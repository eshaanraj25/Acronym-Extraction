# -*- coding: utf-8 -*-
"""ScrapeResearchPapers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h25u1fx4x7ehsvPV_DRdpGTCulb_ONOQ
"""

topics = [
          "neural+networks",
          "linear+algebra",
          "quantum+mechanics",
          "genetic+algorithms",
          "particle+physics",
          "differential+geometry",
          "metallurgy",
          "covid+19",
          "stock+market",
          "drug+discovery"
]

import urllib.request as libreq
from bs4 import BeautifulSoup

id_dict = {}
all_abstracts = []

for topic in topics:
    start = 0
    count = 4000
    with libreq.urlopen(f'http://export.arxiv.org/api/query?search_query=all:{topic}&start={start}&max_results={count}') as url:
        r = url.read()
    soup = BeautifulSoup(r,'xml')
    abstracts = soup.find_all('summary')
    ids = soup.find_all('id')[1:]
    l = 0
    for i in range(len(abstracts)):
        if id_dict.get(ids[i].text, -1) == -1:
            all_abstracts.append(abstracts[i].text)
            id_dict[ids[i].text] = 1
            l += 1
        if l == 200:
            break

import re
all_sentences = []

for abstract in all_abstracts:
    abstract = abstract.replace("\n", " ").replace("\t", " ")
    sentences = re.split("[\.\?!]", abstract)
    for sentence in sentences:
        if(len(sentence) < 2):
            continue
        sentence = sentence.strip()
        words = re.split("[;, ]", sentence)
        for word in words:
            if(len(word) < 2):
                continue
            if sum(1 for c in word if c.isupper()) > len(word)/2:
                all_sentences.append(sentence)
                break

len(all_sentences)

all_sentences

with open("english_sc_sentences.txt", "w+") as fw:
    fw.write('\n'.join(all_sentences) + '\n')
with open("english_sc_visited_pages.txt", "w+") as fw:
    fw.write("\n".join(list(id_dict.keys())) + "\n")

