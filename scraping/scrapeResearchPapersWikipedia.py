# -*- coding: utf-8 -*-
"""Copy of ScrapeResearchPapers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-BxVYrg0u8QAF_MBcT_WxRMA89cNSYOZ
"""

import urllib.request as libreq
from bs4 import BeautifulSoup
import re
import random
import itertools

def preprocess(paragraphs):
    paragraphs = [paragraph.get_text() for paragraph in paragraphs]
    all_sentences = []

    for paragraph in paragraphs:
        paragraph = str(paragraph)
        paragraph = paragraph.replace("\n", " ").replace("\t", " ")
        sentences = re.split("[\.\?!]", paragraph)
        for sentence in sentences:
            if(len(sentence) < 2):
                continue
            if '\\' in sentence:
                continue
            sentence = sentence.strip()
            sentence = re.sub(r'[^a-zA-Z0-9_ ]+', ' ', sentence)
            sentence = re.sub('\s+', ' ', sentence)
            words = re.split("[;, ]", sentence)
            if len(words) < 4:
                continue
            for word in words:
                if(len(word) < 2):
                    continue
                if sum(1 for c in word if c.isupper()) > len(word)/2:
                    all_sentences.append(sentence)
                    break
    
    return all_sentences

def scrape(links):
    i = 0
    sentences = []
    other_links = []
    visited = {}
    while(True):
        if i >= len(links):
            links += other_links[:5]
            other_links = other_links[5:]
        if visited.get(links[i], -1) == 1:
            i += 1
            continue
        print(links[i])
        visited[links[i]] = 1
        with libreq.urlopen(links[i]) as url:
            r = url.read()
        soup = BeautifulSoup(r,'html.parser')
        paragraphs = soup.find_all('p')
        new_sentences = preprocess(paragraphs)
        sentences += new_sentences
        if len(sentences) > 10000:
            return sentences, visited
        see_also = soup.find(id="Voir_aussi")
        if see_also is not None:
            while see_also.parent.parent.parent.get('id') != 'bodyContent':
                see_also = see_also.parent
            allLinks = see_also.find_next_sibling()
            if allLinks is None:
                continue
            while (allLinks.name != 'ul') and (allLinks.find('ul') is None):
                allLinks = allLinks.find_next_sibling()
                if allLinks is None:
                    break
            if allLinks is None:
                continue
            allLinks = ["https://fr.wikipedia.org" + link.a['href'] for link in allLinks.find_all("li") if ((link.a is not None) and (link.a.get('href') is not None) and (link.a['href'].find("/wiki/") == 0))]
            random.shuffle(allLinks)
            links += allLinks
        
        paragraphs = soup.find(id="bodyContent").find_all("p")
        secondaryLinks = [paragraph.find_all('a') for paragraph in paragraphs]
        secondaryLinks = list(itertools.chain.from_iterable(secondaryLinks))
        random.shuffle(secondaryLinks)
        secondaryLinks = ["https://fr.wikipedia.org" + link['href'] for link in secondaryLinks if (link.get('href') is not None) and (link['href'].find("/wiki/") == 0)]

        other_links += secondaryLinks
        
        i += 1

initial_link = 'https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels'
links_to_scrape = [initial_link]
sentences, visited = scrape(links_to_scrape)

with open("french_sentences.txt", "w+") as fw:
    fw.write('\n'.join(sentences) + '\n')
with open("french_visited_pages.txt", "w+") as fw:
    fw.write("\n".join(list(visited.keys())) + "\n")

