[{"text":"entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang,","acronyms":[[131,134],[175,178]],"long-forms":[[104,129],[149,173]]},{"text":"ACL 2006 paper (see experiments). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities","acronyms":[[62,65],[0,3]],"long-forms":[[34,60]]},{"text":"basis on the devising the Princeton English Wordnet.  Arabic Wordnet (AWN) (Elkateb, 2006; Black and Fellbaum, 2006; Elkateb and Fellbaum, 2006) has","acronyms":[[68,71]],"long-forms":[[52,66]]},{"text":"Inspired by the work of web search (Gao et al2010) and question retrieval in community question replying (Q&A) (Zou et al2011), we assume the following generative","acronyms":[[104,107]],"long-forms":[[87,102]]},{"text":"line models: ? Character Segmenter (CS): this model simply divides Chinese sentences into sequences","acronyms":[[36,38]],"long-forms":[[15,34]]},{"text":"Entity linking (EL) recognizes mentions in a text and associates them to their corresponding entries in a knowledge base (KB), for example, Wikipedia","acronyms":[[122,124],[16,18]],"long-forms":[[106,120],[0,14]]},{"text":"This paper recommendations a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder elect appropriate re-","acronyms":[[97,103],[123,126]],"long-forms":[[80,95]]},{"text":" ? WHNP_NN_IN  Syntactic Parse Trees (PT)  72","acronyms":[[38,40],[3,13]],"long-forms":[[25,36]]},{"text":"Hence, it seems plausible to  utilize a back-off mechanism for these sentences  via a combined system (COMB) incorporating  NB only for the sentences that fail to parse.","acronyms":[[103,107],[124,126]],"long-forms":[[86,94]]},{"text":"4.2 Suggests Model : PNB (vs. HMM) Figure 1 exhibitions the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the","acronyms":[[109,112],[21,24],[153,155],[160,163],[30,32]],"long-forms":[[89,107]]},{"text":"Computational Linguistic Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural linguistic processing (NLP) applications. Awareness of MWEs was proven","acronyms":[[156,159],[92,96],[188,192]],"long-forms":[[127,154]]},{"text":"Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three  types of bracketing numbers in all bracketing ","acronyms":[[72,75],[18,21],[43,46]],"long-forms":[[54,70],[0,17],[24,42]]},{"text":"1997). Theory retinement is chiefly used (and has its  origin) in Knowledge Based Systems (KBS) (Craw  and Sleeman, 1990).","acronyms":[[90,93]],"long-forms":[[65,88]]},{"text":"imated conversational characters from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506?513.","acronyms":[[101,104],[71,74]],"long-forms":[[75,99]]},{"text":"AS for word  order,  ther{{ ar<~ basieal\\]}Z two  phrase  'types i n  German:  noun-dependent   phrases,  l i ke  no(:~n phrase  ( NP ) and  prepos i t iona l  phrase  ( !:'","acronyms":[[131,133]],"long-forms":[[114,127]]},{"text":"{kmpark, rim}@nlp.korea.ac.kr 1 Introduction The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Patient, etc.)","acronyms":[[73,76]],"long-forms":[[49,71]]},{"text":"The RASP toolkit (Briscoe et al, 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text.","acronyms":[[141,143],[4,8],[94,97]],"long-forms":[[118,139]]},{"text":"Domains: HT = human transcription factors in blood cells, TCS = two-component systems, BB = germ biology, BS = Bacillus subtilis","acronyms":[[87,89],[9,11],[58,61],[110,112]],"long-forms":[[92,108],[14,33],[64,85],[115,132]]},{"text":"The English supervision NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-alignments sug-","acronyms":[[86,90],[23,25]],"long-forms":[[92,105]]},{"text":"word penalty The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003).","acronyms":[[104,108]],"long-forms":[[75,102]]},{"text":"3.2 Formalizing Paradigmatic Relations with Lexical Functions Lexical functions (LF) are a formal tool designs to describe all types of vera lexical relations","acronyms":[[81,83]],"long-forms":[[62,79]]},{"text":"It  is embedded to the C-value approach for  automatic term recognition (ATR), in the  form of weighted constructs from statisti- ","acronyms":[[73,76],[23,30]],"long-forms":[[45,71]]},{"text":"To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW). Table 2 shows the","acronyms":[[119,121]],"long-forms":[[102,110]]},{"text":" An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-","acronyms":[[91,94]],"long-forms":[[59,89]]},{"text":" 2 Data Kinyarwanda (KIN) and Malagasy (MLG) are lowresource, KIN is morphologically rich, and English","acronyms":[[21,24],[40,43],[62,65]],"long-forms":[[8,19],[30,38]]},{"text":"~  I n  newly  years  the  prob lem o f  man 'mach ine  communicat ion  by  signifies   o e  natura l  language (NL) i s  becoming  a pract i ca l  one .  And the  ","acronyms":[[110,112]],"long-forms":[[90,108]]},{"text":"294  The surface phonetic tones are:  LC = low constant (in Baule, only initial)  HC = high constant (only initial) ","acronyms":[[38,40],[82,84]],"long-forms":[[43,55],[87,100]]},{"text":"and GL.  GL = GR or GL unspec. CC","acronyms":[[14,16],[4,6],[9,11],[31,33]],"long-forms":[[17,19]]},{"text":" The third is the shambles between adjective  (JJ) and noun (NN), when the word in matter  modifies a noun that immediately follows.","acronyms":[[62,64],[48,50]],"long-forms":[[56,60],[36,45]]},{"text":"this mode\\]., the linguistic facts that pertain solely  to the source language (SL) are supposed to be  clearly separated from the facts that pertain solely ","acronyms":[[80,82]],"long-forms":[[63,78]]},{"text":"To standardize the measures to have fixed bounds, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr,Cg) =","acronyms":[[118,121],[127,130]],"long-forms":[[87,116]]},{"text":"changes and there is a addendum particle between complement constituents(COP)  and verbs. Accordingly in the information dictionary, the characteristics of {V(CHA), VP+COP}  should be described.","acronyms":[[152,155],[75,78],[161,164],[158,160]],"long-forms":[[130,145],[51,73]]},{"text":" In Proceedings ofthe 12th International Conference on  Computational Linguistics (COLING), Budapest. ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"2 Dialogue data The conversations corpus used to perform the experiments is the Switchboard database (SWBD). It","acronyms":[[97,101]],"long-forms":[[75,95]]},{"text":"description where i t  is useful.  The posit ion of Linear Precedence (LP) state-  ments in th i s  formalism must now be c la r i f ied .","acronyms":[[71,73]],"long-forms":[[52,69]]},{"text":"step, we apply the CFG IDENTIFICATION to the string  under (2) in order to \"transform\" the sequence of simple  syntactic units into so-called Segmentation Units (SU)  \\[we use the following conventions: \"( )\" for facultativi- ","acronyms":[[162,164],[19,22]],"long-forms":[[142,160]]},{"text":"At the semantic grades, we have included three different families which operate using named entities (NE), semantic roles (TECHNICS), and discourse representations (DR).","acronyms":[[101,103],[122,124],[158,160]],"long-forms":[[85,99],[106,120],[131,156]]},{"text":"gramming. In Association for the Advancement of Artificially Intelligence (AAAI), pages 1050?1055. ","acronyms":[[73,77]],"long-forms":[[13,71]]},{"text":"the earliest in the passage is returned. We used the selective gain computation (SGC) algorithm (Zhou et al, 2003) to select features and estimate","acronyms":[[81,84]],"long-forms":[[53,79]]},{"text":" 1 Introduction Statistical machine translation (SMT) relies on tokenization to split sentences into meaningful units","acronyms":[[49,52]],"long-forms":[[16,47]]},{"text":"A Combinatory Categorial Grammar parsing (CCG) (Steedman, 2000) tool and a Tree Kernel (TK) classifier constitute the core of the system.","acronyms":[[88,90],[42,45]],"long-forms":[[75,86],[2,32]]},{"text":"This  was a motivating factor for the establishment of the  Common Pattern Specification Language (CPSL)  Working Group devoted to formulating a CPSL in ","acronyms":[[99,103],[145,149]],"long-forms":[[60,97]]},{"text":"The relative clause itself has the category S; the incoming edge is labeled RC (relative clause). ","acronyms":[[76,78]],"long-forms":[[80,95]]},{"text":" Figure 1: Graphical representation of the phrase pair themes (PPT) model. ","acronyms":[[62,65]],"long-forms":[[43,60]]},{"text":"(Suchanek et al 2007) have been playing a important role in many AI applications, such as relation extraction(RE), question answers(Q&A), etc. ","acronyms":[[132,135],[108,110],[63,65]],"long-forms":[[113,130],[88,107]]},{"text":" 3 Conditional Random Fields  Conditional Random Fields (CRFs) are a type of  discriminative probabilistic modelled proposed for ","acronyms":[[57,61]],"long-forms":[[30,55]]},{"text":"S# for the count in non-speculative ones)  3 Methods  Conditional random fields (CRF) model was  firstly introduced by Lafferty et al (2001).","acronyms":[[81,84]],"long-forms":[[54,79]]},{"text":"transduction and matching words approximately.  Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.","acronyms":[[57,61]],"long-forms":[[48,55]]},{"text":"egie Group, Inc. (CGI) of Pittsburgh, PA is to promote  and further develop automatic Text Summarization  using a Maximal Marginal Relevance (MMR) metric to  generate summaries of documents hat are directly rele- ","acronyms":[[142,145],[18,21],[38,40]],"long-forms":[[114,140]]},{"text":"fered to punched cards. Abbreviated alphabetical symbol  are used for the syntactic analysis (AP=adJective phrase)  because of the program's 24unlt search constraints.","acronyms":[[95,97]],"long-forms":[[98,114]]},{"text":"patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction pat-","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":"and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shares task; FL=Flagging, FS=Feature stacking, DS=Domains stacking.","acronyms":[[87,89],[100,102],[121,123]],"long-forms":[[90,98],[103,119],[124,139]]},{"text":"Succinct  There has been a long-standing methodology for  evaluating working in speech recognition (SR), but until  recently no community-wide methodology existed for either ","acronyms":[[97,99]],"long-forms":[[77,95]]},{"text":"Engineering neighborhood of bank  bank  Subject Code EG = Engineering  river wall flood thick ","acronyms":[[53,55]],"long-forms":[[58,69]]},{"text":"quences here).  1PARTMOD=participial modifier, PREP=prepositional modifier, POBJ=object of preposition.","acronyms":[[47,51],[16,24],[76,80]],"long-forms":[[52,65],[25,45],[81,102]]},{"text":"particularly helpful in parsing where the sequence  of words forming the MWE is treated as a single  word with a single part of speech (POS) tag. MWE ","acronyms":[[136,139],[73,76],[146,149]],"long-forms":[[120,134]]},{"text":"2 Background 2.1 Surface Realization with Combinatory Categorial Grammatical (CCG) CCG (Steedman, 2000) is a unification-based cat-","acronyms":[[74,77],[79,82]],"long-forms":[[42,72]]},{"text":"Semitic languages (in which vowels are not written), etc.  The problem of word sense disambiguation (WSD) has been described as \"AI-complete,\"  that is, a problem which can be solved only by first resolving all the difficult problems ","acronyms":[[101,104]],"long-forms":[[74,99]]},{"text":"The  motivation for this cooperate is lodged in section 4. Unsupervised Morphology Learner (UML)  framework is presented in section 5.","acronyms":[[90,93]],"long-forms":[[57,88]]},{"text":"on terrorism in a matter of weeks. Since the terrorists  domain knowledge bases (KB's) were developed for Englishman  for MUC-4 already, and since the KB's can be shared across ","acronyms":[[80,84],[118,123],[147,151]],"long-forms":[[63,78]]},{"text":"through IBM, by the Disruptive Technology Office (DTO) Phases III Program for Advanced Question Answered for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06-","acronyms":[[123,130],[8,11],[50,53],[167,170]],"long-forms":[[95,121],[20,48],[140,165]]},{"text":"training data (Surdeanu et al, 2008). We report purity (PU), collocation (CO), and their harmonic mean (F1) evaluated on gold arguments in two set-","acronyms":[[56,58],[74,76]],"long-forms":[[48,54],[61,72]]},{"text":"tional words and notional words. In  the field  of Natural Language Processing(NLP), many  studies on text computing or word meaning ","acronyms":[[79,82]],"long-forms":[[51,77]]},{"text":"utilizing the two cluster class hallmarks. The other selected features and the chosen algorithms (CL) are displayed in Table 1.","acronyms":[[91,93]],"long-forms":[[72,89]]},{"text":" rio deal with unknown words is a big problem in  natural language processing(NLP) too. To recognize ","acronyms":[[78,81]],"long-forms":[[50,76]]},{"text":"our proposed STRAIN approach. The results of using sentence training (STr) and sentence testing (STe) are shown in the STR\/STE row of Table 5.","acronyms":[[70,73],[13,19],[97,100],[119,126]],"long-forms":[[51,68],[79,95]]},{"text":" 2.1 Tree Substitution Grammars A tree substitutions grammar (TSG) is a 4-tuple ?","acronyms":[[61,64]],"long-forms":[[34,59]]},{"text":" 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew","acronyms":[[43,47]],"long-forms":[[22,41]]},{"text":"discussed in section 2, are constituted.  Evaluation of machine translations (MT) systems has to consider the pre-processing of input and","acronyms":[[77,79]],"long-forms":[[56,75]]},{"text":" 2 Tutorial Dialogue Setting and Data My Science Guardian (MyST) (Ward et al, 2010) is a conversational virtual tutor intentioned to improve science learning and comprehension for students in grades 3-5.","acronyms":[[56,60]],"long-forms":[[38,54]]},{"text":"a question written in natural language is called ? Question Answering?(QA), and has gotten a lot of attention recently.","acronyms":[[71,73]],"long-forms":[[51,69]]},{"text":" ? Unaligned word penalties feature (UWP): hUWP (Q,S,A), which is defined as the ratio","acronyms":[[35,38],[41,45]],"long-forms":[[3,25]]},{"text":"tives falls in the middle range and what causes the large and small divergence of the document collection pairs with different topics (DT) and the same topic (ST) or perspective (SP), respectively.","acronyms":[[135,137],[159,161],[179,181]],"long-forms":[[117,133],[147,157],[166,177]]},{"text":"\u0007\u0002 B \u0007\u001eE B \u0004\u001e\u0014\t\u0005E B \u0002\u0014 B \u0019\u0002F\u0013\u0003E\b\u0007 B EA EE B \u0017\u0015 B \u0005\u0013\u0003\u0003\u0006\b) B ,\u0002\u0014 B \tA\u0007E\u0014\b\t\u0007\u0006 EA\u0015 B \u0004E\u0014\u001a\u0002\u0014\u0003\u0006\b) B F\u0002\u0003\u0004\u0002\bE\b\u0007+$\u0006\u0005EB \u0003\u0013A\u0007\u0006\u0004A\u0006F\t\u0007\u0006\u0002\bH0 B \u0002\u001a B$\u0002\u0014\u0019 B  EF\u0007\u0002\u0014\u0005 B ,\u001be\u001eI\u0007JE\" B \/66B0& B(\u001e\u0006\u0005 B \u001e\t\u0005 B \u0007\u001eE B \t\u0019 \t\b\u0007\t)f B \u0007\u001e\t\u0007 B \u0006\u0007 B \u0006\u0005 B \u0006\u0003\u0003E\u0019\u0006\t\u0007EI\u0015B \t\u0004\u0004A\u0006F\t\u0017AEB\u0007\u0002B\t\b\u0015B\u0003\u0002\u0019EAB\u0006\bB$\u001e\u0006F\u001eB$\u0002\u0014\u0019B\u0003E\t\b\u0006\b)BF\t\bB\u0017EBED\u0004\u0014f\u0005\u0005E\u0019B\t\u0005B\tB EF\u0007\u0002\u0014&B(\u001eEB\u0004\u0014\u0006\bF\u0006\u0004AEBA\u0006\u0003\u0006\u0007\t\u0007\u0006\u0002\bB \u0002\u001aBF\u0002\u0013\u0014\u0005EB\u0006\u0005B\u0007\u001e\t\u0007B\u0007\u001e\u0006\u0005B\u0007\t'f\u0005B\b\u0002B\tFF\u0002\u0013\b\u0007B\u0002\u001aB$\u0002\u0014\u0019B\u0002\u0014\u0019E\u0014&B(\u001eE\u0014f\u001a\u0002\u0014E\"B$\u001e\u0006AEB\u0006\u0007B\u001e\t\u0005B\u0004\u0014\u0002 f\bBE\u001a\u001aEES\u0007\u0006 EB\u0006\bB\u0007\u001eEB","acronyms":[[300,307]],"long-forms":[[309,326]]},{"text":"ers. In Proceedings of the 19th International Conference on Compuatational Linguistics (COLING),  pages 556?562.","acronyms":[[88,94]],"long-forms":[[60,86]]},{"text":"We  describe our approach towards building a Wordnet for Tunisian dialect (TD). We proceed, first-","acronyms":[[75,77]],"long-forms":[[57,73]]},{"text":"~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR) ","acronyms":[[74,77],[95,98],[8,11],[23,26],[33,36],[45,48],[55,58],[118,121]],"long-forms":[[61,72],[79,93],[1,7],[13,22],[29,32],[38,44],[50,54],[101,116]]},{"text":"46 2 Related Cooperated and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic mem-","acronyms":[[62,64]],"long-forms":[[40,60]]},{"text":"and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), imple-","acronyms":[[93,95]],"long-forms":[[72,91]]},{"text":"are either too wordy or too ungrammatical. Table 1 shows the compress rates (CompR) for the two 8","acronyms":[[80,85]],"long-forms":[[61,78]]},{"text":" 3.3 Bootstrapped Electoral Experts The Bootstrapped Poll Experts (BVE) algorithm (Hewlett and Cohen, 2009) is an extension to VE.","acronyms":[[66,69],[126,128]],"long-forms":[[37,64]]},{"text":"some plan P such that, if H executes P. then in the re-  sulting state, there exists a \\['F identifiable term P' such  that H knows that Denotation(Pl = Dem;tation(DI),  and 5\" intends that H execute P. ","acronyms":[[164,166]],"long-forms":[[153,163]]},{"text":"several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation","acronyms":[[107,114],[44,48],[163,169]],"long-forms":[[116,135],[50,80],[171,193]]},{"text":" 04 (a) Same Topic (ST) 0.00 0.01 0.02 0.03 0.04 0.05 0.06","acronyms":[[20,22]],"long-forms":[[8,18]]},{"text":"5 23   Proceedings of the Seminars on Rhetoric in Machine Translation (DiscoMT), pages 60?69, Sofia, Bulgaria, August 9, 2013.","acronyms":[[72,79]],"long-forms":[[38,70]]},{"text":"C5 Compound Analyser for Robust Morphological Analyzed]. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.","acronyms":[[85,90]],"long-forms":[[57,83]]},{"text":"multilingual MT system developed at the Laboratoire d'Analyse et de Technologie du Langage (LATL), Academies of Geneva. ","acronyms":[[92,96],[13,15]],"long-forms":[[40,90]]},{"text":"Constituents are tagged with nia class labels from a great, automatically extracted lexicon, using a probabilistic context free grammar (PCFG). ","acronyms":[[137,141]],"long-forms":[[101,135]]},{"text":"2 Data In this study, we use a collection of blog posts from five blogs: Carpetbagger(CB)1, Daily Kos(DK)2, Matthew Yglesias(MY)3, Red State(RS)4, and Right","acronyms":[[86,88],[102,104],[125,127],[141,143]],"long-forms":[[73,84],[92,100],[108,124],[131,140]]},{"text":"known formalisms uch as Head Driven Phrase  Structure Grammar (HPSG), Lexical Functional  Grammar (LFG) or Slot Grammars (SG), because  SUG allows a modular and computational ","acronyms":[[122,124],[63,67],[99,102],[136,139]],"long-forms":[[107,120],[24,61],[70,97]]},{"text":"U, x \/y \/z ,  T, V ~ VYw\\[Y \/X\\ ]   (14)  5Here the 'full' version of (VR) is being used, incorpo-  rating a change of bound variable.","acronyms":[[71,73]],"long-forms":[[59,66]]},{"text":"tween arguments. We thus propose to model the reranking phase (RR) as a HMM sequence labeling task.","acronyms":[[63,65],[72,75]],"long-forms":[[46,55]]},{"text":"Rule 3: Question word followed immediately by a verb (Example (3)).   Qp = question word + headword in the following Verb Phrase(VP) or NP chunk  Rule 4: Question word followed by a passive VP (Example (4)).","acronyms":[[129,131],[136,138],[190,192]],"long-forms":[[117,127]]},{"text":"2003). Recently, Huang et al (2015) revealed that building a conditional random campo (CRF) layer on top of bidirectional LSTM-RNNs performs com-","acronyms":[[85,88],[120,129]],"long-forms":[[59,83]]},{"text":"Whole recall and precision were 0.80 and 0.87 for drugs, and 0.56 and 0.85 for untoward events. 1 Introduction  It is well-known that inauspicious medication reactions (ADRs) are an principal health problem. Indeed, ADRs are the 4th cause of death in hospitalized patients (Wester et al.,","acronyms":[[159,163],[206,210]],"long-forms":[[135,157]]},{"text":"research relies is that the failures of current automatic metrics are not algorithmic: BLEU, Meteor, TER (Translation Edit Rate), and other metrics efficacious and correctly computing informative distance","acronyms":[[101,104],[87,91]],"long-forms":[[106,127]]},{"text":" 2 Related Work Sentiment analysis (SA) and related topics have been extensively studied in recently years.","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":"sides identity (IDENT) we only marked up three associative relations (Hawkins, 1978): set membership (ELEMENT), subset (SUBSET), and ? gen-","acronyms":[[120,126],[16,21],[102,109]],"long-forms":[[112,118],[6,14]]},{"text":"Therefore, we refine our reference performance tiers by combining the ME models (MEM) and handcrafted models (HCM). Suppose the score of a","acronyms":[[111,114],[82,85]],"long-forms":[[91,109],[71,79]]},{"text":"sub-tasks: ? Multimedia Information Network (MiNet) Construction: Erect MiNet from cross-media and cross-genre informational (i.e. tweets, images, sentences of web doc-","acronyms":[[45,50],[76,81]],"long-forms":[[13,43]]},{"text":"  Abstract  A Named Entity Recognizer (NER) generally  has worse performance on machine translated ","acronyms":[[39,42]],"long-forms":[[14,37]]},{"text":"CORROLA TION  NP1 = (COR) (APP) (ADJ) (NC) N  NP2 = (NC) N  NP3 = N ","acronyms":[[53,55],[21,24],[27,30],[33,36],[39,41],[14,17]],"long-forms":[[46,51],[0,12]]},{"text":"Swedish 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55) Turkish 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95) Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL) postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP)","acronyms":[[316,324],[343,351],[406,412],[455,460]],"long-forms":[[279,314],[398,404],[422,432]]},{"text":"the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 41?47, Montre?al.","acronyms":[[116,122],[47,50]],"long-forms":[[89,114],[4,45]]},{"text":"a generalization of the Semitic root-and-template modeling. We use Egyptian Arab (EGY), and German (GER) as our test languages.","acronyms":[[84,87],[102,105]],"long-forms":[[67,75],[94,100]]},{"text":"Winnow and voted-perceptrons (Zhang et al2002;  Collins, 2002), or by using the sequences labeling  models, such as Concealed Markov Models (HMMs)  (Molina and Pla, 2002) and Conditional Random ","acronyms":[[137,141]],"long-forms":[[115,135]]},{"text":"The most comparable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,","acronyms":[[67,72],[29,36],[42,45],[117,121],[122,125]],"long-forms":[[74,108]]},{"text":"been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents.","acronyms":[[160,163]],"long-forms":[[125,158]]},{"text":"tities e1, e2, ei,.., eE on translated Francais documents through aforementioned step, meanwhile, we consider all noun phrases(NP) in original Chinese documents and generating mention candidates","acronyms":[[126,128]],"long-forms":[[113,124]]},{"text":"Within Acquilex IP Project, a reuniting framework  predicated on typed feature structures \\[4\\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptually units corresponding to lexieal senses, lexical ","acronyms":[[113,116],[16,18]],"long-forms":[[118,140]]},{"text":"It is a comprehensive studies but not directly associated to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a","acronyms":[[118,121]],"long-forms":[[91,116]]},{"text":"1969)).  4 Ia the following \\] will utilise the term Uniformity Grammar (UG) aa hyperonym for  GPSG, LFG, FUG, IIPSG etc.,","acronyms":[[70,72],[92,96],[98,101],[103,106],[108,113]],"long-forms":[[49,68]]},{"text":"applied this formula to a vocabulary of single terms.  Subiect Field Code (SFC). This system applies a ","acronyms":[[75,78]],"long-forms":[[55,73]]},{"text":" 53 Creative Information Retrieval (CIR) can be used as a platform for the design of many Web services that offer linguistic creativity on de-mand. By enabling the flexible retrieval of n-gram data for non-literal queries, CIR allows a wide variety of creative tasks to be reimagined as simple IR tasks (Veale 2013).","acronyms":[[36,39],[223,226]],"long-forms":[[4,34]]},{"text":"Schauder 91). The grammar is divided into an LD  (linear supremacy) and an LP (linear precedence) party  so that the piecewise construction of syntactic ","acronyms":[[75,77],[45,47]],"long-forms":[[79,96],[50,67]]},{"text":"Orientation, Ways, and Targeting are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the bygone roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was utilized when a single word in the answer, typically yup, no, agree, disagree, A-D, cetera., stood alone without a significant relationship to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for smaller than 1% of the reference answer facets).","acronyms":[[271,275],[293,297],[315,319]],"long-forms":[[277,290],[299,312],[321,350]]},{"text":"ALO ( in to )   ROO (OST LOC)  PO (PRE)  ON (VO)) ","acronyms":[[34,37],[0,3],[16,18],[20,23],[24,27],[44,46]],"long-forms":[[30,32]]},{"text":"Pos i t i ve  Recal l  (PR)  :  ? Pos i t ive  Prec is ions  (PP)  :  d ?","acronyms":[[61,63],[24,26]],"long-forms":[[34,58],[0,21]]},{"text":"we combine different outlook, the performance enhances and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for executing 2 (LPSSVR), and L+P+S with SVR using transductive learning","acronyms":[[99,104],[84,87],[133,139],[157,160]],"long-forms":[[107,129]]},{"text":" One suggestion is is to use as a natural anguage  grammar the Core Language Engine (CLE)  (Alshawi 1992).","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"Program. It is intended that this interface be coupled to the battle governance system being  developed under DARPA's Fleet Command Center Battle Management Programming(FCCBMP). In the ","acronyms":[[165,171],[110,115]],"long-forms":[[118,163]]},{"text":"hierarchical phrase-based model, but braked so that the English part of the right-hand side is restricted to a Greibach Normal Forme (GNF)like structure: A contiguous sequence of termi-","acronyms":[[138,141]],"long-forms":[[116,136]]},{"text":" We excluded only punctuation; we did no filtering for part of speech (POS). Each word was actually","acronyms":[[71,74]],"long-forms":[[55,69]]},{"text":"ENT E1 E2 (b) Peculiarities Paired Tree(FPT) ENT","acronyms":[[34,37]],"long-forms":[[14,32]]},{"text":"is indicated by the dotted black line.  The receiver operating characteristic (ROC) curves in Figures 2 and 3 demonstrate perfor-","acronyms":[[79,82]],"long-forms":[[44,77]]},{"text":"results. We have utilized the symbol Comp in that instance (e.g., if  ANTI (A)=B and CONV(B)=C, then the relation result-  ing from the composition is simply ANTI(CONV(A))----C).","acronyms":[[61,65],[149,153],[76,80]],"long-forms":[]},{"text":"ailehor ~  The following entry is associated with the class of  verbs taking an NP as indirect objects(IOBJ) which  may be possibly found within a prepositional phrase or ","acronyms":[[103,107],[80,82]],"long-forms":[[86,101]]},{"text":"  *COMPLEXITY: avoid semantic complexity  BC (BE CONCRETE): have a definite meaning   ","acronyms":[[42,44]],"long-forms":[[46,57]]},{"text":" In order to overcome these limitations, some  techniques like word sense induction (WSI) have  been proposed for discovering words?","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"cognition(COG) competition(COMP)  contact(CeNT) motion(MOT)  emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA) ","acronyms":[[85,88],[10,13],[27,31],[42,46],[55,58],[69,72],[102,106],[117,120]],"long-forms":[[74,84],[0,9],[15,26],[34,41],[48,54],[61,68],[91,101],[108,116]]},{"text":" 1 Introduction Coreference resolved (CR) ? the task of determin-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification uses the Ontology Web Language (OWL) to list valid values for objects of the defined class. ","acronyms":[[123,126],[7,10]],"long-forms":[[100,121]]},{"text":"DEP = dependency type ? MOR = morphological features (set) ?","acronyms":[[24,27],[0,3]],"long-forms":[[30,43],[6,16]]},{"text":"Hodellng Temporal Knowledge  Hodellng time, dasoite its olovlous importance, has  proved an elusive goal for artificial Intelligence (AI). ","acronyms":[[134,136]],"long-forms":[[109,132]]},{"text":"ing decisions in multi-party discussions.  Several types of conversations act (DA) are distinguished on the fundamentals of their roles in","acronyms":[[74,76]],"long-forms":[[60,72]]},{"text":" 3.1 Ske le ta l  Phrase  S t ructure  Component   The role of phrase structure (PS) rules in our parser is similar to  their role in Lexical Functional Grammar \\[Kaplan 83\\], however they ","acronyms":[[81,83]],"long-forms":[[63,79]]},{"text":"196  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"ported by Exchanges Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal Story of Your Medical Events (THYME) project (NIH R01LM010090 and U54LM008748).","acronyms":[[117,122],[38,43],[53,56],[133,136]],"long-forms":[[74,115],[10,36]]},{"text":"this, in addition to the four actions of the NonInc algorithm, we introduce two new actions: Left Unveils (LRev) and Right Expose (RRev). For this,","acronyms":[[130,134],[106,110]],"long-forms":[[116,128],[93,104]]},{"text":"End point SIP user agents: These are the SIP end points that exchange SIP signaling messages with the SIP Application server (AS) for call control.","acronyms":[[126,128],[41,44],[70,73],[102,105]],"long-forms":[[106,124],[10,13]]},{"text":"build a bridge between UNL and one of the  internal representations of ETAP, namely  Normalized Syntactic Structure (NormSS), and in  this way link UNL with all other levels of text ","acronyms":[[117,123],[23,26],[71,75],[148,151]],"long-forms":[[85,115]]},{"text":"Thus, we  name our system for generating compressions the  Adjustable Rate Compressor (ARC).   ","acronyms":[[87,90]],"long-forms":[[59,85]]},{"text":" A more flexible approaches is to do it in two measures: complementation, forming a VP (verb phrase) from the verb and the object, and predication","acronyms":[[79,81]],"long-forms":[[83,94]]},{"text":"We conduct a case study of dialectal language in online conversational text by examine African-American English (AAE) on Twitter.","acronyms":[[119,122]],"long-forms":[[93,117]]},{"text":"ability integral transmutation to generate empirical cumulative density functions (ECDF): now instead of the probability density functioning (PDF) space, we are collaborating in the ECDF space where the value of each","acronyms":[[135,138],[79,83],[169,173]],"long-forms":[[105,133],[39,77]]},{"text":"evidences, making the coupled approach efficient enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the first time.","acronyms":[[122,124],[130,133]],"long-forms":[[103,120]]},{"text":"this, in addition to the four actions of the NonInc algorithm, we introduce two new actions: Left Reveal (LRev) and Right Reveal (RRev). For this,","acronyms":[[130,134],[106,110]],"long-forms":[[116,128],[93,104]]},{"text":"Chinese Semantic Dictionary (CSD) for  Chinese-English machine translation, the  Chinese Concept Dictionaries (CCD) for  cross-language text processing, the multi-level ","acronyms":[[109,112],[29,32]],"long-forms":[[81,107],[0,27]]},{"text":" Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each","acronyms":[[155,159],[198,202]],"long-forms":[[129,153],[165,196]]},{"text":" 1 Introduction Weighted Context Free Grammars (WCFG) define an important class of languages.","acronyms":[[48,52]],"long-forms":[[16,46]]},{"text":"contemplate in (14) five salient phases of the passage: at the outset of the process, the parts of river are localized to the exterior EXT(LOC), then to the boundaries FRO(LOC), they arrive in","acronyms":[[139,142],[135,138],[166,169],[170,173]],"long-forms":[[109,118]]},{"text":"Linggle: a Web-scale Linguistic Search Engine for Mots in Context    Joanna Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract  In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in answer to a given query. The query might contain keywords, wildcards, wild parts of rhetoric (PoS), synonyms, and ad-ditional regular expression (RE) operators. For our approach, we incorporated overturned filing in-dexing, PoS information from BNC, and se-mantic indexing basis on Latent Dirichlet Al-location with Google Web 1T. The mode in-volves parsing the query to conversion it in-to assorted keyword retrieval commands.","acronyms":[[584,587],[636,638],[278,283],[708,711],[729,732]],"long-forms":[[567,582],[616,634]]},{"text":"NLP track report. In Proceedings of the 5th  Text salvaging Conference (TREC-5). ","acronyms":[[72,78],[0,3]],"long-forms":[[40,70]]},{"text":"We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems.","acronyms":[[131,134]],"long-forms":[[98,129]]},{"text":"while the NB+E extractor has the worst. Training the CRF with negative examples (CRF+E) gave better precision in extracted information then train-","acronyms":[[81,86],[10,14]],"long-forms":[[53,79]]},{"text":"classes of both test texts  PoS tags  DET (Determiner)  NM (Pronoun) ","acronyms":[[41,44],[59,61]],"long-forms":[[46,56]]},{"text":"According to the fifth rule, the Arabic letter Z may match an empty string on the English side, if there is an English consonant (EC) in the right context of the English side.","acronyms":[[130,132]],"long-forms":[[111,128]]},{"text":"Route INJECTION ORAL SMOKING SNORTING Aspect CHEMISTRY (Pharmacology, TEK) CULTURE (Culture, Setting, Social, Spiritual) EFFECTS (Effects)","acronyms":[[75,82],[70,73],[121,128]],"long-forms":[[84,91],[130,137]]},{"text":"expert users in spoken dialogue systems. The key component of a spoken language understanding (SLU) system is the semantic parser, which translates the users?","acronyms":[[95,98]],"long-forms":[[64,93]]},{"text":"size ? is fixed to 0.0001. We refer to this modelled as Orthogonal Matrix Factorization (OrMF). ","acronyms":[[86,90]],"long-forms":[[53,84]]},{"text":"Section 2  introduces some relevant work in IR and  question answering (QA). Section 3 talks about ","acronyms":[[72,74],[44,46]],"long-forms":[[52,70]]},{"text":"1 In t roduct ion   For some NLP applications, it is pivotal o  identify, \"named entities\" (NE), such as someone  names, organization ames, time, date, or money ","acronyms":[[94,96]],"long-forms":[[77,91]]},{"text":"helpful in identify the embedded words. However  some ambiguous segmentation chain(ASSs) and  unregistered words (i.e. the word that is not registered ","acronyms":[[88,92]],"long-forms":[[57,87]]},{"text":"ing the most similar. This year we set up deux missions: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).","acronyms":[[70,74],[111,116]],"long-forms":[[59,63],[88,93]]},{"text":"integra Chinese word segmentation and NE  identification into a unified framework use a  class-based language paragon (LM).\u0016 &ODVV\u0010EDVHG\u0003\/0 IRU\u00031(\u0003,GHQWLILFDWLRQ The n-gram LM is a stochastic model which ","acronyms":[[121,123],[40,42],[175,177]],"long-forms":[[105,119]]},{"text":"Suppose that the feature f2 is an agreement feature and that a local  tree t which is a projection of this ID rule has been constructed, then  the Agreement Principle (AP) forces X = Y = Z and therefore the  AP has to consider three cases 6: ","acronyms":[[168,170],[107,109],[208,210]],"long-forms":[[147,166]]},{"text":"TEXT), DECL (Declarative), HON (Honorific), IMPER (Imperative), NOM (Nominative), ORTH (ORTHOGRAPHY), PST (Past), SYN (SYNTAX), SEM (SEMANTICS), RELS (RELATIONS), and POS (part of speech).","acronyms":[[102,105],[7,11],[27,30],[44,49],[64,67],[82,86],[114,117],[128,131],[145,149],[167,170]],"long-forms":[[107,111],[13,24],[32,41],[51,61],[69,79],[88,99],[119,125],[133,142],[151,160],[172,186]]},{"text":"For aviation incidents, the advantage of the proposed prior is reflected in the location (LO) and country (CO) slots, which may confuse the various models as they both belong to the entity type loca-","acronyms":[[107,109],[90,92]],"long-forms":[[98,105],[80,88]]},{"text":"State (STT) 42.85 76.93 Gender (GEN) 67.42 84.17 Determiner (DET) 59.71 85.41 Number (NUM) 70.61 87.31","acronyms":[[61,64]],"long-forms":[[49,59]]},{"text":"Attribute F1,344 d Automated Readability Index (ARI) 0.187 0.047 Medium Sentence Length (ASL) 3.870 0.213 Sentence Complexity (COM) 10.93 0.357","acronyms":[[88,91],[46,49],[126,129]],"long-forms":[[63,86],[17,44],[114,124]]},{"text":"sible categories we show the accuracy of the correct hashtag being amongst the top 1,5 or 50 hashtags as well as the Mean Reciprocal Classify (MRR). The","acronyms":[[136,139]],"long-forms":[[114,134]]},{"text":" Other valuable categories are, for example,  pronominal adverbs (PAV) and infinitives of auxil-  iary verbs (VAINF), where the difference between ","acronyms":[[66,69],[110,115]],"long-forms":[[46,64]]},{"text":"1 Int roduct ion   In a recent paper Boguraev and Levin (1990) point out imperfections in common concep-  tions of what a Lexical Knowledge Base (LKB) should be, gaps which stem from  the assumption that a machine-readable thesaurus (MRD) is not only the right provenance for ","acronyms":[[145,148]],"long-forms":[[121,143]]},{"text":"in the lexicon to the following categories: protesters : NP seized : (S\\NP )\/NP several : NP\/NP","acronyms":[[70,74],[57,59],[77,79],[90,95]],"long-forms":[[60,68]]},{"text":"structure come in two different kind: ? Grammatical Functions (GFs) indicate the relationship between the predicate and depen-","acronyms":[[64,67]],"long-forms":[[41,62]]},{"text":"This paper presents the UNL graph matching method for the Semantic Textual Similarity(STS) task. ","acronyms":[[86,89],[24,27]],"long-forms":[[58,84]]},{"text":"   Simpler Segmentation Algorithm(SSA):  1.","acronyms":[[33,36]],"long-forms":[[3,31]]},{"text":"Estimating Proficiency  Item Response Theory (IRT)  Item Response Theories (IRT) is the basis of modern  language tests such as TOEIC, and permit Com-","acronyms":[[74,77],[46,49],[126,131]],"long-forms":[[52,72],[24,44]]},{"text":"competition submissions). Avis that most were using Support Vector Machine (SVM) with bagof-word features in a very minor window, local col-","acronyms":[[78,81]],"long-forms":[[54,76]]},{"text":" In table 1, we present the accuracy of the model trained on the output of the joint inference (JOINT) against that of the self-training baseline (SELF).","acronyms":[[96,101],[147,151]],"long-forms":[[79,84],[123,136]]},{"text":"In: Ernst Buch-berger (ed.): Tagungsband der 7. Konferenz zur Verarbeitung nat?rlicher Sprache (KONVENS), Universit?t Wien, 161?168. Strube, Gerhard (1984).","acronyms":[[96,103]],"long-forms":[[48,94]]},{"text":"3.2 Structural model We go beyond traditional feature vectors by employing structural models (STRUCT), which encode each comment into a shallow syntactic tree.","acronyms":[[94,100]],"long-forms":[[75,85]]},{"text":"line models: ? Character Segmenter (CS): this model simply partition Chinese sentences into sequences","acronyms":[[36,38]],"long-forms":[[15,34]]},{"text":"The thesis will examine two main areas: models cohesive devices within sentences and modelling discourse relations (DRs) across sentences.","acronyms":[[119,122]],"long-forms":[[98,117]]},{"text":"chine learning models based on three different well known techniques, decision trees (C4.5), rule induction (RIPPER) and maximum entropy (MaxEnt), in order to find out which approach is the most suitable","acronyms":[[138,144],[109,115]],"long-forms":[[121,136]]},{"text":", with different projection and SRL training methods. SP=Addition; OW=Overwrite. ","acronyms":[[54,56],[32,35],[69,71]],"long-forms":[[72,81],[57,67]]},{"text":"Abstract We experiment with using different source of distant supervision to guide unsupervised and semi-supervised adjustment of part-of-speech (POS) and named entity taggers (NER) to Twitter. ","acronyms":[[178,181],[147,150]],"long-forms":[[156,176],[131,145]]},{"text":"Again, even the Table 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection with five measures of semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs","acronyms":[[35,37],[48,50],[68,70]],"long-forms":[[24,33],[40,46],[57,66]]},{"text":"The prefer-  ence score (PS) for a pair is determined by the ratio  of its local dominance count (LDC)--the total num-  ber of cases in which the pair is locally dominant--to ","acronyms":[[98,101],[25,27]],"long-forms":[[75,96],[4,23]]},{"text":"In extra, for some nodes it is necessity to insist that adjunction is mandatory at  a node. In such a case, we say that the node has an Obligatory Adjacent (OA)  constraint.","acronyms":[[161,163]],"long-forms":[[139,159]]},{"text":"score word pair for relatedness (on a scale of 0 to 10), which is in contrast to the similarity judgments asked of the Miller and Charles (MC) and Rubenstein and Goodenough (RG) participating.","acronyms":[[144,146],[179,181]],"long-forms":[[124,142],[152,177]]},{"text":"Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable","acronyms":[[75,77]],"long-forms":[[64,73]]},{"text":"I_tt is raining.,  OBJ = Object: Him read a book., ","acronyms":[[19,22]],"long-forms":[[25,31]]},{"text":"navigation instruction to guide the IF to a convenient location at which she can then use a simple referring expression (RE). That is, there is an inter-","acronyms":[[121,123],[36,38]],"long-forms":[[99,119]]},{"text":"1540  Proceedings of the 3rd Workshops on Hybrid Approach to Translation (HyTra) @ EACL 2014, pages 7?14, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"1 Introduction  Scalability in dialog systems is, of course, not only a  matter of the natural language understanding (NLU)  component, but also of the NLG part of the system.2 We ","acronyms":[[119,122],[152,155]],"long-forms":[[87,117]]},{"text":"1978. Longman Thesaurus of  Contemporary lCnglish (LI)OCE). Long\\]nan, liar- ","acronyms":[[51,59]],"long-forms":[[6,50]]},{"text":"In Lawsuits of the Third International Conference on Web Search and Web Data Mining (WSDM), pages 101?110, Novo York, NY, USA, 2010.","acronyms":[[88,92],[120,122],[124,127]],"long-forms":[[56,86]]},{"text":"Narrative Summarization. Journal Traitement automatique des langues (TAL): Particular  issue  on Context:  Automatic Text  Summariza-","acronyms":[[69,72]],"long-forms":[[33,67]]},{"text":"tially lexicalized) syntactic dependencies and patterns. The weight \u0000 is the Local Mutual Informa-tion (LMI) (Evert, 2005) computed on link type frequency (negative LMI values are raised to 0).3.1 Test set","acronyms":[[104,107],[165,168]],"long-forms":[[77,102]]},{"text":"mostly context-free, with some context-sensit ive and  some transformational  ordinance, written in a modif ied  Backus Habitual Form (BNF). Each rule contains the ","acronyms":[[129,132]],"long-forms":[[109,127]]},{"text":"cessing. In Lawsuits of the 2nd International Conference on Knowledge Capture(K-CAP). USA.","acronyms":[[81,86],[89,92]],"long-forms":[[63,80]]},{"text":"In E.M. Voorhees and  D.K. Harman, editors, The 3d Text RE-  trieval Conference (TREC-3). ","acronyms":[[81,87],[3,6],[22,25]],"long-forms":[[48,79]]},{"text":"Pivot, RHS),  generate all subconstituents  generate _rhs ( RHS ),  generate material on path to root ","acronyms":[[60,63],[7,10]],"long-forms":[[54,57]]},{"text":"2007. CRFsuite: a fast implementation of conditional random fields (CRFs). ","acronyms":[[68,72],[6,14]],"long-forms":[[41,66]]},{"text":"Concerned about this inflation of the grammar ceaseless, (DeNero et al, 2009) considering a superset of CNF called Lexical Normal Form (LNF). A rule is","acronyms":[[132,135],[100,103]],"long-forms":[[111,130]]},{"text":"\u0003\u0003\t\f\u0006\u0011\u000b\u0006\b strategies(Lewis, 1992).  We use probability threshold(PT) strategy where each document is assigned to the categories above a thresh-","acronyms":[[65,67]],"long-forms":[[43,64]]},{"text":" 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality ma-","acronyms":[[61,63]],"long-forms":[[40,59]]},{"text":"In a second step, unequalled words are converted into stems or synonyms and then matched. A methods that uses the concept of maximum matching twine (MMS) is presented by Turian, Shen, and Melamed (2003).","acronyms":[[148,151]],"long-forms":[[123,146]]},{"text":" 3.2.6. Candidate word number (WNum)  Because there are candidates that are a multi-","acronyms":[[31,35]],"long-forms":[[18,29]]},{"text":"set we trained on both glosses and statistical MT data, for the OnWN and FNWN test sets we trained on glosses only (OnWN), and for the SMT test set we trained on statistical MT data only (MTnews and","acronyms":[[116,120],[47,49],[64,68],[73,77],[135,138],[174,176],[188,194]],"long-forms":[[99,114]]},{"text":"The types of links  traversed in the search (in the first case) or the checked slots (in the second case) are a  function of the semantic lass (SEM-C) of the first constituent. This function assigns to each ","acronyms":[[144,149]],"long-forms":[[129,137]]},{"text":"cerd,jurafsky,manning@stanford.edu Abstract Minimum error rate training (MERT) is a widely use learning procedure for statistical","acronyms":[[73,77]],"long-forms":[[44,71]]},{"text":"of conditional random fields. Among Annual Meeting of the Association for Computational Linguistics (ACL), pages 870?878.","acronyms":[[98,101]],"long-forms":[[55,96]]},{"text":"for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled attachment score.","acronyms":[[44,47],[76,79]],"long-forms":[[50,74],[82,108]]},{"text":"They displaying betterment of up to 5.3% on two real tasks: pitch accent prediction and optical character recognition (OCR). ","acronyms":[[115,118]],"long-forms":[[84,113]]},{"text":"ALL X X X 0.614 0.186 0.706 0314 0.509 Table 2: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adap-","acronyms":[[82,86]],"long-forms":[[69,80]]},{"text":"The TRIANGLE application  (TRIANGLE) and a new Windows 95  Accessible Graphing Calculator (ACG) both  developed by the SAP uses tone plots to ","acronyms":[[91,94],[27,35],[119,122]],"long-forms":[[59,78],[4,12]]},{"text":"use(USAGE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR)  These are mostly taken from the classifications ","acronyms":[[89,92],[4,7],[16,19],[26,29],[45,48],[66,69]],"long-forms":[[72,87],[0,3],[9,15],[21,25],[32,44],[50,65]]},{"text":" The actual performance of a system is measured in terms of detection error tradeoff (DET) curves and the minimal normalized cost.","acronyms":[[86,89]],"long-forms":[[60,84]]},{"text":"3http:\/\/www.cjk.org 4https:\/\/translit.i2r.a-star.edu.sg\/news2009\/evaluation\/ 5The six metrics are Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank","acronyms":[[122,125]],"long-forms":[[103,111]]},{"text":"fifth tasks) representing fine-grained bio-IE.  2.1 Genia task (GE) The GE task (Kim et al, 2011) maintain the task","acronyms":[[63,65],[71,73],[38,45]],"long-forms":[[51,56]]},{"text":"4.1 Compositional Neural Language Models (C-NLM) Compositional Neurological Language Model (C-NLM) is a combination of a word representation learning","acronyms":[[85,90],[41,46]],"long-forms":[[48,83],[4,39]]},{"text":"value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTOMOBILE), 72.1 (TABLETS) for the sentiment task and 64.1 (MOTOR), 79.3 (TABLETS) for","acronyms":[[70,74],[125,129],[138,145],[83,90]],"long-forms":[]},{"text":"this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), manner (MNR), purpose (PRP), and temporal (TMP).","acronyms":[[112,115],[141,144],[65,68],[82,85],[96,99],[126,129],[161,164]],"long-forms":[[102,110],[132,139],[53,63],[71,80],[88,94],[118,124],[151,159]]},{"text":"Abstract The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate.","acronyms":[[72,74]],"long-forms":[[51,70]]},{"text":"In practice we can find approximate solutions using such algorithms as: Loopy Belief Propagation (BP), Mean Field (MF), Gibbs Sampled (Gibbs). ","acronyms":[[114,116],[97,99],[135,140]],"long-forms":[[102,112],[77,95],[119,124]]},{"text":"a directed acyclic graph, and a set of conditionnal probablities, each node being represented as a Random Variable (RV). Parametrizing the BN","acronyms":[[116,118],[139,141]],"long-forms":[[99,114]]},{"text":"protein interaction as an example. In Proceedings  of the Peaceful Symposium on Biocomputing (PSB),  Hawaii, USA.","acronyms":[[93,96],[108,111]],"long-forms":[[58,91]]},{"text":"demanding.  Latent Semantic Analysis (LSA) (Deerwester et al.,","acronyms":[[38,41]],"long-forms":[[12,36]]},{"text":"blogs which accumulated a large number of posts during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased","acronyms":[[123,125],[140,142],[82,84],[167,170],[181,183],[174,176]],"long-forms":[[105,121],[129,138],[68,80],[150,164]]},{"text":"prior polarity of verb, verb score (V_score).  Verb-PP (prepositional phrase) rules:  1.","acronyms":[[52,54]],"long-forms":[[56,76]]},{"text":"Adding valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? corpus of","acronyms":[[62,65]],"long-forms":[[68,87]]},{"text":"Note that the proponents of the BootCaT method seem to acknowledge this evolution, see for case Marco Baroni?s speaks at this year?s BootCaTters of the world unite (BOTWU) atelier: ?","acronyms":[[166,171],[32,39]],"long-forms":[[134,164]]},{"text":"guages: German, French and Ltalian, with German  ordinarily serving as the source language (SL),  French and Italian as the target language (TL). ","acronyms":[[138,140],[89,91]],"long-forms":[[121,136],[72,87]]},{"text":"ral probabilistic language model. Among Advances in Neural Information Processing Systems (NIPS), 2000.","acronyms":[[88,92]],"long-forms":[[49,86]]},{"text":"Syntactic featured from SLA research (SLASYN) ? Mean length of clause (MLC) ?","acronyms":[[71,74],[24,27],[38,44]],"long-forms":[[48,69]]},{"text":" Assi, S. (1997). Farsi linguistic database (FLDB). ","acronyms":[[45,49]],"long-forms":[[18,43]]},{"text":" More recent, i2b2 organizers furthermore reported a  Maximum Entropy (ME) based approach for the  2009 challenge (Halgrim, Xia, Solti, Cadag, & ","acronyms":[[66,68],[16,20]],"long-forms":[[49,64]]},{"text":"al., ( 2013) but supersedes Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al.,","acronyms":[[78,81]],"long-forms":[[52,76]]},{"text":"diversified respects. While CKY requires a grammar in Chomsky Normal Form (CNF), LSCP takes an customary PG grammar, since no equivalent of the","acronyms":[[71,74],[24,27],[77,81],[100,102]],"long-forms":[[50,69]]},{"text":"determines the nature of such space.  For example, Syntactic Tree Kernel (STK) are used to model complete context free rules as in (Collins","acronyms":[[74,77]],"long-forms":[[51,72]]},{"text":"In order to test the classification rules for the extraction of part?whole relations, we selected two different text collections: the LA Times news articles from TREC 9 and the Wall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomly selected 10,000 sentences that formed two distinct test corpora.","acronyms":[[198,201],[162,166],[134,136]],"long-forms":[[177,196]]},{"text":"For 1)reprocessing the dictionary definitions, we  have experimented with two ditDrent Caggers: the Xe-  rox PAR(J part-of-speech tagger \\[8\\], and the Chop-  per \\[9\\], an optimizing finit, e state luachine-hased tag- ","acronyms":[[109,112],[78,86]],"long-forms":[[115,119]]},{"text":"While named entity recognition (NER) and relation or event extraction are regarded as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more","acronyms":[[128,130]],"long-forms":[[104,126]]},{"text":"2 Description of the France Telecom 3000 Voices Agency corpus The France Telecom 3000 (FT3000) Voice Bodies service, the first deployed vocal service at France","acronyms":[[86,92]],"long-forms":[[65,84]]},{"text":"Table 1: Labelled attachment score on the two test sets of the best single parse, blended with weights set to PoS labelled attachment score (LAS) and blended with learned weights.","acronyms":[[141,144],[110,113]],"long-forms":[[114,139]]},{"text":"guished in Boxer: 1. Discourse Representation Structures (DRSs) 2.","acronyms":[[58,62]],"long-forms":[[21,56]]},{"text":"entire searching process.  (b) EPN for the first optimum solution (EPN-F): The number of the expanded problems when","acronyms":[[64,69]],"long-forms":[[28,45]]},{"text":"eigenvalues are not nothingness, then I? minimizes the multiway normalized cut(MNCut): MNCut(I) = K ??","acronyms":[[72,77],[80,85]],"long-forms":[[48,71]]},{"text":"ratings of IP strategies. In the following we use a Reinforcement Learning (RL) as a statistical planning framework (Sutton and Barto,","acronyms":[[76,78],[11,13]],"long-forms":[[52,74]]},{"text":"The variants of random walk topic models are compared against LDA and the relational topic model (RTM), each with 100 topics (Chang and Blei, 2010).","acronyms":[[98,101],[62,65]],"long-forms":[[74,96]]},{"text":"pound verb (CompV) is framed with these two  verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP).","acronyms":[[80,82]],"long-forms":[[68,78]]},{"text":"For each bracketed phrase, if its FF label does not  fit into the corresponding default pattern, (like for  the noun phrase(NP), the defaulted grammatical  architecture is that the last noun in the phrases is the ","acronyms":[[124,126],[34,36]],"long-forms":[[112,123]]},{"text":"2http:\/\/www.statmt.org\/wmt12\/ by filling in lexical gaps in resource-poor languages with the assist of Appliance Translation (MT). ","acronyms":[[121,123]],"long-forms":[[100,119]]},{"text":"Theorem LG is NP-complete.  Bin Packing (BP) which is NP-complete is  polynomial-t ime Karp reducible to LG.","acronyms":[[41,43],[8,10],[14,16],[105,107],[54,56]],"long-forms":[[28,39]]},{"text":"UMichigan non-Tipster UNK X USouthern California non-Tipster SNAP X USussex (UK) non-Tipster SUSSEX X Table 1 .","acronyms":[[77,79],[22,25],[61,65],[93,99]],"long-forms":[[68,75]]},{"text":"a. the 1000-headlines text (target domain) 1,181 40.2 32.1 35.7 b. the TEC (source domain) 32,954 29.9 26.1 27.9 c. the 1000-headlines text and the TEC (target and source) c.1.","acronyms":[[148,151],[71,74]],"long-forms":[[153,170]]},{"text":" 1 Introduction Synchronous contex free grammars (SCFGs) generalize traditional context-free grammars to generate","acronyms":[[50,55]],"long-forms":[[16,48]]},{"text":"4. Crowdsourcing We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different kinds of opposites.","acronyms":[[53,56]],"long-forms":[[29,51]]},{"text":"This paper presents the UNL diagram matching method for the Semantic Textual Similarity(STS) task. ","acronyms":[[86,89],[24,27]],"long-forms":[[58,84]]},{"text":"Patrizia Paggio University of Copenhagen Centre for Language Technology (CST) Njalsgade 140, 2300-DK Copenhagen","acronyms":[[73,76],[93,100]],"long-forms":[[41,71]]},{"text":"The Multi-Perspective Question-Answering (MPQA) newswire corpus (Wilson and Wiebe, 2005) and the J. D. Power & Associates (JDPA) motorcars review blog post (Eugene et al, 2010)","acronyms":[[123,127],[42,46]],"long-forms":[[97,121],[4,40]]},{"text":"consequently Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.  The parser is able to parse 20 Wall Street Diaries (WSJ) sentences per second on standard hardware, utilizing our best-performing paragon, which compares very favorably with other","acronyms":[[143,146],[44,47]],"long-forms":[[122,141]]},{"text":" 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and sev-","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":"In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI?09), pages 1,058?1,064, Pasadena, CA.","acronyms":[[81,89],[121,123]],"long-forms":[[22,79]]},{"text":"Processing, Hong Hk, Apr. HTK, 2004. Hidden Markov Model Toolkit (HTK) 3.2.","acronyms":[[68,71],[28,31]],"long-forms":[[39,66]]},{"text":"Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.","acronyms":[[74,77]],"long-forms":[[58,72]]},{"text":"77 .73  Problem 2nd 1.45 3.00  GS = Group significant cannot pool by individual  DISCUSS OF THE RESULTS ","acronyms":[[31,33]],"long-forms":[[36,53]]},{"text":"account for these generalizations by decom-  posing the grammar rules to Immediate Dom-  inance(ID) rules and Linear Preeedence(LP)  rules.","acronyms":[[128,130],[96,98]],"long-forms":[[110,126],[73,95]]},{"text":"Learning\". In Proceedings of the 3 rd ACL  Workshop on Very Large Corpora (WVLC95). ","acronyms":[[75,81]],"long-forms":[[43,73]]},{"text":"framework. This algorithm uses grammars in the Chomsky Normal Forms (CNF) so we employed the open origins Natural Language Toolkit2","acronyms":[[68,71]],"long-forms":[[47,66]]},{"text":"900  Lawsuits of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57, Reykjavik, Sweden, April 27, 2014.","acronyms":[[74,79],[83,87]],"long-forms":[[40,72]]},{"text":"Comprehension in 100 days,? publishes by  Chung Hwa Book Co., (H.K.) Ltd.  The  ChungHwa training set includes 100 Anglais ","acronyms":[[63,67]],"long-forms":[[48,56]]},{"text":"is placed sixth out of seventeen systems according to Mean Absolute Error (MAE) and third according to Root Mean Squared Error (RMSE). The","acronyms":[[128,132],[75,78]],"long-forms":[[103,126],[54,73]]},{"text":"lexical chaining.  4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that","acronyms":[[40,46],[52,58]],"long-forms":[[23,38]]},{"text":"e.g. Microsoft 2. Candidate Definition Features (CDs) : These consist of the two following feature classes.","acronyms":[[49,52]],"long-forms":[[18,47]]},{"text":"We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel  corpus.","acronyms":[[83,89],[40,46],[27,31]],"long-forms":[[63,81],[32,38]]},{"text":" 1 Introduction Coreference resolution (CoRe) is the process of finding markables (noun words) referring to the same","acronyms":[[40,44]],"long-forms":[[16,38]]},{"text":"Table 3 presents the total number of training examples extracted from SemCor (SC) and from the background documents (BG). As expected, by","acronyms":[[117,119],[78,80]],"long-forms":[[95,105],[70,76]]},{"text":"perform rather well because the combined recency  bias representation worked well on its own and be-  cause the restricted memory (RM) bias essentially  discards features that are distant from the relative ","acronyms":[[131,133]],"long-forms":[[112,129]]},{"text":"To tackle this challenge, we incorporate multiple graphs probabilistic factorization with two alternatively designed combination strategies into collaborative topic regression (CTR). Experimental results on real dataset demonstrate","acronyms":[[177,180]],"long-forms":[[145,175]]},{"text":"ments. We experimented with multiple classifiers including: SVM, Logistic Regression (LR), and Naive Bayes.","acronyms":[[85,87],[59,62]],"long-forms":[[64,83]]},{"text":"tracting sentence plan construction rules from the only publicly available corpus of discourse trees, the RST Discourse Treebank (RST-DT) (Carlson et al, 2002).","acronyms":[[130,136]],"long-forms":[[106,128]]},{"text":"cific characteristics in form, meaning, function, and distribution. Each entry includes a free text definition, schematic structurally description, definitions of construction components (CEs) and annotated example sentences.","acronyms":[[184,187]],"long-forms":[[161,182]]},{"text":"PP) MDI Missed Samples (MS) Bigram Missed Samples (MS) Figure 4: Values of PP and MS for automata for ad-hoc automata","acronyms":[[51,53],[0,2],[4,7],[24,26],[82,84],[75,77]],"long-forms":[[35,49],[8,22]]},{"text":"al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambigua-","acronyms":[[85,88]],"long-forms":[[56,83]]},{"text":" 2 Methodo logy   A User Centered (UC) approach was adopted for the  design of GEPPETTO.","acronyms":[[35,37],[79,87]],"long-forms":[[20,33]]},{"text":"cation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Supporting Vector Machines(SVMs).","acronyms":[[92,94],[124,128]],"long-forms":[[80,90],[100,123]]},{"text":"their semantic deviation values. The result is a  list of pairs called the ICS (Initial Cluster Set). ","acronyms":[[75,78]],"long-forms":[[80,99]]},{"text":"cal Dirichlet Process (HDP) (Toh et al, 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically deduce the number of topics.","acronyms":[[112,115],[23,26]],"long-forms":[[83,110],[0,21]]},{"text":"constituents. For example, discussing the possible adaptation of Phillips' algorithm to incremental gener-  ation, Lager and Black (1994) point out that some versions of Categorial Grammar (CG) would make the  generator more talkative, by giving rise to \"a more generous notion of constituency\".","acronyms":[[190,192]],"long-forms":[[170,188]]},{"text":"include other language skills such as listening and reading. These constitute the integrated (INT) items.","acronyms":[[94,97]],"long-forms":[[82,92]]},{"text":"settling piping consisted primarily of the C&C parser and Boxer (Curran et al, 2007), which produce Discourse Representation Structures (DRSs). ","acronyms":[[141,145],[47,50]],"long-forms":[[104,139]]},{"text":"and phrases and their compositionality. In Advances in Neural Information Processing Regimes (NIPS). ","acronyms":[[94,98]],"long-forms":[[55,92]]},{"text":"or more previous turns in the dialogue. The third column shows the mean error and standard error (SE) predicted for the model specified by the first two columns. When","acronyms":[[98,100]],"long-forms":[[82,96]]},{"text":" ITSPOKE-WOZ is a semi-automatic version of ITSPOKE (Clever Tutoring SPOKEn dialogue system), which is a speech-enhanced ver-","acronyms":[[44,51],[1,12]],"long-forms":[[53,80]]},{"text":"the secondly sentence provides some further description of that entity. An Entity Relationship (EntRel) was annotated for such sentence pairs as below.","acronyms":[[90,96]],"long-forms":[[73,88]]},{"text":"translation evaluation metrics such as BLEU score (Papineni et al, 2001), NIST score (Doddington, 2002) and Position Independence Word Error Rate (FOR) (Och, 2002).","acronyms":[[146,149],[39,43],[74,78]],"long-forms":[[108,144]]},{"text":"\t\u000f\u000e \u0010\u0011\t\u0013\u0012\u0014\u0010\u0016\u0015\u0016\u0015\u0016\u0015\u0017\u0010\u0011\t\u0019\u0018\u001a\u0007 consists of a directed acyclic graph (DAG) that encodes a set of conditional independence assertions about vari-","acronyms":[[64,67]],"long-forms":[[40,62]]},{"text":"that is neither terminal nor lexical. An interior node is  said to meet he foot condition (FC) iffeach foot feature  that it contains appears also on at least one daughter ","acronyms":[[91,93]],"long-forms":[[75,89]]},{"text":" 2.3 IR Similarity Measures (IR) The information retrieval?based features (IR) were based on a dump of English Wikipedia from Novem-","acronyms":[[75,77],[5,7],[29,31]],"long-forms":[[37,58]]},{"text":"   (2)  LSA-based (Latent Semantic Analysis based)  trigger word similarity: LSA (Deerwester et ","acronyms":[[8,17],[77,80]],"long-forms":[[19,43]]},{"text":"? P5E3N3S3, F W A Computer Processable English (CPE) (Pulman 1996; Sukkarieh and Pulman 1999) is a controlled language that can be ?","acronyms":[[48,51],[2,10]],"long-forms":[[18,46]]},{"text":"Minimum Sub-Structure (MSS) 87.95 87.88 Context-Sensitive MSS (CMSS) 89.11 89.01 Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46","acronyms":[[96,98],[23,26],[58,61],[63,67],[129,131]],"long-forms":[[81,94],[0,21]]},{"text":"When using the classifiers to predict the class of a test example, there are four possible outcomes; true positive (TP), true negative (TN), false positive (FP), and false nega-","acronyms":[[116,118],[136,138],[157,159]],"long-forms":[[101,114],[121,134],[141,155]]},{"text":"ayman,R.Gaizauskas@dcs.shef.ac.uk Abstract Disambiguating named organisations (NE) in running text to their exact interpretations in a specific knowledge base (KB) is an critical problem in NLP.","acronyms":[[74,76],[157,159],[188,191]],"long-forms":[[58,72],[141,155]]},{"text":"er positions less than the penalty of reordering the firstly classification.  iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel,  2008) habitually compares the rankings of two lists.","acronyms":[[122,127],[130,134]],"long-forms":[[74,120]]},{"text":"O-ADVL = Object Adverbial: lie ran two miles.  APP = Apposition: Helsinki, the capital of Finland,  N = Title: King George and Mr. ","acronyms":[[47,50],[0,6]],"long-forms":[[53,63],[9,25],[104,109]]},{"text":"used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to","acronyms":[[69,75]],"long-forms":[[52,67]]},{"text":"model.  Instance Frame Editor (CFE): Maintains the lexical  Case Frame Factbase, a data file of how to infer ","acronyms":[[27,30]],"long-forms":[[8,25]]},{"text":"is shown in PLATE 1. The second part is the  Prompt Piano Server (PPS), which is an IVR  (interactive voice response) server with a Dialogic ","acronyms":[[66,69],[84,87]],"long-forms":[[45,64]]},{"text":"is that of the closest centroid.  The Naive Bayes (NB) classifier is based on a probabilistic model which assumes conditional in-","acronyms":[[51,53]],"long-forms":[[38,49]]},{"text":"2003. MDA Guide Version 1.0.1. Technical report, Object Management Group (OMG). ","acronyms":[[74,77],[6,9]],"long-forms":[[49,72]]},{"text":" Recently researchers have been investigating Amazonian Mechanical Turk (MTurk) as a source of non-expert natural linguistics annotation, which is a","acronyms":[[70,75]],"long-forms":[[53,68]]},{"text":"Figure 1: Top-k Accuracy Level Configure MRR 0 Baseline (BL) 0.6559 1","acronyms":[[61,63],[45,48]],"long-forms":[[51,59]]},{"text":"learning this decision is learnt automatically.  Reinforcement Learning (RL) has been successfully used for learning dialogue management","acronyms":[[74,76]],"long-forms":[[50,72]]},{"text":"The tag B-X (Launch) represents the first word of a named entity of type X, for example, PER (Person) or LOC (Placements). The tag I-X (In-","acronyms":[[88,91],[104,107]],"long-forms":[[93,99],[109,117]]},{"text":"Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Traits Table 8 Model with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional","acronyms":[[151,154],[157,159]],"long-forms":[[138,149]]},{"text":"cf. Webber 1987b), representing the narrative's unfold-  ing contents, and the l inear text structure (LTS), whose  components are linked by rhetorical relations such as ","acronyms":[[103,106]],"long-forms":[[79,101]]},{"text":"To further investigate the effectiveness of our  method, the thirds set of experiments appraisals the  negative transfer detection (NTD) compared to  co-training (CO) without negative transfer ","acronyms":[[129,132],[160,162]],"long-forms":[[100,127],[147,158]]},{"text":"Assessing (LREC?08), Marrakech, Morocco, may.  European Language Resources Association (ELRA). ","acronyms":[[89,93],[12,16]],"long-forms":[[48,87]]},{"text":"Knowing the precise identity of Fisher vector ??(?), we propose a natural measure which we call  Weighted Gradient Uncertainty (WGU) based on the facts explained in the previous paragraph:  ????(???)","acronyms":[[128,131]],"long-forms":[[97,126]]},{"text":"C = connector  TR = terse reply  FS = untrue start  E = echo ","acronyms":[[33,35],[15,17]],"long-forms":[[38,49],[4,13],[20,31],[55,59]]},{"text":"edge mining.  Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain.","acronyms":[[30,37]],"long-forms":[[14,28]]},{"text":"and (W-1,W0,W1) ? Gazetteers (GAZ): We use two sets of gazetteers.","acronyms":[[30,33],[5,8],[9,11],[12,14]],"long-forms":[[18,28]]},{"text":"probabilities. Analysis showed that the correct tag most fre-  quently missing from the lattice was the DT (determiner)  tagging.","acronyms":[[104,106]],"long-forms":[[108,118]]},{"text":"The variants of random walk topic models are comparing against LDA and the relational topic modeling (RTM), each with 100 topics (Chang and Blei, 2010).","acronyms":[[98,101],[62,65]],"long-forms":[[74,96]]},{"text":"In this paper we propose a system which uses  hybrid methods that combine both rule-based  and machine learning (ML)-based approaches  to solve GENIA Event Extraction of BioNLP ","acronyms":[[113,115],[144,149],[170,176]],"long-forms":[[95,111]]},{"text":"These 67? 30 candidate claims were annotated using Amazon?s Mechanical Turk (AMT). In each","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":"employing them simultaneously. We also include the oracle word error rate (WER) of the WCNs and lattices for each ASR configuration.","acronyms":[[75,78],[87,91],[114,117]],"long-forms":[[58,73]]},{"text":"2.2 Keystroke Ratio (KSR) During extra to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al, 2003).","acronyms":[[127,130],[21,24]],"long-forms":[[110,125],[4,19]]},{"text":"mentation in (Zhang et al, 2006) 2.2 OOV Recognition with Accessor Variety Accessor variety (AV) (Feng et al, 2004) is a simple and effective unsupervised method for extrac-","acronyms":[[93,95],[37,40]],"long-forms":[[75,91]]},{"text":" The constructed ontology is evaluated using  cluster purity (CP), instances knowledge (IK),  and relation concept (RC).","acronyms":[[62,64],[88,90],[116,118]],"long-forms":[[46,60],[67,86],[98,114]]},{"text":"A simple solution to this problem is  to compute the probability of words in the target  language as maximum likelihood estimates (MLE)  over a large corpus and reformulate the general ","acronyms":[[131,134]],"long-forms":[[101,129]]},{"text":"of-the-art dependency parser, the 2014 online version. 4) PPAD Naive Bayes(NB) is the same as PPAD but uses a generative model, as opposed to","acronyms":[[75,77],[58,62],[94,98]],"long-forms":[[63,73]]},{"text":"5.2 Results We evaluate SO of words on three different sized corpora: Gigaword (GW) 6.2GB, GigaWord + 50% of web data (GW+WB1) 21.2GB and Gi-","acronyms":[[80,82],[24,26],[119,121],[122,125]],"long-forms":[[70,78]]},{"text":"initions of state significant be sensitive to (sometimes crucial) aspects of the dialogues context, such as the user?s last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of","acronyms":[[134,136]],"long-forms":[[119,132]]},{"text":"Abstract Microblogs are a popular way for users to communicate and have freshly caught the attention of researchers in the natural language treatments (NLP) field. However, regardless of their climb","acronyms":[[153,156]],"long-forms":[[124,151]]},{"text":" 1 Introduction Todays natural user interfaces (NUI) for applications running on smart devices, e.g, cellphones (SIRI,","acronyms":[[48,51],[109,113]],"long-forms":[[23,46]]},{"text":"We also have investigated our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) (Xue et al.,","acronyms":[[102,105]],"long-forms":[[84,100]]},{"text":"Perhaps not shockingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural linguistic generation (NLG) systems attempt to render the output texts in a principled routes.","acronyms":[[184,187],[73,76]],"long-forms":[[155,182],[41,71]]},{"text":"For each dataset, we report Pearson?s correlation r with human judgments on pairs that are found in both resources (BOTH). Otherwise, the re-","acronyms":[[116,120]],"long-forms":[[100,104]]},{"text":"We introduce a  exible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped his-","acronyms":[[61,64]],"long-forms":[[66,87]]},{"text":"by the designation of its author or the author?s place of  work.  In computational linguistic (CL) terms thi s  exert relies on proper noun extra ction.","acronyms":[[88,90]],"long-forms":[[62,86]]},{"text":"To standardize the actions to have fixed boundaries, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr,Cg) =","acronyms":[[118,121],[127,130]],"long-forms":[[87,116]]},{"text":" MEI is one of the four projects elected for  the Johns Hopkins University (JHU) Summer  Workshop 2000.1 Our research focus is on the ","acronyms":[[76,79],[1,4]],"long-forms":[[50,74]]},{"text":"In Proceedings of the NAACL\/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST), pages 33?40, Rochester, NY.","acronyms":[[94,98],[22,32],[125,127]],"long-forms":[[45,92]]},{"text":"bines the output of all the 13 base models lodged anterior. We implemented the meta-classifier using Support Vector Machine (SVMs)15 with a quadratic polynomial kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 exposition the results","acronyms":[[132,136]],"long-forms":[[107,130]]},{"text":"modelled as distributed dense vectors of stealth layers. A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":"? Toral (2013) explores the selection of data to train domain-specific language models (LM) from non-domain specific corpora by means","acronyms":[[88,90]],"long-forms":[[71,86]]},{"text":"We show that the newly proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven","acronyms":[[60,62]],"long-forms":[[35,58]]},{"text":"ILP.  An integer linear program(ILP) is basically the same as a linear program.","acronyms":[[32,35],[0,3]],"long-forms":[[9,30]]},{"text":"We require that the language I has an available Wordnet linked to the Princeton Wordnet (PWN) (Fellbaum, 1998). ","acronyms":[[89,92]],"long-forms":[[70,87]]},{"text":"We show how these strategies are captured in a grammar developed in the Grammatical Framework (GF).1 We evaluated our method by experimenting","acronyms":[[95,97]],"long-forms":[[72,93]]},{"text":"Errors are italicized and marked in red.  LDA with phrases (LDA-P): As aspect-sentiment phrases are often noun phrases, a basic approach","acronyms":[[60,65]],"long-forms":[[42,58]]},{"text":" 3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA, solely uses the anaphoricity information it pro-","acronyms":[[42,45],[73,75]],"long-forms":[[27,40]]},{"text":"Narrative Summarization. Journal Traitement automatique des langues (TAL): Special  issue  on Context:  Automatic Text  Summariza-","acronyms":[[69,72]],"long-forms":[[33,67]]},{"text":"labeled DGs.  (C3) Projectivity constraint(PJC): No arc crosses another arc5","acronyms":[[43,46],[8,11]],"long-forms":[[19,41]]},{"text":"tleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turkic (MTurk), a task commonly mentioned to as crowdsourcing.","acronyms":[[131,136]],"long-forms":[[114,129]]},{"text":"We show that languageindependent features can be used for regression with Support Vector Machines (SVMs) and the Margin-Infused Relaxed Algorithm (MIRA), and","acronyms":[[99,103],[147,151]],"long-forms":[[74,97],[113,145]]},{"text":"1408 then apply our model to the well-established sequence labeling task: noun phrase (NP) chunking. ","acronyms":[[87,89]],"long-forms":[[74,85]]},{"text":"LL; it is calculated from contingency table information as follows: LO = log (O11 + 0.5)(O22 + 0.5)","acronyms":[[68,70]],"long-forms":[[73,76]]},{"text":"and Innovation action). This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Tech-","acronyms":[[92,96]],"long-forms":[[53,90]]},{"text":"C STORE SPEECH WAVE POINT  C  NBUF (NPT) =IFIX(YN)  750 CONTINUE ","acronyms":[[36,39],[47,49]],"long-forms":[[30,34]]},{"text":"words. We model semantic relatedness between two words using the Information Content (IC) of the pair in a method similar to the one used by Lin","acronyms":[[86,88]],"long-forms":[[65,84]]},{"text":" Ours propose a new algorithm: collective iterative classification (CIC) to perform approximate inference to find the maximum a posteriori","acronyms":[[66,69]],"long-forms":[[29,64]]},{"text":"de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assess under an attitude predicate (say), the happenings in Examples (6a) and (6b) are assessed as certain (CT+), whereas the words immeasurably confident in Example (6c) trigger PR+, and may in Instance (6d) leads to PS+.","acronyms":[[181,184],[246,249],[284,287]],"long-forms":[[172,179]]},{"text":"represented in an n ? n matrix of objects by a  multidimensional scaling (MDS) of the distance  between each object.","acronyms":[[74,77]],"long-forms":[[48,72]]},{"text":"subcategorisation information in the form of a Y in col-  umn 8, then we can assign the label for wh-pronou,  head (HWH). An exception list is used to map to the ","acronyms":[[116,119]],"long-forms":[[110,114]]},{"text":" 1 Introduction Question answering (QA) systems have received a huge deal of attention because they provide both","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":" 3.1 LSTMs for sequence generation A Recur Neural Network (RNN) is a generalization of feed forward neural webs to se-","acronyms":[[63,66],[5,10]],"long-forms":[[37,61]]},{"text":" In this paper, we introduce a novel method called Random Manhattan Indexing (RMI). RMI","acronyms":[[78,81],[84,87]],"long-forms":[[51,76]]},{"text":"did. We used files 1-270, 400-554, and 600-931 as source domain training data (STrain), files 271300 as source domain testing data (STest) and files","acronyms":[[79,85],[132,137]],"long-forms":[[50,72],[104,130]]},{"text":"  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":" Finally there is a set of measures relating to the receiver operating characteristic (ROC) curves, which measure the discrimination of the scores for","acronyms":[[87,90]],"long-forms":[[52,85]]},{"text":"tools for the Indian Languages. At Telugu, though a Part Of Discourse(POS) Tagger for Telugu, is available, the accuracy is less when compared to English","acronyms":[[68,71]],"long-forms":[[53,66]]},{"text":"2. TREE ADJO IN ING GRAMMARS- -TAG's   We now introduce tree neighbourhood grammars (TAG's). TAG's ","acronyms":[[81,86],[31,36],[89,94]],"long-forms":[[56,79]]},{"text":" ? LEAD (lead-based): n% punishments are chosen from the beginning of the text.","acronyms":[[3,7]],"long-forms":[[9,13]]},{"text":" tage of the OpenCCG realizer?s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White,","acronyms":[[84,88],[13,20]],"long-forms":[[57,82]]},{"text":"3http:\/\/www.noslang.com 370 are the New York Period (NYT),4 SMS,5 and Twitter.6 The findings are presented in Figure 1.","acronyms":[[52,55]],"long-forms":[[36,50]]},{"text":"lation of term-lists, related studies are found in the  area of target word selection (for content words) in  conventional full-text machine translation (MT). ","acronyms":[[154,156]],"long-forms":[[133,152]]},{"text":"The RASP toolkit (Briscoe et al, 2006) is used for sentence boundaries detected, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text.","acronyms":[[141,143],[4,8],[94,97]],"long-forms":[[118,139]]},{"text":"five COMPARE3, and five RECOMMEND CPs.  Each of the 112 textplans (TPs) that produced 4","acronyms":[[67,70],[34,37]],"long-forms":[[56,65]]},{"text":"semantic links between NEs extracted from the answer sentence and the question focus word, which encodes the expected lexical answer type (LAT). We","acronyms":[[139,142],[23,26]],"long-forms":[[118,137]]},{"text":"Arabic Penn TreeBank POS tagset. Base Phrase (BP) Chunking is the process of establishment non-recursive base phrases such","acronyms":[[46,48],[21,24]],"long-forms":[[33,44]]},{"text":"(BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (Chen et al, 2009).","acronyms":[[113,115],[1,3],[163,166]],"long-forms":[[97,111]]},{"text":"We focused on syntax, esp. noun phrase (NP) syntax from the beginning.","acronyms":[[40,42]],"long-forms":[[27,38]]},{"text":"  1. RecallCorrectTransliteration  (RTrans)  The recall was computed uses the sample as ","acronyms":[[36,42]],"long-forms":[[5,33]]},{"text":"ogy (GO), Cell Sorts Ontology (CTO), BRENDA Tissue Ontology (BTO), Foundational Model of Anatomy (FMA), Cell Cycle Ontology (CCO), and Sequence Ontology (HENCE)?and a small number of","acronyms":[[124,127],[5,7],[30,33],[60,63],[97,100],[153,155]],"long-forms":[[103,122],[10,28],[36,58],[66,95],[134,151]]},{"text":"Certain CT+ (factual) CT? ( counterfactual) CTu (certain but unknown output) Probable PR+ (probable) PR? ( not probable) [NA]","acronyms":[[86,89],[8,11],[22,24],[44,47],[101,104],[122,125]],"long-forms":[[91,99],[111,119],[49,75],[28,42]]},{"text":"sisted of 2000 features, representing the most frequent (head, themes) and (jefe, object) pairs in the British National Corpus (BNC). The feature-","acronyms":[[129,132]],"long-forms":[[104,127]]},{"text":"The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) functions. ","acronyms":[[107,110]],"long-forms":[[78,105]]},{"text":"rules, along with a few lexical rules involving a list of stop phrases, discourse cue phrases and wordlevel parts of speech (POS) tags. First, paragraph","acronyms":[[125,128]],"long-forms":[[108,123]]},{"text":"They are not constrained  by features of any previous utterance in the  discourse segment (DS), and the elements of Cf(Un)  are partially ordered to reflect relative prominence ","acronyms":[[91,93],[119,121],[116,118]],"long-forms":[[72,89]]},{"text":"(SBAR-TMP (IN after) (S (NP (DT the) (NN sell)) (VP (AUX is) (VP (VBN completed))) ))))))))","acronyms":[[66,69],[1,9],[25,27],[29,31],[38,40],[49,51],[53,56],[62,64]],"long-forms":[]},{"text":"573   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59, Sofia, Bulgaria, August 9, 2013.","acronyms":[[71,78]],"long-forms":[[37,69]]},{"text":"collection; nonetheless, to facilitate comparisons with prior work (e.g., McCarthy et al 2004a), all our experiences use the British National Corpus (BNC). In","acronyms":[[146,149]],"long-forms":[[121,144]]},{"text":"KODIAK (Keystone to Overall Design for Inte-  gration and Application of Expertise) is an implemen-  tation of CRT (Cognition Representation Theory), an  approach to knowledge representation that bears simi- ","acronyms":[[111,114],[0,6]],"long-forms":[[116,147],[8,82]]},{"text":"Verbs can similarly involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), etc.","acronyms":[[81,84],[60,67],[97,105]],"long-forms":[[86,94],[69,73],[107,112]]},{"text":"ing different methods: The methods respectively without prediction(NP), with prediction(P),  with prediction and feedback(PF) only using term  frequency (TM), and with prediction and feed-","acronyms":[[122,124],[67,69],[154,156]],"long-forms":[[98,120],[77,87],[137,152]]},{"text":"Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popu-","acronyms":[[108,114]],"long-forms":[[87,106]]},{"text":"ture. Later, AVG were enriched with a statistical ingredient (Abney, 1997): stochastic AVG (SAVG). ","acronyms":[[91,95],[13,16]],"long-forms":[[75,89]]},{"text":"This usage is domain specific and the referent is fixed as a second person subject.  pora: 1) three transcripts of family talks (FaCon) drawn from Australian English Corpus (Monash Colleges 1996~1998) gathered family interviews about their past festivities (Nariyama 2004); 2) three 30-minute-TV Australian tragedy transcripts (TV) (Nariyama 2004); 3) Switchboard corpus consisting of phoning conversation on a variety of specify every day topics (Cote 1996).  Referent   FaCon TV dramas Switchboard I we you he\/she it they ","acronyms":[[136,141],[482,487],[334,336],[301,303]],"long-forms":[[115,134]]},{"text":"CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Backdrop; RC = Right Context There are four different kinds of rules that maggio be","acronyms":[[59,61],[78,80],[0,2],[3,5],[6,8],[9,11],[18,20],[44,46]],"long-forms":[[64,76],[83,96],[23,42],[49,57]]},{"text":"been semi-automatically discover in human reference and machine translations from Englishman (EN) to French (FR) and German (DE) (Section 3). ","acronyms":[[106,108],[91,93],[122,124]],"long-forms":[[98,104],[82,89],[114,120]]},{"text":"+ b) (4) where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. ","acronyms":[[84,88]],"long-forms":[[61,82]]},{"text":"task. As reported in (Liu et al 2013a), we used a genetic algorithm (GA) (Cormen et al 2001) to au80","acronyms":[[69,71]],"long-forms":[[50,67]]},{"text":"pattern, up to mi example questions.  Question Pattern (QPi):  When do Q_PRN Q_MVerb Q_BNP?","acronyms":[[56,59],[71,76],[77,84],[85,90]],"long-forms":[[38,54]]},{"text":"RERANKED 92.7 42.9 92.0 32.6 ORACLE 97.6 81.2 96.7 72.5 Table 4: Word accuracies and error rate reduction (FELA) in percentages for CELEX G2P intensified by Combilex","acronyms":[[108,111]],"long-forms":[[85,106]]},{"text":"? P2E1N3S2, C S D G ASD Simplify Technical English (ASD-STE) (ASD 2013), often abbreviated to Simplified Technical English (STE) or just Simpler English, is a CNL for the aerospace","acronyms":[[54,61],[0,10],[126,129],[164,167],[64,67]],"long-forms":[[20,52],[96,124]]},{"text":"We would expect this to be the case in general, but as repeatedly, cases exist where a conflict between a contrast (CoCo) and a amendments to a method (PModi) emerge:","acronyms":[[112,116],[144,149]],"long-forms":[[83,110]]},{"text":"ROBUST04 WT10G GOV2MAP R-Pr MAPS R-Pr MAPS R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08 Tableau 5: Result with controlled picks of catenae with specifies duration (SF-12, SF-123) are more potency than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD). ","acronyms":[[412,417],[430,435]],"long-forms":[[383,410]]},{"text":"tailment pairs to train an entailment classifier, our paragon was only trained on the 800 text-hypothesis pairs unearthed in the RTE-3 Development Set (DevSet). ","acronyms":[[146,152],[123,128]],"long-forms":[[129,144]]},{"text":"search excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz (LOEWE) as part of the research center Digital Humanities.","acronyms":[[101,106]],"long-forms":[[26,99]]},{"text":"To experiment on MAYA, we compute the  performance punctuation as the Reciprocal Answer  Rank (RAR) of the first correct respond given by  each question.","acronyms":[[89,92],[17,21]],"long-forms":[[83,87]]},{"text":"UAS = unlabeled attachment score; UEM = unlabeled exact match; LA = labeled attachment score. ","acronyms":[[63,66],[0,3],[34,37]],"long-forms":[[69,93],[6,32],[40,61]]},{"text":"er positions less than the penalty of reordering the first ranks.  iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel,  2008) normally compares the rankings of two lists.","acronyms":[[122,127],[130,134]],"long-forms":[[74,120]]},{"text":"Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu  Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu  Abstract  American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents.","acronyms":[[363,366],[87,91],[118,120],[263,267],[299,301],[530,533],[611,614]],"long-forms":[[339,361],[58,85],[108,116],[234,261]]},{"text":"forward and backward apps (FA and BA), implies a  standard notion of constituency, rules like type rises (TR)  and functional composition (FC) give ascent to a more  generous notion of constituency (this is what makes 'non- ","acronyms":[[148,150]],"long-forms":[[124,146]]},{"text":"ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08 Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are more effective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD). ","acronyms":[[412,417],[430,435]],"long-forms":[[383,410]]},{"text":" Because we don't have any other useful resources except ChineseGigaword(CGW),We firstly calculating mutual information for all 3-character words in two","acronyms":[[73,76]],"long-forms":[[57,72]]},{"text":"The parser uses a semantic grammar with approx-  imately 1000 rules which maps the input sentence  onto an interlingua representation (ILT) which rep-  resents the meaning of the sentence in a language- ","acronyms":[[135,138]],"long-forms":[[107,133]]},{"text":"The ISO 639-3 language codes for our eight languages are as follows: Urdu (URD), Thai (THA), Bengali (BEN), Tamil (TAM), Punjabi (PAN), Tagalog (TGL), Pashto","acronyms":[[75,78],[102,105],[4,7],[87,90],[115,118],[130,133],[145,148]],"long-forms":[[69,73],[93,100],[81,85],[108,113],[121,128],[136,143]]},{"text":"User Clustered (WON).4 Research and development  is carried out in close cooperated with user  groups and intellectual property (IP) professionals to ensure solutions and software are delivered ","acronyms":[[128,130],[12,15]],"long-forms":[[105,126]]},{"text":"Central to the predictive dialogue is the topics representation for each scenario, which authorizes the population of a Predictive Dialogue Network (PDN). ","acronyms":[[145,148]],"long-forms":[[116,143]]},{"text":"Way (1991) highlights the importance of this taxonomy by positing a central role for a dynamic type hierarchy (DTH) in metaphor, one that can engender new and com-","acronyms":[[111,114]],"long-forms":[[87,109]]},{"text":"I_tt is raining.,  OBJ = Object: He read a book., ","acronyms":[[19,22]],"long-forms":[[25,31]]},{"text":"The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates.","acronyms":[[338,341],[21,26],[198,200],[205,207],[484,487],[534,537]],"long-forms":[[307,336]]},{"text":"As a starts point, the classes for complements  and trait developed by the New York Univer-  sity Linguistic String Project (LSP) (Fitzpatrick,  1981), were elects sin(x, the coverage is very ","acronyms":[[130,133]],"long-forms":[[103,128]]},{"text":"linear chain, CRFs make a first-order Markov independence assumption, and thus can be understood as conditionally-trained finite state machines(FSMs). ","acronyms":[[144,148],[14,18]],"long-forms":[[122,142]]},{"text":" Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful","acronyms":[[72,75],[104,106]],"long-forms":[[51,70],[81,96]]},{"text":"  2 Dimensionality Reduction   VSM (Vector Space Model) is a basic technique  to conversion text documents to numeric vectors.","acronyms":[[31,34]],"long-forms":[[36,54]]},{"text":"The partitioning of the dataset is listed in the Table 1, where we furthermore give the partitioning of Wall Street Journal (WSJ) (Marcus et al, 1993) used to forming the Englishman grammar.","acronyms":[[118,121]],"long-forms":[[97,116]]},{"text":"emes are represented, and not function words.  Conceptual representations (ConcSs) used by  PRESENTOR are inspired by the characteristics ","acronyms":[[75,81],[92,101]],"long-forms":[[47,73]]},{"text":"Sate (STT) 42.85 76.93 Genre (GEN) 67.42 84.17 Determiner (DET) 59.71 85.41 Number (NUM) 70.61 87.31","acronyms":[[61,64]],"long-forms":[[49,59]]},{"text":"for re-ranking in the context of name tagging.  Maximum Entropy modeling (MaxEnt) has  been extremely successful for many NLP classifi-","acronyms":[[74,80],[122,125]],"long-forms":[[48,63]]},{"text":" Another syntactic phenomena vital to the parser is known as the complex NP  Limitation (CNPC) (Radford 1981); i.e., no transformation rule can move any element  out of a sprawling NP, where a difficult NP (CNP) is an NP containing a relative clause.","acronyms":[[91,95],[181,183],[201,203],[205,208],[216,218],[75,77]],"long-forms":[[79,89]]},{"text":"PIQ(Fy;R) or not.  z Predictive Information Redundancy(PIR) Founded on the above two definitions, we can","acronyms":[[55,58],[0,3],[4,8]],"long-forms":[[21,53]]},{"text":"and labeled by the people on Amazon Mechanical Turk, a web service. Amazon Mechanical Turkish (MTurk) allows individuals to post work on MTurk with a set fee that are","acronyms":[[92,97]],"long-forms":[[75,90]]},{"text":"increasingly more major.  In ordinary phrase structure grammars (PSG's),  the only mechanism for capturing the kinds of merg- ","acronyms":[[69,74]],"long-forms":[[42,67]]},{"text":"Finite State Automata (FSA) have traditionally been used in speech processing, but they are clearly  inappropriate for spoken language systems. In this section, we contrast unification grammars (UGs) with  Context-Free grammars (CFGs) and discuss extensions needed for spoken language systems.","acronyms":[[195,198],[23,26],[229,233]],"long-forms":[[173,193],[0,21],[206,227]]},{"text":"cognition(COG) competition(COMP)  contacted(CeNT) motion(MOT)  emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA) ","acronyms":[[85,88],[10,13],[27,31],[42,46],[55,58],[69,72],[102,106],[117,120]],"long-forms":[[74,84],[0,9],[15,26],[34,41],[48,54],[61,68],[91,101],[108,116]]},{"text":"of conditional random fields. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 870?878.","acronyms":[[98,101]],"long-forms":[[55,96]]},{"text":"In order to normalize Thai input personage  sequences to a canonical Unicode form, we developed a finite state transducer (FST) which  detect and repairs a number of sequencing er-","acronyms":[[123,126]],"long-forms":[[98,121]]},{"text":"When these approaches are applied to normal Twitter users accuracy results significantly diminished.  Sentiment Analysis (SA) has been widely studied in the last decade in various domains. Most work","acronyms":[[120,122]],"long-forms":[[100,118]]},{"text":"The training set consisted of 12,927 texts: News reports (News) about Education (Edu), Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med). ","acronyms":[[154,158],[81,84],[99,103],[121,124],[176,179]],"long-forms":[[131,152],[70,79],[87,97],[112,119],[166,174]]},{"text":"(Joshi, Vijay?Shanker, and Weir, 1989; Weir and Joshi,  1988) have shown that LIGs, Combinatory Categorial  Grammars (CCG), Trees Adjoinig Grammars (labels),  and Head Grammars (HGs) are weakly equivalent.","acronyms":[[148,152],[78,82],[118,121],[175,178]],"long-forms":[[124,146],[84,116],[160,173]]},{"text":"Newly, McDonald et al (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic times relative","acronyms":[[91,94]],"long-forms":[[68,89]]},{"text":"evaluate the quality of the paraphrase collection.  In parcitular, Amazon?s Mechanical Turk1 (MTurk) provides a way to pay people small amounts of","acronyms":[[94,99]],"long-forms":[[76,92]]},{"text":"1 Introduction As they are normally conceived, several tasks relevant to Computational Linguistics (CL), such as text categorization, clustering, and information retrieval, omit the con-","acronyms":[[97,99]],"long-forms":[[70,95]]},{"text":"0 ORIGINS Figure 2: Example of a CoNLL-style annotated sentence. Everyone word (FORM) is numbered (ID), lemmatized (LEMMA), annotated with two levels of part-of-speech label (CPOSTAG and POSTAG), annotated with morpho-","acronyms":[[92,94],[30,35],[73,77],[109,114],[167,174],[179,185]],"long-forms":[[79,90],[97,107],[146,165]]},{"text":"put language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of","acronyms":[[98,101]],"long-forms":[[103,130]]},{"text":" cs.uni-kassel.de\/bibsonomy\/dumps Content Relevance (CRM) model (Iwata et al, 2009) and Tag Allocation Model (TAM) (Si et al,","acronyms":[[53,56],[110,113]],"long-forms":[[34,51],[88,108]]},{"text":" 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity","acronyms":[[53,57]],"long-forms":[[28,51]]},{"text":"The number of sentences with product feature  ? Word level (WL)  ?","acronyms":[[61,63]],"long-forms":[[49,59]]},{"text":"inforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used.","acronyms":[[68,70]],"long-forms":[[53,66]]},{"text":"wk} be its vocabulary. In the monolingual settings, the Vector Space Model (VSM) is a k-dimensional space Rk, in which the text tj ?","acronyms":[[76,79],[106,108]],"long-forms":[[56,74]]},{"text":"novel opportunity: part of Attempto Controlled Englishman (ACE) was mapped to OWL (Kaljurand and Fuchs, 2007), and Processable English (PENG) evolved to Sydney OWL Syntax (SOS) (Cregan et","acronyms":[[131,135]],"long-forms":[[110,129]]},{"text":"segmentable candidates, and picks a correct segmentation candidate from the list by using a value of LEF (Likelihood Evaluation Function, Section 2.1) and so on.","acronyms":[[101,104]],"long-forms":[[106,136]]},{"text":"semaatic grounds, new referent objects must be created. The number of objects to be  created is set equal to the QTY (quantity) attribute of the noun phrase if specified (as in  \"two boys\" (P20)), to two if the noun phrase is plural and not compound, to the number ","acronyms":[[113,116]],"long-forms":[[118,126]]},{"text":" (MAP) adjustment of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.","acronyms":[[81,85],[38,44],[2,5]],"long-forms":[[50,79],[21,36]]},{"text":"~ ~ . ~  5  Object S t r i n g  (OBJL1,ST) Refercn'ce Guide. .............. 9 ","acronyms":[[33,41]],"long-forms":[[12,30]]},{"text":"If we put these two constraints together we obtain the constraint MINS = MAXS, which means that the area where quantifiers take scope (the MAXS-","acronyms":[[73,77],[66,70],[139,143]],"long-forms":[]},{"text":"tongues. In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10). ","acronyms":[[84,90],[12,16]],"long-forms":[[25,54]]},{"text":" 8SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal ob-","acronyms":[[72,74],[1,4],[16,18],[40,43],[85,87],[107,109],[134,136],[150,152]],"long-forms":[[77,83],[7,14],[21,38],[46,70],[90,105],[112,132],[139,148],[155,165]]},{"text":"of FIS model used to resolve each expression.  4.1 Similarity Features (SIM) Similarity features represent the lexical overlap","acronyms":[[72,75],[3,6]],"long-forms":[[51,61]]},{"text":" Ours used Moses (Koehn et al 2007) with lexicalized reordering and a 6-gram language model (LM) trained utilize SRILM (Stolcke et al 2011) to trans-","acronyms":[[91,93],[109,114]],"long-forms":[[75,89]]},{"text":"  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54, Boulder, Colorado, June 2009.","acronyms":[[87,92]],"long-forms":[[46,85]]},{"text":"Budanitsky and Hirst Lexical Semantic Relatedness Figure 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection by measure and scope. ","acronyms":[[70,72],[83,85],[103,105]],"long-forms":[[59,68],[75,81],[92,101]]},{"text":"To model the relations between objects and verbs, we follow the data preparation in (Le et al 2013), uses the Brits National Corpus (BNC) which has been preprocessed and parsed utilize TreeTagger and","acronyms":[[136,139]],"long-forms":[[111,134]]},{"text":" 1 Introduction Referring Expression Generation (REG) is a keytask in NLG, and the subjects of the REG 2008 Chal-","acronyms":[[49,52],[70,73],[96,99]],"long-forms":[[16,47]]},{"text":"If the polarity of manifested statement is not  neutrality and reinforcement is negative, then the  polarity of the statement (PP) is reversed and  score is intensified: ","acronyms":[[123,125]],"long-forms":[[96,104]]},{"text":"  Although we use the same techniques to derive global features (assessor variety (AV) feature with 2~6 grams) from both training and test-","acronyms":[[83,85]],"long-forms":[[65,81]]},{"text":" The LRS representation of (1) is shown in Figure 1, where INCONT (INTERNAL CONTENT) encodes the core semantic contribution of the head, EXCONT","acronyms":[[59,65],[5,8],[137,143]],"long-forms":[[67,83]]},{"text":"experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et","acronyms":[[70,73]],"long-forms":[[43,68]]},{"text":"\u0006? ( PM), tree matching without the auxiliary patterns (TM), and tree matching with the auxiliary patterns","acronyms":[[56,58]],"long-forms":[[49,54]]},{"text":"IDF Approach. Proceedings of International Conference on Language Resources and Evaluation (LREC) 2012, European Language Resources Association","acronyms":[[92,96],[0,3]],"long-forms":[[57,75]]},{"text":"search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for accessing previously resolved crossword puz-","acronyms":[[61,63]],"long-forms":[[51,59]]},{"text":"In Proceedings of the 2nd International Conference on Language Resources and Evaluating (LREC). ","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":" 2 Forced Derivation Tree for SMT A forced derivation tree (FDT) of a sentence pair {f, e} can be defined as a pair G =< D,A >:","acronyms":[[60,63],[30,33]],"long-forms":[[36,58]]},{"text":"2005; Wieling et al, 2007) for string similarity  estimation, and is based on the notion of twine  Edit Distance (ED). String ED is define here as ","acronyms":[[115,117],[127,129]],"long-forms":[[100,113]]},{"text":"REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62 HUE = coloursensorreadning78","acronyms":[[40,45],[0,3],[13,17],[69,75]],"long-forms":[[48,68],[20,39],[78,100],[6,12]]},{"text":" The single product avis summarizer we consider is the Sentiment Aspect Match modelled (SAM) described and evaluated in (Lerman et al, 2009).","acronyms":[[88,91]],"long-forms":[[58,80]]},{"text":"The prefer-  ence score (PS) for a paired is determined by the ratio  of its local dominance counts (LDC)--the total num-  ber of cases in which the pair is domestically dominant--to ","acronyms":[[98,101],[25,27]],"long-forms":[[75,96],[4,23]]},{"text":"grammars. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING-92), pages 426?432, Nantes, 1992.","acronyms":[[98,107]],"long-forms":[[57,96]]},{"text":"For comparative, the graphs in Figures 1 and 2 further demonstrating the curves corresponding to the evaluation of Pointwise Mutual Information (PMI).8 The cooccurrence statistics of the expressions in Disco-En-","acronyms":[[132,135]],"long-forms":[[102,130]]},{"text":"(grammatical knowledge and lexical knowledge respectively). The insights gained from our work  with machine readable dictionaries (MRDs) the Longman Dictionary of Contemporary English,  henceforth LDOCE, and the Van Dale dictionary of contemporary Dutch \"Groot Woordenboek van ","acronyms":[[131,135],[197,202]],"long-forms":[[100,129]]},{"text":"A S O W B E  - as + Object of be  A S S E R T I O N ~ S ~ ~ ~ ~ ~ ~  + Tense + V f r b  Object  ASTG = Adjectives String  C l  MUST -- Subjunctive foYm of ASSERTllgZQ ","acronyms":[[96,100]],"long-forms":[[103,119]]},{"text":"Training uses balanced data (50:50). Testing uses two sorts distributions (C.D.): 50:50 (balanced) and Natural Distribution (N.D.). Refinements of our method are statistically major with p<0.005 based on paired t-test.","acronyms":[[125,129],[75,78]],"long-forms":[[103,123],[54,72]]},{"text":"ferently by (1) and (2)8.  5 The Neutral Edge Direction (NED) Measure","acronyms":[[57,60]],"long-forms":[[33,55]]},{"text":"HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the issues by formulating tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also assists native English speakers. This chore is purely expressed as text-to-text generation or Natural linguistics Generation (NLG). In the 2011 interchange task, all possible errors were covered which made the task uncommonly huge.","acronyms":[[441,444],[0,3]],"long-forms":[[412,439]]},{"text":"1(c)) for each k. 3.3.2 PLEB PLEB (Point Locations in Equal Balls) was outset proposed by Indyk and Motwani (1998) and further","acronyms":[[29,33],[24,28]],"long-forms":[[35,64]]},{"text":"kept on a ventilator for doctor reasons.     Change of state (COS) is most often understood  as an aspectual divergence that is reflected in verb ","acronyms":[[63,66]],"long-forms":[[46,61]]},{"text":"10-fold open test 62.80-58.54 59.15 66.46-65.55 65.55 65.55-64.63 Table 5: Compares of Optimizers (Avis in KNB Corpus) Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data choices Annealing","acronyms":[[141,145],[113,116],[164,166]],"long-forms":[[125,139]]},{"text":"Here we perform a set of experiments where we investigate the potential of multi-source transfer for NER, in German (DE), English (EN), Spanish (ES) and Dutch (NL), using cross-lingual","acronyms":[[131,133],[101,104],[117,119],[145,147],[160,162]],"long-forms":[[122,129],[109,115],[136,143],[153,158]]},{"text":"Onto the SIGHAN Bakeoff 2007, there are five training corpus for word segmentation (WS) task: AS  (Academia Sinica), CityU (City University of  Hong Kong) are traditional Chinaman corpus; CTB ","acronyms":[[115,120],[82,84],[92,94],[185,188]],"long-forms":[[122,137],[63,80],[97,112]]},{"text":"adv Adverbial words(RB, RBR, RBS)  adj Adjunct word(JJ,JJR,JJS)  advP Adverb phrase(ADVP)  punct Punctuation(,) ","acronyms":[[84,88]],"long-forms":[[70,82]]},{"text":"Spurned? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the","acronyms":[[66,70],[45,48]],"long-forms":[[54,64],[24,32]]},{"text":"ABSTRACT  We presenting a progress report on our research  on nominal compounds (NC's). Recent approaches to ","acronyms":[[78,82]],"long-forms":[[59,76]]},{"text":"October-2001 w\/out 2000 SPA 30.88 \u0000 95.68 \u0000 0.08 \u0000 Table 2: Consequences for Identification Speech-Act DATE tags in the October-2001 Broadcaster Corpus, (Dim = Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl.","acronyms":[[203,206],[24,27],[95,99],[147,150],[226,228]],"long-forms":[[209,219],[153,162]]},{"text":"2013).  Currently the most active framenet research teams are working on Krone FrameNet (SweFN) (Borin et al.,","acronyms":[[91,96]],"long-forms":[[73,89]]},{"text":" In this paper, we propose a new, structures vector space model for word meaning (SVS) that addresses these problems.","acronyms":[[82,85]],"long-forms":[[34,57]]},{"text":"Our model incorporates easy priors inspired by the minimum description length (MDL) principle, as well as overlapping peculiarities such as morphemes and","acronyms":[[81,84]],"long-forms":[[53,79]]},{"text":"Lastly, it is clear from Figure 6 that certain relations are particularly knotty for both parsers. For example, indirect object (IObj) dependents are low scoring nodes: this is because they are often attached to the exact head but are","acronyms":[[133,137]],"long-forms":[[116,131]]},{"text":" BIBLIOGRAPHY  ALPAC (Automatic Language Processing Advisory Committee)  1966 Lanquage and Machines - Computers i n  Translation and Linguistics ","acronyms":[[15,20]],"long-forms":[[22,70]]},{"text":"marn et al 2007). The training procedure routinely employs an expectation maximization (EM) procedure and the resultant transducer can be used to","acronyms":[[86,88]],"long-forms":[[60,84]]},{"text":" 4.1 Representing Transformations as FSTs Finite State Transducers (FSTs) provide a natural formalism for representing output transformations.","acronyms":[[68,72],[37,41]],"long-forms":[[42,66]]},{"text":"numeroinen se on (UT = Utterance as actually made by the users, UR = Utterance as acknowledged by the system, SU = System utterance).","acronyms":[[63,65],[18,20],[107,109]],"long-forms":[[68,77],[23,32],[112,128]]},{"text":" 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiment (Brew","acronyms":[[43,47]],"long-forms":[[22,41]]},{"text":"ity. In Proceedings of Treebanks and Linguistic Theories (TLT) 2003, Vaxjo, Sweden. ","acronyms":[[58,61],[71,74]],"long-forms":[[23,56]]},{"text":"from the output of the parser we adopt a uniform meaning representation which is a structured Logical Form(LF). Among other words we cartography our f-","acronyms":[[107,109]],"long-forms":[[94,106]]},{"text":"In this shared task we test the feasibility of eliciting parallel data for Machines Translation (MT) using Mechanical Turkish (MTurk). MT poses an interesting","acronyms":[[123,128],[96,98],[131,133]],"long-forms":[[106,121],[75,94]]},{"text":"gradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS). ","acronyms":[[89,93]],"long-forms":[[50,87]]},{"text":"Abstract  Plus machine transliteration systems  transliterate out of vocabulary (OOV)  words through intermediate phonemic ","acronyms":[[81,84]],"long-forms":[[62,79]]},{"text":"systems. This shared task was partially supported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal His-","acronyms":[[81,86],[96,99]],"long-forms":[[53,79]]},{"text":"puts to a system and the outputs it is intended to produce. In Machine Translation (MT), such resources take the form of sentence-aligned parallel corpora of","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":"mentation model, which we train on the Arabian side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to Anglais,","acronyms":[[139,142],[163,166],[172,175]],"long-forms":[[122,130],[145,154]]},{"text":"The two main Modern Standard Arabic dependency treebanks currently available are the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009) and the Prague Arabic Dependency","acronyms":[[111,116]],"long-forms":[[85,109]]},{"text":" 1 Introductions Multiword Expressions (MWEs) are commonly defined as ?","acronyms":[[39,43]],"long-forms":[[16,37]]},{"text":"manual mapping. In the rest of the paper, we shall first  describe the Switchboard Dialogue Act (SWBD-DA)  Corpus and its annotation scheme (i.e. SWBD-DAMSL).","acronyms":[[97,104],[146,156]],"long-forms":[[71,95]]},{"text":"LA     =   The average length (ALen) of chunks for each  type is the average number of tokens in each chunk ","acronyms":[[31,35],[0,2]],"long-forms":[[15,29]]},{"text":"ysis. The Chinese comma is viewed as a delimiter of rudimentary discourse units (EDUs), in the sense of the Rhetorical Structure Theories (Carlson et al,","acronyms":[[80,84]],"long-forms":[[52,78]]},{"text":"volved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) basis parsing formalism.","acronyms":[[85,88]],"long-forms":[[61,83]]},{"text":"ings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). ","acronyms":[[85,88]],"long-forms":[[58,83]]},{"text":" ? System integration, through SGML (the Standard Generalized Markup Language), both at the leve l of meaning analysis and at the overall application level .","acronyms":[[31,35]],"long-forms":[[41,77]]},{"text":"This paper explores the use of the homotopy method for training a semi-supervised Hidden Markov Model (HMM) used for sequence labeling.","acronyms":[[103,106]],"long-forms":[[82,101]]},{"text":"Diacritization evaluation of our experiments is  reported in terms of word error rate (WER), and  diacritization error rate (DER)5. ","acronyms":[[125,128],[87,90]],"long-forms":[[98,123],[70,85]]},{"text":"1 Reinforcing Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received sublime interest in research on dialogue man-","acronyms":[[107,109]],"long-forms":[[83,105]]},{"text":"social media. During Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), pages 291?300.","acronyms":[[96,100]],"long-forms":[[68,94]]},{"text":" 0.7  0  20  40  60  80  100  120  140 informational density (ID)Anglers information (FIR)query-by-committee (SVE)random CoNLL-03","acronyms":[[60,62],[107,110],[118,126],[83,86]],"long-forms":[[39,58],[63,69]]},{"text":"   Figure 2. Tutor's Priming Ratio aggregated by chore set (TS = Task Set)     Figure 3.","acronyms":[[59,61]],"long-forms":[[64,72]]},{"text":"PLC = partition left context  (has been done)  PRC = partition right context  (yet to be done) ","acronyms":[[47,50],[0,3]],"long-forms":[[53,76],[6,28]]},{"text":"Statistical machine translation based on LDA.  Into Universal Communication Symposium (IUCS), 2010 4th International, pages 286?290.","acronyms":[[85,89],[41,44]],"long-forms":[[47,83]]},{"text":"3.1 Data We use the TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER,","acronyms":[[80,85],[48,53],[92,97]],"long-forms":[[64,78],[20,34]]},{"text":"Into the  past a few year, we have put forward methods for Kazakh morphological analysis, which includes  stem extracting, part of speech(POS) tagging, spelling verifying, etc. Recently, we are working on syntax ","acronyms":[[136,139]],"long-forms":[[121,134]]},{"text":" 1 Introduction  Hallmarks term formalisms (FTF) have proven extremely  useful for the declarative representation f linguistic ","acronyms":[[42,45]],"long-forms":[[17,40]]},{"text":" 2003. Social Communication Questionnaire (SCQ). ","acronyms":[[43,46]],"long-forms":[[7,41]]},{"text":"only from the corresponding fount language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al.,","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"ing in the non-realizable case. In Advances in Neural Information Processing Systems (NIPS), 2010.","acronyms":[[86,90]],"long-forms":[[47,84]]},{"text":"ing (NLP) petitions. In Information Retrieval (IR) and Question Answering (QA) it is routinely termed query\/question expansion (Moldovan and","acronyms":[[78,80],[5,8],[50,52]],"long-forms":[[58,76],[27,48]]},{"text":"RH = Random House dictionary; WSJ = Wall Street Journal; BN = Diffusion News; SWB = Switchboard. ","acronyms":[[78,81],[0,2],[30,33],[57,59]],"long-forms":[[84,95],[5,17],[36,55],[62,76]]},{"text":"71 tweets, called T-NER, is presented which employs Conditional Random Spheres (CRF) for named entity segmentation and labeled topic modelling for","acronyms":[[79,82],[18,23]],"long-forms":[[52,77]]},{"text":"Into Proceedings of the 23rd International Conference on Computational Linguistics (COLING?10), pages 617? ","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"  For all the reasons listed above, a dictionary of  Turkish Language Association (TLA) is used in  this study.","acronyms":[[83,86]],"long-forms":[[53,81]]},{"text":" To combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as ??","acronyms":[[98,101]],"long-forms":[[75,96]]},{"text":"tic attachment. Eight classes of syntactic onstituent were used: sentence (S), noun  phrase (NP), verb expressions (VP), prepositional phrase (PP), wh-noun phrase (WHNP),  adjective or adverbial phrases (AP), any other constituent (O), and both words in the ","acronyms":[[114,116],[141,143],[96,98],[162,166],[201,203]],"long-forms":[[101,112],[119,139],[68,76],[82,94],[146,160],[170,199],[206,227]]},{"text":"PROJECT GOALS  This project involves the integration of speech and natural-  language processing for spoken language systems (SLS). The ","acronyms":[[126,129]],"long-forms":[[101,124]]},{"text":"1 Introduction  In this paper we address the event extraction task  defined in Automatic Content Extraction (ACES)1  program.","acronyms":[[109,112]],"long-forms":[[79,107]]},{"text":"Research in molecular-biology field is discovering enormous amount of new facts, and thus there is an increasing need for information extraction (IE) technology to support database building and to find","acronyms":[[146,148]],"long-forms":[[122,144]]},{"text":" 1 Introduction Many troubles in natural language processing (NLP) involve optimizing some objective function over a set of","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"Our relating extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al, 2011) and applies maximum entropy (MaxEnt) in the MALLET","acronyms":[[119,125],[134,140]],"long-forms":[[102,117]]},{"text":"One part of the work is directed towards developing computational modes to facilitate the guidebooks construction of SweFN. We have so far focuses on three tasks: (1) semantic role labeling (SRL) (Johansson et al.,","acronyms":[[189,192],[115,120]],"long-forms":[[165,187]]},{"text":"is placed sixth out of seventeen systems according to Mean Absolute Mistaken (MAE) and third depending to Root Mean Squared Error (RMSE). The","acronyms":[[128,132],[75,78]],"long-forms":[[103,126],[54,73]]},{"text":"ance improvements in our system.  The OOV recall rates (RRoov) showed in  Table 4 demonstrate that the OOV recognition ","acronyms":[[56,61],[38,41],[103,106]],"long-forms":[[42,54]]},{"text":" 3.3 Prosodic Model Training We opting to use a support vector machine (SVM) classifier1 for the prosodic model based on formerly","acronyms":[[72,75]],"long-forms":[[48,70]]},{"text":"Note: in genera\\], the resultant segments,  such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP), ","acronyms":[[93,95],[73,77],[119,122],[141,143]],"long-forms":[[81,91],[52,72],[98,110],[129,140]]},{"text":"tions for this sentence.  Figure 5: The LF (left) and MRS (right) representations for the sentence ?","acronyms":[[40,42],[54,57]],"long-forms":[[44,48]]},{"text":"{zhongzhi, nght}@comp.nus.edu.sg Abstract Word sense disambiguation (WSD) systems based on overseeing learning","acronyms":[[69,72]],"long-forms":[[42,67]]},{"text":"It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (MP ) in which each one is supposed to be the translation of each","acronyms":[[100,102],[60,62]],"long-forms":[[79,98],[42,58]]},{"text":"Collaboratively build resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem specifically suitable","acronyms":[[75,77]],"long-forms":[[64,73]]},{"text":"1982, 1984; Clark 1992; Cremers 1996; Arts 2004). The present article will analyzed its results for the generation of referring expressions (GRE). In doing this, we dis-","acronyms":[[145,148]],"long-forms":[[108,143]]},{"text":" 1 Introduction Natural Language Processing (NLP) and Machine Learning (ML) are making a significant impact in","acronyms":[[45,48],[72,74]],"long-forms":[[16,43],[54,70]]},{"text":" 5.4 Nonshared Concept Activated with  No Identif ication Intention (NSNI). ","acronyms":[[70,74]],"long-forms":[[40,68]]},{"text":"line debates. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL, pages 35?43.","acronyms":[[88,92],[95,99]],"long-forms":[[52,86]]},{"text":"minimal human THEREBY annotation Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation dictionary, PSL = Pitt subjectivity vocabulary, SWN =","acronyms":[[88,91],[74,76],[115,117],[138,142],[185,188],[218,221]],"long-forms":[[94,113],[52,72],[120,136],[145,183],[191,216]]},{"text":"ity? have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2).","acronyms":[[92,95],[42,44]],"long-forms":[[62,90]]},{"text":"2010. The PASCAL Visual Object Class Challenge 2010 (VOC2010) Results.","acronyms":[[55,62]],"long-forms":[[17,53]]},{"text":"For the discourse  structure analyzed, we suggest a statistical models  with discourse segment boundaries (DSBs)  similar to the idea of gaps suggested for a ","acronyms":[[106,110]],"long-forms":[[76,104]]},{"text":"gls: the definition of the verb  They also defined two alternate search protocols: rika hierarchy exploration (RHE) with no  more than six links and superficial hierarchy explo-","acronyms":[[111,114]],"long-forms":[[83,109]]},{"text":" ADEPT tags documents in a uniform fashion, utilized  Standard Generalized Markup (SGML) according to  OIR standards.","acronyms":[[80,84],[1,6],[100,103]],"long-forms":[[51,78]]},{"text":"matic and paradigmatic associations on the results of the clustering step. We conduct two experiments on SemEval-2012 task 2 and Scholastic Assessment Test (OIN) resemblance quizzes to measure relational similarity to assessments our model.","acronyms":[[157,160]],"long-forms":[[129,155]]},{"text":"5http:\/\/wordnet.princeton.edu\/. 58 that, as in Informtion Retrieval (IR), multiple occurrences in the same document count as just one","acronyms":[[69,71]],"long-forms":[[47,67]]},{"text":" ? Reverse Gap (RG), if (i2 + 1) < i3 for OL and if (i6 + 1) < i1 for OR. (","acronyms":[[16,18]],"long-forms":[[3,14]]},{"text":"120 compared to approximate highest likelihood estimation (MLE). However, this method has not been","acronyms":[[59,62]],"long-forms":[[28,57]]},{"text":" 2. Character Error Rate (CER): Edit distance in terms of characters between the intent sentence","acronyms":[[26,29]],"long-forms":[[4,24]]},{"text":" 3.2 Query by Committee  Consultation by Committee (QBC) was introduced by  Seung, Opper, and Sompolinsky (1992).","acronyms":[[45,48]],"long-forms":[[25,43]]},{"text":"We present an open source, freely available Java implementation of Align, Disambiguate, and Walk (ADW), a state-of-the-art approach for measuring semantic similarity based on","acronyms":[[98,101]],"long-forms":[[67,96]]},{"text":"recursive noun phrases (NPs), primary sneeze groups (MVs), and a common annotation for adjectival and adverbial phrases (APs). Example (3) be-","acronyms":[[116,119],[24,27],[48,51]],"long-forms":[[97,114],[10,22],[30,46]]},{"text":"In order to take the transitivity of outscoping relations into account, we usage the transitive shut (TC) of DAGs. Let G+ =","acronyms":[[103,105],[110,114]],"long-forms":[[83,101]]},{"text":"  Figure 2.  Words Accurate (WC) scores from Teachers  and Machine scoring at the reader level (n = 87).","acronyms":[[28,30]],"long-forms":[[13,26]]},{"text":"ilarity between given texts. The first approach is bases on vector space models (VSMs) (Meadow, 1992).","acronyms":[[81,85]],"long-forms":[[60,79]]},{"text":" ? Fondazione Bruno Kessler (FBK-irst), Italy ?","acronyms":[[29,37]],"long-forms":[[3,27]]},{"text":" First of all, we now first formally introduce DUDES: Definition 1 (DUDES) A DUDES is a 7-tuple (m, l, t, U,A, S,C) consisting of","acronyms":[[68,73],[77,82]],"long-forms":[[54,66]]},{"text":"3  Chinese NER Using CRFs Model Integrating Multiple Features  Besides the text feature(TXT), simplified part-ofspeech (POS) feature, and small-vocabulary-","acronyms":[[88,91],[11,14],[21,25],[120,123]],"long-forms":[[75,87],[105,118]]},{"text":" 2.2 CoSeC CoSeC (Compares Semantics in Context) performs meaning comparison on the basis of an underspec-","acronyms":[[11,16]],"long-forms":[[18,37]]},{"text":"sic and Youth, 2011; Williams, 2010; Young et al., 2010) and Bayesian network (BN)-founded methods (Raux and Ma, 2011; Thomson and Young,","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). ","acronyms":[[116,118],[144,146],[32,36]],"long-forms":[[100,114],[124,142],[20,31]]},{"text":"pus Linguistics 2001 Conference, pages 274?280. Lancaster University (ENGLAND). ","acronyms":[[70,72]],"long-forms":[[58,68]]},{"text":"Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46 Kl w\/o using LM feature (Kl-LM) 84.24 84.06 Composite Kernel (Kc: MST+Kl) 92.98 92.67","acronyms":[[89,94],[15,17],[48,50],[130,133],[134,136]],"long-forms":[[64,79],[0,13]]},{"text":"that have to be defined in a theory-specific way.  Thus a document representation (DocRep) has two components, a DocAttr and a DocRepSeq.","acronyms":[[83,89],[113,119],[127,136]],"long-forms":[[58,81]]},{"text":"of a multi-class document categorization. We introduce PRBEP (precision recall break even point) as a measure which is popular in the area of infor-","acronyms":[[55,60]],"long-forms":[[62,95]]},{"text":" 4.4 SLU Features The SLU (Spoken Language Understanding) peculiarities are used to resolve implicit and explicit REs.","acronyms":[[22,25],[5,8],[109,112]],"long-forms":[[27,56]]},{"text":"158  I. CONSTRUCT THE PROPOSE ANCHORS for Un  (a) Create set of referring expressions (RE's). ","acronyms":[[88,92]],"long-forms":[[65,86]]},{"text":" First of all, we now frst formally introduce DUDES: Definition 1 (DUDES) A FELLAS is a 7-tuple (m, l, t, U,A, S,C) consisting of","acronyms":[[68,73],[77,82]],"long-forms":[[54,66]]},{"text":", VP ...... Others (OTHER): The remaining cases of comma receive the OTHER labels, indicating they do","acronyms":[[20,25]],"long-forms":[[12,18]]},{"text":"of the Annual Meeting of the ACL and the World Joint Conference on Natural Language Treating of the AFNLP (ACL-IJCNLP). ","acronyms":[[117,120],[29,32],[110,115]],"long-forms":[[120,121]]},{"text":"these baseline models for email interviews.  4.1 Latent Dirichlet Allocation (LDA) Our first model is the probabilistic LDA model","acronyms":[[78,81],[120,123]],"long-forms":[[49,76]]},{"text":" Performance has been measured with both the question followed by an enlarging (Q+E), as well as the question followed by the objectives and then","acronyms":[[80,83]],"long-forms":[[45,78]]},{"text":"ual resources on pairwise comparison task (Diff. = Difficulty lexicon, CF = Crowdflower) Features","acronyms":[[71,73]],"long-forms":[[76,87]]},{"text":" Into integrating this approach into a dialog system, we see that the dialog manager (DM) no longer determines surface strings to forwarded to the TTS system, as is often the case in underway dialog systems.","acronyms":[[84,86],[140,143]],"long-forms":[[68,82]]},{"text":"4.3 Experiments with the QA data In the first set of experiments we focus on the Question Answering (QA) domain (CLEF corpus). ","acronyms":[[101,103],[25,27],[113,117]],"long-forms":[[81,99]]},{"text":"Parsing. Onto Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp.","acronyms":[[101,104]],"long-forms":[[58,99]]},{"text":"contains the resulting for Eisner?s algorithm use no transformation (N-Proj), projectivized training data (Proj), and pseudo-projective parsing (P-Proj). The","acronyms":[[144,150],[68,74],[106,110]],"long-forms":[[124,142]]},{"text":"baseline (BAS) system which is close to the system described in (Hu et al 2009), and three variants of our novel divide and conquer (DAC) system. Fea-","acronyms":[[133,136],[10,13]],"long-forms":[[113,131],[0,8]]},{"text":"side were installed from 2003 to 2005.?  3.4 Sentence Reordering (RE) Some of the transforms operations results in","acronyms":[[66,68]],"long-forms":[[54,64]]},{"text":"ceedings of the 10th International Lectures on Text, Speech, and Dialogue (TSD-2007), Lecture Notes in Computer Science (LNCS), Springer-Verlag.","acronyms":[[123,127],[77,80]],"long-forms":[[88,121]]},{"text":"portance of the edge features and the resultant largemargin constraint, we also compare against a standard binary Support Vector Machine (SVM) which uses node features alone to predict whether each","acronyms":[[138,141]],"long-forms":[[114,136]]},{"text":"gle word maze); B-M (beginning of multi-word 72 maze); I-M (in multi-word maze); and E-M (termination of multi-word maze).","acronyms":[[55,58],[16,19],[85,88]],"long-forms":[[60,68],[21,39],[90,103]]},{"text":" Conf. on Language Resources and Evaluation (LREC). ","acronyms":[[45,49]],"long-forms":[[10,28]]},{"text":"Also,  the dependency framework is arguably closer to  semantics than the phrase structure grammar (PSG)  if the dependency relations are judiciously chosen.","acronyms":[[100,103]],"long-forms":[[74,98]]},{"text":"Since by default we return up to RT=100  search engine results to user, we will extract the  top RQ=RT\/(#newQuery+1) entries from results of  each new query and original query.","acronyms":[[97,99],[33,35]],"long-forms":[[100,115]]},{"text":"An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corpora and evaluation metrics.","acronyms":[[336,339],[396,399]],"long-forms":[[341,373],[401,429]]},{"text":"collocations in each sentence.  Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram linguistic","acronyms":[[64,67]],"long-forms":[[32,62]]},{"text":" 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"Japanese  (Jap). Germanic (Get), and Southern Romance (SRom). Only the ","acronyms":[[55,59],[11,14],[27,30]],"long-forms":[[37,53],[0,8],[17,25]]},{"text":" 3.1 Underspecified domains An underspecified realms (UD) represents a partially specified reference domain corresponding to the","acronyms":[[54,56]],"long-forms":[[31,52]]},{"text":"2012. Disclosure and Quality of Answer in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Vista Investigate (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd.","acronyms":[[162,167],[178,180]],"long-forms":[[121,160]]},{"text":"corpora for our experiments. The first is a new corpus of 70 articles from New York Times (NYT) LDC corpus, each describing one or more terrorist events","acronyms":[[91,94],[96,99]],"long-forms":[[75,89]]},{"text":"pulses are possibly found. It is performed with very  simple space management transition networks (SMTNs),  in which $EXP, a distinguished symbol on an arc, ","acronyms":[[100,105]],"long-forms":[[62,98]]},{"text":"HG-ALN 0.266 0.359 Table 1: The Pk and WindowDiff scores of uniform segmentation (UNI), TextTiling (TT), basic alignment (B-ALN), and align with hier-","acronyms":[[100,102],[82,85],[0,6],[125,130]],"long-forms":[[88,98],[60,67],[105,123]]},{"text":" 3 Background Slot filling (SF) is a query-oriented relation extraction (RE) task in the Knowledge Based Popu-","acronyms":[[28,30],[73,75]],"long-forms":[[14,26],[52,71]]},{"text":"Chaired), Khalid Choukri, Bente Maegaard, Yousef Mariani, Jan Odjik, Stelios Piperidis, Mike Rosner, & Daniel Tapias, 310?314, Valletta, Malta. European Language Resources Associations (ELRA).Ferra?ndez, Oscar, Michael Ellsworth, Rafael Mun?oz, & Collin F. Baker. 2010b.","acronyms":[[183,187]],"long-forms":[[142,181]]},{"text":"cessing. In Proceedings of the 2nd International Conference on Knowledge Capture(K-CAP). USA.","acronyms":[[81,86],[89,92]],"long-forms":[[63,80]]},{"text":" Figure 1. Reference answer representation revisions  Typical facets, as in (1a), are derived directly from a dependency parse, in this case preserves its addictions sorts tags, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(twig, to) and PMod(to, nail) in the case of (1c).","acronyms":[[178,182],[283,287],[303,307]],"long-forms":[[184,197]]},{"text":"Deep-syntactic structures (DSyntSs);  ? Surface syntactic structures (SSyntSs);  61 ","acronyms":[[70,77]],"long-forms":[[40,68]]},{"text":"rate expression (PERFECT). Second, we let the parser introduce the EEs itself (INSERT). ","acronyms":[[74,80],[12,19]],"long-forms":[[48,72]]},{"text":"that first contains TOOV and given by the search  engine. Average_Rank (A_Rank) is the average position of TOOV in the returned snippets.","acronyms":[[72,78],[20,24],[107,111]],"long-forms":[[58,70]]},{"text":"Other formats have been suggested for dictionary sharing,  notably those developed under the Text Encoding Initiative  using SGML (Standard Generalized Markup Language). We ","acronyms":[[125,129]],"long-forms":[[131,167]]},{"text":"As a result, a texts to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text grades would be a good indicator of text to text similarity. 4 Sagan for MT evaluated Sagan for MT evaluated is based on a basic devel-opment to approaches the Semantic Textual Similari-ty task(STS). The piloting task STS was recently defines in Semeval 2012 (Aguirre et al, 2012) and has as main purpose measuring the degree of semantic equivalence between two text fragments.","acronyms":[[352,355],[232,234],[256,258],[373,376]],"long-forms":[[318,346]]},{"text":"news headlines (headlines); mapping of lexical funds from Ontonotes to Wordnet (OnWN) and from FrameNet to WordNet (FNWN); and appraise of machine translation (SMT).","acronyms":[[120,124]],"long-forms":[[99,118]]},{"text":"Lowe HJ, Barnett GO. ( 1994) Understanding and using the medical themes headings (MeSH) vocabulary to perform literature searches.","acronyms":[[83,87],[5,7]],"long-forms":[[57,81]]},{"text":"Learning\". In Proceedings of the 3 rd ACL  Workshop on Very Wide Corpora (WVLC95). ","acronyms":[[75,81]],"long-forms":[[43,73]]},{"text":"SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLORATION = coloursensorreadning78 ?","acronyms":[[56,62],[0,4],[27,32]],"long-forms":[[65,87],[7,26],[35,55]]},{"text":"a topic model are compared. Latent Dirichlet Allocation (LDA) (Blei et al 2003) is a widely used type of topic model in which documents can be","acronyms":[[57,60]],"long-forms":[[28,55]]},{"text":"Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the compression rates (CompR) for the three systems and evaluates the quality of their output using grammatical relations F1.","acronyms":[[100,105]],"long-forms":[[81,98]]},{"text":" 1 Introduction Information Extraction (IE) refers to the problem of extracting structure information from unstructured","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":" An surrogate paradigm is to view error correction as a statistical machine translation (SMT) problem from ?","acronyms":[[91,94]],"long-forms":[[58,89]]},{"text":"6. Demonstrative pronoun labels are collapsed to DEM PRON (someone and number information is easily recovered)","acronyms":[[53,57],[49,52]],"long-forms":[[59,65]]},{"text":" ? MEA+LexPageRank (MEALR) : This method applies the proposed mixture-event-aspect model to","acronyms":[[20,25]],"long-forms":[[3,18]]},{"text":"10-fold open test 62.80-58.54 59.15 66.46-65.55 65.55 65.55-64.63 Table 5: Comparison of Optimizers (Opinions in KNB Corpus) Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing","acronyms":[[141,145],[113,116],[164,166]],"long-forms":[[125,139]]},{"text":"MT is one of the oldest and most important areas of Natural Language Processing (NLP) \/ Computational Linguistics (CL).2 From its beginnings we have witnessed some changes in the","acronyms":[[115,117],[0,2],[81,84]],"long-forms":[[88,113],[52,79]]},{"text":"Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent","acronyms":[[109,112],[65,68]],"long-forms":[[88,107]]},{"text":"annotations of sentiment values for individual syntactic phrases in a binarized tree, and an approach based on recursive neural tensor networks (RNTN) which yields significant improvements over the ear-","acronyms":[[145,149]],"long-forms":[[111,143]]},{"text":" Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result.","acronyms":[[66,69]],"long-forms":[[34,64]]},{"text":"well. The formal framework for analysis will be the Discourse Representation Theory (DRT). ","acronyms":[[85,88]],"long-forms":[[52,83]]},{"text":"Here the construction of the Chinese VP involves joining a prepositional phrase (PP) and a smaller verbal phrase (VP-A), with the preposition at the beginning as a PP marker.","acronyms":[[114,118],[37,39],[81,83],[164,166]],"long-forms":[[99,112],[59,79]]},{"text":"5) is blocked by (7), because of the  passinginto the subject'a story about es'; i.e., the specifier of  INFL (in the transformational ccount) or of VP (in theories  like GPSG, etc.).","acronyms":[[105,109],[149,151],[171,175]],"long-forms":[[111,134]]},{"text":"TEXT), DECL (Declarative), SWEETHEART (Honorific), IMPER (Indispensable), NOM (Nominative), ORTH (ORTHOGRAPHY), PST (Past), SYN (SYNTAX), SEM (SEMANTICS), RELS (RELATIONS), and POS (part of speech).","acronyms":[[102,105],[7,11],[27,30],[44,49],[64,67],[82,86],[114,117],[128,131],[145,149],[167,170]],"long-forms":[[107,111],[13,24],[32,41],[51,61],[69,79],[88,99],[119,125],[133,142],[151,160],[172,186]]},{"text":"event coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard information extraction (IE) pipeline. ","acronyms":[[136,138]],"long-forms":[[112,134]]},{"text":"various subsets of the documents in the English Gigaword corpus, chiefly drawn from Novo Yorke Times (NYT) and Agence France Presse (AFP).1 2.1 Are Discounts Constant?","acronyms":[[131,134],[100,103]],"long-forms":[[109,129],[84,98]]},{"text":"were computed for each screenplays: bilingual evaluation under study (BLEU), position freelance error rate (PER) and word error rate (WER). ","acronyms":[[133,136],[67,71],[107,110]],"long-forms":[[116,131],[33,59],[74,105]]},{"text":" 250 Support Vector Machines (SVMs) construction a hyperplane in a multi-dimensional space which yields a well separation between positive and negative training examples, represented as data points.","acronyms":[[30,34]],"long-forms":[[5,28]]},{"text":"find in each row, as good as the level of granularity of analysis in each row.3 2KEY: ABS=abstract, COM=completive, CL=classifier, DEM=demonstrative, E=ergative, EV=evidential, S=sole,","acronyms":[[87,90],[101,104],[117,119]],"long-forms":[[91,99],[105,115],[120,130]]},{"text":"These interfaces stand to play a critical role in the  ongoing migration of interaction fi'oln the desktop  to wireless mobile computing devices (PI)As, next-  generation phones) that offer limits screen genuine es- ","acronyms":[[148,150]],"long-forms":[[120,146]]},{"text":"NER model was shown in Table 4. We use the Peking University (PKU) named entity corpus to train the models.","acronyms":[[62,65],[0,3]],"long-forms":[[43,60]]},{"text":"583  Proceedings of the Analyzing Speaks in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7, Nuevo York City, New York, June 2006.","acronyms":[[68,72],[86,95]],"long-forms":[[24,66]]},{"text":"3. Maximization of 1)arameters, A of at:tire fea-  tures 1)y I IS(hnproved Iterative Sealing) algo-  rithm.","acronyms":[[63,65]],"long-forms":[[75,92]]},{"text":"prove sentiment classification. In Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[100,103]],"long-forms":[[57,98]]},{"text":"User-generated content (UGC), and specially the microblog genre, has become an interesting resource for Natural Language Processing (NLP) tools and applications.","acronyms":[[133,136],[24,27]],"long-forms":[[104,131],[0,22]]},{"text":"systems that learned new representations for opendomain NLP using latent-variable language models like Hidden Markov Models (HMMs). In POS-","acronyms":[[123,127],[54,57],[133,136]],"long-forms":[[101,121]]},{"text":"The Cell or Tissue Type category was split into two fine grained classes, CELL and CLNE (cell line). ","acronyms":[[83,87],[74,78]],"long-forms":[[89,98]]},{"text":"to this tree as the Hidden Factors Tree (HFT). We used Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to building the tree.","acronyms":[[90,93],[41,44]],"long-forms":[[54,88],[20,39]]},{"text":"follows:  1. If the topic is an individual constant (IC), establish  restricts (utilizing the Restricts link), if any, on the ","acronyms":[[53,55]],"long-forms":[[32,51]]},{"text":"The effectiveness of customer care in the email channel is measured using two competing metrics: Average Handling Time (AHT) and Customer Experience Evaluation (CEE).","acronyms":[[120,123],[161,164]],"long-forms":[[97,118],[129,159]]},{"text":"otherwise be very limited annotated data. The resource, the Online Database of INterlinear text (ODIN), makes this data available and provides additional annotation","acronyms":[[97,101]],"long-forms":[[60,90]]},{"text":"4.3 Nested Expressions Not nested expressions will be marked. Even in example where LOCATION (ENAMEX) expressions occur within  TIMEX and NUMEX expressions, they are not to be tagged.","acronyms":[[91,97],[125,130],[135,140]],"long-forms":[[61,89]]},{"text":"the current work, we used a subset of that corpus consisting of examples whose question types were PRC (procedure), RSN (reason) or ATR (atrans). ","acronyms":[[99,102],[116,119],[132,135]],"long-forms":[[104,113],[121,127],[137,143]]},{"text":"UMLS Metathesaurus version 2003AC, the string mammectomy has been assigned the concept-unique identifier C0024881 (CUI), the lemma-unique identifier L0024669 (LUI), and the string-unique identifier S0059711 (SUI).","acronyms":[[115,118],[0,4],[27,33],[159,162],[208,211]],"long-forms":[[105,113],[125,157],[173,206]]},{"text":"In this paper we propose a system which utilize  hybrid methods that combine both rule-based  and machine learning (ML)-based approaches  to solving GENIA Event Extraction of BioNLP ","acronyms":[[113,115],[144,149],[170,176]],"long-forms":[[95,111]]},{"text":"Our model can now be represented like this:  241  Database (DB)  Truths about hotels ","acronyms":[[60,62]],"long-forms":[[50,58]]},{"text":"2 Related Work There have been several studies on supervising dialogue act (DA) modeling. To the best of","acronyms":[[75,77]],"long-forms":[[61,73]]},{"text":"which fixes its results after a given time ? and report the corresponding word blunders rate (WER). This","acronyms":[[91,94]],"long-forms":[[74,89]]},{"text":"therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learns (HRL). ","acronyms":[[129,132]],"long-forms":[[92,127]]},{"text":"3. Generation of Crisp Descriptions Admittedly the most essentials task in the generation of referring expressions (GRE), content determination (CD) requires finding a set of properties that jointly detects the in-","acronyms":[[115,118],[144,146]],"long-forms":[[78,113],[121,142]]},{"text":"Abstract  This article emphasis on the development of  Natural Language Processing (NLP) tools for  Computer Assisted Language Learning ","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":" VP  Figure 3: A tree with some of its partial trees (PTs). ","acronyms":[[54,57]],"long-forms":[[39,52]]},{"text":"contributing to the irregularity of hangul orthography is the differences in spelling between South Korea (S.K.) and North Korea (N.K.). The","acronyms":[[107,111],[130,134]],"long-forms":[[94,105],[117,128]]},{"text":" As with most modern simulators, DISs are controlled via  graphical user interfaces (GUIs). However, the simulation ","acronyms":[[85,89],[33,37]],"long-forms":[[58,83]]},{"text":"categories containing NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition, CONJ = collaborate, DET = determiner, PRON = 1http:\/\/www.wiktionary.org\/","acronyms":[[94,98],[114,117],[31,34],[48,51],[62,65],[76,79],[132,136]],"long-forms":[[101,112],[120,130],[37,46],[54,60],[68,74],[82,92]]},{"text":"In particular, the work of (Pietra et al, 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler","acronyms":[[104,107]],"long-forms":[[76,102]]},{"text":"velopment and 23 for tests. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al, 2005); clauses 001-270 and 440-","acronyms":[[90,93]],"long-forms":[[72,88]]},{"text":"ing decisions in multi-party discussions.  Several types of dialogue act (DA) are distinguished on the basis of their roles in","acronyms":[[74,76]],"long-forms":[[60,72]]},{"text":"2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012).","acronyms":[[52,54]],"long-forms":[[36,50]]},{"text":"ious learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL forms.","acronyms":[[76,79],[117,120]],"long-forms":[[51,74]]},{"text":"weiwei@cs.columbia.edu Abstract In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence,","acronyms":[[64,67]],"long-forms":[[35,62]]},{"text":"In Proceedings of the 19th International Conference on Computational Linguistics (COLING?02), pages 218? ","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"However, a studied regarding maximal recall exhibition that we do not remove too many true positives (TPs) (more details in Section 4.1).","acronyms":[[95,98]],"long-forms":[[79,93]]},{"text":"The lexical idiosyncrasies used are word bigrams. The Portion of Speech (PoS) of the target word and its neighbors make up the the syntactic","acronyms":[[64,67]],"long-forms":[[48,62]]},{"text":"strat1 40.1 24.4 15.0 strat2 38.2 22.5 14.5 Table 4: Word Error Rate (WER), Concept Error Rate (CER) and Interpretation Error Rate (IER) ac-","acronyms":[[70,73],[96,99],[132,135]],"long-forms":[[53,68],[76,94],[105,130]]},{"text":"This data may be presented in multiple form, e.g. as  dictionaries, transition networks for lexical analysis,  growth transition networks (ATN) for syntactic analysis,  semantic networks,  re la t ions ,  end so on.","acronyms":[[142,145]],"long-forms":[[111,140]]},{"text":"patterns. The Ngram features were generated utilizes the Ngram Stats Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction pat-","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":"{csjxu, csluqin}@comp.polyu.edu.hk Abstract The Semantic Textual Similarity (STS) task aims to exam the degree of semantic","acronyms":[[77,80]],"long-forms":[[48,75]]},{"text":" 7.1 Support vector machines  Support vector machines (SVMs) were introduced by (Vapnik, 1995) as an instantiation ","acronyms":[[55,59]],"long-forms":[[30,53]]},{"text":"LR = Lagrangian relaxation; DP = thorough dynamic programming; ILP = integer linear programming; LP = linear programs (LP does not recover an exact solution).","acronyms":[[65,68],[0,2],[28,30],[99,101],[124,126]],"long-forms":[[71,97],[5,26],[44,63],[104,122]]},{"text":"the toolkits. Sizes are yielded for the resulting transducers (VM = Verbmobil). ","acronyms":[[61,63]],"long-forms":[[66,75]]},{"text":"described in Section 3. We then trained linear SVMs (Support Vector Machine) using the LIBLINEAR software (Fan et al, 2008), using L1 loss","acronyms":[[47,51],[87,96]],"long-forms":[[53,75]]},{"text":" 6 Conclusion In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a network of semantically coherent classes of templates and derived semantic relations including entailment","acronyms":[[90,94]],"long-forms":[[54,88]]},{"text":"2 Conditional Random Sphere 2.1 The model Conditional Random Scopes(CRFs), a statistical sequence modeling framework, was first intro-","acronyms":[[68,72]],"long-forms":[[42,66]]},{"text":"Abstract One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assessed user","acronyms":[[68,70]],"long-forms":[[55,66]]},{"text":"MSEAS (MS,MSEA, VP,i, OUT)  (1) Launch with VP = VAR ({X1, \" \" ,X.}), MSEA = f~,  i=1, and OUT = O. When the calculation is finishing, MS is bound to the set of active ","acronyms":[[90,93],[0,5],[7,9],[10,14],[16,18],[22,25],[43,45],[48,51],[69,73],[134,136]],"long-forms":[[96,119]]},{"text":"We experiment with multiple modes to choose a snippet: the first 50 words of the summary (START), the last 50 words (END) and 50 words starting at a randomly electing sentence","acronyms":[[89,94],[116,119]],"long-forms":[[80,87]]},{"text":"course Connectives. Proceedings of the Fourth Workshop on Treebanks and Linguistic Doctrine (TLT). ","acronyms":[[93,96]],"long-forms":[[58,91]]},{"text":"It has been shown in (Ando and Zhang, 2005a) that the optimization problem (3) has a simply solution utilize singular value decomposition (SVD) when we opt square regularization: r(f","acronyms":[[137,140]],"long-forms":[[107,135]]},{"text":"To solve the ILP models we used lp solve, a highly efficient GNU-licence Mixed Integer Programming (MIP) solver11, that implements the Branch-and-Bound algorithm.","acronyms":[[100,103],[13,16],[61,72],[32,34]],"long-forms":[[73,98]]},{"text":"Abstract Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human?s verbal ability includ-","acronyms":[[89,91]],"long-forms":[[66,87]]},{"text":"James Clark. 1999. XSL transformed (XSLT). W3C Recommendation, 16 November.","acronyms":[[40,44]],"long-forms":[[19,38]]},{"text":"For example, both the terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective and shading techniques whereas collage is classified under image-making processes and","acronyms":[[147,151]],"long-forms":[[117,145]]},{"text":"Secondly, there is the group of speech particles which are not part of the core sentence construction,  yet pragmatically cannot stand on their own. These 'sentence external' (SE) elements can be subclassified into two classes.","acronyms":[[176,178]],"long-forms":[[156,173]]},{"text":"accessing semantic information represented in  input specifications, written in the form of the  Sentence Plan Language (SPL) (Kasper, 1989;  Bateman, 1997a), and in the knowledge base of ","acronyms":[[121,124]],"long-forms":[[97,119]]},{"text":"by each of these strategies.  The four Word Sense (WS) disambiguation  strategies resolves sense ambiguity errors.","acronyms":[[51,53]],"long-forms":[[39,49]]},{"text":" [and, thus, so]  Contrastive Connectives (CC)  men den ?","acronyms":[[48,50]],"long-forms":[[23,46]]},{"text":"scikit-learn python library 3 : Naive Bayes (NB), Tightest Neighbor (NN), Decision Tree (DT), Ran-","acronyms":[[45,47],[68,70],[88,90]],"long-forms":[[32,43],[50,66],[73,86]]},{"text":"on the guidance of domain experts, who can devise pedagogically valuable reading lists that order docAutomatic Speech Recognition (ASR) with HMMs Noisy Channel Model Viterbi Decoding for ASR","acronyms":[[131,134],[187,190]],"long-forms":[[101,129]]},{"text":"ing at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a syn-","acronyms":[[100,103]],"long-forms":[[69,98]]},{"text":"the titles of the entity articles titles(e) to represent the entities in the query and two ranking functions, Recursive TFISF (R-TFISF) and LC, 3","acronyms":[[127,134],[140,142]],"long-forms":[[110,125]]},{"text":"ory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g.,","acronyms":[[114,117],[34,37]],"long-forms":[[97,112]]},{"text":"to inspect and easily modified discourse-planning specifications for expedited  iterative refinement. The Explanation Design Package (EDP) formalism  is a convenient, schema-like (McKeown 1985; Paris 1988) programming ","acronyms":[[128,131]],"long-forms":[[100,126]]},{"text":"data. In International Conference on Machine Learning (ICML). ","acronyms":[[55,59]],"long-forms":[[9,53]]},{"text":"The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section","acronyms":[[190,193]],"long-forms":[[163,188]]},{"text":"Table 6 shows the number of true positive (TP), true negative (TN), false positive (FP) and false negative (FN) of models for the stocks. ","acronyms":[[108,110],[43,45],[63,65],[84,86]],"long-forms":[[92,106],[33,41],[48,61],[68,82]]},{"text":"Extraction algorithms: ReV = REVERB; Comp = Compression; Data sets: NS = NewsSpike URLs; All = news 2008-2014.","acronyms":[[83,87],[23,26],[37,41]],"long-forms":[[73,82],[29,35],[44,55]]},{"text":" The system includes four main stages: topic classification, named entity recognition (NER), disease\/location detect, and visualization.","acronyms":[[87,90]],"long-forms":[[61,85]]},{"text":"the anticipated output.  officially a weighted finite state automation (FSA), where V is the set of nodes andE is the set of edges.","acronyms":[[67,70]],"long-forms":[[42,65],[120,124]]},{"text":" Result (in percentages) are for per-logical-predication (PR) and per-whole-graph (GRPH) tagging accurcies. ","acronyms":[[84,88],[59,61]],"long-forms":[[77,82],[46,57]]},{"text":"constructions. The notion of build is similar to the one in Architectural Grammar (CxG)4, as in (Goldberg, 1995), where:","acronyms":[[89,92]],"long-forms":[[67,87]]},{"text":"accessing semantic information represented in  input specifications, written in the form of the  Sentence Scheme Language (SPL) (Caspar, 1989;  Bateman, 1997a), and in the knowledge base of ","acronyms":[[121,124]],"long-forms":[[97,119]]},{"text":"mantic relations between referents. This task has a long tradition in natural language processing (NLP) since the early days of artificial intelligence (Web-","acronyms":[[99,102]],"long-forms":[[70,97]]},{"text":"ditto Shi-fen three-hours ten-minute, i.e. 'three-  ten'), post-position phrases (GPs), preposition  phrases (PPs), br adverbs (ADVs). They all share ","acronyms":[[128,132],[82,85],[110,113]],"long-forms":[[119,126],[88,108],[59,80]]},{"text":"These are the second-order prepositional complement (PC) and directional complement (LD) relations, and the first-order direct object (OBJ1) and theme (SU) relations. Finally, the setting SU+OBJ1 engages expression obtained from subject","acronyms":[[154,156],[53,55],[85,87],[135,138]],"long-forms":[[145,152],[27,51],[127,133]]},{"text":" 5Note that this is a recursive lexical rule, which  Adjunct Extraposition Lexical Regulation (AELR)  \"r,oc \\[\\] ICATIHEAD nou,~ Vverb\\] ","acronyms":[[89,93]],"long-forms":[[53,87]]},{"text":"few open source programs. Since we are interested in a fully supervised WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selected in our","acronyms":[[82,85],[72,75]],"long-forms":[[87,95]]},{"text":"We gratefully acknowledge the support of Turkish  Scientific and  Technological Research Council of  Turkey  (TUBITAK)  and  METU  Scientific  Research  Fund  (no.","acronyms":[[110,117],[125,129]],"long-forms":[]},{"text":"ptishes a rather inconsequential altered with respect o a  previously non-existent connecting or with respect to a link  No impairment (NI) Q Confusion (C)  Q Mislearning (ML) Q Deficient Learning (IL) ","acronyms":[[129,131],[165,167],[194,196]],"long-forms":[[114,127],[152,163],[171,192],[135,144]]},{"text":" ? MEA+LexPageRank (MEALR) : This method applying the proposed mixture-event-aspect model to","acronyms":[[20,25]],"long-forms":[[3,18]]},{"text":"51 Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model of Anatomy, other ontologies identified in the text.","acronyms":[[61,64],[88,91]],"long-forms":[[67,86],[94,123]]},{"text":"1425  Proceedings of the 3rd Workshop on Hybrid Approaches to Translating (HyTra) @ EACL 2014, pages 75?81, Reykjavik, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"5 Finding and Furthermore Worked   In this paper we have proposed a reestimation algorithm and a best-first parsing algorithm  for probabilistic dependency grammars(PDG). The reestimation algorithm is a variation of ","acronyms":[[163,166]],"long-forms":[[129,161]]},{"text":"3. Maximization of 1)arameters, A of at:tyre fea-  tures 1)y I IS(hnproved Iterative Latch) algo-  rithm.","acronyms":[[63,65]],"long-forms":[[75,92]]},{"text":"tence All the indexes dove ., in which All should be tagged as a predeterminer (PDT).10 Biggest occurrences of All, albeit, are as a determiner (DT, 106\/135 vs","acronyms":[[80,83],[143,145]],"long-forms":[[65,78],[131,141]]},{"text":"We suggest a method to improves the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems.","acronyms":[[131,134]],"long-forms":[[98,129]]},{"text":"of the label candidates. For the supervised method, we use a support vector regression (SVR) model (Joachims, 2006) over all of the features.","acronyms":[[88,91]],"long-forms":[[61,86]]},{"text":"Soft cardinality has been demonstrated to be a very strong text-overlapping baseline for the task of measure semantic textual similarity (STS), obtaining 3rd place in SemEval-2012.","acronyms":[[133,136],[162,174]],"long-forms":[[104,131]]},{"text":"unitary operator. Therefore, the primary problem  of building a quantum classifier (QC) is to find  the correct or optimal unitary operator.","acronyms":[[84,86]],"long-forms":[[64,82]]},{"text":"target translation: the gunman was killed by police .  The Penn English Treebank (PTB) (Marcus et al, 1993) is our source of syntactic information, largely","acronyms":[[82,85]],"long-forms":[[59,80]]},{"text":"date translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise lin-","acronyms":[[63,66]],"long-forms":[[41,61]]},{"text":"alignment. Across Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 215?222.","acronyms":[[103,106]],"long-forms":[[60,101]]},{"text":"can formulating natural language-like habits as exploratory queries for relations against a text corpus.  We draw inspiration from the information seeking paradigm of Exploratory Search (ES) (Marchionini, 2006; White and Roth, 2009), where users commencing with a loosely defined information need and - with a mix","acronyms":[[187,189]],"long-forms":[[167,185]]},{"text":" 2.2 CoSeC CoSeC (Comparing Semantics in Context) performs meaning comparison on the basis of an underspec-","acronyms":[[11,16]],"long-forms":[[18,37]]},{"text":"We trained the UKP machine learning classifier initially crafted for the Semantic Textual Similarity (STS) task at SemEval-2012 (B?rs et al 2012) on the averaged binary and senary judge-","acronyms":[[105,108],[15,18],[118,130]],"long-forms":[[76,103]]},{"text":"  Parallel corpora Size of English texts (in  million words (MB))  Size of Chinese texts (in ","acronyms":[[61,63]],"long-forms":[[46,59]]},{"text":"for everything language. All results in percent. LAS = labeled attachment score, UAS = unlabeled attachment score.","acronyms":[[44,47],[76,79]],"long-forms":[[50,74],[82,108]]},{"text":"At the semantic level, we have included three different families which operate using named entities (NE), semantic roles (SR), and discourse representations (DR).","acronyms":[[101,103],[122,124],[158,160]],"long-forms":[[85,99],[106,120],[131,156]]},{"text":"Figure 1: Overall architecture of Sentiment Classifier when a word is used with tremendously auspicious (HP), positive (P), highly negative (HN), counterproductive (N) or objective (O) meaning based on a sentiment sense inven-","acronyms":[[133,135],[97,99]],"long-forms":[[116,131],[80,95],[102,110],[138,146],[154,163]]},{"text":"Nonetheless, one interesting result came from extending the feature space with themes derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).","acronyms":[[124,127]],"long-forms":[[95,122]]},{"text":" The corpora used in our experiences are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the Frenchmen Treebank (FTB) (Abeill?, Cl?ment, and Kinyon","acronyms":[[118,121],[67,70]],"long-forms":[[101,116],[45,65]]},{"text":"representing natural language in a traversable graph, composed of propositions and their semantic interrelations ? A Propositional Knowledge Diagram (PKG). The ensuing structure provides a representation","acronyms":[[148,151]],"long-forms":[[117,146]]},{"text":"cific characteristics in form, meaning, function, and distribution. Each entry includes a free text definition, schematic structural description, definitions of construction elements (CEs) and annotated example sentences.","acronyms":[[184,187]],"long-forms":[[161,182]]},{"text":"This I will  claim to be in contrast with the ability of temporal  subordinate clauses and noun phrases (NPs) to direct the  listener to any position in the evolving structure.)","acronyms":[[105,108]],"long-forms":[[91,103]]},{"text":"BLEU-4 (Papineni et al, 2002) used in the two  experimenting, we design another appraise metrics Reordering Accuracy (RAcc) for forced decoding evaluation.","acronyms":[[118,122],[0,6]],"long-forms":[[97,116]]},{"text":"Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language paragon order (ORDER) and histogram pruning sizes (BEAM) for decipherment of letter","acronyms":[[118,123],[29,32],[56,59],[75,77],[153,157]],"long-forms":[[111,116],[9,26],[35,53],[66,72]]},{"text":"CoTrain v. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain v. LEX(EN) 0.0018 0.0276 0.00329 CoTrain v. SVM(CN) 1.26E-13 6.45E-10 2.7E-14 CoTrain v. SVM(EN) 2.15E-18 1.1E-17 3.46E-13","acronyms":[[105,107]],"long-forms":[[89,96]]},{"text":" 3.2 ,6  Addi t ion  of T rans la t ion  Rules  FinMly, translation rules (TRis) are added to the set  of GLTPC, s. TRis are descriptions in which concepts ","acronyms":[[75,79],[106,111],[116,120]],"long-forms":[[56,73]]},{"text":"By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HM)based tagging model.","acronyms":[[92,95]],"long-forms":[[71,90]]},{"text":"and the speech. The SU detection task is evaluated on both the reference human transcriptions (REF) and speech recognition outputs (STT).","acronyms":[[95,98],[20,22],[132,135]],"long-forms":[[63,72],[104,130]]},{"text":" (Markov) that significantly differed from the output of the noisy channel model (NoisyC), which confirms our finding that Markovized models can pro-","acronyms":[[82,88]],"long-forms":[[61,74]]},{"text":"4 Discussion We provide a brief introduction to the framework of Linear Categorial Grammar (LinCG). One of","acronyms":[[92,97]],"long-forms":[[65,90]]},{"text":"subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links between phrases, and the entire taxonomy is kind of fragmented into a many-rooted DAG (directed acyclic graph). More-","acronyms":[[168,171]],"long-forms":[[173,195]]},{"text":" Conf. on Language Resources and Evaluation (LREC), pages 147?152, Las Palmas, Spain, May.","acronyms":[[45,49]],"long-forms":[[10,28]]},{"text":"noun modification? which generally is shown in the form of a Noun phrase (NP) [A DE B]. A in-","acronyms":[[74,76]],"long-forms":[[61,72]]},{"text":"stood statistical models?statistical dependency parsers, probabilistic context-free grammars (PCFGs), and word translation models (TMs)?can be effectively combined into a unified framework that jointly searches for the best","acronyms":[[131,134],[94,99]],"long-forms":[[111,129],[57,92]]},{"text":"We focussed on predicting an absolute indication of quality rather than only classification the sentences by quality; this is why we use the Mean Absolute Error (MAE) as the primary evaluation measurement rather than Spearman?s correlation or DeltaAvg (Callison-Burch et al.,","acronyms":[[152,155],[226,234]],"long-forms":[[131,150]]},{"text":"that of Visweswariah et al(2011) ? hereby called Travelling Salesman Problem (TSP) modeling ? with","acronyms":[[78,81]],"long-forms":[[49,76]]},{"text":"conditional models are computed directly from data.  In this studied, we use a Maximum Entropy (MaxEnt) classifier to combine the decision trait fea-","acronyms":[[94,100]],"long-forms":[[77,92]]},{"text":" 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), bases on the Rhetorical Struc-","acronyms":[[44,50]],"long-forms":[[20,42]]},{"text":"pendency and constituency parsing.  2.4.1 On Dependency Parsing (DP) ?","acronyms":[[65,67]],"long-forms":[[45,63]]},{"text":"since it appears in the corpus with the six differ-  ent tags: CD (cardinal), DT (determiner), JJ (ad-  jective), NN (noun). NNP (proper noun) and VBP ","acronyms":[[114,116],[63,65],[78,80],[95,97],[125,128],[147,150]],"long-forms":[[118,122],[67,75],[82,92],[104,111],[130,141]]},{"text":"The symptomatic way to address these situations is to jointly model these relationships, e.g., using Markov logic networks (MLN) (Poon and Vanderwende, 2010).","acronyms":[[116,119]],"long-forms":[[93,114]]},{"text":"= Task Defined RAW SCORI~  ((! OMM(~OST x number of messages)  - (INFCOST x nnmber  o f  in fe rences)  ","acronyms":[[31,34]],"long-forms":[[36,60]]},{"text":"wqK ,   and the word sequence of the web page,                    A=WA=wA1, wA2, ?, wAL,  ","acronyms":[[68,70]],"long-forms":[[71,74]]},{"text":"However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting","acronyms":[[98,101]],"long-forms":[[69,96]]},{"text":" 3 BUDS dialogue manager The Bayesian Refreshed of Dialogue State (BUDS) dialogue manager is a POMDP-based dialogue","acronyms":[[64,68],[3,7],[92,97]],"long-forms":[[29,62]]},{"text":" Acknowledgments This work has been funded in party by a research grant from Scientifically Foundation Ireland (SFI) under Grant Number SFI\/12\/RC\/2289 (INSIGHT) and by the EU FP7 programme in the context of the project LIDER","acronyms":[[104,107],[209,214],[164,166],[167,170]],"long-forms":[[76,102]]},{"text":"with their scope and corresponding negated events is an important task that could benefit other natural parlance processing (NLP) task such as extraction of factual information","acronyms":[[125,128]],"long-forms":[[96,123]]},{"text":"ran, and the historical ancestor of the other varieties. Modern Standard Arabic (MSA) is the modern  version of CA and is, broadly speaking, the univer-","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"tive of the gold standard data.  Finally, the aligned error rate (AER) is lower (and hence better) for English?French than Romania?","acronyms":[[68,71]],"long-forms":[[46,66]]},{"text":"con using label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= root mean squared error","acronyms":[[114,117],[45,48],[57,60],[143,147]],"long-forms":[[118,131],[62,76],[107,112],[149,172]]},{"text":"KEA: Practical automatic keyphrase  extraction. Trials of Digital Libraries 99 (DL'99), pp. ","acronyms":[[85,90],[0,3],[93,95]],"long-forms":[[63,83]]},{"text":" FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on","acronyms":[[100,103],[1,7]],"long-forms":[[74,93]]},{"text":"istic conversational systems. In Proceedings of Intelligent User Interfaces 2001 (IUI-01), pages 1?8, Santa Fe, NM, January.","acronyms":[[82,88],[112,114],[108,110]],"long-forms":[[48,80]]},{"text":" We using LibSVM (Chang and Lin, 2011), an implementation of Aided Vector Machines (SVM) (Cortes and Vapnik, 1995), as the underlying tech-","acronyms":[[84,87],[8,14]],"long-forms":[[59,82]]},{"text":"2.1 Conditional Random Realms  Conditional random field (CRF) was an extension  of both Maximum Entropy Modeling (MEMs) and  Hidden Markov Models (HMMs) that was firstly ","acronyms":[[111,115]],"long-forms":[[88,109]]},{"text":"Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The per-","acronyms":[[111,114]],"long-forms":[[84,109]]},{"text":"the MDL methods.  ConVote (CONVOTE) Ours second dataset is taken from segments of speech from United States","acronyms":[[27,34],[4,7]],"long-forms":[[18,25]]},{"text":"characterize the AAV functions mediating this effect, cloned AAV type 2 wild-type or mutant genomes were transfected into simian virus 40 (SV40)-transformed hamster cells together with the six HSV replication genes","acronyms":[[139,143],[17,20],[61,64],[193,196]],"long-forms":[[122,137]]},{"text":"the literature. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree sys-","acronyms":[[69,71]],"long-forms":[[52,67]]},{"text":"cognitive system (Wilkes, 1997). We refer to this  subset of knowledge store (KS) in operation for and in  a given discourse as discourse model (DM) and hold ","acronyms":[[78,80],[145,147]],"long-forms":[[61,76],[128,143]]},{"text":" The improvement of PAS analysis would benefit several natural language processing (NLP) applications, such as information extraction, summariza-","acronyms":[[81,84],[20,23]],"long-forms":[[52,79]]},{"text":"The parsers are: the Berkley parser with gold POS label as entrances (Berk-G), the Berkeley merchandise parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). ","acronyms":[[262,268],[315,321],[47,50],[199,203]],"long-forms":[[236,253],[279,313],[165,180]]},{"text":"See Table 3 for the complete list of non-predicate filters describing restrictions on the role text (RT), role span (RS), and predicate frame (PF) in terms of the semantic type","acronyms":[[101,103],[117,119],[143,145]],"long-forms":[[90,99],[106,115],[126,141]]},{"text":"Figure 1: Top-k Accuracy Level Configuration MRR 0 Baseline (BL) 0.6559 1","acronyms":[[61,63],[45,48]],"long-forms":[[51,59]]},{"text":"parser on merged development PTB\/PRBK data (section 24). Legend of models: ST=Split Tags; EC=augmented connectivity.","acronyms":[[75,77],[29,37],[90,92]],"long-forms":[[78,88],[93,114]]},{"text":"341  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070?1080, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"Due to the existence of CTB-I, we were able to train new automatic Chinese language processing (CLP) tools, which crucially use annotated corpora as training","acronyms":[[96,99],[24,29]],"long-forms":[[67,94]]},{"text":"In this example, only Midas can be chosen for the role of twit, but any member of the class PN (proper names) having the attributes male, brave and handsome can be","acronyms":[[92,94]],"long-forms":[[96,108]]},{"text":"ear (lin) kernel, seconds degree polynomial kernel (d=2), and RBF kernel (rbf); SVM with transductive inference (TSVM) and linear (linseed) kernel or second degree polynomial (d=2) ker-","acronyms":[[112,116]],"long-forms":[[88,110]]},{"text":"(:all it as word accurate(W.A.). We use one  more measure, called character precision(C.A.)  that measures the character edit distance be- ","acronyms":[[85,89],[26,29]],"long-forms":[[66,83],[12,25]]},{"text":"P (di|h(d1,..., di?1)) Using a neural network architecture called Simple Synchrony Webs (SSNs), the histories representation h(d1,..., di?1) is incrementally computed from","acronyms":[[93,97]],"long-forms":[[73,91]]},{"text":"R5   95 7 Antecedent Contained Deletion(ACD)  Further evidence for the proposed analysis comes ","acronyms":[[40,43]],"long-forms":[[10,39]]},{"text":"notator. Pour brevity, we only considered PubMed as the source DB, and named entity recognition (NER)type annotations, which may be purely represented","acronyms":[[96,99]],"long-forms":[[70,94]]},{"text":"occurrence limits (FCRs), feature specification  defaults (FSDs), linear precedence (LP) statements,  and universal feature instantiation principles (UIPs). ","acronyms":[[156,160],[25,29],[65,69],[91,93]],"long-forms":[[112,154],[32,63],[72,89]]},{"text":"stood statistical models?statistical dependency parsers, probabilistic context-free grammars (PCFGs), and word translation models (TMs)?can be efficiently combines into a unified framework that jointly searches for the best","acronyms":[[131,134],[94,99]],"long-forms":[[111,129],[57,92]]},{"text":"This makes it hard to find particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the discussion at a high-level. Our goal in this work was to create a simple tool which would allow people to rapidly ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comment threads. 2 How CoFi works For a given set of comments, we create a distinct CoFi instance.","acronyms":[[453,457],[625,629],[564,568]],"long-forms":[[459,473]]},{"text":"2008) since it also includes structural info of arguments. It is based on the Argumentation Markup Language (AML) that models argument ingredient in a XML-based tree structure.","acronyms":[[116,119],[158,161]],"long-forms":[[85,114]]},{"text":"Hajdinjak and Mihelic? The HEAVEN Evaluation Framework Number of help messages (NHM) and help-message ratio (HMR), i.e., the number and the ratio of system?s pomoc messages;","acronyms":[[82,85],[111,114]],"long-forms":[[57,80],[91,109]]},{"text":"in penalties level are not combined. That is why  most of the verb phrases(VP) are indoors match  (53.28%).","acronyms":[[74,76]],"long-forms":[[61,72]]},{"text":"particularly helpful in parsing where the sequence  of expression forming the MWE is treated as a single  word with a single parties of speech (POS) tag. MWE ","acronyms":[[136,139],[73,76],[146,149]],"long-forms":[[120,134]]},{"text":"icantly better performance than GIZA++. We also evaluated Support Vector Machines (SVM) classifiers on the same first order feature space and","acronyms":[[83,86],[32,38]],"long-forms":[[58,81]]},{"text":"It has been shown in (Ando and Zhang, 2005a) that the optimization problem (3) has a simple solution using singular value decomposition (SVD) when we choose square regularization: r(f","acronyms":[[137,140]],"long-forms":[[107,135]]},{"text":"we used dialog features derived from manual annotations ? dialog acts (DA) and overt displays of power (ODP) ?","acronyms":[[71,73],[104,107]],"long-forms":[[58,69],[79,102]]},{"text":"TDP (target only) 62.60 33.04 Table 2: Results Generalized average precision (GAP) is a more precise measure than P","acronyms":[[78,81],[0,3]],"long-forms":[[47,76]]},{"text":"1 In t roduct ion   The main targets of the proposed project is to develop  a language modelling(LM) that uses syntactic structure. ","acronyms":[[90,92]],"long-forms":[[75,88]]},{"text":"dhi\/NEP road\/NEL. The structure of the tagged  element using the Shakti Standard Format (SSF)5  will be as follows: ","acronyms":[[89,92],[0,7],[8,16]],"long-forms":[[65,87]]},{"text":"are the formal-language theoretic foundation for n-gram mannequins (Garcia et al, 1990), which are widely used in natural language processing (NLP) in portion because such distributions can be estimated","acronyms":[[139,142]],"long-forms":[[110,137]]},{"text":"LPM output using application knowledge  ? Function Generator Module (FGM) converting  SAM output into executable function calls ","acronyms":[[69,72],[86,89],[0,3]],"long-forms":[[42,67]]},{"text":"of Electrical and Computer Engineering Pohang University of Science and Technology (POSTECH) Advanced Information Technology Research Center (AITrc) San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784","acronyms":[[142,147],[84,91]],"long-forms":[[93,140],[39,82]]},{"text":" 3.3.1 Determinantal Point Processes Determinantal point process (DPPs) are distributions over subsets that jointly prefer quality of","acronyms":[[68,72]],"long-forms":[[37,66]]},{"text":"emerge although ModP and FocP are optional.  projections such as NegP (negation phrase) will  not be discussed although we assume there owe ","acronyms":[[65,69],[16,20],[25,29]],"long-forms":[[71,86]]},{"text":"ing the following measuring:   1. PrecisionCorrectTransliterations(PTrans)  2.","acronyms":[[66,72]],"long-forms":[[33,64]]},{"text":"3 Bayes ian  networks   A Bayes ian  network  (Pearl, 1988), or  Bayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol;  of var iab les  and a sel; of d i rec ted  edges  (:on- ","acronyms":[[92,95]],"long-forms":[[65,90]]},{"text":"contributions to sentence similarity. In most cases, the longer common sequence (LCS) the two sentences have, the higher similarity score the sentences","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"Given these restrictions the DAR problem becomes to find that value da of DA that maximises P ( DA = da | f1 = v1, . . . , fn = vn,","acronyms":[[96,98],[29,32],[74,76]],"long-forms":[[101,103]]},{"text":"The partitioning of the dataset is listed in the Table 1, where we also give the partitioning of Wall Street Journal (WSJ) (Marcus et al, 1993) used to train the English grammar.","acronyms":[[118,121]],"long-forms":[[97,116]]},{"text":"Similarity operandi We examine two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008).","acronyms":[[121,125]],"long-forms":[[101,119]]},{"text":"actions and boolean b (> or ?) are utilised to ensure that unary decrease (RU) can only take place once after a SHIFT action.","acronyms":[[73,75],[110,115]],"long-forms":[[61,71]]},{"text":"Natural Language Generation (NLG). For example, STOP is a Natural Language Generation (NLG) system that generates tailored smoking cessation let-","acronyms":[[87,90],[29,32]],"long-forms":[[58,85],[0,27]]},{"text":"based Translation. In Trials of the 13th International Conference on Computational Linguistics (COLING?90), pages 247?252.","acronyms":[[101,110]],"long-forms":[[60,99]]},{"text":"Wiebe, 2000).  4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more","acronyms":[[47,53]],"long-forms":[[21,45]]},{"text":"trieval process. ( Chou and Wade, 2009b) proposed a Latent Dirichlet Allocation (LDA)based method to model the latent structured of ","acronyms":[[81,84]],"long-forms":[[52,79]]},{"text":"To experiment on MAYA, we compute the  performance score as the Reciprocal Answer  Rank (RAR) of the first correct answer given by  each question.","acronyms":[[89,92],[17,21]],"long-forms":[[83,87]]},{"text":"2008) and hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007) using the implementation in (Turian et al, 2010), Hellinger PCA (H-PCA) (Lebret and Collobert, 2014) and our connective-based representation (Bllip).","acronyms":[[143,148]],"long-forms":[[128,141]]},{"text":"1. Introduction  A verb phrase ellipsis (VPE) exists when a  sentence has an auxiliary verb but no verb phrase ","acronyms":[[41,44]],"long-forms":[[19,39]]},{"text":" 2.2 Recognizing subwords An automatic speech recognition (ASR) system (Jelinek, 1998) serves to recognizes both queries","acronyms":[[59,62]],"long-forms":[[29,57]]},{"text":"Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in","acronyms":[[71,74],[90,93]],"long-forms":[[63,69],[80,88]]},{"text":"for each word in the DAL. ( e.g., the sneezing definition for LOL (chuckles out loud) in Wiktionary is ? To laugh","acronyms":[[58,61],[21,24]],"long-forms":[[63,77]]},{"text":" NONHUMAN (wildlife), DNA, RNA, PROTEIN, CONTROL (control measures to contain the disease), BACTERIA, CHEMICAL and SYMPTOM.","acronyms":[[40,47],[21,24],[26,29],[31,38],[91,99],[101,109],[114,121],[1,9]],"long-forms":[[49,56],[11,18]]},{"text":"morphological kinds and variables.  The Encyclopedia Specialist (ES) is able to accessed the Encyclo-  pedia for extracting semantic information and world knowledge.","acronyms":[[65,67]],"long-forms":[[40,63]]},{"text":"these paths. This corresponds to the Viterbi approximation i speech recognition or in  other related areas for which hidden Markov models (HMM's) are used. In all such ","acronyms":[[139,144]],"long-forms":[[117,137]]},{"text":"annotating unlabeled data, for adapting  existing CRF-based named entity acknowledgement (NER) systems to new texts or  domains.","acronyms":[[86,89],[50,53]],"long-forms":[[60,84]]},{"text":"terms encountered.  Shallow Syntactic (SSyn) features consider the number and ratios of common part-of-speech","acronyms":[[39,43]],"long-forms":[[20,37]]},{"text":"for topic models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI), pages 27?34.","acronyms":[[99,102]],"long-forms":[[59,97]]},{"text":"assignment. We use a generative model based on a Dirichlet Process (DP) defined over composed rules.","acronyms":[[68,70]],"long-forms":[[49,66]]},{"text":"ysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al,","acronyms":[[80,84]],"long-forms":[[52,78]]},{"text":"This work was partly supported by UK EPSRC project GR\/N36462\/93: ? Forceful Accurate Statistical Parsing (RASP)?. ","acronyms":[[104,108],[34,36],[37,42]],"long-forms":[[67,102]]},{"text":"without any adaptation.  Laplacian SVM (L-SVM) This is a semisupervised learns method based on label","acronyms":[[40,45]],"long-forms":[[25,38]]},{"text":"On the one hand, we built machine learning classifiers based on Support Vector Machines (SVMs) and Conditional Random Fields (CRFs).","acronyms":[[89,93],[126,130]],"long-forms":[[64,87],[99,124]]},{"text":"Another important note is that, although the audio sets consist of both broadcast news (BN) and broadcast conversations (BC), we did not perform BN or BC-specific tuning.","acronyms":[[121,123],[88,90],[145,147],[151,153]],"long-forms":[[96,119],[72,86]]},{"text":" 1 Introduction Traditionally, Information Retrieval (IR) and Statistical Natural Language Processing (NLP) applica-","acronyms":[[54,56],[103,106]],"long-forms":[[31,52],[74,101]]},{"text":"Xi, where Par(Xi) denotes the parents of Xi.  Conditional potentially distributions (CPDs) can be defined in sundry ways, from look-up tables","acronyms":[[85,89],[10,13]],"long-forms":[[46,83],[30,37]]},{"text":"construction relies on existing natural language processing tools, e.g., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or rich lexical resources such","acronyms":[[130,132]],"long-forms":[[106,128]]},{"text":"2003. MDA Guide Version 1.0.1. Technical report, Object Management Panel (OMG). ","acronyms":[[74,77],[6,9]],"long-forms":[[49,72]]},{"text":" 4.1 KNN classification The basic idea of the K nearest neighbor (KNN) classification algorithm is to use already categorized","acronyms":[[66,69],[5,8]],"long-forms":[[46,64]]},{"text":"Nincc NIA ~ \\]laS beginning moving from toy  problems to ,'eal applications one of the greatest  difficully has been Knowledge Acquisition (KA)  of different lypes (lexical, grammatical, domain ","acronyms":[[137,139]],"long-forms":[[114,135]]},{"text":"collected as follows: Positive Diccn: 3,730 Chinaman positive terms (e.g., \/good-looking, \/ thankfully) were collected from the Chinese Vocabulary for Sentiment Analysis (VSA)20 released by HOWNET.","acronyms":[[165,168],[184,190]],"long-forms":[[130,163]]},{"text":" 2 American National Corpus The American National Corpus (ANC) project (Ide and Macleod, 2001; Ide and Suderman, 2004) has","acronyms":[[58,61]],"long-forms":[[32,56]]},{"text":"Using the observation that LL is correlated with MRR on the same data set, we expect that optimizing LL on a development set (LLdev) will also improve MRR on an evaluation set (MRReval).","acronyms":[[126,131],[27,29],[49,52],[151,154],[177,184]],"long-forms":[[101,120]]},{"text":"Improving part-of-speech tagging for context-free parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai, Thailand.","acronyms":[[144,150]],"long-forms":[[81,142]]},{"text":"X:?  TypeChanging (TCR) X:? ? Y:?(?)","acronyms":[[19,22]],"long-forms":[[5,17]]},{"text":"A Linear Support Vector Machine text classifier (Joachims, 1999) was trained on Web pages from the Open Directory Project (ODP)6. These pages ef-","acronyms":[[123,126]],"long-forms":[[99,121]]},{"text":"   Figure 2. Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3.","acronyms":[[59,61]],"long-forms":[[64,72]]},{"text":"rithms for learning neuropsychological and demographic data which are then used for the prediction of Clinical Dementia Rating (CDR) scores for different sub-types of Dementia and other cog-","acronyms":[[128,131]],"long-forms":[[102,126]]},{"text":"Table 1: Probabilities computed for each type of linguistic information. Error codes correspond to the five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).","acronyms":[[151,159],[224,227],[127,132],[185,187]],"long-forms":[[161,182],[229,251],[189,200]]},{"text":"started.  British National Corpus (BNC)4, American  National Corpus (ANC)5 had been referenced ","acronyms":[[35,38],[69,72]],"long-forms":[[10,33],[42,67]]},{"text":" 2004. Document understanding conferences (DUC). ","acronyms":[[43,46]],"long-forms":[[7,41]]},{"text":"joining grammars. In Proceedings of the 12 th Internationally  Conference on Computational Linguistics (COLING'88),  Budapest, Hungary, August 1988.","acronyms":[[102,111]],"long-forms":[[61,100]]},{"text":"5.1 Overall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG","acronyms":[[74,76],[78,82],[101,103],[105,109],[144,146],[148,152]],"long-forms":[[67,72],[88,99],[118,142]]},{"text":"For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M paral-","acronyms":[[124,126],[24,27]],"long-forms":[[114,122]]},{"text":"interface.  The PRIDES User Interface Layer (PUI) is  responsible for establish and managing the screen ","acronyms":[[45,48]],"long-forms":[[16,37]]},{"text":" A stack based extraction Algorithm 1 was designed to extract a context free grammar (CFG) from the URDU.KON-TB treebank.","acronyms":[[86,89],[100,111]],"long-forms":[[64,84]]},{"text":"to efficiently implement their computation.  In Natural Language Processing (NLP), the typical dimensionality of databases, which are made","acronyms":[[77,80]],"long-forms":[[48,75]]},{"text":" ? Backward Looking (BL)\/Forward Looking (FL) features (14 to 22) are mostly extracted from ut-","acronyms":[[42,44],[21,23]],"long-forms":[[25,40],[3,19]]},{"text":"We focus on predicting an absolute indication of quality rather than only ranking the sentences by quality; this is why we use the Mean Absolute Error (MAE) as the main evaluation measure rather than Spearman?s correlation or DeltaAvg (Callison-Burch et al.,","acronyms":[[152,155],[226,234]],"long-forms":[[131,150]]},{"text":"similar to how Iida et al (2011) computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only).","acronyms":[[118,124],[166,168]],"long-forms":[[126,139]]},{"text":"O-ADVL = Object Adverbial: lied ran two miles.  APP = Apposition: Helsinki, the capital of Finland,  N = Title: King Georges and Mr. ","acronyms":[[47,50],[0,6]],"long-forms":[[53,63],[9,25],[104,109]]},{"text":" The objective of this study is to illustrate a  word support model (WSM) that is able to improve our WP-identifier by achieving better ","acronyms":[[69,72],[102,104]],"long-forms":[[49,67]]},{"text":"HowNet and Its Computation of Meaning. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, M., Pet, M. and Soria, C. (2009). Multilingual resources for NLP in the lexical markup framework (LMF). In journal de Language Resources and Evaluation, March 2009, Volume 43, pp.","acronyms":[[240,243]],"long-forms":[[214,238]]},{"text":"2 Procedures In this IRB-approved researches, we obtained the Shared Annotated Resource (ShARe) corpus originally generated from the Beth Israel Dea-","acronyms":[[81,86]],"long-forms":[[54,79]]},{"text":"template includes, for each sentence, syntactic Infor-  mation that Is represented in a tree whose nodes are  syntacti~ classes such as S (sentence), CL (provisions),  SUBJECT or VERB.","acronyms":[[153,155]],"long-forms":[[157,163],[142,150]]},{"text":"A Linear Helped Vector Machine text classifier (Joachims, 1999) was qualified on Web pages from the Open Directory Project (ODP)6. These pages ef-","acronyms":[[123,126]],"long-forms":[[99,121]]},{"text":"model organism databases (e.g., for mouse3 and  yeast4) as well as various protein databases (e.g.,  Protein Information Resource5 (PIR) or SWISS-                                                                                           tor), a model organism for genetics research: ","acronyms":[[132,135]],"long-forms":[[101,130]]},{"text":"Abstract In this work we present results from utilize Amazon?s Mechanical Turk (MTurk) to annotate translation lexicons between Brits","acronyms":[[78,83]],"long-forms":[[61,76]]},{"text":"computed from the rewriting rules by the examination of the interdependencies of the rules with the helped of  KIT = Ktinsdiche lntelligenz und Textverstehen  (artificial intelligence and text understanding), FAST = ","acronyms":[[107,110],[205,209]],"long-forms":[[113,135]]},{"text":"smoothness. Before creating a POMDP edifice, we used the dynamic Bayesian network (DBN) structure (Fig.","acronyms":[[85,88],[30,35]],"long-forms":[[59,83]]},{"text":"2009. Word sense disambiguation: A surveyed. ACM Computing Surveys (CSUR),41(2):10. ","acronyms":[[66,70],[43,46]],"long-forms":[[47,64]]},{"text":"Entailment [13] was organized by SemEval-2.  Recognized Inference in Text (CEREMONIAL)2 organized by NTCIR-9 in 2011 is the first to expand ","acronyms":[[76,80],[96,101]],"long-forms":[[45,74]]},{"text":"based on such formalisms include Generalized Phrase  Structured Grammar (GPSG) \\[Gazdar et al 1985\\],  Lexical Functional Grammar (LFG) \\[Bresnan 1982\\],  Functional Unification Grammar (insect) \\[Kay 1984\\], ","acronyms":[[130,133],[72,76],[186,189]],"long-forms":[[102,128],[33,70],[165,184]]},{"text":"(CPs) increases the number of Complex Predicates (CPs) entries along with compound verbs  (CompVs) and conjunct verbs (ConjVs). The ","acronyms":[[119,125]],"long-forms":[[103,117]]},{"text":"1540  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 7?14, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":" 2 Methodo logy   A User Centered (UC) approach was adopted for the  designs of GEPPETTO.","acronyms":[[35,37],[79,87]],"long-forms":[[20,33]]},{"text":"stem of JUMP = <jump>.   sense of JUMP = jumping. ","acronyms":[[34,38],[8,12]],"long-forms":[[41,48],[16,20]]},{"text":"paring average precision values obtained 1) against the original, guidebooks chronologies (APC), and 2) against the expert evaluate (APE). These values","acronyms":[[131,134],[87,90]],"long-forms":[[100,129]]},{"text":"2014).  The RST Discourse Treebank (RST-DT) (Carlson et al.,","acronyms":[[36,42]],"long-forms":[[12,34]]},{"text":"+ ??????????)  haphazard Markov Clustering Algorithm (MCL)  (Dongen, 2000) ","acronyms":[[51,54]],"long-forms":[[22,49]]},{"text":"Abstract Separately of language, the standard characteristics set for text messages (SMS) and many other social media platforms is the Roman alphabet.","acronyms":[[79,82]],"long-forms":[[56,77]]},{"text":"Computational Linguistics, Volume 15, Number 1, March 1989 33  Michael C. McCord \\]Designing of LMT: A Prolog-Based Machines Translation System  sions in a logical forma language (LFL)  (McCord 1985a,  1987).","acronyms":[[175,178],[93,96]],"long-forms":[[152,173]]},{"text":"Another important note is that, although the audio sets consist of both broadcast news (BN) and spreading conversations (BC), we did not fulfill BN or BC-specific tuning.","acronyms":[[121,123],[88,90],[145,147],[151,153]],"long-forms":[[96,119],[72,86]]},{"text":"We performed the same experiments on three dif-  ferent corpora:  Corpus SN (Spanish Novel) train: 15Kw, testing:  2Kw, tag set caliber: 70.","acronyms":[[73,75]],"long-forms":[[77,90]]},{"text":"Note Table 3: Statistic of training and test corpus for the Canadian Hansards task (PP=perplexity). ","acronyms":[[85,87]],"long-forms":[[88,98]]},{"text":"lem of opting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will enable for accurate","acronyms":[[128,130]],"long-forms":[[102,126]]},{"text":"sequence of ATN arcs which is matched against the  input string. A pattern archangel (PAT) has been adding to  the ATN formalism with a form identical to that of oth- ","acronyms":[[80,83],[108,111]],"long-forms":[[67,74],[12,15]]},{"text":" 1 Introduction Question answering(QA) system aims at finding exact answers to a natural language question.","acronyms":[[35,37]],"long-forms":[[16,33]]},{"text":" 1 Introduction Grammatical Framework (GF) (Ranta, 2004) is a grammar formalism intended in particular to serve","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":"list of Frames alphabetically surrounding Compliance runs as follows: Compatibility, Competition, Complaining, Completeness, Accordance, Concessive.... Next, we endeavours to catalogue the Lexical Units (LUs) associated with the frame. ","acronyms":[[201,204]],"long-forms":[[186,199]]},{"text":"ing is how to detect a temporal relation between a pair of temporal entities such as events (EVENT) and times expressions (TIMEX) in a narrative. Af-","acronyms":[[124,129]],"long-forms":[[106,122]]},{"text":"terminates 1 <= V I ;   SPAN (SPANS, 'CONSTITUENTS ' ) = <TUP , CONSTITUENTS>;  TODO = TUP -t TODO;  r e t u r n ;  ","acronyms":[[80,83],[51,54]],"long-forms":[[84,86]]},{"text":"equivalent m Dutch For a sampling of 59 Ital,an noun  s)nsets there is at least an overlap of 30% (20) with  Dutch Case are Arbeltszeitverkurzung (DE)  = arbeidstijdverkortmg (NL) = (declines of work- ","acronyms":[[149,151],[178,180]],"long-forms":[[107,121]]},{"text":" 1 Introduction Spoken Dialogue Systems (SDSs) play a imperative role in achieving natural human-machine interaction.","acronyms":[[41,45]],"long-forms":[[16,39]]},{"text":"native (see \\[2\\]).  Task  Manager  (TM)   In our previous experience, speech acknowledge systems ","acronyms":[[37,39]],"long-forms":[[21,34]]},{"text":" 1 Introduction Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verbal constituent has been omitted.","acronyms":[[38,41]],"long-forms":[[16,36]]},{"text":" 3 Background Slot filling (SF) is a query-oriented relation extraction (RE) task in the Knowledge Base Popu-","acronyms":[[28,30],[73,75]],"long-forms":[[14,26],[52,71]]},{"text":"their semantic deviation valuing. The result is a  list of pairs called the ICS (Initial Cluster Set). ","acronyms":[[75,78]],"long-forms":[[80,99]]},{"text":"number of correcVi'abeled-constituents in suggests parse  number of correct matched constituent inproposed parse  6) Sentence parsing percentages(SPg) =  number\" of sentences having a proposed parse by parser ","acronyms":[[140,143]],"long-forms":[[117,133]]},{"text":"strategies for reducing ambiguity.  4.2.1 Longest ending filtering (LEF) The first approach to ambiguity reduction is based","acronyms":[[68,71]],"long-forms":[[42,66]]},{"text":"SEPA parameters are S = 13, 000, N = 20. In both rows, SEPA results for the in-domain (left) and adaptation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines. The","acronyms":[[183,185],[0,4],[55,59],[159,161]],"long-forms":[[167,181]]},{"text":" In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain.","acronyms":[[74,77],[85,87]],"long-forms":[[53,72]]},{"text":"nce,...,tM i=2 i=N+I   (7)  which is a Nth-order Markovian chain for the language model (MLM). ","acronyms":[[88,91]],"long-forms":[[48,86]]},{"text":"[NP : [XNOUNS : MERINO'S (NOUN) HOME (NOUN)] ] [NP : [XNOUNS : MERINO'S (NOUN)] ] [VP : [VERB_GROUP : HOME (VERB)] ] [XPPS : [PP : IN (PREPOSITION )","acronyms":[[102,106],[48,50],[83,85],[126,128],[7,13],[54,60],[118,122]],"long-forms":[[89,99],[1,3]]},{"text":"The model consists of two subtasks of boundary identifying(BI) and semantic role classification(SRC). ","acronyms":[[99,102],[62,64]],"long-forms":[[70,97],[38,61]]},{"text":"17 classes set in Sun et al(2008).  We used the spectral clustering (SPEC) modes and configure as in Sun and Korhonen (2009) but","acronyms":[[69,73]],"long-forms":[[48,67]]},{"text":"New York, N.Y. 10027  Introduction  COMET (COordinated Multimedia Explanation  Testbed) is an experimental system that generates inter- ","acronyms":[[36,41],[10,14]],"long-forms":[[43,86],[0,8]]},{"text":" 1 Introduction Statistical machine translation (SMT) starts from sequence-based models.","acronyms":[[49,52]],"long-forms":[[16,47]]},{"text":"tics.  Linguistic Data Consortium (LDC). 2013.","acronyms":[[35,38]],"long-forms":[[7,33]]},{"text":"Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). ","acronyms":[[104,106],[122,124]],"long-forms":[[94,102],[112,120]]},{"text":"2 Algorithms and Data  2.1 Task Definition and Data  The named entity (NE) task use for this  evaluation requires the system to determine all ","acronyms":[[71,73]],"long-forms":[[57,69]]},{"text":"data. In Globally Conference on Machine Learning (ICML). ","acronyms":[[55,59]],"long-forms":[[9,53]]},{"text":"Patrizia Paggio University of Copenhagen Centres for Language Technology (CST) Njalsgade 140, 2300-DK Copenhagen","acronyms":[[73,76],[93,100]],"long-forms":[[41,71]]},{"text":"unitary operator. Consequently, the primary problems  of building a quantum classifier (QC) is to find  the correct or optimal unitary operator.","acronyms":[[84,86]],"long-forms":[[64,82]]},{"text":"the second sentence provides some further description of that entity. An Entity Relation (EntRel) was annotated for such sentence pairs as below.","acronyms":[[90,96]],"long-forms":[[73,88]]},{"text":"*Event, *Mtrans-Action), and plans (i.e. *Pick-Up-  Gun). A hierarchy of Concept Class (CC) entities  stores knowledge both declaratively and procedurely ","acronyms":[[88,90]],"long-forms":[[73,86]]},{"text":"that are of interest o specific users. An cases of IE is the  Named Ent i ty  (NE) task, which has becomes established  as the essential first step in many other IE tasks, provid- ","acronyms":[[81,83],[53,55],[163,165]],"long-forms":[[64,78]]},{"text":"computations the posterior probability.  Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has","acronyms":[[73,78],[99,103]],"long-forms":[[42,52]]},{"text":"In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado. ","acronyms":[[93,97]],"long-forms":[[22,91]]},{"text":"Figure 1(a), the node @VP indicates that a binarization has been performed on the subtree VP (VBD PRT PP). All remaining rules that","acronyms":[[90,92],[23,25]],"long-forms":[[94,101]]},{"text":"noisy and potentially unreliable observations.  While scenario template inception (STC) is a complex problem, its evaluation is arguably more dif-","acronyms":[[82,85]],"long-forms":[[54,80]]},{"text":"1 Intro  Scalability in dialog systems is, of course, not only a  matter of the natural language understanding (NLU)  component, but also of the NLG party of the system.2 Ours ","acronyms":[[119,122],[152,155]],"long-forms":[[87,117]]},{"text":"WordNet  In another related effort, SRI performed experiments  in utilizing WordNet (WN) as a knowledge source for  IE.","acronyms":[[85,87],[36,39],[116,118]],"long-forms":[[76,83]]},{"text":"2 Complexity of GPSG Components  A generalized phrase structure grammar contains five language-  particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence ","acronyms":[[145,147],[16,20],[189,191]],"long-forms":[[124,143],[170,187]]},{"text":" 1 Introduction Amazon?s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in","acronyms":[[41,44]],"long-forms":[[16,39]]},{"text":"on three official testsets.  NIST 2008 Open Machine Translate (OpenMT) Evaluation9 has distributed test data from 2 realms: Newswire and Web.","acronyms":[[65,71]],"long-forms":[[39,63]]},{"text":" OOV Handling Techniques and their Combination We compare our baseline system (BASELINE) to each of our basic techniques and their full combi-","acronyms":[[79,87],[1,4]],"long-forms":[[62,77]]},{"text":" The CBDF similarity values between  100,000-word subsets of Original French (OF),  French translated from English (EF), from ","acronyms":[[78,80],[5,9],[116,118]],"long-forms":[[61,76],[84,114]]},{"text":" Fond most of the successful AQUAINT QA systems,  LCC?s system uses an answer type (AT) ontology for  the ranked of AT categories.","acronyms":[[84,86],[29,36],[37,39],[50,53],[124,126]],"long-forms":[[71,82]]},{"text":"in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story.","acronyms":[[81,84]],"long-forms":[[68,79]]},{"text":"3.1 Data We use the TiGer treebank freed 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER,","acronyms":[[80,85],[48,53],[92,97]],"long-forms":[[64,78],[20,34]]},{"text":"\/NN ?? \/NN ]   Input: wi: word index (ID) in a yielded sentence. ","acronyms":[[38,40]],"long-forms":[[31,36]]},{"text":"case where estimated user?s acquaintances and preference are represented as separate binary parameters instead of probability distributions (PDs). That is, the estimated","acronyms":[[137,140]],"long-forms":[[110,135]]},{"text":"In Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89), volume 2, pages 1511?1517.","acronyms":[[86,94]],"long-forms":[[27,84]]},{"text":"method of ADN. ADN is constructed by  Restricted Boltzmann Machines (RBM)  with unsupervised learning using labeled ","acronyms":[[69,72],[10,13],[15,18]],"long-forms":[[38,67]]},{"text":"ing different training processes. The effects of discriminative training (CRF) and elongated feature sets (lower section) are more than additive.","acronyms":[[72,75]],"long-forms":[[50,70]]},{"text":"Abbreviations: BI=Bioinformatician, NLP=Natural Linguistic Processing researcher, ML=Machine Learning researcher, L=Linguist, Doorman=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency transforming, Dict=Dictionary UTurku VIBGhent ConcordU HCMUS","acronyms":[[187,189],[15,17],[36,39],[80,82],[124,130],[147,152],[222,226]],"long-forms":[[190,209],[18,34],[40,67],[83,99],[114,122],[131,137],[153,178],[227,237]]},{"text":" The obtained Spanish dozens as compares to the  scores from the initial English experiment (E-E-E)  are shown in figure 5.","acronyms":[[93,98]],"long-forms":[[73,91]]},{"text":"This research has been supported in part by DARPA (under contract number FA8750-13-2-0005), NIH (NICHD award 1R01HD07532801), Keck Foundation (DT123107), NSF (IIS0835797), and","acronyms":[[92,95],[44,49],[73,75],[143,145],[154,157]],"long-forms":[[97,102]]},{"text":"\u0007\u0002 B \u0007\u001eE B \u0004\u001e\u0014\t\u0005E B \u0002\u0014 B \u0019\u0002F\u0013\u0003E\b\u0007 B AE EA B \u0017\u0015 B \u0005\u0013\u0003\u0003\u0006\b) B ,\u0002\u0014 B \tA\u0007E\u0014\b\t\u0007\u0006 EA\u0015 B \u0004E\u0014\u001a\u0002\u0014\u0003\u0006\b) B F\u0002\u0003\u0004\u0002\bE\b\u0007+$\u0006\u0005EB \u0003\u0013A\u0007\u0006\u0004A\u0006F\t\u0007\u0006\u0002\bH0 B \u0002\u001a B$\u0002\u0014\u0019 B  EF\u0007\u0002\u0014\u0005 B ,\u001bF\u001eI\u0007JE\" B \/66B0& B(\u001e\u0006\u0005 B \u001e\t\u0005 B \u0007\u001eE B \t\u0019 \t\b\u0007\t)E B \u0007\u001e\t\u0007 B \u0006\u0007 B \u0006\u0005 B \u0006\u0003\u0003E\u0019\u0006\t\u0007EA\u0015B \t\u0004\u0004A\u0006F\t\u0017AEB\u0007\u0002B\t\b\u0015B\u0003\u0002\u0019EAB\u0006\bB$\u001e\u0006F\u001eB$\u0002\u0014\u0019B\u0003E\t\b\u0006\b)BF\t\bB\u0017EBED\u0004\u0014E\u0005\u0005E\u0019B\t\u0005B\tB EF\u0007\u0002\u0014&B(\u001eEB\u0004\u0014\u0006\bF\u0006\u0004AEBA\u0006\u0003\u0006\u0007\t\u0007\u0006\u0002\bB \u0002\u001aBF\u0002\u0013\u0014\u0005EB\u0006\u0005B\u0007\u001e\t\u0007B\u0007\u001e\u0006\u0005B\u0007\t'E\u0005B\b\u0002B\tFF\u0002\u0013\b\u0007B\u0002\u001aB$\u0002\u0014\u0019B\u0002\u0014\u0019E\u0014&B(\u001eE\u0014E\u001a\u0002\u0014E\"B$\u001e\u0006AEB\u0006\u0007B\u001e\t\u0005B\u0004\u0014\u0002 E\bBE\u001a\u001aEF\u0007\u0006 EB\u0006\bB\u0007\u001eEB","acronyms":[[300,307]],"long-forms":[[309,326]]},{"text":"to such an extent that?)  Multiword interjections (MWI) are a small category with expressions such as mille sabords (?","acronyms":[[51,54]],"long-forms":[[26,49]]},{"text":"I  zwemmen  (17) is ttms obtained by setting VPo = VPh Zo = Yo,  attd Zl = Vl.","acronyms":[[51,54],[45,48],[70,72]],"long-forms":[[55,57]]},{"text":"tion device, a Nippon Electric Company DP-200, was  addendum to an existing natural anguage processing system,  the Natural Language Computer (NLC) (Ballard 1979,  Biermann and Ballard 1980).","acronyms":[[144,147],[43,45]],"long-forms":[[117,142]]},{"text":"The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF)","acronyms":[[69,75],[112,115]],"long-forms":[[52,67],[85,110]]},{"text":"person names, organization names, location names, etc. The template element (TE) task extracts information centered around an entity, like the acronym,","acronyms":[[77,79]],"long-forms":[[59,75]]},{"text":"Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Janitor stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency transform annotation process introduced by Pyysalo et al","acronyms":[[177,179],[15,22],[41,47],[64,68],[84,92],[111,116],[151,159]],"long-forms":[[180,199],[32,39],[48,54],[69,82],[93,101],[117,142],[160,168]]},{"text":"knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954).","acronyms":[[181,185]],"long-forms":[[149,179]]},{"text":"must be stored at each step of the decoding algorithm.  This information includes: the current score (SCORE),  the pointer to the previous lexical item (BPO) on the best ","acronyms":[[102,107],[153,156]],"long-forms":[[95,100]]},{"text":"2007) from 4 domains, labeled propitious or negative. We apply logistic regression (LR) and SVM utilised unigram and bigram features.","acronyms":[[82,84],[90,93]],"long-forms":[[61,80]]},{"text":" The headline  generation system we present utilized  Singular Value Decomposition (SVD) to  guide the generation of a headline ","acronyms":[[80,83]],"long-forms":[[50,78]]},{"text":"J = Japanese ????? S = Spanish  JV = Joint Ventures ?????????? ME = Microelectronics ","acronyms":[[32,34],[62,64]],"long-forms":[[37,50],[4,12],[67,83]]},{"text":"evidenced in (1).2  2~Vc utilised lhe fo l low ing  abbrev ia t ions :  NOM : nominat ive ;   KAB = accusat ive ;  AI)N = adnomina l ;  CI. = c lass i l ier ;  ARGSTR ","acronyms":[[85,88],[106,110]],"long-forms":[[91,98],[113,121]]},{"text":"question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to","acronyms":[[78,80],[20,22]],"long-forms":[[55,76],[0,18]]},{"text":"Abstract  This paper provides a description of the Hong  Kong Polytechnic University (PolyU) System  that participated in the task #5 of SemEval-2, ","acronyms":[[86,91]],"long-forms":[[62,84]]},{"text":"2http:\/\/www.nist.gov\/rhetoric\/tests\/mt\/ Table 1: Training, development and test data from Core Travel Expression Corpus(BTEC) Japanese English","acronyms":[[119,123]],"long-forms":[[88,117]]},{"text":"class. Among these are: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al, 2002), H (Meila, 2001), clustering F-measure","acronyms":[[89,91]],"long-forms":[[64,87],[27,33]]},{"text":"Suffixes (S): able, est, ful, ic, ing, ive, ness etc.  Word Sentiment Polarity (SP): POS, NEG, NEU Pivoting on the head aspect, we look forward and","acronyms":[[80,82]],"long-forms":[[60,78]]},{"text":"sorts. Among these are: L (Larson, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al, 2002), H (Meila, 2001), clustering F-measure","acronyms":[[89,91]],"long-forms":[[64,87],[27,33]]},{"text":"tagger (Cutting et al 1992) and LT POS tagger  (Mikheev 1997). Maximum Entropy (MaxEnt)  based taggers also seem to perform very well        ","acronyms":[[80,86],[32,34],[35,38]],"long-forms":[[63,78]]},{"text":"8. THE SAIL INTERFACING SYSTEM  The SAIL Interfacing System (S.I.S.) is the f ramework   where a user  can interact with SAIL in developing NL ","acronyms":[[61,67],[140,142],[121,125]],"long-forms":[[36,59]]},{"text":"2https:\/\/www.mturk.com\/mturk\/. 3http:\/\/tartarus.org\/martin\/PorterStemmer\/ 4Reviews with NDr = NTr are regarded as incorrectly classified by TopicSpam.","acronyms":[[94,97],[88,91]],"long-forms":[[98,125]]},{"text":"sides identity (IDENT) we only marked up three associative relations (Hawkins, 1978): set membership (ASPECT), subset (SUBSET), and ? gen-","acronyms":[[120,126],[16,21],[102,109]],"long-forms":[[112,118],[6,14]]},{"text":"evaluating the attribute subsets. Their evaluation is based on consistency (CBF) and correlation (CFS). ","acronyms":[[76,79],[98,101]],"long-forms":[[63,74],[85,96]]},{"text":"email: mal@aber.ac.uk Stephen Pulman University of Oxford (UK) email: sgp@clg.ox.ac.uk","acronyms":[[59,61]],"long-forms":[[37,57]]},{"text":"n - c-dow British English American English Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical type tagset (LTT).","acronyms":[[115,117],[157,160]],"long-forms":[[97,113],[136,155]]},{"text":"categories of both test texts  PoS tags  DET (Determiner)  NM (Pronoun) ","acronyms":[[41,44],[59,61]],"long-forms":[[46,56]]},{"text":"query, and their performance asymptotes by the time they get to the second query.  This effect is confirmed by an analysis of variance (ANOVA)8, which shows a highly significant effect of order of presentation (F = 9.8427; p< .0001).","acronyms":[[136,141]],"long-forms":[[114,134]]},{"text":"In International Conference on Autonomy Agents and Multiagent Systems (AAMAS). ","acronyms":[[73,78]],"long-forms":[[31,71]]},{"text":"In this paper we investigate the relation between positive and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE","acronyms":[[101,103],[157,159]],"long-forms":[[81,99]]},{"text":"during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Mads Yglesias (MY),3 Red Sate (RS),4 and Right Wing News (RWN).5 CB and MY ceased as independent bloggers in Augusto 2008.6 Because","acronyms":[[119,122],[34,36],[51,53],[75,77],[92,94],[133,135],[126,128]],"long-forms":[[102,117],[20,32],[40,49],[57,73],[81,90]]},{"text":" ? Named entity (NE) representation in KBs posed another NED challenge.","acronyms":[[17,19],[39,42],[57,60]],"long-forms":[[3,15]]},{"text":"characters ? are often referred to by pronouns or definite noun phrases (NPs) instead of explicit repetition. ","acronyms":[[73,76]],"long-forms":[[59,71]]},{"text":"F6: \"TO PRODUCE GOLF CLUBS\"  (VP (AUX (TO \"TO\"))  (VP (V \"GENERATE\")  (NP (N \"GOLF\") (N \"CLUBS\")))) ","acronyms":[[51,53],[30,32],[34,37],[70,72]],"long-forms":[[55,58]]},{"text":"Results on final tests sets. LAS = labeled attachment score. UAS = unlabeled attachment score. ","acronyms":[[60,63],[28,31]],"long-forms":[[66,92],[34,58]]},{"text":"transcripts of user utterances, and included lexical, syntactic, numeric, and features from the output of Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al.,","acronyms":[[141,145]],"long-forms":[[106,139]]},{"text":"(MBF).  4 Multilingual PRF (MultiPRF) The schematic of the MultiPRF approach is shown","acronyms":[[28,36],[1,4],[59,67]],"long-forms":[[10,26]]},{"text":"   In the SUM (Summarization) setting, the  entailment pairs were generated using two proce-","acronyms":[[10,13]],"long-forms":[[15,28]]},{"text":" 1 Introduction Information extraction (IE), defined as the task of removing structured information (e.g., events, bi-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"1 Introduction  Syntax parsing is one of the most essentials  tasks in natural language processing (NLP) and  has attracted extensive beware during the past ","acronyms":[[101,104]],"long-forms":[[72,99]]},{"text":"The basic aim of Acquilex is the  development of techniques and methods in order to use  Machine Readable Dictionaries (MRD) * for build lexical  elements for Natural Language Processing Systems.","acronyms":[[120,123]],"long-forms":[[89,118]]},{"text":" ? True Positive (TP), the predicted e was justly referred to by s.   ","acronyms":[[18,20]],"long-forms":[[3,16]]},{"text":"Linear Program relaxation based on single-commodity flow. LP(meters): Linear Program relaxation based on multi-commodity flow.","acronyms":[[58,60]],"long-forms":[[65,79]]},{"text":"1 Int roduct ion   In a recent paper Boguraev and Levin (1990) point out inadequacies in common concep-  tions of what a Lexical Knowledge Base (LKB) should be, inadequacies which stem from  the assumption that a machine-readable dictionary (MRD) is not only the right source for ","acronyms":[[145,148]],"long-forms":[[121,143]]},{"text":" The SU detection task is conducted on two corpora: Broadcast News (BN) and Conversational Telephoned Speech (CTS).","acronyms":[[68,70]],"long-forms":[[52,66]]},{"text":"F is a frame name, E a frame element name, and t and s are sequences of word indices (t is for the target (FEE)) Using this measure of partial agreement, we now","acronyms":[[107,110]],"long-forms":[[91,105],[7,17],[29,41]]},{"text":"obtain a bilingual database. The database is  called the ATR Dialogue Database(ADD). ","acronyms":[[79,82]],"long-forms":[[57,77]]},{"text":"interchange of LRs. It also demonstrate how MLI  can be applied to Asian Language Resource (ALR)  through making the results of collaborative en-","acronyms":[[92,95],[15,18],[44,47]],"long-forms":[[67,90]]},{"text":"nese kanji and words. The currently available  JWAD Version 1 (JWAD-V1) consists of  104,800 free word association responses col-","acronyms":[[63,70]],"long-forms":[[47,61]]},{"text":"based chunking; 3. MEMM-based word segmenter with Support Vector Machines (SVM)-based chunking.","acronyms":[[75,78],[19,23]],"long-forms":[[50,73]]},{"text":"each other.  Normalized common neighbors (NCN). Nor-","acronyms":[[42,45]],"long-forms":[[13,40]]},{"text":"to be explained by a set of unobserved (latent) topics. Hidden Markov Model LDA (HMM-LDA) (Griffiths et al, 2005) is a topic model that simul-","acronyms":[[81,88]],"long-forms":[[56,79]]},{"text":"only limited discontinuities in each tree.  Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive","acronyms":[[75,79]],"long-forms":[[44,73]]},{"text":"tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al, 1998) has also been used to measure dis-","acronyms":[[91,94]],"long-forms":[[65,89]]},{"text":"mentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English,","acronyms":[[139,142],[163,166],[172,175]],"long-forms":[[122,130],[145,154]]},{"text":"http:\/\/www.ukp.tu-darmstadt.de Abstract In this paper, we present a machine learning approach for word sense alignment (WSA) which combines distances between senses in the graph representations of lexical-semantic resources","acronyms":[[120,123]],"long-forms":[[98,118]]},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is an important component in many information organization","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"ability of reordering models to capture this tag sequence in system translations. Popovic et al (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors.","acronyms":[[139,142],[165,168]],"long-forms":[[144,159],[170,206]]},{"text":"outer: the perceived external frame or point of referencing for  the action, event, or state as a whole  Means (MNS):  inboard: the perceived immediate affeetor or effeetor of the ","acronyms":[[110,113]],"long-forms":[[103,108]]},{"text":"We apply LDA on  the user-word matrix UW:  UW = UM * MW  , where UM is the user-hidden matrix, MW is the ","acronyms":[[48,50],[9,12],[38,40],[43,45],[53,55],[65,67],[95,97]],"long-forms":[[51,52],[21,37]]},{"text":"exist solely to guide processing. These expression, known  as operandi words (FW's), are quite common, and  include articles, prepositions, and auxiliary verbs.","acronyms":[[73,77]],"long-forms":[[57,71]]},{"text":"clude substitution, splitting and merging statistics. Given  an input (ASCII) word, and the above statistics, candidate  (corrupted) words are generated based on simulating and pro- ","acronyms":[[71,76]],"long-forms":[[61,69]]},{"text":"New York, N.Y. 10027  Introduction  COMET (COordinated Multimedia Explanations  Testbed) is an experimental system that generates inter- ","acronyms":[[36,41],[10,14]],"long-forms":[[43,86],[0,8]]},{"text":"Syntactic Tree  Vague Semantic  Expression (EFL)  Unambiguous Semantic ","acronyms":[[48,51]],"long-forms":[[36,46]]},{"text":"grammars is denoted CFGS.  For a linear indexed grammar (LIG),2 strings are derived from nonterminals with an associated","acronyms":[[56,59],[20,24]],"long-forms":[[32,54]]},{"text":"For Task 2-2, we design two kinds of evaluation metrics:  1) POS accuracy (POS-A)  This index is used to evaluate the performance ","acronyms":[[75,80]],"long-forms":[[61,73]]},{"text":" Phrasometer ? The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word","acronyms":[[40,42]],"long-forms":[[19,30]]},{"text":"4.2 Outcome Table 3 demonstrating the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL).","acronyms":[[85,90],[126,129],[140,147]],"long-forms":[[67,83]]},{"text":" 1 In t roduct ion :  a problem  Grammar development environments (GDE's) for  analysis and for generation have not yet come to- ","acronyms":[[67,72]],"long-forms":[[33,65]]},{"text":"tions. Following Bahdanau et al (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing long-distance depen-","acronyms":[[76,79]],"long-forms":[[54,74]]},{"text":"Another dictionary instruments deals wlth  unrecognized elements - the so-called  transducing dictionary (TD). TD re- ","acronyms":[[101,103]],"long-forms":[[77,99]]},{"text":" 2.3 Graphical User Interface The graphical user interface (GUI) is an important idiosyncrasies that has been recently add to Clairlib","acronyms":[[60,63]],"long-forms":[[34,58]]},{"text":"mvzaanen@uvt.nl Gerhard van Huyssteen Centre for Text Technique (CTexT) North-West University","acronyms":[[66,71]],"long-forms":[[38,64]]},{"text":"Given an occurrence of a word \u0002 in a natural language text, the task of word sense disambiguation (WSD) is to determine the correct sense of \u0002 in that context.","acronyms":[[99,102]],"long-forms":[[72,97]]},{"text":"The data are packaged as the payload of incremental units (IU) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that","acronyms":[[172,175],[59,61],[101,104],[199,203],[241,244]],"long-forms":[[154,170],[40,57],[181,197]]},{"text":"A mobile real-time speech-to-speech translation (S2ST) device is one of the grand challenges in natural language processing (NLP). It involves","acronyms":[[125,128],[49,53]],"long-forms":[[96,123],[19,47]]},{"text":"2013 Association for Computational Linguistics FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG","acronyms":[[151,154],[47,50],[202,205]],"long-forms":[[129,149]]},{"text":"an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun, in converting, is replaced by HD (head). Such unary branching NPs are add on superior of nouns","acronyms":[[109,111],[3,5],[42,44],[66,68],[141,144]],"long-forms":[[113,117],[20,40]]},{"text":"1 Introduction 1.1 Background Back in 2004, ETV (Eenadu Television), Chennai, felt a need for a texts editor to prepare news","acronyms":[[44,47]],"long-forms":[[49,66]]},{"text":"them, and finally generating and displaying them.  The Input Analyzer (IA) of the system is the  most stable end experimented component and it is ","acronyms":[[71,73]],"long-forms":[[55,69]]},{"text":"Notes also that there is some overlap of infer-  marion between the Lexical Plan analysis and  the Brandeis analysis, such as SUISCAT(TRAN)  and DO.","acronyms":[[136,140],[147,149],[128,135]],"long-forms":[]},{"text":"2 Symmetrical Tversky?s Ratio Model In the field of mathematical psychology Tversky proposed the ratio model (TRM) (Tversky, 1977) motivated by the imbalance that humans have on","acronyms":[[110,113]],"long-forms":[[93,108]]},{"text":" 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":" 1 Introduction Currently the Machine Translation (MT) researching community attempts to seamlessly integrate both","acronyms":[[51,53]],"long-forms":[[30,49]]},{"text":"O N S  *****  SCAN TERMED AT 1 I  ANTEST CALL'EC FOR 'I \"REDVO U (AACC) ,SD= 2 .  RES= 6.","acronyms":[[49,52],[66,70],[73,75],[82,85]],"long-forms":[[34,48]]},{"text":"  This approach has been employed in Augmentative  and Alternative Communication (AAC), in the  form of multimodal vocabularies in assistive de-","acronyms":[[82,85]],"long-forms":[[37,80]]},{"text":"Normalization (WCCN) (Dehak et al., 2011) and Eigen Factored Radial (EFR) (Bousquet et al., 2011).","acronyms":[[67,70],[15,19]],"long-forms":[[46,65]]},{"text":"Social media in general exhibit a rich variety of  information sources. Question answering (QA) has  been particularly amenable to social media, as it ","acronyms":[[92,94]],"long-forms":[[72,90]]},{"text":"2 Textual Entailment for MT Evaluation 2.1 Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (","acronyms":[[100,102],[25,27],[66,68]],"long-forms":[[80,98]]},{"text":"Else function such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive","acronyms":[[79,82],[95,98],[41,43],[64,66],[120,123]],"long-forms":[[69,77],[85,93],[32,39],[109,117],[46,62]]},{"text":" 2.2 LNRE Nature  o f  the  Data   The LNRE (Large Number of Rare Phenomena)  zoning (Chitashvili & Baayen, 1993) is defined as ","acronyms":[[39,43],[5,9]],"long-forms":[[45,72]]},{"text":"3.3 Aspect term extract Our approach for aspect term extraction is based on Conditional Random Spheres (CRF). The choice","acronyms":[[106,109]],"long-forms":[[79,104]]},{"text":"The majority of dependency-based features are constructed using the properties of edges and vertices along the shortest path (SP) of an entity pair. ","acronyms":[[126,128]],"long-forms":[[111,124]]},{"text":"1 In t roduct ion   For some NLP applications, it is important o  identify, \"named entities\" (NE), such as person  names, organization ames, time, date, or money ","acronyms":[[94,96]],"long-forms":[[77,91]]},{"text":"con. In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci). ","acronyms":[[84,90]],"long-forms":[[57,82]]},{"text":"Findings for the Mention-Pair Paragon 1 Foundations 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5 2 Bases+YAGO Typing (YUKON) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8 3 Foundations+YAGO Method (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9","acronyms":[[121,123],[204,206]],"long-forms":[[104,119],[192,202]]},{"text":"three modules: (1) a question processing (QP) module; (2) a passage retrieval (PR) module; and (3) an answer processing (AP) module. Questions","acronyms":[[121,123]],"long-forms":[[102,119]]},{"text":"aKemp|es  de  t. raL~uct lon  du  russe  eD f ranca ls  rea l  i s le  par   le  G~TA a Grenob le ,   MUTS-CLES:  TAO ( t raduct ion  ass i s t ,  co  par  o rd lnateur )e   ana lyse  morpno l  og ique~ t rans fer t  lex lca l~ 0enerat  1o~ ","acronyms":[[114,117],[81,85],[102,111]],"long-forms":[[120,168]]},{"text":"Data Annotated. The data to be annotated in WSsim-1 were taken primarily from Semcor (Miller et al1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea, Chklovski, and Kilgarriff 2004).","acronyms":[[147,151]],"long-forms":[[112,122]]},{"text":"any parser; the third requirement is not easy met  in all languages, but even in those languages where  nonrestrictives are not easily identifiable, (II)  work reasonable well.","acronyms":[[152,154]],"long-forms":[[137,149]]},{"text":"three items:  (1) a new formalism for logic grammars, which we  call modifier structure grammars (MSGs),  (2) an translator (or parser) for MSGs that takes all ","acronyms":[[99,103],[142,146]],"long-forms":[[70,97]]},{"text":"each new domain and scenario, as discussed in the next section.  The lexical analysis module (LexAn) is responsible for splitting the document into sentences, and the sentences into tokens.","acronyms":[[94,99]],"long-forms":[[69,85]]},{"text":"Learning dependency-based compositional semantics.  Throughout Association for Computational Linguistics (ACL). ","acronyms":[[98,101]],"long-forms":[[55,96]]},{"text":"Natural Language Generation (NLG). Per example, HALTS is a Natural Language Generation (NLG) system that generates tailored smoking cessation let-","acronyms":[[87,90],[29,32]],"long-forms":[[58,85],[0,27]]},{"text":"belief updating model. In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approaches","acronyms":[[81,85]],"long-forms":[[40,79]]},{"text":" The machine receives natural language input (text)  with referring expressions (RE), and possibly other  input (e.g. mouse clicks on a screen) with pseudo- ","acronyms":[[81,83],[46,50]],"long-forms":[[58,79],[39,44]]},{"text":"(Joachims, 1999) software). During it, we implemented: the String Kernel (SK), the Syntactic Tree Kernel (STK), the Superficial Semantic Tree Kernel","acronyms":[[70,72]],"long-forms":[[55,68]]},{"text":"In our particular application, access to Getty?s Art and Architecture Thesaurus (AAT), to other museum and collection databases or online auction cata-","acronyms":[[81,84]],"long-forms":[[49,79]]},{"text":"the Natural Language Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output.","acronyms":[[119,122],[33,36]],"long-forms":[[103,117],[4,31]]},{"text":"translators with the help of computer-aided translation paraphernalia (GATO), (3) rule-based MT systems (RBMT) and (4) statistical MT systems (SMT). ","acronyms":[[134,137],[63,66],[96,100]],"long-forms":[[110,132],[29,55],[73,86]]},{"text":" 658     We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x: a","acronyms":[[70,75]],"long-forms":[[46,68]]},{"text":"both in the form of documents and factual  databases. These knowledge sources (KSs) are  intrinsically heterogeneous and dynamic.","acronyms":[[79,82]],"long-forms":[[60,77]]},{"text":"AT(- +)  Stems to which the suffixes +ation and +ative may  attach are marked as (FOR +), while those taking the  corresponding formulas +ion and +ive are (AT -).","acronyms":[[82,86],[152,154],[0,2]],"long-forms":[[60,66]]},{"text":"such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrases(INF), and Verb Phrase(VP),  are grand structures with some.k~y ","acronyms":[[97,99],[29,33],[49,51],[75,78]],"long-forms":[[85,95],[8,28],[37,48],[54,74]]},{"text":"use the two cluster class features. The other selected features and the chosen algorithms (CL) are displayed in Table 1.","acronyms":[[91,93]],"long-forms":[[72,89]]},{"text":"be intuitively characterized as a way of trigger-  ing semantically related concepts which define for  each role the projective conclusion space (PCS). ","acronyms":[[146,149]],"long-forms":[[117,144]]},{"text":"traction) consists of two steps: keyphrase candidate extraction and the hereditary programming of keyphrase scoring actions (KSMs).1 3.1 Step 1: Keyphrase candidate extraction","acronyms":[[123,127]],"long-forms":[[95,121]]},{"text":"ically used as the purpose. For example, the NIST Open Machine Translation Evaluation (OpenMT) 2009 (Garofolo, 2009) constrained Arabic-English","acronyms":[[86,92],[44,48]],"long-forms":[[49,84]]},{"text":"2003). Another kind of verb-based multiword expression is light verb constructions (LVCs), such as the case in (1).","acronyms":[[84,88]],"long-forms":[[58,82]]},{"text":"mentary ASR systems, a technique first suggests in the context of NIST?s ROVER system (Fiscus, 1997) with a 12% relative error reduced (RER), and after widely employed in many ASR","acronyms":[[138,141],[8,11],[185,188]],"long-forms":[[112,136]]},{"text":"word-level characters described in Section 5.  4.1 Ranking by regression (RR) The first ranking strategies is based on training a re-","acronyms":[[72,74]],"long-forms":[[49,70]]},{"text":"from Association. ACM Transactions on Information Systems (TOIS) 21:315-346. ","acronyms":[[59,63],[18,21]],"long-forms":[[22,57]]},{"text":"Second, we demonstrate correlation to a database of real-world international disputing events, the Militarized Interstate Quarrel (MID) dataset (Jones et al, 1996).","acronyms":[[130,133]],"long-forms":[[98,128]]},{"text":"SEMANTICS. The main component of SeSyn is a rule system (the syntax) which transforms the Semantic Analysis  (SA) of any given sentence into a Surface Structure (SS) of that sentence. The SAs represent meanings ina higher order ","acronyms":[[162,164],[33,38],[110,113],[188,191]],"long-forms":[[143,160],[90,107]]},{"text":"A closer, more detailed, look at the  LOCATION data suggests that he high payoff indicated  by the average REC and precision (PRE) scores was  achieved because most of the data were listable.","acronyms":[[126,129],[107,110]],"long-forms":[[115,124]]},{"text":"In Proceedings of the 5th International Conference on Language Resources and Evaluating (LREC), pages 1262?1267, Genoa, Italy, May.","acronyms":[[89,93]],"long-forms":[[54,87]]},{"text":"didate substitutes, as described below.  Lexical Baseline (LB): During this approach we use the pre-existing lexical resources to supplying a rank-","acronyms":[[59,61]],"long-forms":[[41,57]]},{"text":"Their study with three different learners ? na??ve Bayes, maximum entropy (MaxEnt) and the support vector machine (SVM) ?","acronyms":[[75,81],[115,118]],"long-forms":[[58,73],[91,113]]},{"text":"at meeting the needs of CSR research, and at serving in  a complementary ole to the corpora being collected in  the interactive Air Travel Information System (ATIS) do-  main.","acronyms":[[159,163],[24,27]],"long-forms":[[128,157]]},{"text":"semantic tree setups are depicted as follows:  TP2TP1 (a) Baggies Of Features(BOF) ENT","acronyms":[[74,77],[47,53],[79,82]],"long-forms":[[58,72]]},{"text":"Following the ideas of (Collobert et al, 2011), Zeng et al (2014) first solve relation ranking used convolutional neural network (CNN). The","acronyms":[[138,141]],"long-forms":[[108,136]]},{"text":"2. Define of Stochastic Context-Free Grammars  We will now define stochastic ontext free grammars (SCFGs) and establish some  notation.","acronyms":[[103,108]],"long-forms":[[70,101]]},{"text":"The metrics Precision (P), Recall (R),  F-score (F) (F=2PR\/(P+R)), Recall of OOV  (ROOV) and Recall of IV (RIV) are uses to  assessments the results.","acronyms":[[107,110],[83,87]],"long-forms":[[93,105],[12,21],[27,33],[40,47],[67,80]]},{"text":"low navigational directions. In Proceedings of the Association for Computational Linguistics (ACL), 2010.","acronyms":[[94,97]],"long-forms":[[51,92]]},{"text":"processing. This paper explores grammatical issues in Scottish Gaelic by methods of dependency tagging and combinatory categorial grammatical (CCG), which we see as complementary approaches. As such it","acronyms":[[137,140]],"long-forms":[[105,135]]},{"text":"ity. We presented an assessing of these parameters for preposition sense disambiguation (PSD). ","acronyms":[[90,93]],"long-forms":[[56,88]]},{"text":"SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Rule-based","acronyms":[[45,47],[0,3],[26,28]],"long-forms":[[50,66],[6,25],[31,44]]},{"text":"inspect  ? Product feature level (PFL)  ?","acronyms":[[33,36]],"long-forms":[[10,31]]},{"text":"researches on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) phrase almost in news do-","acronyms":[[116,119],[9,12],[82,85],[97,100],[127,130],[144,147]],"long-forms":[[103,114],[75,81],[88,96],[122,126],[136,143]]},{"text":"LOC(at. ACROSS) The ACT (Actor) can be any noun in the subjective instance (the abbreviation n), the PAT (Patient)","acronyms":[[16,19],[0,3],[93,96]],"long-forms":[[21,26],[98,105]]},{"text":"C STORE SPEECH WAVE POINT  C  NBUF (NPT) =IFIX(YN)  750 UNINTERRUPTED ","acronyms":[[36,39],[47,49]],"long-forms":[[30,34]]},{"text":"for alignments.  Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art per-","acronyms":[[43,46]],"long-forms":[[17,41]]},{"text":"We use multiple epochs of minibatch stochastic gradient descent and update all parameters to minimize the untoward logging likelihood (NLL) of our training set.","acronyms":[[131,134]],"long-forms":[[106,129]]},{"text":"strat1 40.1 24.4 15.0 strat2 38.2 22.5 14.5 Tableau 4: Word Mistakes Rate (WER), Idea Error Rate (CER) and Interpretation Error Rate (IER) ac-","acronyms":[[70,73],[96,99],[132,135]],"long-forms":[[53,68],[76,94],[105,130]]},{"text":"These are the second-order prepositional complement (PC) and directional complement (LD) relations, and the first-order direct object (OBJ1) and subject (SU) relations. Finally, the setting SU+OBJ1 joins words obtained from subject","acronyms":[[154,156],[53,55],[85,87],[135,138]],"long-forms":[[145,152],[27,51],[127,133]]},{"text":"transcripts. The print news consisted of 22 New York Times (NYT) articles from January 1998.","acronyms":[[60,63]],"long-forms":[[44,58]]},{"text":"Table 4: Entailment judgment in closed essays  of mutual information (T=True, F=False,  MI=mutual information). ","acronyms":[[86,88]],"long-forms":[[89,107],[70,74],[78,83]]},{"text":"The most successful stochastic language models  have been base on finite-state descriptions such  as n-grams or hides Markov models (HMMs)  (Jelinek et al, 1992).","acronyms":[[135,139]],"long-forms":[[113,133]]},{"text":"ductive transfer learning. In Proceedings of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Bio-","acronyms":[[95,99],[49,53]],"long-forms":[[54,93]]},{"text":"the verb that contained in a subordinate clause.  We use semantic role labeling (SRL) to help  solve this problem in which the coordinated can ","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"0 ROOT Figure 2: Example of a CoNLL-style annotated sentence. Each word (FORM) is numbered (ID), lemmatized (LEMMA), annotated with two levels of part-of-speech tags (CPOSTAG and POSTAG), annotated with morpho-","acronyms":[[92,94],[30,35],[73,77],[109,114],[167,174],[179,185]],"long-forms":[[79,90],[97,107],[146,165]]},{"text":"2214  Proceedings of the of the EACL 2014 Seminars on Conversation in Motion (DM), pages 1?9, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[74,76],[32,36]],"long-forms":[[54,72]]},{"text":"3.4 Innovative Features of SPTK The most similar kernel to SPTK is the Syntactic Semantic Tree Kernel (SSTK) proposed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Mos-","acronyms":[[103,107],[27,31],[59,63]],"long-forms":[[81,101]]},{"text":"egories (left side) and with respect to the assessor?s own expertise (right side). ( Key: B=beneficial, LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE) the assessor had no idea from which condition an","acronyms":[[150,152],[104,106]],"long-forms":[[153,172],[92,102],[107,124],[128,137],[141,148],[176,183],[187,190]]},{"text":"(6) health issues (HI) trend other (7) personal issues (PI) trend decreasing (8) lectures attended (LA) trend other (9) revision (R) trend decreasing","acronyms":[[100,102],[19,21],[56,58]],"long-forms":[[81,98],[4,16],[39,53],[120,128]]},{"text":"This representation, together with target language words, are fed to a deep neural network (DNN) to shape a strongest NNJM.","acronyms":[[92,95],[116,120]],"long-forms":[[71,90]]},{"text":"are not very demanding on resources: Inverse Consultation (IC) (Tanaka and Umemura, 1994) and Distributional Similarity (DS) (Kaji et al, 2008), their strong points and weaknesses, and proposed","acronyms":[[121,123],[59,61]],"long-forms":[[94,119],[37,57]]},{"text":"For Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 217?220, Va?xjo?. ","acronyms":[[73,76]],"long-forms":[[38,71]]},{"text":"Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus. ","acronyms":[[146,148],[15,17],[36,39],[80,85],[120,128],[181,183]],"long-forms":[[149,168],[18,34],[40,67],[86,111],[129,137],[184,186]]},{"text":"ple sentences. Such knowledge is also important for Textual Entailment (TE), a generic framework for model semantic inference.","acronyms":[[72,74]],"long-forms":[[52,70]]},{"text":"Associative Texture Is Lost For Translation. In Proceedings  of the Seminars on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, ","acronyms":[[113,120],[137,140]],"long-forms":[[79,111]]},{"text":"ble. The model makes heavy utilise of single-category Ambiguity Categories (AC)3, which (being independent on the tagger?s intermediate decisions) can be","acronyms":[[69,71]],"long-forms":[[50,67]]},{"text":"294  The surface phonetic tones are:  LC = low constant (in Baule, only initial)  HC = high permanent (only initial) ","acronyms":[[38,40],[82,84]],"long-forms":[[43,55],[87,100]]},{"text":"5.1 Data  We have two kinds of training data from general  domain: Labeled Data (LD) and Unlabeled Data  (UD).","acronyms":[[81,83],[106,108]],"long-forms":[[67,79],[89,103]]},{"text":" The LRS representative of (1) is shown in Figure 1, where INCONT (INTERNAL CONTENT) encodes the nuclei semantic contribution of the head, EXCONT","acronyms":[[59,65],[5,8],[137,143]],"long-forms":[[67,83]]},{"text":" First, we employ a multiple output GP basis on the Intrinsic Coregionalization Model (ICM) (","acronyms":[[87,90],[36,38]],"long-forms":[[52,85]]},{"text":"the natural (CC natural) and strong (CC strong)  levels; and (b) advanced level texts from a popular  science magazine phoned Ci?ncia Hoje (CH). Tableau ","acronyms":[[140,142],[13,15],[37,39]],"long-forms":[[126,138]]},{"text":" 2 System Overview Our system, named PML Tree Query (PML-TQ), consists of three main components (discussed fur-","acronyms":[[53,59]],"long-forms":[[37,51]]},{"text":"email: elenimi@linc.cis.upenn.edu Jerry R. Hobbs University of Southern California (USA) email: hobbs@isi.edu","acronyms":[[84,87]],"long-forms":[[49,82]]},{"text":"monly used tasks: small vocabulary recognition (TI-digits), read and spontaneous text dictation (WSJ), and goal-oriented spoken dialog (ATIS). The broadcast news task is quite general, covering a","acronyms":[[136,140],[48,57],[97,100]],"long-forms":[]},{"text":"Together NN + LR (w\/o alternate grammar) 54.38 41.90 Together NN + LR (w\/o synthetic data) 53.98 42.41 Tabled 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our system with numerous configurations.","acronyms":[[144,146],[173,175],[9,16],[62,69]],"long-forms":[[128,142],[152,171]]},{"text":"appliance learning tasks. We used character n-grams, word n-grams, Parts of Speech (POS) tag n-grams, and perplexity of characters trigrams as features.","acronyms":[[82,85]],"long-forms":[[65,80]]},{"text":"with common sensing knowledge.       Natural language processing (NLP) techniques  such as part of speech tagging and parse tree gen-","acronyms":[[64,67]],"long-forms":[[35,62]]},{"text":"{EVENT 2 {AND {SUBTYPE DIE} {PERSON  $foo}}}  2.4 Graphical User Interface (GUI)  For some applications such as database ","acronyms":[[76,79]],"long-forms":[[50,74]]},{"text":"Y? >l LY?l, s.t. SYl = SY?l (1) 1","acronyms":[[17,20]],"long-forms":[[23,27]]},{"text":"details of this collection.  AECMA Simplified English (AECMA-SE) (AECMA 1986) was the predecessor of ASD Simplified Technical Frenchman.","acronyms":[[55,63],[66,71],[101,104]],"long-forms":[[29,53]]},{"text":"VBL (Light Verb) is used in complex predicates (Butt 1995), but its syntactic similarity with  VB (Verb) is a major source of confusion in automatic tagging.","acronyms":[[95,97],[0,3]],"long-forms":[[99,103],[5,15]]},{"text":"ing measure of the loss in modeling accuracy:     Likely Loss (PL):   )()()(),( vuvuvu +?","acronyms":[[68,70]],"long-forms":[[50,66]]},{"text":"egorization. ACM Transactions on Information Systems (TOIS), 12(3):233?251. ","acronyms":[[54,58],[13,16]],"long-forms":[[17,52]]},{"text":" 3.3 Linear-chain CRF for Extraction The alignment CRF (AlignCRF) model described in Section 3.1 is able to predict labels for a text","acronyms":[[56,64],[18,21]],"long-forms":[[41,54]]},{"text":"z that maps sentences x to logical expressions z. We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xi, zi)|i = 1 . . .","acronyms":[[106,110]],"long-forms":[[87,104]]},{"text":"Data. We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009).","acronyms":[[85,91]],"long-forms":[[63,83]]},{"text":"precision, recall and f-measure. Precision measures the number of correct Named Entities(NEs) in the 107","acronyms":[[89,92]],"long-forms":[[74,87]]},{"text":"Haile) and (?????, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al, 2009a).","acronyms":[[98,100],[135,138],[185,188],[189,191],[30,32]],"long-forms":[[75,96],[106,133]]},{"text":"text refer to the same entity in veritable international or not.  Noun Phrase CRR (NP-CRR) considers all noun  phrases as entities, while Named Entity CRR ","acronyms":[[70,76],[138,141]],"long-forms":[[53,68]]},{"text":"interpreters. In International Lectures on Learning Representations (ICLR). ","acronyms":[[71,75]],"long-forms":[[17,69]]},{"text":"2011a.  Overview of the Infectious Disease (ID) task of BioNLP Shared Task 2011.","acronyms":[[45,47],[57,63]],"long-forms":[[24,43]]},{"text":"milieu method in DUC 2005. Proceedings of the 5th Document Understanding Conference (DUC). Van-","acronyms":[[82,85],[14,17]],"long-forms":[[47,80]]},{"text":"(Associativity)  \\[A\\[BC\\]\\] = \\[\\[AB\\]C\\]  (A(BC)) = ((AB)C)  (L, -singleton bidirectionality) ","acronyms":[[56,60]],"long-forms":[[45,49]]},{"text":"Figure 3: A Graphical Representation of the Infinite Trees Model than a simple Dirichlet process (DP)2 (Ferguson, 1973) is that we have to introduce coupling in","acronyms":[[97,99]],"long-forms":[[78,95]]},{"text":"(disharmonic) combinators to broadened the expressive power of the model.  \u0001 KZGS10 (Kwiatkowski et al2010) uses a restricted higher-order monotheism procedure, which iteratively breaks up a logical form into","acronyms":[[76,82]],"long-forms":[[84,105]]},{"text":"Hidden topic markov models. In Artificial Intelligence and Statistics (AISTATS), San Juan, Puerto Rico. ","acronyms":[[71,78]],"long-forms":[[31,69]]},{"text":"This work investigated four well-known specifications created by four different organizations: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (Beijing)","acronyms":[[112,114],[147,152]],"long-forms":[[95,110],[117,132]]},{"text":"Table 9 (Hindi). Here, precision measurements the number of correct Named Agencies (NEs) in the machine tagged file over the total number of NEs in the ma-","acronyms":[[80,83],[137,140]],"long-forms":[[64,78]]},{"text":"ings. They evaluate their work against the SENSEVAL 2 AAAAH test data (SV2AW). They tune the","acronyms":[[68,73]],"long-forms":[[43,56]]},{"text":"tween arguments. We thus propose to model the reranking phase (RR) as a HMM sequence labeling chore.","acronyms":[[63,65],[72,75]],"long-forms":[[46,55]]},{"text":"case. In this paper, we investigate using Amazon?s Mechanical Turk (MTurk) to make MT test sets cheaply.","acronyms":[[68,73]],"long-forms":[[51,66]]},{"text":"meaning of the other. Examples are:  * Senior research assistant at the Belgian National Fund for Scientific Research (F.N.R.S.). ","acronyms":[[119,127]],"long-forms":[[89,117]]},{"text":"Whilst named entity recognition (NER) and relating or event extraction are regarded as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more","acronyms":[[128,130]],"long-forms":[[104,126]]},{"text":"for alignments.  Repetitive neural network (RNN)-based models have recently demonstrated state-of-the-art per-","acronyms":[[43,46]],"long-forms":[[17,41]]},{"text":" verb. The third is termination position (EP), after a predi-  cate.","acronyms":[[34,36]],"long-forms":[[20,32]]},{"text":"of binary relations that differ in length and since a  question representation (SRq) can be answered by a  sentence candidate (SRc) that includes more information  than the question specified, the Arity restrictions i~ revisited ","acronyms":[[125,128],[78,81]],"long-forms":[[105,123],[53,76]]},{"text":"Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate)) from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive byphrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate pronoun, REFL=Anaphoric reference by reflexive pronoun).","acronyms":[[203,208],[126,130],[151,154],[163,166],[180,184],[249,254],[297,301]],"long-forms":[[209,239],[155,161],[167,175],[185,192],[255,287],[325,334]]},{"text":"Each accuracy measure is shown in a column, including the segmentation F-score (SF ), the overall tagging 894","acronyms":[[80,82]],"long-forms":[[58,72]]},{"text":"and opportunities. In Proceedings of the 1st International Temporal Web Analytics Workshop (TWAW), pages 1?8.","acronyms":[[92,96]],"long-forms":[[59,90]]},{"text":"51 Table 1: Semantic restrictions on Task 2 incidents arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model of Anatomy, other ontologies identify in the text.","acronyms":[[61,64],[88,91]],"long-forms":[[67,86],[94,123]]},{"text":"We set aside the blind TEST set for evaluating the final performance of our named entity recognition (NER) and relation extraction (RE) 2http:\/\/code.google.com\/apis\/ajaxsearch","acronyms":[[132,134],[102,105]],"long-forms":[[111,130],[76,100]]},{"text":"RERANKED 56.2 13.5 57.3 12.7 ORACLE 85.0 70.3 80.4 60.0 Table 2: Word accuracies and error rate reductions (ERR) in percentages for English-to-Japanese MTL augmented","acronyms":[[108,111],[152,155]],"long-forms":[[85,106]]},{"text":"These templates, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic modelling, or, used the term we will adopts, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree","acronyms":[[174,178]],"long-forms":[[142,172]]},{"text":"Many investigator have attempted dissimilar tech-  niques to deal with extragrammatical sentences  such as Augmented Transition Network(ATN)  (Kwasny and Sondheimer, 1981), network-based ","acronyms":[[132,135]],"long-forms":[[103,130]]},{"text":"HowNet and Its Computation of Mean. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, meters., Pet, metres. and Soria, C. (2009). Multilingual funds for NLP in the lexical markup framework (LMF). In journal of Language Resources and Evaluation, March 2009, Volume 43, pp.","acronyms":[[240,243]],"long-forms":[[214,238]]},{"text":"below.  4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary English","acronyms":[[38,41]],"long-forms":[[14,36]]},{"text":"category. The following transfer knowledge involves  sets of three common ouns (CNs):  3A' is the transferred expression of A ","acronyms":[[80,83]],"long-forms":[[67,78]]},{"text":"should not be allowed?.  Gay Marriageable (GM) 5","acronyms":[[39,41]],"long-forms":[[25,37]]},{"text":"guided learns. The approaching taken has been to en-  code an artificial neural network (ANN) in a genome  which stores its architecture and learning rules.","acronyms":[[88,91]],"long-forms":[[61,86]]},{"text":"4.2 Proposed Model : PNB (vs. UM) Figure 1 shows the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the","acronyms":[[109,112],[21,24],[153,155],[160,163],[30,32]],"long-forms":[[89,107]]},{"text":"2 Symmetrical Tversky?s Ratio Model In the field of mathematical psychology Tversky recommendation the ratio paragon (TRM) (Tversky, 1977) motivated by the imbalance that humans have on","acronyms":[[110,113]],"long-forms":[[93,108]]},{"text":"(O?Shaughnessy, 2000), lip aperture (LA) is the normalized Euclidean distance between the lips, and lip protrusion (LP) is the normalized 2nd principal component of the midpoint between the lips.","acronyms":[[116,118],[36,39]],"long-forms":[[100,114],[23,35]]},{"text":" 4.2 Data  We used the Wall Street Journal (WSJ) of the years  88-89.","acronyms":[[44,47]],"long-forms":[[23,42]]},{"text":"We then build three pairwise comparison matrices: one comparing pairs of typically developing (TD) children; one comparing pairs of children with ASD; and a third com-","acronyms":[[95,97],[146,149]],"long-forms":[[73,93]]},{"text":"1 Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tan-","acronyms":[[98,101]],"long-forms":[[65,96]]},{"text":"The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al.,","acronyms":[[104,106],[63,66]],"long-forms":[[78,102],[42,61]]},{"text":" ? Fondazione Bruno Munoz (FBK-irst), Italy ?","acronyms":[[29,37]],"long-forms":[[3,27]]},{"text":"Since we are going to be  concerned with definability, we first translate CFGs  into CFTs (Backgrounds Free Theories). The ","acronyms":[[85,89],[74,78]],"long-forms":[[91,112]]},{"text":"take scope over another.  Those natural language processing (NLP) systems that have manage to supplying some sort of account of quantifier scope preferences have done so by using a separated","acronyms":[[61,64]],"long-forms":[[32,59]]},{"text":"proven sentiment classification. In Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[100,103]],"long-forms":[[57,98]]},{"text":"minimal set of defaults.  A Preferential Default Description Logic (PDDL)  based on weigthed defaults has been developed in ","acronyms":[[68,72]],"long-forms":[[28,66]]},{"text":" In addition, NE alignment can be very useful for  Statistical Machine Translation (SMT) and CrossLanguage Information Retrieval (CLIR).","acronyms":[[84,87],[14,16],[130,134]],"long-forms":[[51,82],[93,128]]},{"text":"(iv) Embedded appositional phrases.  (2) Very long PE's (Phrasal Elements) appear occasionally. ( eg.","acronyms":[[51,55]],"long-forms":[[57,73]]},{"text":" 2.2 Thread-level analysis Next, we perform named entity recognition (NER) over each thread to identify entities such as package","acronyms":[[70,73]],"long-forms":[[44,68]]},{"text":"NNS?, in this paper; other worked makes a distinction between ESL (Anglais as a Second Language) speakers (who live and speak in a primarily English-speaking environment) or EFL","acronyms":[[60,63],[0,4],[172,175]],"long-forms":[[65,93]]},{"text":"question answering (QA). However, few models in ad hoc information retrieval (IR) using paths for document ranking due to","acronyms":[[78,80],[20,22]],"long-forms":[[55,76],[0,18]]},{"text":"afforded threshold, this binary features fires.  4.3 Variant Feature (VAR) In the variant cipher, the plaintext is written into","acronyms":[[66,69]],"long-forms":[[49,64]]},{"text":" 2 Graphical Model Framework A Graphical Model (GM) represents a factorization of a family of joint probability distributions over a","acronyms":[[48,50]],"long-forms":[[31,46]]},{"text":"Translation Equivalents and Semantic  Relations   Note that two translation equivalents (TE)  in a doublet of languages standing in a lexical semantic ","acronyms":[[89,91]],"long-forms":[[64,87]]},{"text":"posed web-based semantic similarity measures: Jaccard, Dice, Overlap, PMI (Bollegala et al, 2007), Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007), Sahami and Heil-","acronyms":[[127,130],[70,73]],"long-forms":[[99,125]]},{"text":"Between the NEs we select six of them as the recognizes objects, that is, personal designation (PN), date or time (DT), location name (LN), team name (TN), competition title (CT) and per-","acronyms":[[106,108],[126,128]],"long-forms":[[92,104],[111,124]]},{"text":"2007; Noh and Pad?o, 2013).  2.2 Entailment Core (EC) The Entailment Core perform the actual entail-","acronyms":[[50,52]],"long-forms":[[33,48]]},{"text":"phases that are performed sequentially without feedback: Question Processing (QP), Passage Retrieval (PR) and Answer Extraction (AE). More","acronyms":[[129,131],[78,80],[102,104]],"long-forms":[[110,127],[57,76],[83,100]]},{"text":"word if it surpassed a certain threshold.  4.2 LDA Graph Method (LDA-GM) The LDA-GM algorithm creates a similarity graphs","acronyms":[[63,69],[75,81]],"long-forms":[[45,61]]},{"text":"( NN?? ) speech 1:1 ( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1 ( NP ( NR?? ) (","acronyms":[[34,36],[2,6],[59,61],[64,68]],"long-forms":[[22,29]]},{"text":" 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilis-","acronyms":[[49,52]],"long-forms":[[26,47]]},{"text":" 2.2 Human Intelligence Tasks A Human Intelligence Task (HIT) is a short paid task on MTurk.","acronyms":[[57,60]],"long-forms":[[32,55]]},{"text":"match with a supposition. These weights are  confidence measures: Logical Sufficiency (LS)  and Logical Necessity (LN).","acronyms":[[86,88],[114,116]],"long-forms":[[65,84],[95,112]]},{"text":"error criteria: ? WER (word error rate): The WER is computed as the minimum","acronyms":[[18,21],[45,48]],"long-forms":[[23,38]]},{"text":"The begin point for the approach followed here was a dissatisfaction with certain  aspects of the doctrines of quasi-logical form as outlined in Alshawi (1990, 1992), and  implemented in SRI's Core Linguistics Engine (CLE). In the CLE-QLF approach, as ra- ","acronyms":[[216,219],[188,192],[229,236]],"long-forms":[[194,214]]},{"text":"There is no person boiling noodles A woman is boiling noodles in water Example 9051 (ENTAILMENT) A pair of kids are sticking out blue and green colored tongues","acronyms":[[85,95]],"long-forms":[[71,83]]},{"text":"Associative Texture Is Lost In Translation. In Proceedings  of the Workshop on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, ","acronyms":[[113,120],[137,140]],"long-forms":[[79,111]]},{"text":"step, we implement the CFG IDENTIFICATION to the rope  under (2) in order to \"transmutation\" the sequence of simple  syntactic units into so-called Segmentation Units (SU)  \\[we use the following conventions: \"( )\" for facultativi- ","acronyms":[[162,164],[19,22]],"long-forms":[[142,160]]},{"text":"The pair of loudspeakers KI-KA is in the superior quin-tile (>13.6%). Bases on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which maggio be read as follows: speakers KI and KA have the maximum level of expressive disagreement in dialogue-1. This measure is supplement by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a centigrade of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of loudspeakers, the CDX values are as-signed to each group participants and indicate the degree of disharmony that each person generates.","acronyms":[[338,341],[21,26],[198,200],[205,207],[484,487],[534,537]],"long-forms":[[307,336]]},{"text":"\u0000 OBST(obstacle): The boy stumbled over a stumb.  \u0000 INTT (intent): He came there to look for Jane. ","acronyms":[[52,56],[2,6]],"long-forms":[[58,64],[7,15]]},{"text":"2 We utilizing a new related measure, which we call the overall percentage error shrinking (OPER), that uses the entire area under the curves given by","acronyms":[[86,90]],"long-forms":[[50,84]]},{"text":" In Proceedings of the 3rd Internationally Conference on Language Resources and Evaluation (LREC), pages 1698?1703.","acronyms":[[90,94]],"long-forms":[[55,73]]},{"text":"even for very high-dimensional data.  4.2 Open Directory Project (ODP) Opened Directory Project (ODP)7 is a multilingual","acronyms":[[66,69],[95,98]],"long-forms":[[42,64],[71,93]]},{"text":"Total 2,910 1,086 3,996 Table 1: Number of annotated elements for categories in our gold standard (CR=controlled requirements, UR=uncontrolled requirements)","acronyms":[[97,99],[125,127]],"long-forms":[[100,123],[128,140]]},{"text":"recording  information pertinent to treatment of a patient that consists of a number of subsections such as Chief Complaint (CC), History of Present Illness (HPI),","acronyms":[[125,127],[158,161]],"long-forms":[[108,123],[130,156]]},{"text":"probabilistic Earley?s, and minimum edit distance algorithms). Dynamic programming (DP) involves solving certain kinds of recursive equations","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":"for evaluating the ASR,  2. Concept F-measure (ConF) ? the F-measure of ","acronyms":[[47,51],[19,22]],"long-forms":[[28,37]]},{"text":"However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al, 2001), can compute word indices pairs?","acronyms":[[125,128]],"long-forms":[[99,123]]},{"text":"As shown in the table, all models perform equally well on identification, which is ascertained by the frame matcher (FM); i.f., any excerpt argument receiving one or more candidate roles is ?","acronyms":[[116,118]],"long-forms":[[101,114]]},{"text":"Frustration Frustrated (F), Neutral (N), Correctness Correct (C), Incorrect (I) Partially Correct (PC) Percent Correct 50-100% (High), 0-50% (Low)","acronyms":[[99,101]],"long-forms":[[80,97],[12,22],[28,35],[41,60],[66,75]]},{"text":"Th~se de l'Universitt~ Joseph  Fourier, Grenoble I, Mars 1990  \\[8\\] TEl (Text Encoding Initiative), Guidelines for the  Encoding and lnterchange of Machine Readable Texts.","acronyms":[[69,72]],"long-forms":[[74,98]]},{"text":" 658     We investigates the effect of thyroid transcription factor 1 (TTF-1) ...x: a","acronyms":[[70,75]],"long-forms":[[46,68]]},{"text":"4.4 Word Sense Induction In this section, we present an evaluation of our model on the word sense induction (WSI) tasks. The","acronyms":[[109,112]],"long-forms":[[87,107]]},{"text":"for each word in the DAL. ( e.g., the verb definition for LOL (laugh out loud) in Wiktionary is ? To laugh","acronyms":[[58,61],[21,24]],"long-forms":[[63,77]]},{"text":"concept, BLESS contains several relata,  connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER),  meronymy (MERO) or no-relation (RANDOM-N).2 ","acronyms":[[118,123]],"long-forms":[[107,116]]},{"text":"  1 Introduction  Many Natural Language Processing (NLP)  applications need to recognize when the meaning ","acronyms":[[52,55]],"long-forms":[[23,50]]},{"text":"the traditional k-nearest neighbor (kNN) algorithm.  Maximum a posteriori (MAP) principle is used to determine which emotion set is related to the giv-","acronyms":[[75,78],[36,39]],"long-forms":[[53,73],[16,34]]},{"text":"Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes.","acronyms":[[123,127]],"long-forms":[[98,121]]},{"text":"ing different training methods. The effects of discriminative training (CRF) and extended feature sets (lower section) are more than additive.","acronyms":[[72,75]],"long-forms":[[50,70]]},{"text":"mark of language attainment at different stages of learning. The English Profile (EP)2 research programme aims to enhance the learning, teaching","acronyms":[[82,84]],"long-forms":[[65,80]]},{"text":"Table 1 FactBank annotation scheme. CT = certain; PR = probable; PS = possible; U = underspecified; + = positive; ?","acronyms":[[36,38],[50,52],[65,67]],"long-forms":[[41,48],[55,63],[70,78],[84,98],[104,112]]},{"text":" The algorithm was first proposed by the Institute for Computer Science  and Technology (ICST) of the National Bureau of Standards (NBS') in 1973. ","acronyms":[[132,136],[89,93]],"long-forms":[[102,130],[41,87]]},{"text":"Deployments bureaucratic, enabled fastest deployment of locally tested charac-ters to highly available internet servers as well as reviewed and data warehousing functions for both analytic and refineries purposes. The information model is implemented in a re-lational database that fully specifies, relates and enables inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also enabled asks expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime.","acronyms":[[420,423]],"long-forms":[[385,418]]},{"text":"category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operational Gastrointestinal, RAD = Radiology and ","acronyms":[[75,77],[27,29],[52,56],[103,105],[136,139]],"long-forms":[[80,100],[32,49],[59,73],[118,134],[142,151]]},{"text":" 2 Extended typed  A-ca lcu lus   CU(\\] (Categorial U,,ificAtion (l:ra,nma,r) \\[8\\] is a,d-  vantageous, compared to other expression structured ","acronyms":[[34,36]],"long-forms":[[41,64]]},{"text":"  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787?798, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF","acronyms":[[105,109],[84,86],[130,132],[173,177]],"long-forms":[[91,103]]},{"text":"of Data-to-Speech systems have been and are be-  ing formulated on the basis of D2S. Examples are  the Dialing Your Disc (DYD)-system, which presents  information in Frenchman about Mozart compositions ","acronyms":[[118,121],[79,82]],"long-forms":[[102,116]]},{"text":"of which are limited in scope. Ours here restraining inferables to the particular subset de-  fined by Hahn, Markert, and Strube (1996), which we call functional anaphora (FA). ","acronyms":[[167,169]],"long-forms":[[146,165]]},{"text":"ing the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).","acronyms":[[70,74],[111,116]],"long-forms":[[59,63],[88,93]]},{"text":"larities of MSA. ARET has deux subparts tools : the  Arabic Reading Facilitation Tool (ARFT) and the  Arabic Reading Assessment Tools (ARAT).","acronyms":[[86,90],[17,21],[12,15],[133,137]],"long-forms":[[52,84],[101,131]]},{"text":"edge associated with it.  Definition 3 (Informative Feature Extraction (IF)) We define the Informative-Features(IF ) feature","acronyms":[[72,74],[112,114]],"long-forms":[[40,59],[91,110]]},{"text":"representation, a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (HMM) for language modeling.","acronyms":[[131,134],[55,61]],"long-forms":[[110,129],[18,53]]},{"text":"state%1:03:00; ? AB = abstraction%1:03:00 \\ (event%1:03:00 ? state%1:03:00).","acronyms":[[17,19]],"long-forms":[[22,41]]},{"text":"sity is developing the Anglais Resource Grammar, an HPSG grammar for Frenchman, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).","acronyms":[[123,128],[52,56]],"long-forms":[[95,121]]},{"text":"ture. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). ","acronyms":[[91,95],[13,16]],"long-forms":[[75,89]]},{"text":"special case describing whether\/t \/  is glottalized. A context other than preceding phoneme and following  phoneme is incorporated; the first split (nodes 0 and 10) in this tree is on syllable boundary (SYLL-BDRY),  indicating that when \/ t \/  is glottalized it is generally in syllable-final position.","acronyms":[[203,212]],"long-forms":[[184,201]]},{"text":"(Bikel et al, 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Assist Vector Machines (SVM) (Asahara and Matsumoto, 2003), and similarly semi-supervised","acronyms":[[132,135]],"long-forms":[[107,130]]},{"text":" 1 Introduction  Feature term formalisms (FTF) have proven extremely  useful for the declarative representation f linguistic ","acronyms":[[42,45]],"long-forms":[[17,40]]},{"text":"Founded on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their com-","acronyms":[[52,58],[9,17]],"long-forms":[[60,75]]},{"text":"One means of achieving a fiat structure with extrinsic  orders is by utilizing the ID\/LP formalism, a subformalism of  GPSG that allows immediate dominance (ID) information to be  specified separately from linear precedence (LP) notion. (","acronyms":[[155,157],[81,86],[117,121],[223,225]],"long-forms":[[134,153],[204,221]]},{"text":"NP and selection of the head NP by the relative 1The following abbreviations are used in glosses: NOM = nominative, ACC = accusative, PRES = non-past and POT = potential. ( )","acronyms":[[116,119],[29,31],[0,2],[98,101],[134,138],[154,157]],"long-forms":[[122,132],[104,114],[160,169]]},{"text":"2 Data Into this study, we utilized a compiling of blog posts from five blogs: Carpetbagger(CB)1, Daily Kos(DK)2, Matthew Yglesias(MY)3, Red State(RS)4, and Right","acronyms":[[86,88],[102,104],[125,127],[141,143]],"long-forms":[[73,84],[92,100],[108,124],[131,140]]},{"text":"CHBWGE, H l V E  CSERCH ONTO HERGEF IN 10  combine specifics  i n t o  node 10 (making it a schwa)  ANTEST CALLED FOB 211SCFIHDA (AACC) , SD= 3. RES= 7.","acronyms":[[126,130],[134,136],[0,6],[8,15],[17,23],[28,34],[24,27],[141,144]],"long-forms":[[96,124]]},{"text":"Cleveland Families study dceweb1.case.edu\/ serc\/collab\/project_family.shtml), CHS (the Cardiovascular Heart Study www. ","acronyms":[[76,79]],"long-forms":[[85,111]]},{"text":"Hypernym Hyponyms Co-Hyponyms Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the state-of-the-art skip-gram model; The right part represents the semantic constraints).","acronyms":[[78,81]],"long-forms":[[53,76]]},{"text":"include other linguistic skills such as listening and reading. These constitute the integrated (INT) items.","acronyms":[[94,97]],"long-forms":[[82,92]]},{"text":"This convexity given the n4 Discounted cumulative gain (DCG) is widely used in information retrieval learning-to-rank settings.","acronyms":[[56,59]],"long-forms":[[28,54]]},{"text":"_ _ae$X=A\u0007I&HLH7K5HOG\u0007X5HOGPMLHLK ^ CWX=A$X=APH U\u0003I&K5X\u0010K5HOG\u0007X5H&GflMLHJa\u0007HLK5MOI5CEc\u0007CEG! ","acronyms":[[44,47]],"long-forms":[[48,90]]},{"text":"apart recorded in the miscellaneous information field of the lexeme. Similarly, Gene Ontology (GO) (Consortium.,","acronyms":[[94,96]],"long-forms":[[79,92]]},{"text":"ters instead of Y or N as labels. The character-level appliance translation (MT) approach (Pennell and Liu, 2011) was changes in (Li and Liu, 2012a)","acronyms":[[75,77]],"long-forms":[[54,73]]},{"text":"tion algorithm. We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al, 2011), for extraction.","acronyms":[[83,85]],"long-forms":[[63,81]]},{"text":"and a segment relation are identified.  Topic intermission index (TBI) takes the value of 1 or 2: the frontiers with TBI=2 is less con-","acronyms":[[59,62],[109,112]],"long-forms":[[40,57]]},{"text":"gathered training data from parallel texts for the set of most frequently occurring noun, adjective, and verb types in the Brown Corpus (BC). These word","acronyms":[[137,139]],"long-forms":[[123,135]]},{"text":" We propose a new algorithm: collective iterative classification (CIC) to perform approximate inference to find the maximum a posteriori","acronyms":[[66,69]],"long-forms":[[29,64]]},{"text":" The task addressed in this paper is likewise related to the Semantic Textual Resemblance (STS) task (Agirre et al, 2012).","acronyms":[[86,89]],"long-forms":[[57,84]]},{"text":"The evaluation strategy follows the globally standard as  Text Retrieval Conference (TREC)8 metrics. It ","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":"Thus, it is essential to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese vocabulary and  English ECS (E-ECS) \\[7\\] for English language. Onto the translation ","acronyms":[[132,137],[41,44],[79,84]],"long-forms":[[119,130],[65,77]]},{"text":"markert@l3s.de Abstract Automatic timeline summarization (TLS) generates precise, dated overviews over","acronyms":[[58,61]],"long-forms":[[34,56]]},{"text":"PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Classe GD = Gender MD = Methods  PF ~ Predicate Form PS = Person TN = Tense  NU = Number ","acronyms":[[131,133],[0,3],[14,20],[45,47],[55,57],[76,78],[88,90],[99,101],[119,121],[143,145]],"long-forms":[[136,141],[6,13],[23,43],[50,54],[70,75],[81,87],[93,97],[104,118],[124,130],[148,154]]},{"text":"A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proc. Conference on Empirical Methods in Natural Language Processing 2009, (EMNLP-09). David Yarowsky.","acronyms":[[158,166],[82,86]],"long-forms":[[102,155]]},{"text":"v2i = v3i ? a circular convolution model (CON) v1 ? v2 = v3","acronyms":[[42,45]],"long-forms":[[23,34]]},{"text":"In this section, we compare the running time5 of our sampling algorithm (FAST) and our algorithm with the refined bucket (RB) against the unfactored Gibbs sampler (NAI?VE) and examine the effect of sorting.","acronyms":[[122,124],[73,77],[164,170]],"long-forms":[[106,120]]},{"text":"hypernym, hyponym, near-synonym, holonym, and mernoym are listed as below:  Hypernym(HYP) (a) THOUGH x=ANT ","acronyms":[[85,88],[99,102]],"long-forms":[[76,83]]},{"text":"A set of the hyponym candidates extracted from a single itemization or list is called a hyponym candidate set (HCS). For the itemization","acronyms":[[111,114]],"long-forms":[[88,109]]},{"text":"we will depict in detail in Section 3. They then  introduced a ClueWordSummarizer (CWS), a  graph-based unsupervised summarization ap-","acronyms":[[85,88]],"long-forms":[[65,83]]},{"text":"its title  read{ARG0, ARG1}  Figure 2: A sentence parse tree with two predicative tree structures (PAST s) which is equal 1 if the target fi is rooted at node n","acronyms":[[99,105],[16,20],[22,26]],"long-forms":[[70,97]]},{"text":"our propositions STRAIN approach. The results of using sentence training (STr) and sentence experiment (STe) are shown in the STR\/STE row of Table 5.","acronyms":[[70,73],[13,19],[97,100],[119,126]],"long-forms":[[51,68],[79,95]]},{"text":" Proceedings of 24th International Conference on Computational Linguistics (COLING): Posters. ","acronyms":[[76,82]],"long-forms":[[49,74]]},{"text":"during the last two years. In this firstly of four installerments, the  Association of Data Treat Service Organisation, Inc. (ADAPSO) is  considered with respect to its membership, charter, organization and ","acronyms":[[130,136]],"long-forms":[[70,122]]},{"text":"PIQ(Fy;R) or not.  z Predictive Information Redundancy(PIR) Based on the above two definitions, we can","acronyms":[[55,58],[0,3],[4,8]],"long-forms":[[21,53]]},{"text":"DO: parent:number := node:number; parent:gender := node:gender; 3.1.5 Preposition without children (PrepNoCh) In our dependency trees, the preposition is the","acronyms":[[100,108],[0,2]],"long-forms":[[70,98]]},{"text":"Granted an input pair (q,a), where q is a question and a is a candidate answer, frst we retrieve the word embeddings (WEs) of both q and a. Later, we separately apply a","acronyms":[[117,120]],"long-forms":[[100,115]]},{"text":"Table 1: Mean relative frequencies and standard deviation for each schoolroom (A(nimate) vs. I(nanimate)) from idiosyncratic extraction (SUBJ=Transitive Themes, OBJ=Object, GEN=Genitive -s, PASS=Passive byphrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric references by inanimate pronoun, REFL=Anaphoric reference by reflexive pronoun).","acronyms":[[203,208],[126,130],[151,154],[163,166],[180,184],[249,254],[297,301]],"long-forms":[[209,239],[155,161],[167,175],[185,192],[255,287],[325,334]]},{"text":"In Proceedings of the Globally Conference on Data Engineering (ICDE). ","acronyms":[[68,72]],"long-forms":[[22,66]]},{"text":"2008).  The semantic role labeler (SRL) encompasses of a pipeline of independent, local classifiers that iden-","acronyms":[[35,38]],"long-forms":[[12,33]]},{"text":"The paper fiirst provides a brief overview of Lexical Functional Grammar, and the Penn Arabic Treebank (ATB). The next section introduces","acronyms":[[103,106]],"long-forms":[[86,101]]},{"text":"al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually repaired by Englishman instruc-","acronyms":[[91,94]],"long-forms":[[57,89]]},{"text":"models use a word embedding size of 200, whereas the hidden layer(s) size is fixed at 400, with all hidden units using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x) as activation function.","acronyms":[[146,150],[165,168]],"long-forms":[[123,144]]},{"text":"seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure.","acronyms":[[131,133]],"long-forms":[[105,129]]},{"text":"In Proceedings of the 19th Worldwide Conference on Computational Linguistics (COLING?02), pages 218? ","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"namely person name, location name, organization name and miscellaneous name to apply  Support Vector Machine (SVM) based machine  learning technique.","acronyms":[[110,113]],"long-forms":[[86,108]]},{"text":"storing our storage. We built a pattern matching  system based on Finite State Automata(FSA). Upon ","acronyms":[[88,91]],"long-forms":[[66,87]]},{"text":"Each word is considered as an  instance. Maximum Entropy (MaxEnt) is used in  this paper.","acronyms":[[58,64]],"long-forms":[[41,56]]},{"text":"grammatical person and number (1PS, 1PP, 2P, 3PS, 3PP), the quantified pronouns (QUANT), and a group including all other expressions (OTHER). ","acronyms":[[134,139],[81,86]],"long-forms":[[115,132],[60,79]]},{"text":"the predicted margin. Dredze and Crammer (2008a) illustrated how Confidence Weight (CW) learning could be used to generate a more informative mea-","acronyms":[[81,83]],"long-forms":[[60,79]]},{"text":"user gender (GEN), the user identity (UID) (e.g. the user could be a anybody or an organise), and the source document ID (DID). We also mark the lan-","acronyms":[[125,128],[13,16],[38,41]],"long-forms":[[112,123],[5,11],[23,36]]},{"text":"lead to over-fitting. Therefore, we propose another mode, Probabilistic Soft Logic (PSL) (Broecheler et al, 2010).","acronyms":[[86,89]],"long-forms":[[60,84]]},{"text":"query, some form of translation is required. One might conjecture that a combination of two existing fields, IR and machine translation (MT), would be satisfactory for accomplishing the combined translation and retrieval task.","acronyms":[[137,139]],"long-forms":[[116,135]]},{"text":" 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural","acronyms":[[52,55]],"long-forms":[[28,50]]},{"text":"the first reference in this study. ( 3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the","acronyms":[[79,83]],"long-forms":[[59,77]]},{"text":"BP as a computational expression charts ? Automatic differentiation (AD) 7.","acronyms":[[68,70],[0,2]],"long-forms":[[41,66]]},{"text":"Semantic Characteristics Performance  The semantic features include Named Entity  (NE), Noun Hypernym (NHype) and Head Verb  Synset (HVSyn).","acronyms":[[95,100],[75,77],[125,130]],"long-forms":[[80,93],[60,72],[106,123]]},{"text":" 2.3 Evaluation We use the Dutch portion of EuroWordNet (DWN) (Vossen, 1998) for evaluation of our hypernym ex-","acronyms":[[54,57]],"long-forms":[[27,52]]},{"text":" 1.1 Language Modeling Formally, a language model (LM) is a probability distribution over strings of a parlance:","acronyms":[[51,53]],"long-forms":[[35,49]]},{"text":"the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: K\u0002L^M_NQP\u0012R`NQS S54%6\u001a7","acronyms":[[118,121],[146,149],[134,143]],"long-forms":[[100,116]]},{"text":"Ensemble NN + LR (w\/o alternate grammar) 54.38 41.90 Ensemble NN + LR (w\/o synthetic data) 53.98 42.41 Table 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our system with various configurations.","acronyms":[[144,146],[173,175],[9,16],[62,69]],"long-forms":[[128,142],[152,171]]},{"text":" 1 Introduction Biomedical Text Mining (TM) has become increasingly popular due to the pressing need to furnishes","acronyms":[[40,42]],"long-forms":[[27,38]]},{"text":"creation (CR)  emotion (EM)  motion (MO)  perception (PC) ","acronyms":[[37,39],[10,12],[24,26],[54,56]],"long-forms":[[29,35],[0,8],[15,22],[42,52]]},{"text":"In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI). ","acronyms":[[84,87],[18,21]],"long-forms":[[55,82]]},{"text":"SVO = Subject-Verb-Object GE = General Incidents PE = Predefined Event Grammar Module","acronyms":[[45,47],[0,3],[26,28]],"long-forms":[[50,66],[6,25],[31,44]]},{"text":"Accept? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, granted that the","acronyms":[[66,70],[45,48]],"long-forms":[[54,64],[24,32]]},{"text":"2.1 Knowledge Fount We employ BabelNet 2.5.1 as our reference expertise base (KB). BabelNet is a multilingual","acronyms":[[79,81]],"long-forms":[[63,77]]},{"text":"1 Introduction As they are normally conceived, many tasks relevant to Computational Linguistics (CL), such as text categorization, clustering, and information retrieval, ignore the con-","acronyms":[[97,99]],"long-forms":[[70,95]]},{"text":"142 ? National University of Mongolia (NUM),  Mongolia ","acronyms":[[39,42]],"long-forms":[[6,37]]},{"text":"2004. Evaluation of a Deidentification (De-Id) Software Engine to Share Pathology Reports and Clinical Documents for","acronyms":[[40,45]],"long-forms":[[22,38]]},{"text":"Entropy Guided Transformation Learning (ETL) is a new machine taught strategy that combines the advantage of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos","acronyms":[[128,130],[40,43],[167,170]],"long-forms":[[112,126],[0,38],[136,165]]},{"text":"ing different linguistic aspects and an effective routes to combine such info is made available by Support Vector (SV) regression, described in (Smola and Scho?lkopf, 2004).","acronyms":[[119,121]],"long-forms":[[103,117]]},{"text":" LTP. LTP (Language Technology Platform  developed by HIT) is a package of tools to ","acronyms":[[6,9],[1,4],[54,57]],"long-forms":[[11,39]]},{"text":"by uses multiple trainees and a label integrator.  We have developed a forward (FR) and a backward relationship (BR) learner to learn relation-","acronyms":[[81,83],[114,116]],"long-forms":[[72,79],[91,112]]},{"text":".  Tallest TM Candidate Indicator (LTC):  Which indicates whether the given  is the ","acronyms":[[35,38]],"long-forms":[[3,33]]},{"text":"Tokens (T) .72 \/ .77 .83 \/ .84 .72 \/ .76 .85 \/ .84 .82 \/ .84 .88 \/ .86 Named Organizations (NORTHEASTERN) .75 \/ .80 .84 \/ .79 .75 \/ .77 .85 \/ .78 .89 \/ .78 .89 \/ .73 NORTHEASTERN aim (NE-T) .54 \/ .55 .49 \/ .47 .66 \/ .64 .60 \/ .57 .64 \/ .64 .57 \/ .58 Hosted (H) .72 \/ .57 .64 \/ .48 .67 \/ .51 .58 \/ .41 .67 \/ .63 .59 \/ .55","acronyms":[[164,168]],"long-forms":[[151,162]]},{"text":"Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright","acronyms":[[131,134]],"long-forms":[[112,129]]},{"text":"Taalkommissie van die Suid-Afrikaanse Akademie vir Wetenskap en Kuns. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa. ","acronyms":[[98,103]],"long-forms":[[70,96]]},{"text":"In t roduct aeon   This paper discusses the relationstfip between Tree Adjoin-  ing Grammars (TAG's) and :Chief Grammars (HG's). TAG's ","acronyms":[[120,124],[93,98],[127,132]],"long-forms":[[105,118],[65,91]]},{"text":"qn  e.g., ? MP (Member of Parliament)? ","acronyms":[[12,14]],"long-forms":[[16,36]]},{"text":"Two-level rules are generally of the form CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context","acronyms":[[60,62],[86,88],[42,44],[45,47],[48,50],[51,53],[101,103],[120,122]],"long-forms":[[65,84],[91,99],[106,118],[125,138]]},{"text":" 6 Conclusion In this cooperating, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a network of semantically coherent classes of templates and derivative semantic relations consisting entailment","acronyms":[[90,94]],"long-forms":[[54,88]]},{"text":"sentence length. This gives us five metrics of complexity: sentence length (SL), tree depth (TD), branching factor (BF), normalized tree depth","acronyms":[[76,78],[93,95],[116,118]],"long-forms":[[59,74],[81,91],[98,114]]},{"text":"on a questionnaire provided to them. And a Mean Opinion Score(MoS) of 62.27% was achieved.","acronyms":[[62,65]],"long-forms":[[43,61]]},{"text":"In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 75? ","acronyms":[[92,95]],"long-forms":[[49,89]]},{"text":"Abstract Crowd-sourcing approaches such as Amazon?s Mechanical Turk (MTurk) make it possible to annotate or collect grandes quantities of","acronyms":[[69,74]],"long-forms":[[52,67]]},{"text":"In Processdings of Sixth International Conference on  Language Resources and Evaluation (LREC),  pages 2961-2968, Marrakech, Morocco.","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":"an examples of a low-pass filter. The concept of recursion is next introduced in order to pave the way for a discuss of IIR (Infinite Stimulus Response) filters. High-, low-, and","acronyms":[[122,125]],"long-forms":[[127,152]]},{"text":" The best method ASSVM outperforms other methods most clearly on METH (Method) category. Al-","acronyms":[[65,69],[17,22]],"long-forms":[[71,77]]},{"text":"of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 aljosav@gmail.com     Abstract  We report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system.","acronyms":[[259,262],[299,301],[385,388]],"long-forms":[[232,257]]},{"text":"tions (Abe et al, 1996).  1.1 Question answering (QA) Unlike IR systems which reverted a list of documents","acronyms":[[50,52]],"long-forms":[[30,48]]},{"text":" This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most","acronyms":[[78,81]],"long-forms":[[62,76]]},{"text":" The focus of this paper is a discussion of various methods used to create a set of acoustic models for  characterizing the PLU's used in large vocabulary recognition (LVR). The set of context independent ","acronyms":[[168,171],[124,129]],"long-forms":[[138,166]]},{"text":"{yvchen, yww, anatoleg, air}@cs.cmu.edu Abstract Talked dialogue systems (SDS) typically require a predefined semantic ontology","acronyms":[[74,77]],"long-forms":[[49,72]]},{"text":"The ISO 639-3 language codes for our eights languages are as follows: Urdu (URD), Thai (THA), Bengali (BEN), Tamil (TAM), Punjab (PAN), Tagalog (TGL), Pashto","acronyms":[[75,78],[102,105],[4,7],[87,90],[115,118],[130,133],[145,148]],"long-forms":[[69,73],[93,100],[81,85],[108,113],[121,128],[136,143]]},{"text":"the MACH-III expert system, we should begins with  a brief description. Functional hierarchy (FH) is a  new paradigm for organize expert system ","acronyms":[[93,95],[4,12]],"long-forms":[[71,91]]},{"text":"audiences have scant trouble mapping a collection  of noun phrases during the same entity, this task of  noun expressions (NP) coreference r solution can present  a formidable challenge to an NLP system.","acronyms":[[117,119],[186,189]],"long-forms":[[104,115]]},{"text":"The next-to-last  column shows the precision (PRE)--the true positives divided by all verbs that Lerner  judged to be +S. The final column shows the recall (REC)--the true positives divided  by all verbs that were judged +S by hand.","acronyms":[[157,160],[46,49]],"long-forms":[[149,155],[35,44]]},{"text":"Topic 1 Other modules Greeting(GR) Keep silence(KS) Figure 2: Overview of the information navigation","acronyms":[[31,33],[48,50]],"long-forms":[[22,29],[35,46]]},{"text":"This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based","acronyms":[[110,112]],"long-forms":[[89,108]]},{"text":"ily' (lists truncated). Score = log-likelihood score; f = occurrence frequency of keyterm; NN = noun; VV = verb; AR =  article; AP = article+preposition; JJ = adjective; CC = con-","acronyms":[[91,93],[24,29],[102,104],[113,115],[128,130],[154,156],[170,172]],"long-forms":[[96,100],[69,78],[47,52],[107,111],[119,126],[133,152],[159,168]]},{"text":" ? ALGN (alignment-based): We ran a sentences alignment algorithm (Gale and Church, 1993)","acronyms":[[3,7]],"long-forms":[[9,18],[45,54]]},{"text":"Table 3: Example of retrieve Wikipedia pages from the four various methods tested in this paper.  Results of diverse merging (DivM) appear to cover more themes relevant to the conversation fragment than other methods.","acronyms":[[129,133]],"long-forms":[[112,127]]},{"text":" 4.1 Lexical Sampling Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sampling tasks and also","acronyms":[[70,73],[91,94]],"long-forms":[[58,68],[79,89]]},{"text":"act (2004). This frames for linguistic semantics is called Uni\u0002ed Eventity Representation (UER), because it is a true extension of the UML and not","acronyms":[[94,97],[138,141]],"long-forms":[[62,92]]},{"text":"string is a false positive (FP). Each gold standard gene mention is counted as a false negative (FN) if it is not identified by the approach.","acronyms":[[97,99],[28,30]],"long-forms":[[81,95],[12,26]]},{"text":"apply shallow semantic (selectlonal) constraints, to filter out semantically anomalous parses, in a  second experiment. This procedure used PUNDIT's Selection Pattern Query and Response (SPQR)  component ~Lang1988\\].","acronyms":[[187,191]],"long-forms":[[149,185]]},{"text":"composition process.  4.1 Tag Guided RNN (TG-RNN) We propose Tag Guiding RNN (TG-RNN) to re-","acronyms":[[42,48],[77,83]],"long-forms":[[26,40],[61,75]]},{"text":"Into support to the on going project of Multilingual  Machine Translation Sytem for Asian Language organized by  Center for Internationale Cooperaliou inComputerization (CICC)-  Japan and other Asian cotmtries (China.","acronyms":[[167,171]],"long-forms":[[111,165]]},{"text":"This system tags, lemmatizes and parses corpus data utilised the current version of the RASP (Solid Accuracy Statistical Parsing) toolkit (Briscoe et al, 2006), and on the basis of resulting","acronyms":[[85,89]],"long-forms":[[91,126]]},{"text":"idea was an early version of branching entropy, one of the experts in VE, and they developed an algorithm called Phoneme to Morpheme (PtM) around it. ","acronyms":[[134,137],[70,72]],"long-forms":[[113,132]]},{"text":" 5.3   Learning Algorithm: Conditional Random Field  Conditional Random Fields (CRF) is a formalism well-suited for learning and projections on consecutive data.","acronyms":[[80,83]],"long-forms":[[53,78]]},{"text":"contribution: Functional GENDER and NUMBER features contribute more than their form-based counterparts, in both gold and predicted conditions; rationality (RAT) as a single feature on top of the POS tag set helps in gold (and with Easy-First Parser, also in predicted conditions)?but when used in combination with","acronyms":[[156,159],[195,198]],"long-forms":[[143,154]]},{"text":"The current   representat ion copilot r  t h a t  sentence i n  pur system would be:  Z V l  =Ncorn(elephant,X1) PI =P.P(size,X1 ,scant)  & =Ncom(animal,X1) P2 =P(size ,XI ,gros) ","acronyms":[[108,110],[89,94],[81,86],[136,140],[112,115],[152,154],[164,166],[104,106]],"long-forms":[]},{"text":"started.  Briton National Corpus (BNC)4, American  National Corpus (ANC)5 had been referenced ","acronyms":[[35,38],[69,72]],"long-forms":[[10,33],[42,67]]},{"text":"correspond to the point P.  Once a n  object has been adding to the geometric modelling by specifying values  for its GSTART, GSIZE, and ROTN (rotation), the geometric coordinates for any  locations on the object may be obtained by calling the funtion EXECLOCA with the ","acronyms":[[135,139],[249,257],[116,122],[124,129]],"long-forms":[[141,149]]},{"text":" IV. RETROSPECTIVE SSL (R-SSL). After","acronyms":[[24,29]],"long-forms":[[5,22]]},{"text":" ? Adjuncts (AM-): General controversies that any verb may take optionally.","acronyms":[[13,16]],"long-forms":[[3,11]]},{"text":"4} is not a most frequent token  and will reach at bi-gram queue manager only  after passing  through all forms generator (AFG).  ","acronyms":[[123,126]],"long-forms":[[102,121]]},{"text":"Cognitive Science Department at Xiamen University (XMU) ? ?  Harbin Institute of Technology Shenzhen Graduate Teaching (HITSZGS)    National Chengdu University of Technology (NTUT)   ","acronyms":[[118,125],[51,54],[172,176]],"long-forms":[[61,116],[32,49],[130,170]]},{"text":"The cooperating describing in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programmed.  To promote more linguistically-oriented writing process investigative, Inputlog aggregates the logged process data from the personage tiers (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) paraphernalia to further annotate the logged process data with varied genus of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.","acronyms":[[382,385]],"long-forms":[[353,380]]},{"text":"been semi-automatically detected in human reference and machine translations from English (EN) to French (FR) and German (DE) (Section 3). ","acronyms":[[106,108],[91,93],[122,124]],"long-forms":[[98,104],[82,89],[114,120]]},{"text":"(Schubert 1987; Maxwell & Schubert 1989), and fig-  ure 1 shows the dependency trees for this examples,  cross-coded for translation units (TUs). Everybody ellipse ","acronyms":[[139,142]],"long-forms":[[120,137]]},{"text":"functions(CPU: Celeron TM 366, RAM: 64M).  2) Predicted precision(PP) =  number of words with correct BPs(CortBP) ","acronyms":[[67,69],[10,13],[23,25],[103,106],[107,113],[31,34]],"long-forms":[[46,66]]},{"text":"Though, LSI has known a resurging interest. Supervised Semantic Indexing (SSI) (Bai et al.,","acronyms":[[80,83],[14,17]],"long-forms":[[50,78]]},{"text":"but more often there is only one.  FN=pseudo negative, etc.). I also consider micro- and","acronyms":[[35,37]],"long-forms":[[38,52]]},{"text":"Texts The performance of punctuation prediction on both Chinese (CN) and English (EN) texts in the correctly recognized output of the BTEC and CT datasets are","acronyms":[[65,67],[82,84],[134,138],[143,145]],"long-forms":[[56,63],[73,80]]},{"text":"refers to the fact that either a particular lexical item or a particular grammatical construction must be present for the omission of a frame element (FE) to occur.","acronyms":[[151,153]],"long-forms":[[136,149]]},{"text":"PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labelled precision (LP), remind (LR), F 1","acronyms":[[98,100],[0,5],[111,113]],"long-forms":[[79,96]]},{"text":" ? Max Similarity (MaxSim): For tuple ? in an","acronyms":[[19,25]],"long-forms":[[3,17]]},{"text":"other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al 1997), Unequivocal Semantic Analyzed (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis","acronyms":[[124,127],[68,71]],"long-forms":[[96,122],[42,66]]},{"text":" A more flexible approach is to do it in two steps: complementation, forming a VP (verb phrase) from the verb and the object, and predication","acronyms":[[79,81]],"long-forms":[[83,94]]},{"text":"1 Scope and Anterior Work We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.","acronyms":[[120,123]],"long-forms":[[88,118]]},{"text":" In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 865? ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-","acronyms":[[154,157]],"long-forms":[[132,152]]},{"text":"quency and its character string frequency is  less than or equal to 1%, it is a SWBS;  BMM-ASM (BMM ambiguity string mapping  table: the BMM-ASM table lists all the ","acronyms":[[87,94],[137,144],[80,84]],"long-forms":[[96,124]]},{"text":"the impact of various kinds of physical degradation that pages may endure before they are scanned and processed using optical character recognition (OCR) software. ","acronyms":[[149,152]],"long-forms":[[118,147]]},{"text":"this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), fashion (MNR), purposes (PRP), and temporal (TMP).","acronyms":[[112,115],[141,144],[65,68],[82,85],[96,99],[126,129],[161,164]],"long-forms":[[102,110],[132,139],[53,63],[71,80],[88,94],[118,124],[151,159]]},{"text":"While later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) uses Integer Linear Programming (ILP) to decode a global optimized result, the input scores","acronyms":[[108,111]],"long-forms":[[80,106]]},{"text":"2007). Deux 5 DOF sensors - TT (Tongue Tip)  and TB (Tongue Organ Back) - were attached on  the midsagittal of the tongue.","acronyms":[[48,50],[27,29],[13,16]],"long-forms":[[52,63],[31,41]]},{"text":"gorithm used belongs to the family of algorithms described by Covington (2001), and the classifiers are trained using support vector machines (SVM) (Vapnik, 1995).","acronyms":[[143,146]],"long-forms":[[118,141]]},{"text":"In contrast to standard 357 multi-class Word Sense Disambiguation (WSD), it uses a coarse-grained sense inventory that allows to","acronyms":[[67,70]],"long-forms":[[40,65]]},{"text":"ber of occurrence for this feature per patient narrative is obtained based on the frequency of the coordinating conjunction PoS tag (CC) detected in the parse tree structure.","acronyms":[[133,135]],"long-forms":[[112,123]]},{"text":"6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference","acronyms":[[81,84]],"long-forms":[[65,79]]},{"text":"expunged to meet United States HIPAA standards, (U.S. Health, 2002) and approved for release by the local Institutional Review Board (IRB); the sample must represent problems that medical records coders","acronyms":[[134,137],[49,52],[31,36]],"long-forms":[[106,132]]},{"text":"Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen {v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl Center for Language and Cognition Groningen (CLCG) University of Groningen, The Netherlands","acronyms":[[163,167]],"long-forms":[[118,161]]},{"text":" Issues, Tasks and Program Structures to Roadmap Research in Question & Answering (Q&A) http:\/\/www-nlpir.nist.gov\/projrcts\/","acronyms":[[83,86]],"long-forms":[[61,81]]},{"text":"get, with results deteriorating as we move to information retrieval (IR), multi-document summarization (SUM), and information extraction (IE). ","acronyms":[[138,140]],"long-forms":[[114,136]]},{"text":"Figure 1: Na??ve Bayes Model The model outlines above is commonly renowned as a na??ve Bayes (NB) model. NB models have","acronyms":[[92,94],[103,105]],"long-forms":[[78,90],[10,22]]},{"text":"3 The  S imulat ion  How l   The computational simulating supports the evolu-  tion of a population of Language Agents (LAgts),  similar to Holland's (1993) Echo agents.","acronyms":[[121,126]],"long-forms":[[104,119]]},{"text":" 1 Introduction Natural Language Processing (NLP) systems often consist of a series of NLP components, each trained","acronyms":[[45,48],[87,90]],"long-forms":[[16,43]]},{"text":"future research which are suggested by some af the techniques used in this program.  The SFRAME (semantic frame) concept. in which a sernantirl interpretation ","acronyms":[[89,95]],"long-forms":[[97,111]]},{"text":"32  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178, Seoul, South Korea, 5-6 July 2012.","acronyms":[[100,107]],"long-forms":[[50,98]]},{"text":"Following are the two broad types of social events that were annotated: Interaction happenings (INR): When both entities embroiled in an event are aware of each other and of","acronyms":[[91,94]],"long-forms":[[72,83]]},{"text":"Question is define as a Question term (QTerm).  The Answer Term (ATerm) is the Answered given by the KM corpus.","acronyms":[[66,71],[40,45],[100,102]],"long-forms":[[53,64],[25,38]]},{"text":"226  Trials of the 13th Annual Meeting of the Specially Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313, Seoul, South Korea, 5-6 July 2012.","acronyms":[[101,108]],"long-forms":[[51,99]]},{"text":"2115  Proceedings of the of the EACL 2014 Workshop on Discussions in Motion (DM), page 63?67, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[74,76],[32,36]],"long-forms":[[54,72]]},{"text":"them as \"LIG-equivalent formalisms\". LIG is a vari-  ant of indexed grammar (IG) (Aho, 1968). Like CFG, IG ","acronyms":[[75,77],[37,40],[9,12],[97,100],[102,104]],"long-forms":[[60,73]]},{"text":"Table 2: The rating for each combination of agents. LB = ListenerBot; DB = DialogBot. ","acronyms":[[56,58],[74,76]],"long-forms":[[61,72],[79,88]]},{"text":"We used 1300 texts (DEV) as our training set, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as a test set. All 1700 docu-","acronyms":[[95,104]],"long-forms":[[88,93]]},{"text":"It is well known that for English, the automatic conversion of a constituency parser?s output to dependency format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output trained on automatically converted trees","acronyms":[[168,171]],"long-forms":[[139,159]]},{"text":"for unsupervised learning. Onto this paper, we are interested in partitioning words into several clusters without any label priori using unguarded LP (Un-LP) algorithm. Firstly we randomly opt K (K ?","acronyms":[[152,157]],"long-forms":[[135,150]]},{"text":"Abstract  Since statistical machine translation (SMT)  and translation memory (TM) complement  each other in matched and unrivalled regions, ","acronyms":[[79,81],[49,52]],"long-forms":[[59,77],[16,47]]},{"text":"2008; Metzler and Cai, 2011).  Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that","acronyms":[[70,74]],"long-forms":[[39,68]]},{"text":"an ASR system. The main idea was to design a language model (LM) to combine the trigram language model probability with the translation","acronyms":[[61,63],[3,6]],"long-forms":[[45,59]]},{"text":"we utilizes three categories for the identically spelled words: (a) we used the term true equivalents (TE) to refer to the pairs that have the same","acronyms":[[97,99]],"long-forms":[[79,95]]},{"text":"Pro = percent of the expression as pronominals. WPS = Words per sentence. 6LTR = percent of phrases that are longer than 6 letters.","acronyms":[[43,46],[70,73]],"long-forms":[[49,67]]},{"text":"Nevertheless, LSI has known a resurging interest. Supervised Semantic Indexing (SSI) (Bai et al.,","acronyms":[[80,83],[14,17]],"long-forms":[[50,78]]},{"text":"CONN =  nil;  konj( KONJ )  FUNDF = fundf n( NOMINAL ); \/* No nought *\/ ","acronyms":[[20,24],[0,4],[28,33]],"long-forms":[[14,18]]},{"text":"Combining keyphrase and collocation Yamamoto and Iglesias (2001) compare two metrics, MI and Residual IDF (RIDF), and remarked that MI is suitable for finding collocation and RIDF","acronyms":[[105,109],[84,86],[130,132],[173,177]],"long-forms":[[91,103]]},{"text":"pus (Mitchell et al, 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based par-","acronyms":[[75,80],[48,53]],"long-forms":[[59,73],[38,46]]},{"text":"Abstract   This paper describes two algorithms which construct two differ-  ent types of generators for lexical functional grammars (LFGs). The ","acronyms":[[133,137]],"long-forms":[[104,131]]},{"text":"9http:\/\/disi.unitn.it\/moschitti\/Tree-Kernel.htm 10for STS-2012 we similarly report the conclusions for a concatenation of all five test sets (ALL) provement with the Mean = 0.7416 and Persson","acronyms":[[133,136]],"long-forms":[[113,116]]},{"text":" Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a","acronyms":[[70,75]],"long-forms":[[53,68]]},{"text":"Ravi and Knight (2008) solved 1:1 replaces ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while Corlett and Penn (2010) settling the problem using","acronyms":[[134,137]],"long-forms":[[110,132]]},{"text":"Phrase Extrac\"on GIZA++ Figure 3: Building a twin phrase table (PT). First, sep-","acronyms":[[64,66],[17,23]],"long-forms":[[50,62]]},{"text":"In this part, we introduce how to make use of the  original and opposite training\/test data together  for dual training and dual prediction (DTDP). ","acronyms":[[141,145]],"long-forms":[[106,139]]},{"text":"using the written and spoken language corpora.  Occurrence probabilities (OPs) of expressions in the written and spoken language corpora can be used to dis-","acronyms":[[74,77]],"long-forms":[[48,72]]},{"text":"ajaynagesh@cse.iitb.ac.in Abstract Information Extraction (CI) has become an key tool in our quest to handle the data","acronyms":[[59,61]],"long-forms":[[35,57]]},{"text":"Next subsections show the results obtained by TIPSem system in each one of the TempEval-2 tasks for English (EN) and Spanish (ES). More-","acronyms":[[109,111],[46,52],[79,89],[126,128]],"long-forms":[[100,107],[117,124]]},{"text":"Hong Kong Laws, Sinorama, Xinhua News, and English translations of Chinese Treebank), available from the Language Data Consortium (LDC). To","acronyms":[[132,135]],"long-forms":[[104,130]]},{"text":"During Proceedings of the 15th International Conference on Computational Linguistics (COLING?94), pages 1145?1150, Kyoto, Japan.","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"mantic representation is not so clear cut. Generalising only verbs to semantic files (SFv) was the best option in most of the experiments, particularly","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"matic speech recognizers in Sermons Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output. For the","acronyms":[[128,131],[75,79]],"long-forms":[[97,126],[28,73]]},{"text":" In order to rate models M1, M2, M3 in comparison to the vector space model (VS) using MSTs, STs and CTs as alternative hierarchi-","acronyms":[[77,79],[87,91],[93,96],[101,104]],"long-forms":[[57,69]]},{"text":"Tipster (ADEPT) Program is a demonstrations project  aimed at alleviating problems currently being encountered by  the Office of Information Resources (OIR). OIR has ","acronyms":[[145,148],[9,14],[151,154]],"long-forms":[[112,143]]},{"text":"Text recovering Conference (TREC)1. The TREC 1The Text REtrieval Conference (TREC) is a series of evaluations of fully automatic Q\/A scheme","acronyms":[[76,80],[27,31],[39,43],[128,131]],"long-forms":[[49,74]]},{"text":"include: 1) candidate frequency and its distribution in different Web pages, 2) length ratio between source terms and goals hopefuls (S-T), 3)  distance between S-T, and 4) keywords, key ","acronyms":[[137,140],[164,167]],"long-forms":[[101,135]]},{"text":"4  At the highest level, the text is a petition treated to CCC  members to vote against making the nuclear freeze initiative (NFI)  one of the issues about which CCC actively lobbies and stimulates ","acronyms":[[127,130],[60,63],[163,166]],"long-forms":[[100,125]]},{"text":"Learning dependency-based compositional semantics.  In Association for Computational Linguistics (ACL). ","acronyms":[[98,101]],"long-forms":[[55,96]]},{"text":" The optimum method ASSVM outperforms other methods most clearly on METH (Method) category. Al-","acronyms":[[65,69],[17,22]],"long-forms":[[71,77]]},{"text":"concept, BLESS consists different relata,  connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER),  meronymy (MERO) or no-relation (RANDOM-N).2 ","acronyms":[[118,123]],"long-forms":[[107,116]]},{"text":"retrieval with locality information using smart. In  Text retrieval conferenc (TREC-1) (pp. 59-72).","acronyms":[[79,85],[88,90]],"long-forms":[[53,77]]},{"text":"a new father node. The following simple rule forms a  noun phrases (NP). ","acronyms":[[67,69]],"long-forms":[[54,65]]},{"text":"collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al 2004a), all our experiments use the British National Corpus (BNC). In","acronyms":[[146,149]],"long-forms":[[121,144]]},{"text":"event extraction essentially rely on elaborately designed features and complicated natural language processing (NLP) tools. ","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"3.1 Div is ion  and  L inear l i za t ion  o f   Cases   At first, we define a translation pattern (TPi) as fol-  lows.","acronyms":[[100,103]],"long-forms":[[79,98]]},{"text":"derivations obtained from all stop states in the chart.  3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a","acronyms":[[81,84]],"long-forms":[[61,79]]},{"text":" Definition 1  A lexical conceptually structure (LCS) is a alter version of the representation proposed  by Jackendoff (1983, 1990) that conforms to the following structural form: ","acronyms":[[47,50]],"long-forms":[[17,45]]},{"text":"niscent of balanced-tree structures using left and right  rotations. A left rotation changes a (A(BC)) structure to  a ((AB)C) structural, and vice inverse for a right rotation.","acronyms":[[96,100],[120,126]],"long-forms":[[69,92]]},{"text":"qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, qnp), qdet?1.0 DET(the),","acronyms":[[52,54],[77,80],[30,32],[8,10]],"long-forms":[[56,60]]},{"text":"  Parallel corpora Size of Francais texts (in  million words (MB))  Size of Chinese texts (in ","acronyms":[[61,63]],"long-forms":[[46,59]]},{"text":"computational semantics. With the climbs of massive and easily-accessible digital corpora, computation of co-occurrence statistically has authorizes researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas.","acronyms":[[201,205],[156,159]],"long-forms":[[169,199]]},{"text":"Results on the dev set using two metrics: instance classification accuracy (CA), and soundbite name  recognition accuracy (RA). The oracle RA is 79.1%.","acronyms":[[123,125],[76,78],[139,141]],"long-forms":[[101,121],[51,74]]},{"text":"one of German, Englishman or Japanese. The system has been destined  around the task of conference r gistration (CR). It has initially been ","acronyms":[[110,112]],"long-forms":[[85,108]]},{"text":"There were four data sources utilizing in the  training set: the Wall Street Newspaper, Associated  Press, Federal Register (FR), and Department of  215 ","acronyms":[[118,120]],"long-forms":[[100,116]]},{"text":"School of Computer Science, University of Manchester, BRITISH ? National Centre for Text Mining (NaCTeM), UK ?","acronyms":[[92,98],[54,56],[101,103]],"long-forms":[[59,90]]},{"text":"event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. ","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"among other ways). We refer to cases in which the subject and object of the relation are contained within a phrase headed by a Noun as Relational NP?s (RNP). ","acronyms":[[152,155]],"long-forms":[[135,150]]},{"text":"the procedures that manipulate an  intermediate representat ion of the query,  cal culminated the Meta Query Language (MQL). ","acronyms":[[112,115]],"long-forms":[[91,110]]},{"text":"vp?np?pp .83 .80 .83 .80 .83 .80 vp?pp?pp .75 .74 .75 .74 .75 .74 Lexical association (LAsim) Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR","acronyms":[[87,92]],"long-forms":[[66,85]]},{"text":"and GL.  ACCOUNTANCY = GR or GL unspec. CC","acronyms":[[14,16],[4,6],[9,11],[31,33]],"long-forms":[[17,19]]},{"text":"TF (Term Frequency)  is the word frequency within a documenting;  IDF (Inverse Document Frequency) is the  logarithm of the ratio of the overall number of ","acronyms":[[63,66],[0,2]],"long-forms":[[68,94],[4,18]]},{"text":"We extend the SPARSELDA (Yao et al, 2009) inference scheme for latent Dirichlet alocation (LDA) to tree-based topic models.","acronyms":[[91,94],[14,23]],"long-forms":[[63,89]]},{"text":"We implement MVM using generative models primitives drawn from Latent Dirichlet Allocation (LDA) and the Dirichlet Process (DP). |M | diverse","acronyms":[[123,125],[13,16],[91,94]],"long-forms":[[104,121],[62,89]]},{"text":"employing them meanwhile. We also include the oracle word error rate (WER) of the WCNs and lattices for each ASR setups.","acronyms":[[75,78],[87,91],[114,117]],"long-forms":[[58,73]]},{"text":"........ ? ................... ? ............... + ................... +  CLS = Clause NP = Noun Phrase (BARRISTER 2)  PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL ","acronyms":[[87,89],[74,77],[105,108],[113,117],[130,135],[165,167]],"long-forms":[[92,103],[80,86],[120,129],[139,146]]},{"text":"edges. The NEs includes personal name(PRE), location name(LOC) and organizations name(ORG). ","acronyms":[[85,88],[11,14],[38,41],[58,61]],"long-forms":[[67,79],[24,32],[44,52]]},{"text":"UMichigan non-Tipster UNK X USouthern California non-Tipster SNAP X USussex (UK) non-Tipster SUSSEX X Tables 1 .","acronyms":[[77,79],[22,25],[61,65],[93,99]],"long-forms":[[68,75]]},{"text":" Reference:  MedLine sample # 6  Autonym:  decoy receptor 3 (DcR3)  Information a soluble decoy receptor  ","acronyms":[[61,65]],"long-forms":[[43,59]]},{"text":"3.2 To resolve gapping under serial verb construction Serial verb construction (SVC) (Baker, 1989) is construction in which a sequence of verbs appears in","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":" For each combination, we measure the attachment score (AS) and the exact match (EM). A signif-","acronyms":[[81,83],[56,58]],"long-forms":[[68,79],[38,54]]},{"text":"2003; Shen et al, 2006; Wubben et al, 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al, 2004), consisting of 5,801 sentence","acronyms":[[129,133]],"long-forms":[[98,127]]},{"text":" A statistical classification technique based  on the use of Hidden Markov Modelled (HMM) was  used as a language discriminator.","acronyms":[[83,86]],"long-forms":[[61,81]]},{"text":"2.2 Graphical Representation Recently, Ding et al (2008) use skip-chain and 2D Conditional Random Fields (CRFs) (Lafferty et al, 2001) to perform the relational learning for","acronyms":[[106,110],[76,78]],"long-forms":[[79,104]]},{"text":"plains the text? Our approach is related to minimum description length (MDL). We formulate our","acronyms":[[72,75]],"long-forms":[[48,70]]},{"text":"cal search. We examine this by using three different parsers in phase 0: (i) MS (Marecek and Straka, 657","acronyms":[[77,79]],"long-forms":[[81,99]]},{"text":"of General Linguistics  MAINE, JUNE '94  SEMANTIC SYNTAX (SeSyn) is a direct continuation of work done in the '60s and '70s under the name of GENERATIVE  SEMANTICS.","acronyms":[[58,63]],"long-forms":[[41,56]]},{"text":"Sotelo, 2007).  4.3 Polarity Lexicon (LEX) We have built a polarity thesaurus with both Positive","acronyms":[[38,41]],"long-forms":[[29,36]]},{"text":" minimizes the multiway normalized cutting(MNCut): MNCut(I) = K ?","acronyms":[[39,44],[47,52]],"long-forms":[[15,37]]},{"text":"terns for them. In the terminology of Frame Semantics, the roles are called frame elements (FEs), and the words which evoke the frame are referred","acronyms":[[92,95]],"long-forms":[[76,90]]},{"text":"performs rather well because the combined recency  bias representation worked well on its own and be-  cause the restricted souvenir (RM) bias overwhelmingly  discards features that are distant from the relative ","acronyms":[[131,133]],"long-forms":[[112,129]]},{"text":"In second method we compare CLIR performance of the two systems using Cross Language Evaluation Forum (CLEF) 2007 ad-hoc bilingual track (Hindi-English) docu-","acronyms":[[103,107],[28,32]],"long-forms":[[70,101]]},{"text":" 3.1 Ske le ta l  Expressions  S t ructure  Element   The role of phrase structural (PS) rules in our parser is similar to  their role in Lexical Functional Grammar \\[Kaplan 83\\], however they ","acronyms":[[81,83]],"long-forms":[[63,79]]},{"text":"tion system; then the output is classified into a  small number of domain-specific classes called  Domain Acts (DAs) that can indicate directly to  the dispatcher the general intended meaning of ","acronyms":[[112,115]],"long-forms":[[99,110]]},{"text":"Phrase Extrac\"on GIZA++ Figure 3: Building a twin phrase table (PT). Outset, sep-","acronyms":[[64,66],[17,23]],"long-forms":[[50,62]]},{"text":"spect to the ISSC. We will refer to this expanded version of the SSC as the processed SSC (PSSC). ","acronyms":[[91,95],[13,17],[65,68]],"long-forms":[[76,89]]},{"text":" From the set of erroneous instances: True Positive (TP) ML class 6= pupil class False Negative (FN) ML class = learner class","acronyms":[[53,55],[99,101],[57,59],[103,105]],"long-forms":[[38,51],[83,97]]},{"text":"provided for the slots over the course of the dialog.  These are our String Consistency (SC) features. ","acronyms":[[89,91]],"long-forms":[[69,87]]},{"text":"followed by a orally suffix\". This is to cover general  verb inflection, for both auxiliaries (TO +) and main  verbs (AUX -).","acronyms":[[95,100],[119,122]],"long-forms":[[82,93]]},{"text":"  2 Dimensionality Reduction   VSM (Vector Space Model) is a basic technique  to transform text documents to numeric vectors.","acronyms":[[31,34]],"long-forms":[[36,54]]},{"text":" Association for Computational Linguistics.                       ACL Specific Interest Group on the Dictionaries (SIGLEX), Philadelphia,                   Unsupervised Lexical Acquisition: Proceedings of the Workshop of the","acronyms":[[109,115],[66,69]],"long-forms":[[70,107]]},{"text":" won out on F-measure while giza++ syllabized reached better alignment error rate (AER). Refer to Table 3 for","acronyms":[[84,87]],"long-forms":[[62,82]]},{"text":"edges the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract No.","acronyms":[[132,136],[64,69]],"long-forms":[[101,130],[21,62]]},{"text":"a PP (Mirroshandel and Ghassem-Sani, 2011), the prepositional lexeme of the PP if e2 is governed by a PP, the POS of the leiter of the sneezing words (VP) if e1 is headed by a VP, the POS of the head of the","acronyms":[[146,148],[2,4],[76,78],[102,104],[110,113],[173,175],[181,184]],"long-forms":[[133,144]]},{"text":"A Unified Model of Phrasal and Sentential Evidence for Information Extraction. Across Proc. Conference on Empirical Modes in Natural Language Processing 2009, (EMNLP-09). David Yarowsky.","acronyms":[[158,166],[82,86]],"long-forms":[[102,155]]},{"text":"comparison with SMO-n) 6 Conclusions Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more accessible to a wider audience.","acronyms":[[68,71],[16,21]],"long-forms":[[37,66]]},{"text":"domain-oriented semantics of the GENIA event corpus, and suggests a factor for utilizing NLP techniques for Text Mining (TM) in the bio-medical domain.","acronyms":[[121,123],[33,38],[89,92]],"long-forms":[[108,119]]},{"text":"Figure 5: Example GMM fitting 2. Gaussian mix model (GMM)-based POI probability (prior) calculation","acronyms":[[57,60],[18,21],[68,71]],"long-forms":[[33,55]]},{"text":" The first principle of a search engine is based  on cursory Natural Language Treat (NLP)  techniques, for instance, string matching, while ","acronyms":[[90,93]],"long-forms":[[61,88]]},{"text":"we used two other error-annotated learner corpora.  The NUS Corpus of Learner English (NUCLE) includes one billions words of academic writing","acronyms":[[87,92]],"long-forms":[[56,85]]},{"text":"mathematical models from piloting data. The  algorithm is similar to Genetic Programming (GP),  but uses fixed-length character twine (called chro-","acronyms":[[94,96]],"long-forms":[[73,92]]},{"text":"tion of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA).","acronyms":[[105,110],[40,43],[57,60],[80,85],[161,169]],"long-forms":[[88,103],[28,38],[46,55],[63,78],[121,159]]},{"text":"figure 5).  Base clauses (BC)  are subclauses of type sub-  junctive and subordinate.","acronyms":[[26,28]],"long-forms":[[12,24]]},{"text":"(2) dobj? det:DT NN prep:IN 7DT\/det=determiner, NN=noun, IN\/prep=preposition, dobj=directly object","acronyms":[[48,50],[14,16],[17,19],[60,64],[78,82],[28,35],[4,8],[20,24]],"long-forms":[[51,55],[65,76],[83,96],[36,46]]},{"text":" 2.2 LNRE Nature  o f  the  Data   The LNRE (Large Number of Rare Events)  zone (Chitashvili & Baayen, 1993) is defined as ","acronyms":[[39,43],[5,9]],"long-forms":[[45,72]]},{"text":" Assi, S. (1997). Farsi language database (FLDB). ","acronyms":[[45,49]],"long-forms":[[18,43]]},{"text":"occurrence restrictions (FCRs), feature specification  defaults (FSDs), linear precedence (LP) statements,  and universal feature instantiation principles (UIPs). ","acronyms":[[156,160],[25,29],[65,69],[91,93]],"long-forms":[[112,154],[32,63],[72,89]]},{"text":"S i 3.3.3 Methods 3: TrueSkill (TS) TrueSkill is an adaptive, online system that em-","acronyms":[[31,33]],"long-forms":[[20,29]]},{"text":"WI = big; NA = narrow; CR = critical; CL = closed; ALV = alveolar; P-A = palato-alveolar; RET = retroflex. ","acronyms":[[91,94],[0,2],[11,13],[24,26],[39,41],[52,55],[68,71]],"long-forms":[[97,106],[5,9],[16,22],[29,37],[44,50],[58,66],[74,89]]},{"text":"In this paper, I present a lexical representation  of the light  verb  ha  'do'  used in two types of  Korean light verb constructions (LVCs). These ","acronyms":[[136,140]],"long-forms":[[110,134]]},{"text":"sider this sentence is correctly tagged.  Test data set 1 (TDS 1): contains about 10%  of the sentences from the complete emotion-","acronyms":[[59,64]],"long-forms":[[42,57]]},{"text":"to efficiently implement their computation.  Throughout Natural Language Processing (NLP), the typical dimensionality of databases, which are made","acronyms":[[77,80]],"long-forms":[[48,75]]},{"text":"description where i t  is beneficial.  The posit aeon of Linear Precedence (LP) state-  ments in th i s  formalism must now be c la r i f ied .","acronyms":[[71,73]],"long-forms":[[52,69]]},{"text":"compiled two datasets consisting of research papers from two top-tier machine learning conferences: Worldwide Wide Web (WWW) and Knowledge Uncovering and Data Mining (KDD).","acronyms":[[116,119],[162,165]],"long-forms":[[100,114],[125,153]]},{"text":"the titles of the entity articles titles(e) to represent the organizations in the consultations and two ranking functions, Recursive TFISF (R-TFISF) and LC, 3","acronyms":[[127,134],[140,142]],"long-forms":[[110,125]]},{"text":"tongues. Throughout Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10). ","acronyms":[[84,90],[12,16]],"long-forms":[[25,54]]},{"text":" respectively display the results obtained without and with the use of subcat features (SF). The sec-","acronyms":[[88,90]],"long-forms":[[71,86]]},{"text":"rate words (PERFECT). Second, we let the parser introduce the EEs itself (INSERT). ","acronyms":[[74,80],[12,19]],"long-forms":[[48,72]]},{"text":"........................................ LTH  ........... Liaison between STH and LTHs  TLink (Translation Link) between parlance LTHs  Figure 2: Example of an STH linked to a Fragment ","acronyms":[[85,90],[71,74],[79,83],[127,131],[157,160],[41,44]],"long-forms":[[92,108]]},{"text":"The last column lists the Spearman classification order correlation (?) of the rankings with the Berlin and Kay (B&K) categorized. ","acronyms":[[103,106]],"long-forms":[[87,101]]},{"text":"a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al, 1993) are converted into CCG derivations and a","acronyms":[[118,121],[18,25],[163,166]],"long-forms":[[103,116]]},{"text":"represented in Table 3 based on the place (bilabial  (BL), lab-dental (LD), dental (DE), alveopalatal  (AP), velar (VL), uvular (UV) and glottal (GT))  and manner of articulation (stops (ST), fricatives ","acronyms":[[146,148],[54,56],[71,73],[84,86],[104,106],[116,118],[129,131],[187,189]],"long-forms":[[137,144],[43,51],[59,69],[76,82],[89,101],[109,114],[121,127],[180,185]]},{"text":" The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test","acronyms":[[64,67]],"long-forms":[[46,62]]},{"text":"tor. Together, these alter reduce the BLEU score by 1.49 BLEU points (BP)9 at the largest training calibre.","acronyms":[[78,80],[46,50]],"long-forms":[[65,76]]},{"text":"exploited in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that uses ranking","acronyms":[[112,117]],"long-forms":[[90,110]]},{"text":"2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (","acronyms":[[88,90]],"long-forms":[[68,86]]},{"text":"shown in Figure 2. It is observed that the numbers of case of Conjunct Verb (ConjV),  Passives (Pass), Auxiliary Erected (AC) ","acronyms":[[82,87],[101,105],[132,134]],"long-forms":[[67,80],[108,130],[91,99]]},{"text":"gree (TTCD) 1, tongue body constriction location (TBCL) and degree (TBCD), lower tooth height (LTH), and glottal vibration (GLO). For example,","acronyms":[[124,127],[6,10],[50,54],[68,72],[95,98]],"long-forms":[[105,122],[15,48],[75,93]]},{"text":"n of  PSP Positive?penalties percentage (PSP) statistics  ","acronyms":[[40,43],[6,9]],"long-forms":[[10,38]]},{"text":"518 ? SS = Stanford parser style:5 the first conjunct is the head and the remaining conjuncts (as","acronyms":[[6,8]],"long-forms":[[11,32]]},{"text":"utterance. The interface default authorizing this is called Noun meaning extended (NMExt)13 NMExt: Noun(u), sem(u) is ?","acronyms":[[80,85],[89,94]],"long-forms":[[57,78]]},{"text":"making methods.  Latent semantic analysis (LSA) (Deerwester et al.,","acronyms":[[46,49]],"long-forms":[[20,44]]},{"text":"erences to the trainer in the posts etc.  3.3 Linear Chain Markov Model (LCMM) The logistic regression modeling is good at exploit-","acronyms":[[77,81]],"long-forms":[[50,75]]},{"text":"4.1 Selection of PPs in the Terminology Ours parser makes use of the computational lexicon HaGenLex (Hagen Deutsch Lexicon, see (Hartrumpf et al, 2003)), which is a general do-","acronyms":[[86,94],[17,20]],"long-forms":[[96,116]]},{"text":"The resulting unit denominates a concept which belongs to the language for special target (LSP). ","acronyms":[[93,96]],"long-forms":[[62,91]]},{"text":"should not be allowed?.  Gay Marriage (GM) 5","acronyms":[[39,41]],"long-forms":[[25,37]]},{"text":"Proc. of the IEEE International Conferences on Data Mining (ICDM). ","acronyms":[[59,63]],"long-forms":[[54,57]]},{"text":"2 Preposition Semantic Role Disambiguation in Penn Treebank Significant numbers of prepositional phrases (PPs) in the Penn treebank [1] are tagged with their semantic role relative to the governing verb.","acronyms":[[106,109]],"long-forms":[[83,104]]},{"text":"English?German 45.59 43.72 Automatically aligned corpora average 47.99?4.20 45.75?3.64 Table 1: The grammatical coverage (GC) of NF-ITG for different corpora dependent on the interpretation of word alignments: contiguous Translation Equivalence or discontiguous Translation Equivalence","acronyms":[[122,124],[129,135]],"long-forms":[[100,120]]},{"text":"At  the top level, >,sb,,~ denotes the fundamental relation for the  overall ranking of information structure (IS) patterns. ","acronyms":[[105,107]],"long-forms":[[82,103]]},{"text":"grammars. In Proceedings of the Fourteenth International Conference on Computational Linguistic (COLING-92), pages 426?432, Nantes, 1992.","acronyms":[[98,107]],"long-forms":[[57,96]]},{"text":"3.4 Innovative Trait of SPTK The most analogue kernel to SPTK is the Syntactic Semantic Tree Kernel (SSTK) proposed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Mos-","acronyms":[[103,107],[27,31],[59,63]],"long-forms":[[81,101]]},{"text":"Centro de Sondi E Imagen S.L. (Spain)  - Lead Industrial User  University of Sunderland (UK)  - Academic Research ","acronyms":[[89,91],[25,28]],"long-forms":[[63,87],[31,36]]},{"text":"tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17], and helps avoid local extrema by introducing inverse temperature ?.","acronyms":[[148,152],[13,16],[80,83]],"long-forms":[[120,146],[0,11]]},{"text":"859  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217?226, Seoul, South Korea, 5-6 July 2012.","acronyms":[[101,108]],"long-forms":[[51,99]]},{"text":"a first-order statistical language model can reduced perplex-  ity by at least a factor of 10 with little computation, while  applying complete natural vocabulary (NL) models of syn-  tax and semantics to all partial hypothesis typically requires ","acronyms":[[161,163]],"long-forms":[[143,159]]},{"text":" This work deals a major bottleneck in the development of Statistical MT (SMT) plan: the lack of sufficiently large parallel corpora for most","acronyms":[[78,81]],"long-forms":[[62,76]]},{"text":"ral probabilistic language model. In Advances in Neural Information Processing Systems (NIPS), 2000.","acronyms":[[88,92]],"long-forms":[[49,86]]},{"text":" To adapt for Chinese phonetic rule, we divide the  constants CLs into independent CLs(IC) and  schism structure of CL+VL+CL into CL+VL and ","acronyms":[[88,90],[63,66],[117,119],[120,122],[123,125],[131,133],[134,136]],"long-forms":[[72,86]]},{"text":" The SU detection task is conducted on two corpora: Broadcast News (BN) and Conversational Telephone Speech (CTS).","acronyms":[[68,70]],"long-forms":[[52,66]]},{"text":"CoTrain vs. BaseCN2 0.000144 4.77E-05 0.000247 CoTrain versus. BaseCN3 0.0009 0.000287 0.00139 CoTrain versus. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain versus. LEX(EN) 1.87E-05 1.64E-05 8.92E-07","acronyms":[[107,109],[150,153],[154,156]],"long-forms":[[91,98]]},{"text":"Since we are going to be  concerned with definability, we first translate CFGs  into CFTs (Context Free Theories). The ","acronyms":[[85,89],[74,78]],"long-forms":[[91,112]]},{"text":"\u0000 OBST(obstacle): The boy stumbled over a stumb.  \u0000 INTT (objective): He came there to look for Jane. ","acronyms":[[52,56],[2,6]],"long-forms":[[58,64],[7,15]]},{"text":"Abbreviations: BI=Bioinformatician, NLP=Naturel Language Processing investigators, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus. ","acronyms":[[146,148],[15,17],[36,39],[80,85],[120,128],[181,183]],"long-forms":[[149,168],[18,34],[40,67],[86,111],[129,137],[184,186]]},{"text":"\u0006? ( PM), tree matching without the auxiliary patterns (TM), and tree matching with the auxiliary habits","acronyms":[[56,58]],"long-forms":[[49,54]]},{"text":"scheme that includes: (1) a pre-annotation that segments the dialogue into turns which are further segmented into Elementary Discourse Units (EDUs) with the author of each turn automatically given;","acronyms":[[142,146]],"long-forms":[[114,140]]},{"text":"After applying the linguistic manifestations  resolution algorithm we obtains a new slot  structure (SS) that will store both the anaphora  and their antecedents.","acronyms":[[95,97]],"long-forms":[[84,93]]},{"text":"ducted. The bureaucratic body governing these decisions is the Institutional Review Board (IRB). ","acronyms":[[93,96]],"long-forms":[[65,91]]},{"text":"- harmonizing (COORD) : traite les c,'~s imples de  coordination,  - statistique (THERMOSTAT) : utilis6 sur des s6quences qu'il  est impossible de d6sambigui'ser A l'aide ","acronyms":[[83,87],[16,21]],"long-forms":[[70,81],[0,14]]},{"text":"2013 temporal summarization. In Proceedings of the 22nd Text Retrieval Conference (TREC), November.","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":" The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?, Cl?ment, and Kinyon","acronyms":[[118,121],[67,70]],"long-forms":[[101,116],[45,65]]},{"text":"(Joshi, Vijay?Shanker, and Weir, 1989; Weir and Joshi,  1988) have shown that LIGs, Combinatory Categorial  Grammars (CCG), Tree Adjoinig Grammars (TAGs),  and Head Grammars (HGs) are weakly equivalent.","acronyms":[[148,152],[78,82],[118,121],[175,178]],"long-forms":[[124,146],[84,116],[160,173]]},{"text":" 3 Bridgeman Art Bookstores Bridgeman Art Library (BAL)2 is one of the world?s top visuals libraries for art, culture and history.","acronyms":[[48,51]],"long-forms":[[25,46]]},{"text":"one of German, English or Japanese. The system has been designed  around the task of conference r gistration (CR). It has initially been ","acronyms":[[110,112]],"long-forms":[[85,108]]},{"text":"(see, e.g., (Moschitti, 2006) for more details).  Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps","acronyms":[[73,76]],"long-forms":[[50,71]]},{"text":"three statistical models: Conditional Random  Fields(CRF), Maximum Entropy(ME), and  Support Vector Machine(SVM), which have  good performance and used widely in the ","acronyms":[[108,111],[53,56],[75,77]],"long-forms":[[85,106],[26,52],[59,74]]},{"text":"Source Material (SMaterial) e.g., As, InGaAs   ? Source Material Features  (SMChar) e.g. ,(111)B  ","acronyms":[[82,88],[17,26],[38,44]],"long-forms":[[49,79],[0,15]]},{"text":"hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.  The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standard hardware, using our best-performing model, which compares very favorably with other","acronyms":[[143,146],[44,47]],"long-forms":[[122,141]]},{"text":" B. MTE features We use the following MTE metrics (MTFEATS), which compare the similarity between the question and a candidate answer:","acronyms":[[51,58],[4,7],[38,41]],"long-forms":[[42,49]]},{"text":"before the initiating of the current utterance.  Overlapping label (OL) an utterance on another channel with a particular DA tag superposition the","acronyms":[[63,65],[117,119]],"long-forms":[[44,61]]},{"text":"The SCBD structure.  prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases  (APs).","acronyms":[[44,47],[64,67],[4,8],[94,97]],"long-forms":[[21,42],[50,62],[74,91]]},{"text":"(diminishes dimensions). The general idea behind the  Pseudo Relevance Commentary (PRF) (Croft &  Harper, 1979) or its more recent variation called ","acronyms":[[78,81]],"long-forms":[[51,76]]},{"text":"<AbstractText Tags=?RESULTS? NlmCategory=?CONCLUSIONS?>Premature delivery rate was higher (p = 0.048) in the CKC clusters (14\/36, 38.88%) than in supervise group (14\/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985); and premature delivery was related to cone depth, OR was noticeably enhanced when the cone depth was more than","acronyms":[[188,190],[106,109],[268,270]],"long-forms":[[176,186]]},{"text":"Ihe maohine translation problem has recently been replacing  by much narrower goals and computer processing of vocabulary has  become part df artificial intelligence (AI), speech recognition,  and structures pattern recognition.","acronyms":[[164,166]],"long-forms":[[139,162]]},{"text":"pendencies is only context-free (section 2.1). Our argument is based on our desire to use a discourse grammar in natural language generation (NLG). It is well-known that","acronyms":[[142,145]],"long-forms":[[113,140]]},{"text":" CONCRETE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0 ZC05 (Zettlemoyer and Collins 2005) 79.3 ? ","acronyms":[[54,58]],"long-forms":[[60,88]]},{"text":" 1 Introduction Interactive question answering (QA) has been identified as one of the important directions in QA","acronyms":[[48,50],[110,112]],"long-forms":[[28,46]]},{"text":"2013. Overview of the pathway curation (PC) task of bioNLP shared task 2013.","acronyms":[[40,42]],"long-forms":[[22,38]]},{"text":"= li), ? Hierarchical loss (H-Loss) operandi is defined as:","acronyms":[[28,34]],"long-forms":[[9,26]]},{"text":"Electronic Text Encoding and Interchange were first published in April 1994 and were initially predicated on Standard Pervasive Markup Language (SGML).  The","acronyms":[[142,146]],"long-forms":[[104,140]]},{"text":" 5.2 Corpus Benchmark Tool The Corpus Benchmark Tool(CBT) is one of the components in GATE which enables automatic evaluation of an","acronyms":[[53,56]],"long-forms":[[31,51]]},{"text":"{tvu, aaiti, mzhang}@i2r.a-star.edu.sg  Abstract  Term Extraction (TE) is an pivotal component of many NLP applications.","acronyms":[[67,69],[105,108]],"long-forms":[[50,65]]},{"text":"Roget (RG) ablaze aglow alight argent auroral beaming blazing brilliant WordNet (WN) burnished sunny shiny lustrous undimmed sunshiny brilliant TransGraph (TG) nimble ringing fine aglow keen glad light picturesque Lin (LN) red yellow orange pink blue brilliant green white dark","acronyms":[[156,158],[219,221],[81,83],[7,9]],"long-forms":[[144,154],[214,217],[72,79],[0,5]]},{"text":" 3 The Discourses Model Informally, a DiscourseModel (DM)may be outline as the set of entities \"specified\" in a discourse, linked together by the relations they participate in.","acronyms":[[53,55]],"long-forms":[[37,51]]},{"text":"ing and understanding convolutional networks. In European Lectures on Computer Vision (ECCV). ","acronyms":[[89,93]],"long-forms":[[49,87]]},{"text":"performance\" (MOPs) presented here with  results we still requisite from system-external  \"measures of effectiveness\" (MOEs)25 MOE-  based methods appraise (i) baseline unaided ","acronyms":[[114,118],[14,18],[122,125]],"long-forms":[[86,111]]},{"text":"Pierre PN Noting that a preposition (PR) with lemma de and a determiner (DT) with lemma le and the same gender and number as the commons noun have been","acronyms":[[71,73],[7,9],[35,37]],"long-forms":[[59,69],[22,33]]},{"text":" ? Adjuncts (AM-): General arguments that any verb may take optionally.","acronyms":[[13,16]],"long-forms":[[3,11]]},{"text":"DC-10-30?s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.?","acronyms":[[126,129],[47,55],[0,8]],"long-forms":[[104,124]]},{"text":"\u0000 -movement (mostly wh-movement: WH), empty complementizers (COMP), empty units (UNIT), and traces representing pseudo-attachments","acronyms":[[61,65],[33,35],[81,85]],"long-forms":[[44,59],[74,78]]},{"text":"between interdependent ? E actions (as might arise for an  assumption such as ((ANB)?C)). It is straightforward ","acronyms":[[78,84]],"long-forms":[[57,72]]},{"text":"137 pare our system with a non-sequential classifier,  a supports vector machines (SVM), with the same  settings as those described above.","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"corporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features.","acronyms":[[95,97],[45,47],[113,115]],"long-forms":[[87,93]]},{"text":"Figure 3: The system architecture.   CA = communicative act. ","acronyms":[[37,39]],"long-forms":[[42,59]]},{"text":"predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy  score, H_AVE = human average score.9   ","acronyms":[[109,114],[36,38],[51,55],[79,83]],"long-forms":[[117,130],[28,34],[41,48],[58,71],[86,100]]},{"text":"we maximize the log likelihood J(?) using a simple optimization technological called stochastic gradient descent (SGD). N,W","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted","acronyms":[[93,95],[116,119]],"long-forms":[[75,91]]},{"text":"In Proceedings of the 10th World Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October. ","acronyms":[[117,120],[134,136]],"long-forms":[[104,115]]},{"text":"Discourse Relations (DR) 48.04 Entity Grid (EG) 67.74 Lexical Cohesion (LC) 61.63 Document Length 69.40","acronyms":[[72,74],[21,23],[44,46]],"long-forms":[[54,70],[0,19],[31,42]]},{"text":"DSTG = Adverb s t rs i rs ig   mRTOVO = For + Subject -+ to -+ Object  NA = N t- Adjective  NASOBJBE - N + as - t- Object  of be -","acronyms":[[69,71],[0,4],[29,35],[90,98]],"long-forms":[[74,88]]},{"text":"that together with the BOW it yields higher accuracy. Their results show a significant 1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant answer.","acronyms":[[110,112],[23,26]],"long-forms":[[93,108]]},{"text":"Universitat Polit`ecnica de Catalunya (UPC), Barcelona 2 Centro de Investigaci?on en Computaci?on (CIC), Instituto Polit?ecnico Nacional (IPN), Mexico 1","acronyms":[[99,102],[138,141],[39,42]],"long-forms":[[57,97],[105,136],[0,37]]},{"text":"Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). ","acronyms":[[85,88],[29,32],[44,47],[62,65]],"long-forms":[[72,83],[20,27],[35,42],[50,60]]},{"text":"tual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,","acronyms":[[92,96]],"long-forms":[[60,90]]},{"text":" The set of all homonyms constructing for a sentence is  called its morphological structure (MorphS). ","acronyms":[[86,92]],"long-forms":[[61,84]]},{"text":"Table 9 Number of times a core grammatical function was annotated more than once in the treebank (TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted morphology (PRED-M).","acronyms":[[140,146],[98,102],[194,200]],"long-forms":[[123,138],[88,96],[172,192]]},{"text":"however domain sublanguages are characterized by specific vocabularies, a well-defined border between specific sublanguages (SLs) and general language (GL) vocabularies is arduous to establish","acronyms":[[124,127],[151,153]],"long-forms":[[110,122],[133,149]]},{"text":"these 153,014 verb-noun collocations.  We utilizes 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)  as the Japanese thesaurus.","acronyms":[[65,68],[71,75]],"long-forms":[[48,63]]},{"text":" 2Regularized parses (henceforth, \"parse trees\") are  like F-structures of Lexical Ftmction Grammar (LFG),  except, sombrero a dependency architecture is used.\"","acronyms":[[101,104]],"long-forms":[[75,99]]},{"text":"(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE), WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning","acronyms":[[220,222],[8,11],[40,42],[73,75],[94,96],[118,121],[145,149],[176,179],[200,202],[250,257],[289,292],[310,319],[330,332],[347,350]],"long-forms":[[205,218],[23,38],[55,71],[78,92],[99,116],[124,143],[152,174],[182,198],[225,248],[267,279],[295,308],[322,328],[335,345]]},{"text":"true positive (TP) (i.e., a correct match), and an appropriate NNS triple not found in the gold standard set a false negative (FN) (i.e., an incorrect nonmatch), as shown in Table 4.","acronyms":[[127,129],[15,17],[63,66]],"long-forms":[[111,125],[0,13]]},{"text":"PP Fu?r diese Behauptung has the grammatical functioning OAMOD, which indicates that it is a modifier (MOD) of a direct object (OA) elsewhere in the structural (in this lawsuit keinen","acronyms":[[100,103],[125,127],[0,2],[54,59]],"long-forms":[[90,98],[105,109]]},{"text":"Dependency-Based Open Information Extraction Pablo Gamallo and Marcos Garcia Centro de Investigac?a?o sobre Tecnologias da Informac?a?o (CITIUS) Universidade de Santiago de Compostela","acronyms":[[137,143]],"long-forms":[[77,119]]},{"text":"of the reparandum tallies with the termination of  the fluent portion of the utterance, which we term the  INTERRUPTION SITES (IS). The DISFLUENCY INTERVAL ","acronyms":[[128,130]],"long-forms":[[109,126]]},{"text":"mental state labels that are vitally similar to the context of the scene in a latent, conceptual vector space; and an informational retrieval (IR) model that determines labels commonly appearing in sentences","acronyms":[[140,142]],"long-forms":[[117,138]]},{"text":"3 Results  The experiments were completed using the revised  RTE3 development set (RTE3Devmt) before the  RTE3Test findings were released.","acronyms":[[83,92],[106,114]],"long-forms":[[61,81]]},{"text":"computation of distributional thesauri (Lin, 1998) has been around for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications.","acronyms":[[154,157]],"long-forms":[[125,152]]},{"text":"understanding as follows: ? if the POS-tag of current word  is VB (Verb) and  its word-form  is ? can?","acronyms":[[60,62],[32,39]],"long-forms":[[64,68]]},{"text":"Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distance measures","acronyms":[[87,89],[51,53]],"long-forms":[[65,85],[33,49]]},{"text":" First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) (","acronyms":[[87,90],[36,38]],"long-forms":[[52,85]]},{"text":"However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers endeavour to jeu the system by submitting","acronyms":[[98,101]],"long-forms":[[69,96]]},{"text":"8  end if  Active characters are discussed in the section about identifying the SC (Section 8),  because the raison d'etre of the active-character component of an interpretation is that ","acronyms":[[80,82]],"long-forms":[[84,91]]},{"text":"For ex-  ample, temporal PPs, such as \"in 1959\", where  the prepositional object is tagged CD (cardi-  nal), favor attachment to the VP, because tile ","acronyms":[[91,93],[133,135],[25,28]],"long-forms":[[95,106]]},{"text":"SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Grammar Module","acronyms":[[45,47],[0,3],[26,28]],"long-forms":[[50,66],[6,25],[31,44]]},{"text":"LA     =   The average length (ALen) of chunks for each  genre is the average number of tokens in each chunk ","acronyms":[[31,35],[0,2]],"long-forms":[[15,29]]},{"text":" 1 LR  Parser  Generat ion   Tree Adjoining Grammars (label) are tree rewrit-  ing scheme which combine trees with the sin- ","acronyms":[[54,58],[3,5]],"long-forms":[[29,52]]},{"text":"For Task 2-2, we design two kinds of valuation metrics:  1) POS accuracy (POS-A)  This index is used to assess the performance ","acronyms":[[75,80]],"long-forms":[[61,73]]},{"text":"4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another popularly utilise decision rule is minimum Bayes risk (MBR): y?","acronyms":[[125,128],[18,26],[52,55]],"long-forms":[[105,123]]},{"text":"4.4.2 Optimization To maximize the objective in (6), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al, 2009).","acronyms":[[94,97]],"long-forms":[[65,92]]},{"text":"Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used similarity measure in Information Retrieval(IR). It has also been shown","acronyms":[[112,114],[43,49]],"long-forms":[[90,110],[0,41]]},{"text":"usually similar with that in word sense disambiguation (WSD), consisting luggage of word lemmas  in the sentence, n-grams and parts of speech (POS)  in a window, etc.","acronyms":[[138,141],[56,59]],"long-forms":[[121,136],[29,54]]},{"text":"pooled by parts of discourse (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) Figure 3: Plots of imageability scores for literal vs. nonliteral\/metaphorical words in the VUAMC, clusters by parts of discourse (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) clearest distinction between literal vs. nonliteral item.","acronyms":[[223,225],[39,41],[177,182]],"long-forms":[[226,236],[30,37],[42,52],[56,60],[64,68],[72,83],[214,221],[240,244],[248,252],[256,267]]},{"text":"S2LS. The finest parameters were then used on the Senseval-3 English Lexical Sample task (S3LS), where a similar semi-supervised method was utilize","acronyms":[[88,92],[0,4]],"long-forms":[[48,86]]},{"text":"known as conditional random minefields (CRFs) (Lafferty et al, 2001), when all variables are observed, and as disguising conditional random campos (HCRFs) (Quattoni et al, 2007), when only a subset of the variables are","acronyms":[[140,145],[36,40]],"long-forms":[[106,138],[9,34]]},{"text":" Doubly it is labeled as a noun phrase (NP) and once as a prepositional phrase (PP). ","acronyms":[[79,81]],"long-forms":[[57,77]]},{"text":"(http:\/\/ieee.rkbexplorer.com\/) repository7. The corpus of cancer research (COCR) contains 3334 domain specific abstracts of scientific publica-","acronyms":[[75,79]],"long-forms":[[48,73]]},{"text":"of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The data","acronyms":[[132,136],[71,74],[87,90],[107,110]],"long-forms":[[117,130],[63,69],[77,85],[93,105]]},{"text":" In Proceedings of the International Conference on Computational Linguistics (COLING-04). ","acronyms":[[78,87]],"long-forms":[[51,76]]},{"text":"tried two types of expansion, one mainly using synonyms (SYN), and one mainly utilised hypernyms or related links (LNK). ","acronyms":[[112,115],[57,60]],"long-forms":[[105,110],[47,55]]},{"text":"RLP = rule  learner prediction. RS = Reference Standard   ","acronyms":[[32,34],[0,3]],"long-forms":[[37,55],[6,30]]},{"text":"fying the native language based on the manner of speaking and writing a second language is borrowed from Second Language Acquisition (SLA), where this is known as language transfer.","acronyms":[[134,137]],"long-forms":[[105,132]]},{"text":"and (W-1,W0,W1) ? Gazetteers (GAZ): We use deux sets of gazetteers.","acronyms":[[30,33],[5,8],[9,11],[12,14]],"long-forms":[[18,28]]},{"text":"ceedin.qs, IEEE-1ECEJ-ASJ htternational Con-  ference on Acoustics, Speech, and Signal Process-  ing (ICASSP), 2bkyo, April 1986. ","acronyms":[[102,108],[11,25]],"long-forms":[[26,100]]},{"text":"e-maih ide@cs,  vassar ,  edu   Abstract. The Text Encoding Initiative (phone) is an  international project established in 1988 to develop ","acronyms":[[72,75]],"long-forms":[[42,70]]},{"text":" The objective of this examining is to illustrate a  word support model (WSM) that is able to enhance our WP-identifier by achieving better ","acronyms":[[69,72],[102,104]],"long-forms":[[49,67]]},{"text":" 2.3 Named Entity Recognition Named Entity Recognising (NER) is the task of conclusions all instances of explicitly named entities","acronyms":[[56,59]],"long-forms":[[30,54]]},{"text":"Sematnic data models are systems for  constructing precise descriptions of protions of  the real world - semantic data description (SDD)-  using terms that come from the real world rather ","acronyms":[[132,135]],"long-forms":[[105,130]]},{"text":"In ? Proceedings of  the Eighth Text REtrieval Conference (TREC-9)?, ","acronyms":[[59,65]],"long-forms":[[25,57]]},{"text":"Semantic Specialization (ISS) (Girju, Badulescu,  and Moldovan, 2006), Na?ve Bayes (NB) 3  and  Highest Entropy (ME)4.  ","acronyms":[[113,115],[25,28],[84,86]],"long-forms":[[96,111],[0,23],[71,82]]},{"text":"composition process.  4.1 Tag Guided RNN (TG-RNN) We propose Tag Guided RNN (TG-RNN) to re-","acronyms":[[42,48],[77,83]],"long-forms":[[26,40],[61,75]]},{"text":"? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples  The difficulty of identifying unknown words in ","acronyms":[[43,47],[11,15]],"long-forms":[[34,42],[2,10]]},{"text":"We used four machine learning algorithms implemented in Mallet (McCallum, 2002): decision tree, Naive Bayes, maximum entropy (MaxEnt), and conditional random field (CRF).5 Table 4 shows the","acronyms":[[126,132],[165,168]],"long-forms":[[109,124],[139,163]]},{"text":"Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).  (MFCCs)1 and Teager Energy Operator (TEO)2 (Kaiser, 1990) based features have also been con-","acronyms":[[151,154],[115,120]],"long-forms":[[127,149]]},{"text":"grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) Figure 3: Plots of imageability scores for literal vs. nonliteral\/metaphorical words in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) clearest distinction between literal vs. nonliteral items.","acronyms":[[223,225],[39,41],[177,182]],"long-forms":[[226,236],[30,37],[42,52],[56,60],[64,68],[72,83],[214,221],[240,244],[248,252],[256,267]]},{"text":"while the NB+E extractor has the worse. Training the CRF with negative examples (CRF+E) gave better precision in extracted informational then train-","acronyms":[[81,86],[10,14]],"long-forms":[[53,79]]},{"text":"dominated by different Root Nodes. In this table, for each  possible Root Node category (RN), its corresponding Head  Node (HN), Dependent Nods\/8 (DN) and Control Features (CF), ","acronyms":[[89,91],[124,126],[147,149],[173,175]],"long-forms":[[69,78],[112,122],[129,145],[155,171]]},{"text":"that have to be defined in a theory-specific way.  Therefore a document representation (DocRep) has two constituents, a DocAttr and a DocRepSeq.","acronyms":[[83,89],[113,119],[127,136]],"long-forms":[[58,81]]},{"text":"only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al.,","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"shown by Kusner and colleagues (2015), semantic representations such as Smouldering Semantic Indexing and Inactive Dirichlet Allocation (LDA) can outperform a BoW representation.","acronyms":[[130,133],[152,155]],"long-forms":[[101,128]]},{"text":"The most  important formulas of discourse of interest o  Natural Language Processing (NLP) are text  and dialogue.","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":" 1 Introduction Word sense disambiguation (WSD) is the task of assigning sense tags to ambiguous lexical items","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"The TRAVEL system here uses a wide range of statistic driven preprocessing, including parties of speech tagging, constituent bracketing, inter-pretation of unspecified phrases using WordNet, and named-entity recognition (Allen et al 2008). All these are generic off-the-shelf resources that ex-tend and help guide the deep parsing process.  The TRIPS LF (logical form) ontology1 is de-signed to be linguistically motivated and domain independant. The semantic types and selectional limitations are driven by linguistic considera-tions rather than requirements from reasoning ingredients in the system (Dzikovska et al 2003).","acronyms":[[345,347],[339,344],[4,9]],"long-forms":[[349,361]]},{"text":" 1997) has led us to employ, among other param-  eters, mutual information (MI) bits of individ-  ual characteristics derivative from large hierarchically ","acronyms":[[76,78]],"long-forms":[[56,74]]},{"text":"? All adjectives appearing after an article and a particle (PART) have the diploma positive (Pos) (39 of 39 cases).","acronyms":[[60,64],[92,95]],"long-forms":[[50,58],[82,90]]},{"text":"When ready and mature,  technology and language processing techniques will be  incorporated into Foreign Broadcast Information Service (FBIS)  processing.","acronyms":[[136,140]],"long-forms":[[97,134]]},{"text":"\\[ Class (Tag) Kernel Nouns  act (AC)  an~ (AN)  art~fact (AR) ","acronyms":[[44,46]],"long-forms":[[39,42]]},{"text":"3.1 Data and preprocessing Totality embeddings are trained on 22 mln tokens from the the North American News Text (NANT) corpus (Graff, 1995).","acronyms":[[114,118]],"long-forms":[[88,112]]},{"text":"Dispersal Lexicalised hallmarks and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268?275.","acronyms":[[143,148],[53,56]],"long-forms":[[88,141]]},{"text":"of the set of terminal symbols) or empty strings. A Phrase Structure Tree (PST) is a tree in which all and only the leaf nodes are labeled with words or","acronyms":[[75,78]],"long-forms":[[52,73]]},{"text":" 2 CFN and Its SRL task Chinese FrameNet(CFN) (Doyou et al, 2005) is a research project that has been elaborated by Shanxi","acronyms":[[41,44],[3,6],[15,18]],"long-forms":[[24,39]]},{"text":"Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). The number of entities","acronyms":[[116,121]],"long-forms":[[100,114]]},{"text":"ing to their different degree of specification. In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect","acronyms":[[90,93],[112,116],[127,130]],"long-forms":[[79,88],[103,110],[119,125]]},{"text":"for si ? Bm do Utilizes Breadth First Search (BFS) to check if ?","acronyms":[[41,44]],"long-forms":[[19,39]]},{"text":"At LREC 2006, Genoa Yes?im Aksan and Mustafa Aksan 2012. Constructs of the Turkish National Corpus (TNC). In LREC 2012,","acronyms":[[102,105],[3,7]],"long-forms":[[77,100]]},{"text":"topic distributions for all phrase pairs in the phrase table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underly-","acronyms":[[137,140]],"long-forms":[[108,135]]},{"text":"on Artificial Intelligence (KI2002), volume 2479 of Lecture Notes in Artificial Intelligence (LNAI), pages 18?32, Aachen, Germany, September. ","acronyms":[[94,98],[28,34]],"long-forms":[[52,92]]},{"text":"data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from","acronyms":[[76,78],[95,97],[10,12],[104,106]],"long-forms":[[60,74],[84,93]]},{"text":"Abstract In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"142 ? National University of Mongolia (NUM),  Meng ","acronyms":[[39,42]],"long-forms":[[6,37]]},{"text":"Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on text-based features for automatically predicting eight different speech acts derived from a taxonomy called Verbal Response Modes (VRM). The experiments are conducted","acronyms":[[217,220]],"long-forms":[[194,215]]},{"text":"details of this collection.  AECMA Simplified English (AECMA-SE) (AECMA 1986) was the predecessor of ASD Simplified Technical English.","acronyms":[[55,63],[66,71],[101,104]],"long-forms":[[29,53]]},{"text":"ductive transfer learning. Across Trials of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Bio-","acronyms":[[95,99],[49,53]],"long-forms":[[54,93]]},{"text":"NNS?, in this paper; other work makes a distinction between ESL (English as a Second Language) speakers (who live and speak in a primarily English-speaking environment) or EFL","acronyms":[[60,63],[0,4],[172,175]],"long-forms":[[65,93]]},{"text":"text corpus, to be made available without royalties for scientific investigate. The text will  be formatted utilizing SGML (the Standard Generalized Markup Language). To date ","acronyms":[[111,115]],"long-forms":[[121,157]]},{"text":"services (person or company information bureau), ht this  situation, the event aml the attitude of archival inforumtion  holders (IP) is transported to a speaker (SP);journalist. ","acronyms":[[161,163],[128,130]],"long-forms":[[152,159],[104,126]]},{"text":"removed for expository reasons.  rewrites into an (optional) sentence adjunct (SA), a  subject, a verbphrase and subject's right adjunct ","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"typically expressed inTopic Statements.  The Subject Field Coder (SFCoder) uses an  establishet~ semantic oding scheme from the machine- ","acronyms":[[66,73]],"long-forms":[[45,64]]},{"text":"PROP  VP  I PROP = proposition  These-fragments would match a locative object use of a p r epos~ .~ lu r~  arlu L H ~ .","acronyms":[[12,16]],"long-forms":[[19,30]]},{"text":"Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus","acronyms":[[111,113]],"long-forms":[[94,109]]},{"text":"proposed by (Jia and Zhao, 2013). We will mainly consider MIU accuracy (MIU-Acc) which is the ratio of the number of completely corrected gen-","acronyms":[[72,79]],"long-forms":[[58,70]]},{"text":"cal patterns and the impacts of different  constraints that are usage to identify the  Complex Predicates (CPs). System ","acronyms":[[106,109]],"long-forms":[[86,104]]},{"text":"to pour) method that the entity (wine) is localized to externally locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle).","acronyms":[[117,120]],"long-forms":[[107,112]]},{"text":"son to visit Udaipur.  Parse: December to March is [NP (np the  best season) [SBAR [S (dcP to visiting Udaipur)]]] .","acronyms":[[53,55],[79,83]],"long-forms":[[57,59]]},{"text":"plementation of SVR, with tuned parameters.  Classifications: An SVM model for ranking (SVMRank) is trained using as classifications pairs all pairs of stu-","acronyms":[[80,87],[16,19]],"long-forms":[[57,78]]},{"text":"onomy using a Japanese-English bilingual vocabulary as  a \"bridges\", in order to support semantic processing in a  knowledge-based machine translation (MT) system. ","acronyms":[[151,153]],"long-forms":[[130,149]]},{"text":"URL?, excluded utterances with the symbols that indicate the re-posting (RT) or quoted (QT) of others?","acronyms":[[89,91],[0,3],[73,75]],"long-forms":[[80,87],[61,71]]},{"text":"Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning tech-","acronyms":[[110,112]],"long-forms":[[89,108]]},{"text":" 1 Introduction Interactive question replying (QA) has been identified as one of the important directions in QA","acronyms":[[48,50],[110,112]],"long-forms":[[28,46]]},{"text":"LS (List item markers) LS  MD (Modal) MD  NN (Noun, singular or mass) N  NNS (Noun, plural) N ","acronyms":[[41,43],[0,2],[22,24],[26,28],[37,39],[72,75]],"long-forms":[[45,49],[30,35],[77,81]]},{"text":"There are four basic phrases in Korean: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP). Thus, chunking by rules is","acronyms":[[122,124],[53,55],[71,73],[91,95]],"long-forms":[[102,120],[40,51],[58,69],[76,89]]},{"text":"semantic treatments component. A careful descrip-  tion of the lexical conccplual structure (LCS) which  serves as the interlingua is not given here, but see ","acronyms":[[94,97]],"long-forms":[[64,92]]},{"text":"user?s state in the given session. For this research, the assistance vector machine (SVM) is used as a classifier.","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). ","acronyms":[[262,268],[315,321],[47,50],[199,203]],"long-forms":[[236,253],[279,313],[165,180]]},{"text":" 2011b. Overview of the entity relations (REL) supporting task of BioNLP Shared Task 2011.","acronyms":[[42,45]],"long-forms":[[31,40]]},{"text":"The relative clause itself has the category S; the incoming edge is labelled RC (relative clause). ","acronyms":[[76,78]],"long-forms":[[80,95]]},{"text":"Linear Program relaxation based on single-commodity flow. LP(M): Linear Program relaxation based on multi-commodity flow.","acronyms":[[58,60]],"long-forms":[[65,79]]},{"text":"a baseline.  Language Model (LM): We model the semantic fit of a candidate substitute within the given context","acronyms":[[29,31]],"long-forms":[[13,27]]},{"text":" ? Subordinated clause reordering (SubCR) which involve moving postnominal relative","acronyms":[[34,39]],"long-forms":[[3,32]]},{"text":"We implement MVM using generative model primitives drawn from Latent Dirichlet Allocation (LDA) and the Dirichlet Process (DP). |M | disparate","acronyms":[[123,125],[13,16],[91,94]],"long-forms":[[104,121],[62,89]]},{"text":"integer linear programming (ILP) conditional haphazard field (CRF) support vector machine (SVM) latent semantic analysis (LSA)","acronyms":[[88,91],[28,31],[59,62],[119,122]],"long-forms":[[64,86],[0,26],[33,57],[93,117]]},{"text":" rio treating with unknown words is a big problem in  natural language processing(NLP) too. To recognize ","acronyms":[[78,81]],"long-forms":[[50,76]]},{"text":" Single-tokenization of compound verbs  and named entities (NE) provides significant gains over the baseline PB-SMT ","acronyms":[[60,62],[109,115]],"long-forms":[[44,58]]},{"text":"derived from these MDPs: (1) Diff?s: the number of states whose policy ranged from the Baseline 2 policy, (2) Percent Policy modifying (P.C.): the weighted amount of amendments between the two policies (100%","acronyms":[[134,138],[19,23]],"long-forms":[[119,132]]},{"text":" 1 LR  Parser  Generat ion   Tree Adjoining Grammars (TAGs) are tree rewrit-  ing systems which combine trees with the sin- ","acronyms":[[54,58],[3,5]],"long-forms":[[29,52]]},{"text":"placed in a transcript. Here, we accent on the syndrome of primary progressive aphasia (PPA). PPA","acronyms":[[87,90],[93,96]],"long-forms":[[58,85]]},{"text":"side were installed from 2003 to 2005.?  3.4 Sentence Reordering (RE) Some of the transformation operations results in","acronyms":[[66,68]],"long-forms":[[54,64]]},{"text":"We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter.","acronyms":[[119,122]],"long-forms":[[93,117]]},{"text":"For the bilingual corpus, we  utilised the BTEC and PIVOT data of IWSLT 2008,  KNOCKED corpus 5  and other Chinese LDC (CLDC)                                                            ","acronyms":[[112,116],[39,43],[48,53],[62,67],[75,78]],"long-forms":[[99,110]]},{"text":"n of  PSP Positive?sentence percentage (PSP) statistics  ","acronyms":[[40,43],[6,9]],"long-forms":[[10,38]]},{"text":" 2. Effort of Association (EA): a mc~sure of the effort  required to associate some entity with lira description ","acronyms":[[27,29]],"long-forms":[[4,25]]},{"text":"Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection  (RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All). ","acronyms":[[135,143],[59,63],[97,106],[161,165],[178,185]],"long-forms":[[113,133]]},{"text":"  SVM Classification  SVM (Support Vector Machines) has enticed  much attention since it was introduced in (Boser et ","acronyms":[[22,25],[2,5]],"long-forms":[[27,50]]},{"text":"Beijing University of Posts and Telecommunications (BUPT) ? ?  Beijing Institute of Technology (BIT) ?  ","acronyms":[[96,99],[52,56]],"long-forms":[[63,94],[0,49]]},{"text":"(3) The measurement introduced by Resnik (Resnik, 1995) (RES) restitution the information content (IC) of the LCS of two concepts:","acronyms":[[91,93],[53,56],[102,105]],"long-forms":[[70,89]]},{"text":"Common errors measurements are the Word Mistakes Rate (WER) and the Position Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and","acronyms":[[98,101],[47,50],[161,165]],"long-forms":[[60,96],[30,45]]},{"text":"3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al.,","acronyms":[[99,102],[46,50]],"long-forms":[[67,97]]},{"text":"In contrast to standard 357 multi-class Word Sense Disambiguation (WSD), it utilized a coarse-grained sense inventory that allows to","acronyms":[[67,70]],"long-forms":[[40,65]]},{"text":"3.4 Case  of  in tegrat ion   Figure 3 demonstrates the initiates point of an integra-  tion process with the trigger word (TW) lelter, its  definition, its temporary graph (TG), the concept ","acronyms":[[119,121],[169,171]],"long-forms":[[105,117],[152,167]]},{"text":" 3 Keystroke Data Collection Amazon?s Mechanical Turk (MTurk) is a web service that enables crowdsourcing of tasks that are dif-","acronyms":[[55,60]],"long-forms":[[38,53]]},{"text":"adjectives, and specifies the participants and properties of the situation it outlining, the so called frame elements (FEs). ","acronyms":[[119,122]],"long-forms":[[103,117]]},{"text":"Hence, it seems plausible to  use a back-off mechanism for these sentences  via a combined system (COMB) integrating  NB only for the sentences that fail to parse.","acronyms":[[103,107],[124,126]],"long-forms":[[86,94]]},{"text":"3.5 A Novel Lattice Statistical Language Model Representation Our final statistical language model is a novel latent-variable statistical language model, called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The","acronyms":[[184,190]],"long-forms":[[163,182]]},{"text":"In addition, we also  experimented with multiple combinations of  translate models (TM), phrase-based and  factor-based, trained on various datasets to ","acronyms":[[87,89]],"long-forms":[[67,85]]},{"text":"6 6   Proceedings of the 13th Annual Meeting of the Special Interest Panels on Discourse and Dialogue (SIGDIAL), pages 79?83, Seoul, Southern Korea, 5-6 July 2012.","acronyms":[[102,109]],"long-forms":[[52,100]]},{"text":"3. Formal descriptions about specific games which are classified as servant texts (FoT) 4.","acronyms":[[82,85]],"long-forms":[[68,80]]},{"text":"Thus, we  name our system for generating compressions the  Adjustable Rate Compression (ARC).   ","acronyms":[[87,90]],"long-forms":[[59,85]]},{"text":"? airplane?. We will call this the GN (general noun) lexicon.","acronyms":[[35,37]],"long-forms":[[39,51]]},{"text":"tical problems in class. It might similarly be useful in massive open online courses (MOOCs). In this for-","acronyms":[[81,86]],"long-forms":[[52,79]]},{"text":"R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms  13th International Conferences on  Computational Linguistics (COLING'90). ","acronyms":[[125,134]],"long-forms":[[98,123]]},{"text":"In support to the on going project of Multilingual  Machine Translation Sytem for Asian Language organized by  Center for International Cooperaliou inComputerization (CICC)-  Japan and other Asian cotmtries (China.","acronyms":[[167,171]],"long-forms":[[111,165]]},{"text":"Semitic languages (in which vowels are not written), etc.  The problem of word sense disambiguation (WSD) has been described as \"AI-complete,\"  that is, a problem which can be solving only by first dissipating all the uphill problems ","acronyms":[[101,104]],"long-forms":[[74,99]]},{"text":"for a comprehensive comparison: ? Mean absolute error (MAE) measures how closely predictions resemble their observed","acronyms":[[55,58]],"long-forms":[[34,53]]},{"text":" Twice it is labeled as a noun phrase (NP) and once as a prepositional phrase (PP). ","acronyms":[[79,81]],"long-forms":[[57,77]]},{"text":"Draw Team (DT_DT); Team ? Contests (TM_CP); Team ? City \/ Province \/ Country ","acronyms":[[39,44],[11,16]],"long-forms":[[33,37],[0,9]]},{"text":"We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence.","acronyms":[[408,411],[612,615]],"long-forms":[[379,406],[586,610]]},{"text":"AO = all objects  MO = matched objects  TF = text filtered  FM = F-measures ","acronyms":[[40,42],[0,2],[18,20],[61,63]],"long-forms":[[45,59],[5,16],[23,38],[66,76]]},{"text":"But still we can say that even in languages with  that kind of structural properties like Slavic languages have, named entities (NE) form a subset of  natural language expressions that demonstrates ","acronyms":[[129,131]],"long-forms":[[113,127]]},{"text":"The lack of a  fully comprehensive bilingual dictionary including  the entries for all named entities (NEs) renders the  task of transliteration necessary for certain natural ","acronyms":[[103,106]],"long-forms":[[87,101]]},{"text":"Based on these two conditions we have come up with a linear combination of two cost components, similar to Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998).","acronyms":[[135,138]],"long-forms":[[107,133]]},{"text":"son to visit Udaipur.  Parse: September to March is [NP (np the  best season) [SBAR [S (dcP to visit Udaipur)]]] .","acronyms":[[53,55],[79,83]],"long-forms":[[57,59]]},{"text":" 4.4 Tokenizing Multiword Expressions      Multiword Phrase (MWEs) are two or  more expression that behave like a single word syntac-","acronyms":[[66,70]],"long-forms":[[43,64]]},{"text":"post-process of the internal diacritization task  utilize the same machines learning approach that  was trained on Base phrase (BP)-Chunk as well  as POS feature of individual tokens with correct ","acronyms":[[125,127],[147,150]],"long-forms":[[112,123]]},{"text":"Lima or Jessica Alba??. Therefore, we decided to employ a Conditional Random Fields (CRF) tagger (Lafferty et al, 2001) to the task, since CRF","acronyms":[[85,88],[139,142]],"long-forms":[[58,83]]},{"text":"After applying the linguistic phenomena  resolution algorithm we obtain a new slot  structure (SS) that will store both the anaphora  and their antecedents.","acronyms":[[95,97]],"long-forms":[[84,93]]},{"text":"approach to structuring knowledge is based  on:  z automatic term recognition (ATR)  z automatic term clustering (ATC) as an ","acronyms":[[79,82],[114,117]],"long-forms":[[51,77],[87,112]]},{"text":" FERGUS was originally qualified on the Penn Tree Bank corpus consisting of Wall Rue Journal text (WSJ). The results on","acronyms":[[100,103],[1,7]],"long-forms":[[74,93]]},{"text":" 3.2 Krone Constructicon The Swedish Constructicon (SweCcn) 4","acronyms":[[54,60]],"long-forms":[[31,52]]},{"text":"tourist resources was made by the Direcc?a?o Geral de Turismo (DGT) and afterwards the Inventory of Tourist Resources (IRT) emerged. ","acronyms":[[119,122],[63,66]],"long-forms":[[87,117],[45,61]]},{"text":"regression model. At our regression task we utilizes a Generalised Linear Model (GLM) via penalized maximum likelihood (Friedman et al, 2010).","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":" 2.2 W3C Semantic Web The Monde Wide Web (WWW) was once designed to be as uncomplicated, as decentralized and as interop-","acronyms":[[42,45],[5,8]],"long-forms":[[26,40]]},{"text":"of Petrov et al. ( 2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (prepositions), DET (determiners and clause), ADP","acronyms":[[54,57],[72,75]],"long-forms":[[59,69],[77,84]]},{"text":"Furthermore, to create a fully text-bound subset, family memberships relations (MEMBER) were resolved into single edges and suitable references to","acronyms":[[80,86]],"long-forms":[[57,78]]},{"text":" 3.1.1 Pre-Processing At Feature Extraction Phrase Analyzes(PA): Using basic syntactic analysis (shallow parsing), the PA module re-builds","acronyms":[[61,63],[120,122]],"long-forms":[[45,59]]},{"text":"above.  Symbolic Inflectional Classes (IICs) are ICs that are manually fully annotated, i.e., they have all the tem-","acronyms":[[37,41]],"long-forms":[[8,35]]},{"text":"tres modules: (1) a question processing (QP) module; (2) a passage recoup (PR) module; and (3) an answer processing (AP) module. Questions","acronyms":[[121,123]],"long-forms":[[102,119]]},{"text":"of the first studies to researches such constancy is Genzel and Charniak (2002), in which the authors suggests the Unchangeable Entropy Rate (CER) hypothesis: in written text, the entropy per sig-","acronyms":[[139,142]],"long-forms":[[116,137]]},{"text":"ALL X X X 0.614 0.186 0.706 0314 0.509 Table 2: Pierson?s ? for each featured set (FSet), as well as combinations of attribute sets and adap-","acronyms":[[82,86]],"long-forms":[[69,80]]},{"text":"is a very laborious and costly process. Silver standard corpus (SSC) annotation is a very recent direction of corpus development which","acronyms":[[64,67]],"long-forms":[[40,62]]},{"text":" 2 Abstract Syntax Trees We describe abstract syntax trees (ASTs) using an example from CFR Section 610.11:","acronyms":[[60,64],[88,91]],"long-forms":[[37,58]]},{"text":"1 This work has been developed in the project KFr-FAST (KIT = Kilnstliche  Intelligenz und Textverstehen (Artificial Intelligence and Text  Understand); SPEEDY = Functor Argument S ructure for Translation), which  constitutes the Berlin component of the complementary research projects of ","acronyms":[[156,160]],"long-forms":[[163,189]]},{"text":"full PTB, using 1st sense information. All results  are exhibited as labelled attachment score (LAS). ","acronyms":[[92,95],[5,8]],"long-forms":[[65,90]]},{"text":"tive standard deviation of three intervals, left  edge to anchor (LE-A), center to anchor (CC-A),  right edge to anchor (RE-A), calculated across  productions of pot, sot, spot, lot, plot, and splot ","acronyms":[[121,125],[91,95],[66,70]],"long-forms":[[99,119],[73,89],[44,64]]},{"text":" 2 Sign language phenomena Sign Languages (SLs) involve simultaneous manual and non-manual components for conveying mean-","acronyms":[[43,46]],"long-forms":[[27,41]]},{"text":"1985) in which Tree Adjoining Grammars slumps. 3 Since the  set of Tree Neighbors Languages (TALs) is a strict super-  set of the set of Context Free Languages, in order to determining ","acronyms":[[91,95]],"long-forms":[[65,89]]},{"text":"the predicted margin. Dredze and Crammer (2008a) showed how Confidence Weighted (CW) learning could be used to generate a more informative mea-","acronyms":[[81,83]],"long-forms":[[60,79]]},{"text":"Since it is costly to compute the partition function over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and","acronyms":[[118,121]],"long-forms":[[87,116]]},{"text":"Above all, our targeted is to integrate cross-media inference and create the linkage amongst the information extracted from those heterogenous data. Unser novel Multi-media Information Networks (MiNets) representation initializes our idea about a basic ontology of the ranking system.","acronyms":[[187,193]],"long-forms":[[159,185]]},{"text":"S2LS. The best parameters were then used on the Senseval-3 English Lexical Sample task (S3LS), where a similar semi-supervised method was used","acronyms":[[88,92],[0,4]],"long-forms":[[48,86]]},{"text":"modeled as distributed dense vectors of hidden layers. A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":" Performance has been measured with both the question followed by an extension (Q+E), as well as the question followed by the target and then","acronyms":[[80,83]],"long-forms":[[45,78]]},{"text":" MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and period information.","acronyms":[[68,71],[1,4],[22,25],[43,46]],"long-forms":[[52,60],[11,20],[28,41]]},{"text":"phrases to handling indeterminate phrases when translating from English into Chinese (E2C) and from Castellano into English (S2E). For all baselines we","acronyms":[[116,119],[80,83]],"long-forms":[[94,114],[58,78]]},{"text":"AS for word  edict,  ther{{ ar<~ basieal\\]}Z two  phrase  'types i n  German:  noun-dependent   phrase,  l i ke  no(:~n expression  ( NP ) and  prepos i t iona l  phrase  ( !:'","acronyms":[[131,133]],"long-forms":[[114,127]]},{"text":"show how to construct MWE-aware training resources for them.  The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?,","acronyms":[[128,131],[22,31],[179,182]],"long-forms":[[111,126],[162,177]]},{"text":"PARSEVAL scores for our parse results. We focus on the Labeled Precision (LP) and Labeled Recall (LR) scores only in this paper, as these are","acronyms":[[74,76],[0,8],[98,100]],"long-forms":[[55,72],[82,96]]},{"text":"markert@l3s.de Abstract Automation timeline summarization (TLS) generates precise, dated overviews over","acronyms":[[58,61]],"long-forms":[[34,56]]},{"text":"et al, 1993). The learning algorithm was inspired  by several Inductive Logic Programming (ILP) sys-  tems and primarily consists of a specific-to-general ","acronyms":[[91,94]],"long-forms":[[62,89]]},{"text":"In Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5), pages 269?290, Fr?jus.","acronyms":[[62,66]],"long-forms":[[42,60]]},{"text":"provement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent","acronyms":[[80,86]],"long-forms":[[64,78]]},{"text":"that these heuristics have much effect not only in  the inductive inference (regular SVM) but similarly in  transductive inference (TSVM), especially when  the untagged data is gigantic.","acronyms":[[127,131],[85,88]],"long-forms":[[103,125]]},{"text":"Two document collections are utilizes in this study.  CETTE (Chinese Encyclopedia): This is from the electronic version of the Chinese Encyclopedia.","acronyms":[[50,52]],"long-forms":[[54,74]]},{"text":"The approach involved training a norms Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al.,","acronyms":[[104,106],[63,66]],"long-forms":[[78,102],[42,61]]},{"text":"Proc. ACM Multimedia (MM), ACM, Florence, Italy. pp.","acronyms":[[22,24]],"long-forms":[[15,20]]},{"text":" 3 Data The RST Discourse Treebank (RST-DT) (Carlson et al, 2002) was uses for training and testing.","acronyms":[[36,42]],"long-forms":[[12,34]]},{"text":"BP as a computational expression graph ? Automatic differentiation (AD) 7.","acronyms":[[68,70],[0,2]],"long-forms":[[41,66]]},{"text":"ambiguous verb structure in a garden-path in two shapes; one is as a subordinate clause (MV), the other is a Shrunk Relative (RR). He defined","acronyms":[[125,127],[87,89]],"long-forms":[[107,123]]},{"text":"bilities. Experience has shown that this kind of  full-fledged question answer (QA) over texts  from a wide range of domains is so laborious for ","acronyms":[[83,85]],"long-forms":[[63,81]]},{"text":"represents the UTB statistics. For Telugu, the Telugu Treebank (TTB) released for ICON 2010 Shared Task (Husain et al. ( 2010)) was used for evaluation.","acronyms":[[64,67],[15,18],[82,86]],"long-forms":[[54,62]]},{"text":"ules, distributed on two computers.  The graphical user interface (GUI) has a close link to the dialogue manager since it integrates sev-","acronyms":[[67,70]],"long-forms":[[41,65]]},{"text":"It is a comprehensive study but not directly related to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a","acronyms":[[118,121]],"long-forms":[[91,116]]},{"text":"were computed for each scenario: bilingual evaluation under study (BLEU), position independent error rate (PER) and word error rate (WER). ","acronyms":[[133,136],[67,71],[107,110]],"long-forms":[[116,131],[33,59],[74,105]]},{"text":" CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA)","acronyms":[[81,84],[1,6],[135,138]],"long-forms":[[51,79],[106,133]]},{"text":"This problem  is of realistic interest for the design of various types  of natural anguage interfaces (NLI's) that make employs of  different knowledge sources.","acronyms":[[103,108]],"long-forms":[[75,101]]},{"text":"plains the text? Our approach is related to minimum description length (MDL). We formulated our","acronyms":[[72,75]],"long-forms":[[48,70]]},{"text":"tends the comparison set to players of AS Roma.  Prepositional phrases (PPs), gerunds, and relative clause introduce additional complexity.","acronyms":[[72,75],[39,41]],"long-forms":[[49,70]]},{"text":"Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame.","acronyms":[[504,506],[14,17],[242,245],[265,267]],"long-forms":[[494,502],[255,263]]},{"text":"erature for further particulars.  Semantic Role Marking (SRL) Our first task is that of identifying arguments of verbs in a sen-","acronyms":[[54,57]],"long-forms":[[30,52]]},{"text":"lines and web-gathered word lists. Theses grammars are represented by Finite State Machines (FSMs) (thanks to the AT&T GRM\/FSM toolkit (Allauzen et","acronyms":[[93,97],[114,118],[119,126]],"long-forms":[[70,91]]},{"text":" 3.4.1 Arabic Named Entity Recognition Named Entity Recognition (NER) is a subtask of information extraction, where each proper name in","acronyms":[[65,68]],"long-forms":[[39,63]]},{"text":"In Proceedings of the International Conference on World Wide Web (WWW), pages 641?650. ","acronyms":[[66,69]],"long-forms":[[50,64]]},{"text":"the ratio of system?s moves stating that the requested information is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the ratio of the information-providing games","acronyms":[[115,118]],"long-forms":[[85,113]]},{"text":"formation of globally dispersed virtual communities, one of which is the very active and growing movement of Open Source Software (OSS) development.","acronyms":[[131,134]],"long-forms":[[109,129]]},{"text":"ity, to validate Boosting NER hypotheses. We also use three Markov chain Monte Carlo (MCMC) algorithms for probabilistic inference in MLNs.","acronyms":[[86,90],[26,29],[134,138]],"long-forms":[[60,84]]},{"text":"The generator  uses as its linguistic resource a lexicon encoded in a version  of Categorial Grammar (CG), the extension of which with  rules of function composition gives rise to a problem of ","acronyms":[[102,104]],"long-forms":[[82,100]]},{"text":"vantages while reviewing the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints","acronyms":[[123,126]],"long-forms":[[96,121]]},{"text":"teams employed vector-based linear classifiers of different types: Hacioglu et al (2004) and Park et al (2004) utilised Supports Vector Machines (SVM) with polynomial kernels, Carreras et al (2004) utilised Voted Percep-","acronyms":[[141,144]],"long-forms":[[116,139]]},{"text":"quency and its personages string frequency is  less than or equal to 1%, it is a SWBS;  BMM-ASM (BMM ambiguity string mapping  table: the BMM-ASM tableau lists all the ","acronyms":[[87,94],[137,144],[80,84]],"long-forms":[[96,124]]},{"text":"translation from one fount language to multiple target languages, inspired by the recently recommends neural machine translation(NMT) framework proposed by Bahdanau et al (2014).","acronyms":[[128,131]],"long-forms":[[101,126]]},{"text":"We labeled the Reddit dataset using crowdworkers on Amazon Mechanical Turk (AMT), creating the first public corpus annotated with levels of dogmatism.","acronyms":[[76,79]],"long-forms":[[52,74]]},{"text":"We introduce a  exible historic reference mechanism termed an ACT (arboreal context tree; an extension of the context tree to tree-shaped his-","acronyms":[[61,64]],"long-forms":[[66,87]]},{"text":"\/fi:\/. 2.1 Foreign Words From an information retrieval (IR) perspective, foreign words in Arabic can be classified into two gen-","acronyms":[[56,58]],"long-forms":[[33,54]]},{"text":"2.3.1 KiF (Knowledge in Frame) The whole system, training and prediction, has been implemented in KiF (Knowledge in Frame), a script language that has been implemented into","acronyms":[[98,101],[6,9]],"long-forms":[[103,121],[11,29]]},{"text":"phrase structure grammar (PSG) as thc tagging  formalisms(Lecch & Garside 1991), and some  adopt dependence grammar(DG) 1993, Komatsu,  Jane, & Yasuhara, 1993).","acronyms":[[116,118],[26,29]],"long-forms":[[97,114],[0,24]]},{"text":"pora. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 993?1000.","acronyms":[[88,94]],"long-forms":[[61,86]]},{"text":"facts or splits the  targeting into newest subgo~ls uch as to shows the facts in the premises of n. The derivation of a fact is  shipped by so-called mathematics ommunicating acts (MCAs) and accompanied by storing the  fact as a chunk in the declarative memory.","acronyms":[[173,177]],"long-forms":[[142,171]]},{"text":"The target set is built use the ? 88-?89 Wall Street Journal Corpus (WSJ) tagged uses the (Ratnaparkhi, 1996) tagger and","acronyms":[[71,74]],"long-forms":[[43,62]]},{"text":"dominates the other.  Marcu?s Nuclearity Principle (NP) Marcu 1996 furnishes an alternative to the immediate interpretation and","acronyms":[[52,54]],"long-forms":[[30,50]]},{"text":"SELF = talking to oneself  TQ = terse question  TI = terse information  INT = interrupted ","acronyms":[[48,50],[0,4],[27,29],[72,75]],"long-forms":[[53,70],[7,25],[32,46],[78,89]]},{"text":"Keyword 0.168 0.168 0.157 6.3 Table 3: Title quality as compared to the reference for the hierarchical discriminative (HD), flat discriminative (FD), hierarchical generative (HG), flat","acronyms":[[119,121],[145,147],[175,177]],"long-forms":[[90,117],[124,143],[150,173]]},{"text":"We therefore chose to perform ASR using a statistical language model (LM) and employ CMU?s Sphinx to generate an n-best list of recogni-","acronyms":[[70,72],[30,33],[85,90]],"long-forms":[[54,68]]},{"text":"toolkit that contains a suit of modules for generic tasks in Natural Language Processing (NLP), Information Recuperation (IR), and Networks Analysis (NA). ","acronyms":[[146,148]],"long-forms":[[128,144]]},{"text":"(ADV), reason (CAU), direction (DIR), extent (EXT), location (LOC), forma (MNR), and time (TMP), modal verbs (MOD), negative markers (NEG), and discourse connectives (DIS). ","acronyms":[[134,137],[1,4],[14,17],[31,34],[45,48],[61,64],[75,78],[91,94],[110,113],[167,170]],"long-forms":[[116,124],[7,12],[20,29],[37,43],[51,59],[67,73],[85,89],[97,102],[144,153]]},{"text":"ing default parameters. Error analyzing was done by means of Mean Squared Error estimate (MSE). ","acronyms":[[89,92]],"long-forms":[[60,78]]},{"text":"PER (PN), only PER candidates beginning with  the family naming is considered. For PER (FN), a  nominee is generated only if all its composing ","acronyms":[[86,88],[0,3],[5,7],[15,18]],"long-forms":[[77,84]]},{"text":"wards a Shared Task for Multiword Expressions. ACL Special Interest Group on the Lexicon (SIGLEX), Marrakech.","acronyms":[[90,96],[47,50]],"long-forms":[[51,88]]},{"text":"]} (7) This optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).","acronyms":[[78,80]],"long-forms":[[52,76]]},{"text":" org\/wiki\/California?. It is distinct from named entity extraction (NEE) in that it identifies not the occurrence of names but their reference.","acronyms":[[68,71]],"long-forms":[[43,66]]},{"text":"tially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test","acronyms":[[137,143],[70,74],[104,106]],"long-forms":[[114,135],[33,68],[78,102]]},{"text":"ture subset selecting [2][4][8]. Yang and Pederson  found informations gain (IG) and chi-square test (CHI)  most effective in aggressive term removal without los-","acronyms":[[76,78],[101,104]],"long-forms":[[58,74],[84,99]]},{"text":"1995. A bi-directional Russian-toEnglish machine translation system (ETAP-3). For","acronyms":[[69,75]],"long-forms":[[33,67]]},{"text":"back. We define a currency for annotation cost as Annotation cost Units (AUs). For an annotation bud-","acronyms":[[73,76]],"long-forms":[[50,71]]},{"text":"Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage","acronyms":[[128,131],[8,11],[64,67],[84,87],[176,180]],"long-forms":[[96,126],[34,62],[137,174]]},{"text":"DEPICTION (DPC) TOPIC (TPC) SYNONYMY-NAME (SYN) CAUSALITY (CSL) PART-WHOLE (PW) MANNER (MNR) ANTONYMY (ANT) JUSTIFICATION (JST) HYPERNYMY (ISA) MEANS (MNS) PROBABLE OF EXISTENCE (PRB) INTENTS (GOL) ENTAIL (ENT) ACCOMPANIMENT (ACC) POSSIBILITIES (PSB) BELIEF (BLF)","acronyms":[[182,185],[43,46],[11,14],[23,26],[193,196],[206,209],[226,229],[244,247],[257,260],[59,62],[76,78],[88,91],[103,106],[123,126],[151,154],[139,142]],"long-forms":[[156,167],[28,36],[0,9],[16,21],[187,191],[198,204],[211,224],[231,242],[249,255],[48,57],[64,74],[80,86],[93,101],[108,121],[144,149]]},{"text":" In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698?1703.","acronyms":[[90,94]],"long-forms":[[55,73]]},{"text":"534 3.1 Cross Validation on the Training Consultations Random Walk with Resetting (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a","acronyms":[[75,78]],"long-forms":[[49,73]]},{"text":"Aware the precise identity of Fisher vector ??(?), we recommends a natural measure which we call  Weighted Gradient Uncertainty (WGU) based on the facts explained in the previous paragraph:  ????(???)","acronyms":[[128,131]],"long-forms":[[97,126]]},{"text":"A S O W B E  - as + Object of be  A S S E R T I O N ~ S ~ ~ ~ ~ ~ ~  + Tense + V e r b  Object  ASTG = Adjective String  C l  SHOULD -- Subjunctive foYm of ASSERTllgZQ ","acronyms":[[96,100]],"long-forms":[[103,119]]},{"text":"Stephanie Strassel and Zhiyi Song and Kawa Ellis University of Pennsylvania Linguistic Data Consortium (LDC) Philadelphia, PA, USA","acronyms":[[103,106],[126,129],[122,124]],"long-forms":[[75,101]]},{"text":" 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), based on the Rhetorical Struc-","acronyms":[[44,50]],"long-forms":[[20,42]]},{"text":"would also be doable.  Customize PageRank similarity (PPR) (Agirre and Soroa, 2009) measures the semantic relatedness between two word senses s","acronyms":[[59,62]],"long-forms":[[25,57]]},{"text":"NG = ngrams, E = emoticon replaces, IR = informal register replacement, TL = tweet length, RT = retweet count, SVO = subject-verbobject structures. %","acronyms":[[94,96],[0,2],[39,41],[75,77],[114,117]],"long-forms":[[99,106],[5,11],[17,25],[44,61],[80,92],[120,138]]},{"text":" The graph in figure 2 shows that as the number of relevant documents increases, average precision (AveP) after feedback increases considerably for each extra relevant","acronyms":[[100,104]],"long-forms":[[81,98]]},{"text":"fornls of words. For instance, the rule  \\[ S= ion I=  (NNS VBZ) R= (NN) M=8\\]  says if by deleting the suffix \"ion\" from a word ","acronyms":[[60,63],[69,71]],"long-forms":[[56,59]]},{"text":"generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). ","acronyms":[[148,151]],"long-forms":[[114,146]]},{"text":"  4.1 Speech recognise   The automatic speech recognition module (ASR)  is groundwork on the Sphinx 4 system (Lamere et al, ","acronyms":[[68,71]],"long-forms":[[31,59]]},{"text":"editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http:\/\/www.lrec-","acronyms":[[172,176],[96,103]],"long-forms":[[131,170],[61,94]]},{"text":"representation and transformation are imperative.  (2)  Latent Semantic Indexing (LSI) without  transformation (LSI-Com): we first merge the ","acronyms":[[81,84],[111,118]],"long-forms":[[55,79]]},{"text":"One of  the most commonly used methods is the  Latent Semantic Analysis (TAS). In this ","acronyms":[[73,76]],"long-forms":[[47,71]]},{"text":"Performing proper Arabic dialect identification may positively impact many Natural Vocabulary Processing (NLP) application.","acronyms":[[104,107]],"long-forms":[[75,102]]},{"text":"expunged to meet United States HIPAA standards, (U.S. Gesundheit, 2002) and approved for liberating by the local Institutional Review Board (IRB); the sample must represent problems that physicians records coders","acronyms":[[134,137],[49,52],[31,36]],"long-forms":[[106,132]]},{"text":"example, the prepObject )f a LOCATION-PP must be a  PLACE-NOUN. A description of \"on AI\" (as in \"book on  AI\") as a LOCATION-PP c~Id  not be constructed since AI ","acronyms":[[85,88],[159,161],[29,40],[52,62],[116,127]],"long-forms":[]},{"text":"1 ? Active Node List (ANL): a list that registering all ?","acronyms":[[22,25]],"long-forms":[[4,20]]},{"text":"determines the traits of such space.  For example, Syntactic Tree Kernel (STK) are used to modeling complete context free rules as in (Collins","acronyms":[[74,77]],"long-forms":[[51,72]]},{"text":"spect to the ISSC. We will refer to this expand version of the SSC as the processed SSC (PSSC). ","acronyms":[[91,95],[13,17],[65,68]],"long-forms":[[76,89]]},{"text":"5.3 Evaluation Metrics For YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) (Manning et al, 2008).","acronyms":[[102,105],[27,29]],"long-forms":[[80,100]]},{"text":"At Proceedings of the 15th International Conference on  Computational Linguistics (COLING'94),  Kyoto, Japan, August.","acronyms":[[83,92]],"long-forms":[[56,81]]},{"text":" Prior we discuss the significant sentence in answer mails, we sorted replies mails into three types: (1) direct answer (DA) mail, (2) questioner?s reply (QR) mail, and (3) the others.","acronyms":[[125,127],[159,161]],"long-forms":[[110,123],[139,157]]},{"text":"level for naturel  language unders tand ing  system. In case of a  Module  Package Layer( MPL ), there are two genus of program  packages.","acronyms":[[90,93]],"long-forms":[[67,88]]},{"text":"Two-level rules are generally of the form CP OP LC RC where CP = Correspondence Party; OP = Operator; LC = Left Context; RC = Right Background","acronyms":[[60,62],[86,88],[42,44],[45,47],[48,50],[51,53],[101,103],[120,122]],"long-forms":[[65,84],[91,99],[106,118],[125,138]]},{"text":"semaatic rationale, novo referent objects owes be engendered. The number of objects to be  created is set equal to the QTY (quantity) attribute of the noun phrase if specified (as in  \"two boys\" (P20)), to two if the noun phrase is plural and not compound, to the number ","acronyms":[[113,116]],"long-forms":[[118,126]]},{"text":"We then builds three pairwise comparison matrices: one comparing pairs of conventionally developing (TD) children; one comparing pairs of children with ASD; and a third com-","acronyms":[[95,97],[146,149]],"long-forms":[[73,93]]},{"text":"see sections 4.3.  WIV(1): Weighted Identity Value (with the weight 1):  see chapter 2.2.","acronyms":[[18,21]],"long-forms":[[26,49]]},{"text":"ponent Analytical (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al.,","acronyms":[[63,65]],"long-forms":[[67,87]]},{"text":"sual scenes. In their seminal work Dale and Reiter (1995) present the Incremental Algorithm (IA) for GRE.","acronyms":[[93,95],[101,104]],"long-forms":[[70,91]]},{"text":"of-the-art in more detail.  The field of Information Extraction (IE) has been heavily influenced by the Information Retrieval (IR)","acronyms":[[65,67]],"long-forms":[[41,63]]},{"text":"  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 143?147, October 25, 2014, Doha, Qatar.","acronyms":[[80,84],[21,26]],"long-forms":[[44,78]]},{"text":"in a fully automatic fashion. Again, this is an exciting possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methods (namely, that sense-tagged corpora are very costly to acquire).","acronyms":[[150,153]],"long-forms":[[123,148]]},{"text":"3.3  Parameter  es t imat ion   In supervised lcarning~ the simpliest parameter  estimation is the maximum likelihood(ML) cs-  t imation(Duda et al, 1973) which lnaximizes ","acronyms":[[118,120]],"long-forms":[[99,116]]},{"text":"cal machine translation. The 41th Annual meeting of the Association for Computational Linguistics (ACL), 311-318.","acronyms":[[99,102]],"long-forms":[[56,97]]},{"text":"1. Construct word representation models for  corpus in the bases time, D(TB), and in the  target time, D(TT). (","acronyms":[[71,73],[103,105]],"long-forms":[[54,62],[88,99]]},{"text":"2 Classifiers 2.1 NB Naive Bayes(NB) probabilistic classifiers are habitually studied in machine learning(Mitchell, 1996).","acronyms":[[33,35]],"long-forms":[[21,31]]},{"text":"Understanding (NLU) framework (see below), while ASR includes features such as speech\/nonspeech (SNS) detection and automatic gain control (AGC).","acronyms":[[97,100],[15,18],[49,52],[140,143]],"long-forms":[[79,95],[116,138]]},{"text":"Collins et al. ( 2008) recommendation simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF).","acronyms":[[63,65],[107,110]],"long-forms":[[39,61],[81,105]]},{"text":"This  was a motivating factor for the establishment of the  Common Pattern Specification Language (CPSL)  Works Grouped devoted to formulating a CPSL in ","acronyms":[[99,103],[145,149]],"long-forms":[[60,97]]},{"text":"puterization, Technical Report 6-CICC-MT55 (1995)  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182, Boulder, Colorado, June 2009.","acronyms":[[136,141],[33,42]],"long-forms":[[95,134]]},{"text":"BUS (business) Belgium Labour Federation 9 4440 11.0 NOB (nobility) Albert II 6 4179 15.1 COM (comics) Suske and Wiske 3 4000 10.5 MUS (music) Sandra Kim, Urbanus 3 1296 14.6","acronyms":[[90,93],[0,3],[53,56],[131,134]],"long-forms":[[95,101],[5,13],[58,66],[136,141]]},{"text":"Table 1: Probability computed for each type of linguistic info. Error codes correspond to the five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (prepositions), SVA (subject-verb arrangements) and Vform (sneeze form).","acronyms":[[151,159],[224,227],[127,132],[185,187]],"long-forms":[[161,182],[229,251],[189,200]]},{"text":" 1 Introduct ion  Many of the Natural Language Generation (NLG)  systems that produce flexible output, i.e. sentences ","acronyms":[[59,62]],"long-forms":[[30,57]]},{"text":"structure by first computing the similarity of each proposition to the others using a Latent Dirichlet Allocation (LDA) model. LDA is a genera-","acronyms":[[115,118],[127,130]],"long-forms":[[86,113]]},{"text":" 1 Introduction As Machine Translation (MT) systems becoming widely adopted both for gisting purposes and to","acronyms":[[40,42]],"long-forms":[[19,38]]},{"text":"for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. ","acronyms":[[145,149],[49,52]],"long-forms":[[106,143]]},{"text":"637  Proceedings of the Seminars on Discourse in Machine Translation (DiscoMT), pages 27?32, Sofia, Bulgaria, August 9, 2013.","acronyms":[[70,77]],"long-forms":[[36,68]]},{"text":"run in parallel. This sorting of parallelism is a good fitted for the Same Instruction Multiple Thread (SIMT) hardware paradigm implemented by modern GPUs.","acronyms":[[98,102],[144,148]],"long-forms":[[64,96]]},{"text":"NP NP\\NP (S\\NP )\/NP NP< >NP S\\NP <S Fruit flies like bananas NP S\\NP (S\\S)\/NP NP< >S S\\S <S (b) The search space in this work, with one node for each","acronyms":[[64,68],[0,2],[3,8],[61,63],[25,27],[28,32]],"long-forms":[[70,77]]},{"text":"generation of textual descriptions from optic data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). ","acronyms":[[148,151]],"long-forms":[[114,146]]},{"text":"applied this formula to a vocabulary of single terms.  Subiect Field Code (SFC). This system apply a ","acronyms":[[75,78]],"long-forms":[[55,73]]},{"text":"sion with LSA and filtering according to the ET.  Further on, we apply sentiment analysis (SA)  using the approach described in Section 5.3 and ","acronyms":[[91,93],[10,13],[45,47]],"long-forms":[[71,89]]},{"text":" 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al.,","acronyms":[[36,38],[65,67],[122,124]],"long-forms":[[16,34],[44,63]]},{"text":"c?2009 Association for Computational Linguistics Report on the Firstly NLG Challenge on Generating Guideline in Virtual Environments (GIVE) Donna Byron","acronyms":[[135,139],[69,72]],"long-forms":[[86,133]]},{"text":"WordNet  Into another related effort, SRI performed experiments  in utilizing WordNet (WN) as a knowledge source for  IE.","acronyms":[[85,87],[36,39],[116,118]],"long-forms":[[76,83]]},{"text":" The sentence-level extraction is completed with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was exhibited to give good re-","acronyms":[[68,71]],"long-forms":[[51,66]]},{"text":"the idiosyncrasies N, vs, or 0 which denotes a nominal parent (NP or PP), a verbal parent (VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is propagated to the head child.","acronyms":[[155,158],[57,59],[63,65],[85,87]],"long-forms":[[136,153],[70,83],[41,55]]},{"text":"we used dialog features derived from manual annotations ? dialog acts (DA) and blatant displays of power (ODP) ?","acronyms":[[71,73],[104,107]],"long-forms":[[58,69],[79,102]]},{"text":"lected randomly from some reference corpus.  Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of","acronyms":[[62,64]],"long-forms":[[45,60]]},{"text":"POS Entropy (luxury) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11 Shunt measuring (Stride) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24 Word Duration (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00 Tables 1: Correlations between (mean-centered) predictors.","acronyms":[[163,167],[13,17],[90,94]],"long-forms":[[150,161],[0,11],[83,88]]},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is a difficult Natural Language Processing task which requires","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"results in Die event. Recent improvements of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and","acronyms":[[76,80]],"long-forms":[[45,74]]},{"text":" 3.2 ,6  Addi t ion  of ton rans la t ion  Rule  FinMly, translate rules (TRis) are added to the set  of GLTPC, s. TRis are descriptions in which concepts ","acronyms":[[75,79],[106,111],[116,120]],"long-forms":[[56,73]]},{"text":"Hence, any numerical optimization strategy may be employs and practical solutions include scant memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popu-","acronyms":[[108,114]],"long-forms":[[87,106]]},{"text":"lead to over-fitting. Therefore, we propose another method, Probabilistic Soft Logic (PSL) (Broecheler et al, 2010).","acronyms":[[86,89]],"long-forms":[[60,84]]},{"text":"tially lexicalized) syntactic dependencies and patterns. The weighs \u0000 is the Local Mutual Informa-tion (LMI) (Evert, 2005) computed on link type frequency (negatives LMI valuing are raised to 0).3.1 Test set","acronyms":[[104,107],[165,168]],"long-forms":[[77,102]]},{"text":" Definition  Default Unification (first version) AU!B = A ~ U B, where A ~ is the maximal (i.e. most  specific) element in the subsumption ordering such that A' r- A and A ~ U B is defined.","acronyms":[[49,53]],"long-forms":[[56,63]]},{"text":", with different projection and SRL training methods. SP=Supplement; OW=Overwrite. ","acronyms":[[54,56],[32,35],[69,71]],"long-forms":[[72,81],[57,67]]},{"text":"The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to sepa-","acronyms":[[90,92]],"long-forms":[[73,88]]},{"text":"ence.  Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale","acronyms":[[36,39]],"long-forms":[[7,34]]},{"text":"+ raising verb - non-raising verb KEYAGR (key pacts) ?","acronyms":[[34,40]],"long-forms":[[42,55]]},{"text":" The motivation for that work is twofold: on the one hand it builds on the strength of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of choosing themost commonly used sense of a word, irrespective of the context in which","acronyms":[[143,146]],"long-forms":[[116,141]]},{"text":"  1 Introduction  INTERA (Embedded European language data  Repository Area, Contract 22076Y2C2DMAL2) is ","acronyms":[[18,24]],"long-forms":[[26,76]]},{"text":" A manually  prepared seeds list that is used to frame the  lexical patterns for conjunct verbs (ConjVs)  contains frequently employs Light Verbs (LVs).","acronyms":[[96,102],[143,146]],"long-forms":[[80,94],[130,141]]},{"text":"ilarity between given texts. The first approach is based on vector space models (VSMs) (Meadow, 1992).","acronyms":[[81,85]],"long-forms":[[60,79]]},{"text":"The scores are lowercased BLEU calculated on the held-out devtest set. NE = named entities. ","acronyms":[[71,73],[26,30]],"long-forms":[[76,90]]},{"text":"tion. So we developed a method to optimize the CRFs towards the alignment mistakes rate (AER) or the F-score with sure and might links as introduced","acronyms":[[86,89]],"long-forms":[[64,84]]},{"text":"   grandmother. CL.1SG.GEN ALL ART=aerodrome  my grandmother to the airport ","acronyms":[[31,34],[16,26]],"long-forms":[[35,42]]},{"text":"The processing of BADGER is not significantly different than that of the CIRCUS system used i n previous MUC evaluations [4, 5, 6] . Concept node (CN) definitions are still used to create case frame instantiation s and multiple CN definitions can apply to the same text fragment .","acronyms":[[147,149],[105,108],[228,230]],"long-forms":[[133,145]]},{"text":"In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December. ","acronyms":[[62,66],[9,13]],"long-forms":[[14,60]]},{"text":" verb. The third is end position (EP), after a predi-  cate.","acronyms":[[34,36]],"long-forms":[[20,32]]},{"text":"on the labels from DSlabels+MinCut: (4) MaxEnt with named entities (NE); (5) MaxEnt with NE and semantic (SEM) features; (6) CRF with NORTHEASTERN; (7) MaxEnt with NORTHEASTERN and sequential (SQ) characters;","acronyms":[[106,109],[68,70],[89,91],[125,128],[134,136],[142,148],[154,156],[173,175],[40,46]],"long-forms":[[96,104],[52,66]]},{"text":"hypernym, hyponym, near-synonym, holonym, and mernoym are listed as below:  Hypernym(HYP) (a) IF x=ANT ","acronyms":[[85,88],[99,102]],"long-forms":[[76,83]]},{"text":"? Toral (2013) explores the choices of data to train domain-specific language models (LM) from non-domain specific corpora by means","acronyms":[[88,90]],"long-forms":[[71,86]]},{"text":"terns for them. In the terminology of Frame Semantics, the roles are telephoned frame elements (FEs), and the words which evoke the framework are referred","acronyms":[[92,95]],"long-forms":[[76,90]]},{"text":"Pierre PN Note that a preposition (PR) with lemma de and a determiner (DT) with lemma le and the same gender and number as the common noun have been","acronyms":[[71,73],[7,9],[35,37]],"long-forms":[[59,69],[22,33]]},{"text":" 8. Adjacent Variety(AV) of the candidate. We","acronyms":[[21,23]],"long-forms":[[4,19]]},{"text":"We guess that the possessor of a noun phrase is  the subjected or the noun phrase's nearest opic that  has a semantic mark,er HUM (human) or a seman-  tic markers AN I (animal).","acronyms":[[126,129],[162,164]],"long-forms":[[131,136]]},{"text":"MT is one of the elder and most major areas of Natural Language Processing (NLP) \/ Computational Linguistics (CL).2 From its beginnings we have witnessed some changes in the","acronyms":[[115,117],[0,2],[81,84]],"long-forms":[[88,113],[52,79]]},{"text":" 6 Experiments and Results We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain","acronyms":[[59,62]],"long-forms":[[38,57]]},{"text":"Background (BKG) the background of the scrutinize Problem (PROB) the research problem Manner (METH) the methods used Result (RES) the results achieved","acronyms":[[89,93],[12,15],[54,58],[120,123]],"long-forms":[[81,87],[0,10],[45,52],[112,118]]},{"text":" 2 Tales Segmentation using Modified Kmeans (MKM) Clustering The first step in multi-document summarization is","acronyms":[[45,48]],"long-forms":[[28,43]]},{"text":"tributed to idiosyncrasies in the translation. For instance, Emma (EM) seems very difficult to align, which can be attributed to the use of an old transla-","acronyms":[[67,69]],"long-forms":[[61,65]]},{"text":"3.2 To resolve gapping under serial sneezing construction Serial verb construction (SVC) (Baker, 1989) is edifice in which a sequence of verbs appears in","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":"ules, distributed on two computers.  The graphical user interface (KYE) has a nearing link to the dialogue manager since it integrates sev-","acronyms":[[67,70]],"long-forms":[[41,65]]},{"text":"These  probabilities could be estimated from training cases  with Maximum Likelihood Estimation (MLE). Let l ","acronyms":[[97,100]],"long-forms":[[66,95]]},{"text":"A Listings of POS-tags ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (clear articles), DEM (demonstratives),","acronyms":[[71,73],[85,87],[10,13],[19,22],[37,40],[52,54],[104,106],[128,131]],"long-forms":[[75,82],[89,101],[24,34],[42,49],[56,68],[108,125],[133,147]]},{"text":"Apple Events  1 Introduction  The SlmSum (Slmulatmn of Summarizing)  system does what its name pronuses It simu- ","acronyms":[[34,40]],"long-forms":[[42,66]]},{"text":"1 will refer to pa~rs   primarily oriented towards the former goal as Practical Parsers (PP) and refer  to the others as Performance Model Parsers (PMP). With these distinctions ","acronyms":[[148,151],[89,91]],"long-forms":[[121,146],[70,86]]},{"text":"The columns in the first section of the table represent different settings of the p# parameter, with highest performance for each adjusted count model shown in bold. p# values were selected to show a representative range of performance. P = phoneme model; OR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM = local minimum strategy. The best performing local minimum model is shaded.","acronyms":[[300,302],[330,332],[256,258]],"long-forms":[[305,317],[335,348],[241,248],[261,272],[284,292]]},{"text":" 1 Introduction  Spoken language translation (SLT) has become  more important due to globalization.","acronyms":[[46,49]],"long-forms":[[17,44]]},{"text":"  (a)  Support vector machine (SVM) (Vapnik,  1998) is a supervised learning algorithm proposed ","acronyms":[[31,34]],"long-forms":[[7,29]]},{"text":"minimal human SO annotation Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN =","acronyms":[[88,91],[74,76],[115,117],[138,142],[185,188],[218,221]],"long-forms":[[94,113],[52,72],[120,136],[145,183],[191,216]]},{"text":"` and survival variables S`. Languages shown are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern","acronyms":[[75,77],[126,128],[56,58],[95,97],[109,111],[144,146]],"long-forms":[[61,73],[114,124],[49,54],[80,93],[100,107],[135,142],[6,24]]},{"text":"5 Conclusion In this pilot experiment, we explore the possibility of using Amazon Mechanical Turk (MTurk) to collect bilingual word alignment data to assist automatic word align-","acronyms":[[99,104]],"long-forms":[[82,97]]},{"text":"bilities. Experience has shown that this kind of  full-fledged question answering (QA) over texts  from a wide range of domains is so difficult for ","acronyms":[[83,85]],"long-forms":[[63,81]]},{"text":" - 19  -  Both L I and L 2 are CSL's (Context sensitive languages). They ","acronyms":[[31,36]],"long-forms":[[38,65]]},{"text":"seeds more sophisticated models with simpler models, and the parameters of each model are estimated through an Expectancy Maximization (EM) procedure.","acronyms":[[131,133]],"long-forms":[[105,129]]},{"text":"(UT = Utterance as actually introduced by the user, UR = Utterance as recognized by the system, SU = System utterance). ","acronyms":[[90,92],[1,3],[46,48]],"long-forms":[[95,111],[6,15],[51,60]]},{"text":"The IXM2 is the first massively parallel associative  processor that clara demonstrates the computing  power of a large Associative Recollection (AM). The AM ","acronyms":[[142,144],[4,8],[151,153]],"long-forms":[[122,140]]},{"text":"rately hand-designing a grand set of features (idiosyncrasies engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers","acronyms":[[135,138]],"long-forms":[[106,133]]},{"text":"4 Experiments 4.1 Phenomena Extraction Ours extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).","acronyms":[[92,95],[105,110]],"long-forms":[[68,90]]},{"text":"NE translation.   Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT.","acronyms":[[90,92],[0,2],[27,29],[136,138],[175,177]],"long-forms":[[69,88]]},{"text":"KEA: Practical automatic keyphrase  extraction. Proceedings of Digital Libraries 99 (DL'99), pp. ","acronyms":[[85,90],[0,3],[93,95]],"long-forms":[[63,83]]},{"text":"questions evaluated in TREC1. For example questions 1The Text REtrieval Conferences (TREC) are evaluation workshops in which Information Retrieval tasks are annually","acronyms":[[85,89],[23,27]],"long-forms":[[57,83]]},{"text":"scribe the relation R. Most previous schemes perform these steps by first using named entity recognizing (NER) to identify possible arguments and then using a simple rope match, but this crude","acronyms":[[106,109]],"long-forms":[[80,104]]},{"text":"been annotated and nourished into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialising (ISS) method applied to noun phrase constituents.","acronyms":[[160,163]],"long-forms":[[125,158]]},{"text":"morphologically very rich. Different suffixes  may be attached to a Light Verb (LVs) (in this  case [YYY]) depending on the various features ","acronyms":[[80,83]],"long-forms":[[68,78]]},{"text":"These features have been obtained using the knowledge contained into the Multilingual Central Repository (MCR) of the MEANING project3 (Atserias et al, 2004).","acronyms":[[106,109]],"long-forms":[[73,104]]},{"text":"refers to the fact that either a particular lexical item or a particular grammatical architectural must be present for the omission of a frame facet (FE) to occur.","acronyms":[[151,153]],"long-forms":[[136,149]]},{"text":"matical device for handling coordination in computa-  tional linguistics has been the SYSCONJ facility for  augmented transition networks (ATNs) (Woods 1973;  Bates 1978).","acronyms":[[139,143],[86,93]],"long-forms":[[108,137]]},{"text":"Mauser et al 2009). One promising approach is the Discriminative Word Terminology (DWL). In this","acronyms":[[79,82]],"long-forms":[[50,77]]},{"text":"mechanism for accurate named entity  (NE) translation in English?Chinese  question answering (QA). This mecha-","acronyms":[[94,96],[38,40]],"long-forms":[[74,92],[23,35]]},{"text":"The language is defined by about 50 simple grammar rules. ? P5E2N3S4, F W A Standard Language (SLANG). See Section 4.1. ?","acronyms":[[95,100]],"long-forms":[[76,93]]},{"text":"lists are derived automatically from the training data.  Frequent Word Lists (FWL) This list consists of words that occur in more than 5 different papers.","acronyms":[[77,80]],"long-forms":[[57,75]]},{"text":"cal work is broad. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking.","acronyms":[[90,93]],"long-forms":[[57,88]]},{"text":"Langman dictionary.  Maxima Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Rat-","acronyms":[[38,44]],"long-forms":[[21,36]]},{"text":" 2007. The Syntax Augmented MT (SAMT) system at the Shared Task for the 2007 ACL Workshop on","acronyms":[[32,36],[77,80]],"long-forms":[[11,30]]},{"text":"2,5 y1,5 Figure 1: The Partial Lattice MRF (PL-MRF) Modelled for a 5-word penalties and a 4-layer lattice.","acronyms":[[44,50]],"long-forms":[[23,42]]},{"text":"by the connectives will yield better readability.  Entity Grid (EG) Along with the previous cooperation (Pitler and","acronyms":[[64,66]],"long-forms":[[51,62]]},{"text":"sources Tony Mullen and Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku","acronyms":[[73,76]],"long-forms":[[38,71]]},{"text":"and documents created by three or four New York Times columnists (TF = Thomas Friedman, PK = Paolo Krugman, MD = Maureeen Dowd, GC = Gayle Collins).","acronyms":[[88,90],[66,68],[107,109],[127,129]],"long-forms":[[93,105],[71,86],[112,125],[132,144]]},{"text":"erences to the instructors in the posts etc.  3.3 Linear Chain Markov Model (LCMM) The logistic regression model is good at exploit-","acronyms":[[77,81]],"long-forms":[[50,75]]},{"text":" 3.2.6. Candidate word number (WNum)  Because there are hopefuls that are a multi-","acronyms":[[31,35]],"long-forms":[[18,29]]},{"text":" 2.4 Stanford Parser The Stanford Parser (SP) is an unlexicalized parser that competitor state-of-the-art lexical-","acronyms":[[42,44]],"long-forms":[[25,40]]},{"text":"The contextual information about social status and  sentence-external individuals can bc included in the  attribute CONTEXT (CONX). Ill order to see values the ","acronyms":[[125,129]],"long-forms":[[116,123]]},{"text":"resolution pipeline consisted primarily of the C&C parser and Boxer (Curran et al, 2007), which produce Discourse Representation Structures (DRSs). ","acronyms":[[141,145],[47,50]],"long-forms":[[104,139]]},{"text":"for PTB III data valuation by label accuracy system test additional resources JESS-CM (CRF\/UM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data","acronyms":[[87,94],[4,7],[78,85],[102,109]],"long-forms":[]},{"text":" 4 Classifier We using a linear support vector machine (SVM) classifier, which is standard for text data.","acronyms":[[55,58]],"long-forms":[[31,53]]},{"text":"among other methods). We refer to case in which the theme and object of the relation are contained within a phrase headed by a Noun as Relational NP?s (RNP). ","acronyms":[[152,155]],"long-forms":[[135,150]]},{"text":"We applied 3 MCMC algorithms: Gibbs sampling (GS), MCSAT and Simulated Tempering (ST) for inference and the comparative TNE findings are shown in Table 1.","acronyms":[[82,84]],"long-forms":[[61,80]]},{"text":"This representation uses the logical formulation of feature structures  as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach  to the logical formulation of Functional Unification Grammar (FUG) given by Rounds  and Manaster-Ramer (1987).","acronyms":[[218,221]],"long-forms":[[186,216]]},{"text":"  Abstract  The Colorado Literacy Tutor (CLT) is a  technology-based literacy program, design ","acronyms":[[41,44]],"long-forms":[[16,39]]},{"text":"VB (Verb) is a major source of confusion in automatic tagging.  It is disintegrated with VB (Verb). ","acronyms":[[85,87],[0,2]],"long-forms":[[89,93],[4,8]]},{"text":"Because of a scarcity of such corpora, most work has used the International Corpus of Learner English (ICLEv2) (Granger et al 2009) for training and evaluation","acronyms":[[103,109]],"long-forms":[[62,101]]},{"text":"produced by a transliteration system 1. Word Accuracy in Top-1 (ACC) Further known as Word Error Rate, it measurement correct-","acronyms":[[64,67]],"long-forms":[[45,53]]},{"text":"\/(cs + |N |?)  P (GR = gri|SCF = s) = (csgri +?) \/(cs + |gram|?)","acronyms":[[18,20]],"long-forms":[[23,30]]},{"text":" The obtained Spanish scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in figure 5.","acronyms":[[93,98]],"long-forms":[[73,91]]},{"text":" of the International Joint Lectures on Artificial Intelligence (IJCAI). ","acronyms":[[67,72]],"long-forms":[[8,65]]},{"text":"future research which are suggested by some af the techniques utilize in this program.  The SFRAME (semantic frame) idea. in which a sernantirl interpretation ","acronyms":[[89,95]],"long-forms":[[97,111]]},{"text":"573   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), page 51?59, Sofia, Bulgaria, August 9, 2013.","acronyms":[[71,78]],"long-forms":[[37,69]]},{"text":" 2.1 Tree Substitution Grammars A tree substitution grammar (TSG) is a 4-tuple ?","acronyms":[[61,64]],"long-forms":[[34,59]]},{"text":"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system (Choueka, 1990; Ja?ppinen and Niemisto?,","acronyms":[[121,123]],"long-forms":[[98,119]]},{"text":"We use the two representing corpora mentioned above, Penn Chino Treebank (CTB) and PKU?s People?s Daily (PPD) in our experiments.","acronyms":[[109,112],[78,81],[87,92]],"long-forms":[[93,107],[60,76]]},{"text":"is different from the annotation scheme (Abbas, 2012; Abbas, 2014) of phrase structure (PS) and the hyper dependency structure (HDS) of the URDU.KON-TB treebank along with the different","acronyms":[[128,131],[88,90],[140,151]],"long-forms":[[100,126],[70,86]]},{"text":"640,000 non-empty abstracts were found.  Query Set (QSet): We download from PubMed the abstracts that mention a bestowed gene naming and its syn-","acronyms":[[52,56]],"long-forms":[[41,50]]},{"text":"rent involvement week (Curr) and the second usage data from the beginning participation week till the current week (TCurr). For the second setup,","acronyms":[[118,123]],"long-forms":[[100,111]]},{"text":" 2 Named Entity Extraction  Named Entity Recognition (NER) is useful in  NLP applications such as question answering, ","acronyms":[[54,57],[73,76]],"long-forms":[[28,52]]},{"text":"form is  employed to denote funct ion  i t se l f .   b) Conceptual  Phrase St ructure  (CPS) is  a data  s t ruc ture  in which syntact i c  and semant ic  in fo rma-  ","acronyms":[[89,92]],"long-forms":[[57,86]]},{"text":"method of ADN. ADN is constructed by  Confined Boltzmann Machines (RBM)  with unsupervised learning using labeled ","acronyms":[[69,72],[10,13],[15,18]],"long-forms":[[38,67]]},{"text":"Results for the Mention-Pair Model 1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5 2 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8 3 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9","acronyms":[[121,123],[204,206]],"long-forms":[[104,119],[192,202]]},{"text":" University of Texas at Austin Word sense disambiguation (WSD) is an old and important task in computational linguistic that however remained challenging, to machines as well as to human annotators.","acronyms":[[58,61]],"long-forms":[[31,56]]},{"text":"ducted. The administrative body governing these decisions is the Institutional Review Board (IRB). ","acronyms":[[93,96]],"long-forms":[[65,91]]},{"text":"Table 4: Reranking results (%BLEU on TEST).  Discriminative Word\/Tag LMs (DISC): For each language pair, we generated 10,000-best lists for","acronyms":[[74,78],[69,72],[37,41],[29,33]],"long-forms":[[45,59]]},{"text":"Figure 8: Alternative English lexical entry for *WORK-  FUNCIONAR  (GENRE and NUMBER) tally\/mass (COUNT) and  a trinary distinction of ANIMACY (human, animal, ","acronyms":[[79,85],[56,65],[99,104],[136,143],[68,74]],"long-forms":[[87,92]]},{"text":"we sample polysemous phrases from wide-domain {French,Chinese}-English corpora, and employs Amazon?s Mechanical Turk (MTurk) to annotate word sense on the English side.","acronyms":[[112,117]],"long-forms":[[95,110]]},{"text":" 2.3 Linear Programming A Linear Program (LP) is an optimization problem where a linear objective function is minimized (or maximized) under linear constraints.","acronyms":[[42,44]],"long-forms":[[26,40]]},{"text":"ing. In Proceedings of the A CL Fifth Conference  on Applied Natural Language Processing (ANLP),  pages 139-146, Washington, DC.","acronyms":[[90,94],[29,31],[125,127]],"long-forms":[[53,88]]},{"text":"Budanitsky and Hirst Lexical Semantic Relatedness Figure 5 Precision (PD), rappel (RD), and F-measure (FD) for malapropism detecting by measure and scope. ","acronyms":[[70,72],[83,85],[103,105]],"long-forms":[[59,68],[75,81],[92,101]]},{"text":"be compared, for any section of the corpus. The tool also calculates the majority tag (MajTag). Av-","acronyms":[[87,93]],"long-forms":[[73,85]]},{"text":"76 4 Multi-media Information Networks una Multimedia Information Network (MINet) is a structured collection effected up of a set of multimedia documents (e.g., texts and imagery) and links between these documents.","acronyms":[[72,77]],"long-forms":[[40,70]]},{"text":"the expected output.  formally a weighted finite state automation (FSA), where V is the set of nodes andE is the set of edges.","acronyms":[[67,70]],"long-forms":[[42,65],[120,124]]},{"text":"This I will  claim to be in contrast with the skill of temporal  subordinate clauses and noun phrases (NPs) to directly the  listener to any position in the evolving structured.)","acronyms":[[105,108]],"long-forms":[[91,103]]},{"text":"Lioma C. and Ounis I., A Syntactically-Based Query  Reformulation Technique for Information Retrieving,  Informations Processing and Management (IPM), Elsevier Science, 2007 ","acronyms":[[143,146]],"long-forms":[[104,141]]},{"text":"pute probability scores of word sequences. The general conversational language model (LM) is based on data from the SWITCHBOARD corpus and a small","acronyms":[[86,88]],"long-forms":[[70,84]]},{"text":"Table 2  Summary of error rates with the language model only (LM), the prosody model only (PM), the  combined ecision tree (CM-DT), and the combined HMM (CM-HMM). ( a) shows word-based ","acronyms":[[124,129],[154,160],[62,64],[91,93]],"long-forms":[[101,122],[140,152],[41,55],[71,84]]},{"text":"den Markov Models (HMMs), Conditional Random Domains (CRFs), Maximum Entropy Markov  Models (MEMMs), etc. ","acronyms":[[92,97],[19,23],[53,57]],"long-forms":[[60,90],[4,17],[26,51]]},{"text":"describing each video. We then clustered these verbs using Hierarchical Agglomerative Clustering (HAC) using the res metric from WordNet::Similarity by","acronyms":[[98,101]],"long-forms":[[59,96]]},{"text":"(? 2.2), we propose an induction algorithm based on Integer Linear Programming (ILP). Figure 2","acronyms":[[80,83]],"long-forms":[[52,78]]},{"text":"Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P) pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score; NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.","acronyms":[[167,170],[80,82],[241,244],[201,204],[135,138]],"long-forms":[[173,199],[141,165],[257,271]]},{"text":"S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics  Brief History of the Message Understanding Conferences","acronyms":[[43,45],[13,15]],"long-forms":[[48,64],[4,11],[18,31]]},{"text":"ABSTRACT  We present a progress report on our research  on nominal compounds (NC's). Recent approaches to ","acronyms":[[78,82]],"long-forms":[[59,76]]},{"text":" One example of this would be in providing support for the curation of the Gene Expression Database (GXD).4 This support could come in the form of a named entity recog-","acronyms":[[101,104]],"long-forms":[[75,99]]},{"text":"229 2 Participation in STS-SEM2013  The Semantic Textual Similarity (STS) task consists of estimated the value of semantic likeness ","acronyms":[[69,72],[23,34]],"long-forms":[[40,67]]},{"text":"The second approach is based on statistical modeling. We adopted one classic  fulfilment called the \"vector space modelling\" (VSM) (Frakes and Baeza-Yates 1992;  Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch\/itze 1992), which has ","acronyms":[[126,129]],"long-forms":[[105,123]]},{"text":"? All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).","acronyms":[[60,64],[92,95]],"long-forms":[[50,58],[82,90]]},{"text":"Hidden topic markov models. Artificial Intelligence and Statistics (AISTATS). ","acronyms":[[68,75]],"long-forms":[[28,66]]},{"text":"ear (lin) kernel, second degree polynomial kernel (d=2), and RBF kernel (rbf); SVM with transductive inference (TSVM) and linear (lin) kernel or second degree polynomial (d=2) ker-","acronyms":[[112,116]],"long-forms":[[88,110]]},{"text":"this results in minimum expected word mistakes rate (WER) presumption (Mangu et al, 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim","acronyms":[[122,125]],"long-forms":[[102,120]]},{"text":" 4.3 MORPHOTACT1C MODEL  An associative Morphotactic Model (MTModel) is a pair  <{MRi},<*>, where {MRi} is a set of morphotactic rules ","acronyms":[[60,67],[82,85],[99,102]],"long-forms":[[40,58]]},{"text":"NE = Named Entity CE = Correlated Entity EP = Entity Profile SVO = Subject-Verb-Object","acronyms":[[41,43],[0,2],[18,20],[61,64]],"long-forms":[[46,60],[5,17],[23,40],[67,86]]},{"text":"There are two other functional tags which, unlike those listed above, can moreover be associated with numbered arguments in the frames archive. The firstly one, EXT (extent), indicates that a constituent is a costed argument on its verb, as in climbed 15%","acronyms":[[153,156]],"long-forms":[[158,164]]},{"text":"In Proceedings of the 15th International Conference on Computational Linguistics (COLING?94), pages 1145?1150, Kyoto, Japan.","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"The DM is bracketed between two other components, the Input Administrator (IM) and the Output Manager (OM). The","acronyms":[[97,99],[4,6],[69,71]],"long-forms":[[81,95],[54,67]]},{"text":"CC = coordinating conjunction; CD = cardinal number; JJ = adjectives; MD = modal; NN = singular noun; NNP = suitable noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB = base forma verb; VBD = past tense verb; VBZ = third-person singular presenting verb.","acronyms":[[179,181],[0,2],[31,33],[53,55],[69,71],[81,83],[101,104],[120,124],[147,150],[166,168],[188,190],[209,212],[232,235]],"long-forms":[[184,186],[5,29],[36,44],[58,67],[74,79],[95,99],[107,118],[127,145],[153,164],[171,177],[203,207],[215,230],[238,273]]},{"text":"tic models, which in this case are hidden Markov models (HMM), and described in terms of wellknown Mel frequency cepstral coefficients (MFCCs) (Benesty et al, 2008).","acronyms":[[136,141],[57,60]],"long-forms":[[99,134],[35,54]]},{"text":" ? REL = relationship + property; ARG = NP\/VP\/ADJ","acronyms":[[3,6],[30,33]],"long-forms":[[9,17],[36,45]]},{"text":"The methods for scoring the Template Element, Template Relation, Scenario Template, and Named Entity tasks are  very similar. From the standpoint of calculating scores, The template element (TE) task is the basic task of these four. ","acronyms":[[191,193]],"long-forms":[[173,189]]},{"text":"tagging. These errors in the training corpora affects badly to the machine learning (ML) based models. ","acronyms":[[85,87]],"long-forms":[[67,83]]},{"text":"There are two other functional tags which, unlike those listed above, can also be associated with numbered arguments in the frames files. The first one, EXT (extent), indicates that a constituent is a numerical argument on its verb, as in climbed 15%","acronyms":[[153,156]],"long-forms":[[158,164]]},{"text":"at a supermarket 2 1 1 0 able unable 1 0 0 1 Table 1: Word-based Levenshtein distance (LD) featured and separated edit operation (D = deletions, I","acronyms":[[87,89]],"long-forms":[[65,85]]},{"text":"unable to touch the robotics?s screen or to verbalize a sermons command (e.g. after a stroke) is the brain computers interface (BCI) of the robot (Hintermu?ller et al, 2011).","acronyms":[[123,126]],"long-forms":[[97,121]]},{"text":"otherwise be very limited annotated data. The resource, the Online Database of INterlinear text (ODIN), makes this data available and prescribes extra annotation","acronyms":[[97,101]],"long-forms":[[60,90]]},{"text":" 3 Dataset & Pilot Setup We use the First Certificate in English (FCE) ESOL examination scripts2 (upper-intermediate level as-","acronyms":[[73,76]],"long-forms":[[43,71]]},{"text":"TI = fTW; F; ADV; AUX; V A; V C; V V; Pg Each type of the indicators, e.g. TW , contains a set of words, such as TW = twlist = ftw","acronyms":[[113,115],[75,77],[18,21],[13,16],[0,2]],"long-forms":[[118,124]]},{"text":"1998. Protein folding in the hydrophobic-hydrophilic(HP) model is NPcomplete.","acronyms":[[53,55],[66,68]],"long-forms":[[29,52]]},{"text":" Among Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 1?9.","acronyms":[[63,69],[71,76]],"long-forms":[[50,61]]},{"text":" 4. Template Relation(TR) recognition: Finding the relation between TEs and a question","acronyms":[[22,24]],"long-forms":[[4,20]]},{"text":" 4.3 Counting and Calculation The SRI Vocabulary Modelling Toolkit (SRILM) (Stolcke and others, 2002) is utilise to count the frequencies in our work.","acronyms":[[66,71]],"long-forms":[[34,56]]},{"text":"3 HCMUS 6L OpenNLP OpenNLP Dict Rules - Table 2: Participants and abstract of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Treatments searchers, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,","acronyms":[[113,115]],"long-forms":[[116,132]]},{"text":"post-process of the internal diacritization task  using the same machine learning approach that  was trained on Base phrase (BP)-Chunk as well  as POS features of individual tokens with correct ","acronyms":[[125,127],[147,150]],"long-forms":[[112,123]]},{"text":"EUD1.2 has the adding benefits of being natively annotated with gold-standard Universal Dependencies (UD) parses (Nivre et al, 2015).","acronyms":[[100,102],[0,3]],"long-forms":[[76,98]]},{"text":"proaches, we apart report on supplying them with disparate reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learning on the same data but with automatic","acronyms":[[129,132]],"long-forms":[[106,114]]},{"text":"data with which to test research hypotheses. We de-  scribe the Air Travel Information System (ATIS) pilot  corpus, a corpus designed to measure progress in Spo- ","acronyms":[[95,99]],"long-forms":[[64,93]]},{"text":"? A chunking stipulations:  PP = prep, NP#1, if (pythontest(#1)). ","acronyms":[[20,22]],"long-forms":[[25,29]]},{"text":"In STS, we encoded only similarity traits between the two sentences. Thus, we used two classes of kernels: (1) the syntactic\/semantic category (SS) with the final kernel define as K(p 1","acronyms":[[142,144],[3,6]],"long-forms":[[126,140]]},{"text":"We first experiences with the regardless trained supertagger and parser, which are then combined using belief propagation (BP) and dual decomposition (DD).","acronyms":[[122,124],[150,152]],"long-forms":[[102,120],[130,148]]},{"text":"Abstract In this work we present results from using Amazon?s Mechanical Turk (MTurk) to annotate translation lexicons between English","acronyms":[[78,83]],"long-forms":[[61,76]]},{"text":"tive standard deflection of three intervals, left  edge to anchor (LE-A), center to anchor (CC-A),  right edge to anchor (RE-A), calculated across  productions of pot, sot, blot, lots, plot, and splot ","acronyms":[[121,125],[91,95],[66,70]],"long-forms":[[99,119],[73,89],[44,64]]},{"text":"to five possible values (rules have been presented with the sentence pairs from which they have been acquiring): entailment=yea (OUI), i.e. correctness of the rule; entailment=more-phenomena (+PHEN), i.e.","acronyms":[[128,131],[192,196]],"long-forms":[[123,126],[180,189]]},{"text":"unable to touch the robot?s screen or to verbalize a speech command (e.g. after a stroke) is the brain computer interface (BCI) of the robot (Hintermu?ller et al, 2011).","acronyms":[[123,126]],"long-forms":[[97,121]]},{"text":"*Event, *Mtrans-Action), and plans (i.e. *Pick-Up-  Gunpoint). A hierarchy of Concept Class (CC) entities  stores acquaintance both declaratively and procedurely ","acronyms":[[88,90]],"long-forms":[[73,86]]},{"text":" Acknowledgments This work has been funded in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI\/12\/RC\/2289 (INSIGHT) and by the EU FP7 program in the context of the project LIDER","acronyms":[[104,107],[209,214],[164,166],[167,170]],"long-forms":[[76,102]]},{"text":"Following the ideas of (Collobert et al, 2011), Zeng et al (2014) first solve relation classification using convolutional neural network (CNN). The","acronyms":[[138,141]],"long-forms":[[108,136]]},{"text":".  Reattachment Heuristic (RH) targets nonargument leader errors that occur if a TL argument","acronyms":[[27,29],[79,81]],"long-forms":[[3,25]]},{"text":"are not very exacting on resources: Inverse Consultation (IC) (Tanaka and Umemura, 1994) and Distributional Similarity (DS) (Kaji et al, 2008), their strong points and deficiencies, and proposed","acronyms":[[121,123],[59,61]],"long-forms":[[94,119],[37,57]]},{"text":"To solve the ILP models we use lp solve, a extraordinarily efficient GNU-licence Mixed Integer Programming (MIP) solver11, that implements the Branch-and-Bound algorithm.","acronyms":[[100,103],[13,16],[61,72],[32,34]],"long-forms":[[73,98]]},{"text":"tion database (OID) and provides the correspond interface;  - Knowledge Retriever (KR) ? retrieves KSs from ","acronyms":[[86,88],[15,18],[102,105]],"long-forms":[[65,84]]},{"text":"Run 3 100% 18 (9.0%) 38 (19.0%)  Table 7.  Effect of Translation (E-C)   ","acronyms":[[66,69]],"long-forms":[[43,49]]},{"text":"mance on the NER task.  Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is","acronyms":[[56,62],[13,16],[69,75]],"long-forms":[[24,54]]},{"text":" 3.1 Annotating message-level data Amazon?s Mechanical Turk (MTurk) was used to annotate the 5,000 random Anglais (Americans)","acronyms":[[61,66]],"long-forms":[[44,59]]},{"text":"PCFG scoring (SP ) 49.5 50.0 TSG score (ST ) 49.5 49.7 Charniak punctuation (SC) 50.0 50.0 l + S3 61.0 64.3","acronyms":[[69,71],[0,4],[12,14],[27,30],[38,40]],"long-forms":[[62,67]]},{"text":" We observe the following: First, pre-enrollment reviews have noun phrases(NP) that containing smaller leaf nodes than in the post-enrollment reviews.","acronyms":[[75,77]],"long-forms":[[62,73]]},{"text":" We integrate two sets of linguistic features into a maximum entropy (MaxEnt) model and develop aMaxEnt-based binary classifier to predict the cat-","acronyms":[[70,76],[97,103]],"long-forms":[[53,68]]},{"text":"these areas. Speci\fcally, our goals when enter MUC-7 were to: \u000f Increase the accuracy in the Template Component (TE) task and the Template Relation (TR) task su\u000eciently for operational utilise, i.e., F-Measures of 85% and 80% respectively,","acronyms":[[114,116],[150,152],[50,55]],"long-forms":[[96,112],[131,148]]},{"text":"the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). ","acronyms":[[73,76]],"long-forms":[[40,71]]},{"text":"ber of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book","acronyms":[[134,136],[151,153],[60,62]],"long-forms":[[118,132]]},{"text":"   grandmother. CL.1SG.GEN ALL ART=airport  my grandmother to the airport ","acronyms":[[31,34],[16,26]],"long-forms":[[35,42]]},{"text":"volution model (DTCNN). Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).","acronyms":[[80,82],[16,21]],"long-forms":[[65,78]]},{"text":"1 Motivation  Question Answering has emerged as a principal area in  natural language treating (NLP) to apply question parsing, information extraction, summariza-","acronyms":[[92,95]],"long-forms":[[63,90]]},{"text":" School of Informational Science Japan Advanced Institute of Science and Technology (JAIST), Japan nthnhung@jaist.ac.jp","acronyms":[[83,88]],"long-forms":[[31,81]]},{"text":"dresses such conflicting constraints. In this method, the owner of the TM generates a Phrase Table (PT) from it, and makes it accessible to the user following","acronyms":[[100,102],[71,73]],"long-forms":[[86,98]]},{"text":"  Abstract  In an try to expand Penn Discourse Tree Bank (PDTB) \/ Turkish Discourse Bank (TDB)  style annotations to spoken Turkish, this paper presents the firstly attempt at annotating the explicit ","acronyms":[[62,66],[94,97]],"long-forms":[[36,60],[70,92]]},{"text":"2 Shared-task evaluation in HLT Over the past twenty yrs, virtually every campo of research in human language technology (HLT) has introduced STECs.","acronyms":[[124,127],[28,31],[144,149]],"long-forms":[[97,122]]},{"text":"words. We model semantic relatedness between two words using the Information Content (IC) of the couple in a method analog to the one used by Lin","acronyms":[[86,88]],"long-forms":[[65,84]]},{"text":"and Chinese (ZH). The second section gives the result for the English (EN) testing set, PTB Section 23. ","acronyms":[[71,73],[13,15],[85,88]],"long-forms":[[62,69],[4,11]]},{"text":" They use Only Word-Seg (OWS), Whole Layer Weight (WLW), SC (SC) and FeedBack mechanism (FB) separately.","acronyms":[[51,54],[25,28],[57,59],[61,63],[89,91]],"long-forms":[[43,49],[10,23],[69,77]]},{"text":" Two genus of recursion can be distinguished: 1)  middle field (MF) recursion, where the embedded  foundation clause is framed by the left and right verb parts ","acronyms":[[64,66]],"long-forms":[[50,62]]},{"text":"Varied traditional  information salvaging(IR) techniques combined  with natural language processing(NLP) tech-  niques have been re-targeted to enable efficient ","acronyms":[[101,104],[43,45]],"long-forms":[[73,99],[21,42]]},{"text":"10 ESA on senses and Wikipedia Links Measure (WLM) compute similarity on a sense-level, however, sim-","acronyms":[[45,48],[3,6]],"long-forms":[[21,43]]},{"text":" (e.g., hand, heart, blood, DNA) Diseases and Symptoms (DisSym): Diseases and symptom.","acronyms":[[56,62],[28,31]],"long-forms":[[33,54]]},{"text":"Noun I Nominalized verb(NIO  Determinative modifier ::= Adjective I Differentiable Adjective(DA) I Verb I Noun I  Location I String l Numeral + Classifier ","acronyms":[[93,95],[24,27]],"long-forms":[[68,91]]},{"text":"4.1 Preprocessing  HTML Page Parsing  The Document Object Model (DOM) is an application programming interface used for parsing ","acronyms":[[65,68],[19,23]],"long-forms":[[42,63]]},{"text":"model, it then shows how meaning specificity affects the linguistic behavior and semantic content of Chinese resultative verb compounds (RVCs). ","acronyms":[[137,141]],"long-forms":[[109,135]]},{"text":"ically used as the target. For example, the NIST Open Machine Translation Evaluation (OpenMT) 2009 (Garofolo, 2009) constrained Arabic-English","acronyms":[[86,92],[44,48]],"long-forms":[[49,84]]},{"text":"Winnow and voted-perceptrons (Zhang et al2002;  Collins, 2002), or by using the sequence labeling  models, such as Hidden Markov Models (HMMs)  (Molina and Pla, 2002) and Conditional Random ","acronyms":[[137,141]],"long-forms":[[115,135]]},{"text":"   FIGURE 1: Community of Inquiry (CoI) models   (Adapted from: Garrison et al 2000) ","acronyms":[[35,38]],"long-forms":[[13,33]]},{"text":"columbia, edu  Abstract  Concept To Speech (CTS) systems are  closely related to two other types of ","acronyms":[[44,47]],"long-forms":[[25,42]]},{"text":"837 (a) (b) Figure 1: Deep recurrent neural network (DRNN) architectures: arrowheads constituted connection matrices; white, black, and grey circles represent input frames, ulterior states, and output frames, respectively; (a): L intermediate layer DRNN with recurrent connections","acronyms":[[53,57],[241,245]],"long-forms":[[22,51]]},{"text":"timing. This flexibility is in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech","acronyms":[[85,89]],"long-forms":[[60,83]]},{"text":"5.3 Valuation Metrics For YA, we used the standard implementations for P@1 and mean mutual rank (MRR) (Manning et al, 2008).","acronyms":[[102,105],[27,29]],"long-forms":[[80,100]]},{"text":"based on a probabilistic model. We investigate two methods using Smouldering Dirichlet Allocation (LDA) (Blei, 2003) in ?","acronyms":[[94,97]],"long-forms":[[65,92]]},{"text":"Here, we assume :  P(t i I G~ )  IP(tt I Pi-i PiWi) Pi-I piwi E dp  \\[ P(ti \/ Pi ) Pi-I Pi Wi ~ 1I) ","acronyms":[[33,35]],"long-forms":[[39,43]]},{"text":"guistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates","acronyms":[[92,94]],"long-forms":[[72,90]]},{"text":"74.80 ?  Table 2: Parsing accuracy; AS = attachment score; ER = error reduction w.r.t. projective baseline (%)","acronyms":[[36,38],[59,61]],"long-forms":[[41,57],[64,79]]},{"text":"2.2 Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al, 2003).","acronyms":[[127,130],[21,24]],"long-forms":[[110,125],[4,19]]},{"text":"successful method presented by Bendersky and Croft (2008) for selection and weighting of query noun phrases (NPs). It also extends work for deter-","acronyms":[[109,112]],"long-forms":[[95,107]]},{"text":"We then review some standard online learners (e.g. perceptron) before presenting the Bayes Point Machine (BPM) (Herbrich et al, 2001; Harrington et al, 2003).","acronyms":[[106,109]],"long-forms":[[85,104]]},{"text":"conjuncts depend on it. Nilsson et al (2007)  vindicator the Mel?cuk styles (MS) for parsing  Czech, taking the first conjunct as the head, ","acronyms":[[74,76]],"long-forms":[[59,72]]},{"text":"6.1 Corpus The training and testing data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to explore event coreference","acronyms":[[81,84]],"long-forms":[[65,79]]},{"text":"new opportunity: part of Attempto Controlled English (ACE) was mapped to OWL (Kaljurand and Fuchs, 2007), and Processable English (PENG) evolved to Sydney OWL Syntax (SOS) (Cregan et","acronyms":[[131,135]],"long-forms":[[110,129]]},{"text":" 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988).","acronyms":[[67,69]],"long-forms":[[50,65]]},{"text":"In the SIGHAN Bakeoff 2007, there are five training corpus for word segmentation (WS) task: AS  (Academia Sinica), CityU (City University of  Hong Kong) are traditional Chinese corpus; CTB ","acronyms":[[115,120],[82,84],[92,94],[185,188]],"long-forms":[[122,137],[63,80],[97,112]]},{"text":" 1 Overv iew o f  the  IPS  pro jec t   The IPS (Interactive Parsing System) researches  projects, at the Linguistics Departement of the ","acronyms":[[44,47],[23,26]],"long-forms":[[49,75]]},{"text":"as automatic speech recognising (ASR), natural language understanding (NLU), dialogues management (DM), natural language generation (NLG), and speech synthesis (TTS).","acronyms":[[132,135],[33,36],[71,74],[98,100],[160,163]],"long-forms":[[103,130],[3,31],[39,69],[77,96]]},{"text":"unified into one model. We refer to this model as the Unified Transition(UT) model. ","acronyms":[[73,75]],"long-forms":[[54,72]]},{"text":"integer linear programming (ILP) conditional random field (CRF) support vector machine (SVM) latent semantic analysis (LSA)","acronyms":[[88,91],[28,31],[59,62],[119,122]],"long-forms":[[64,86],[0,26],[33,57],[93,117]]},{"text":" Unser correction model makes use of a minimum divergence (MD) model (Berger et al., 1996),","acronyms":[[57,59]],"long-forms":[[37,55]]},{"text":"ported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal Histories of Your Medical Events (THYME) project (NIH R01LM010090 and U54LM008748).","acronyms":[[117,122],[38,43],[53,56],[133,136]],"long-forms":[[74,115],[10,36]]},{"text":"In this paper we investigate the relating between auspicious and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE","acronyms":[[101,103],[157,159]],"long-forms":[[81,99]]},{"text":"estimation the quality of the paraphrase collection.  In parcitular, Amazon?s Mechanical Turk1 (MTurk) provides a way to pay people petit amounts of","acronyms":[[94,99]],"long-forms":[[76,92]]},{"text":"` and survival variables S`. Languages shown are Latin (LA), Skanky Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Noting that only modern","acronyms":[[75,77],[126,128],[56,58],[95,97],[109,111],[144,146]],"long-forms":[[61,73],[114,124],[49,54],[80,93],[100,107],[135,142],[6,24]]},{"text":"ical relations may be at the head of multiple arcs.  For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ).","acronyms":[[87,92],[140,145]],"long-forms":[[70,85],[124,138]]},{"text":"Research in molecular-biology field is detection gigantic amount of newer facts, and thus there is an increasing need for information extraction (IE) technology to support database building and to find","acronyms":[[146,148]],"long-forms":[[122,144]]},{"text":"grammatically correct (readability). We engaged the services of Amazon Mechanical Turks (AMT) to judge the generated sentences based on a discrete","acronyms":[[89,92]],"long-forms":[[64,87]]},{"text":"They are not constrained  by characteristics of any precedents utterance in the  discourse segment (DS), and the elements of Cf(Un)  are partially ordered to reflect relative prominence ","acronyms":[[91,93],[119,121],[116,118]],"long-forms":[[72,89]]},{"text":"bouts in lexical processing. In Lawsuits of SWAP (Spoken Word Access Processes), A. Cutler, J. M McQueen, and R. Zondervan, ed.,","acronyms":[[50,54]],"long-forms":[[56,84]]},{"text":"and the neural language model (NLM), for each expressions jumpsuit: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj). For each","acronyms":[[100,103],[122,127],[31,34],[82,87]],"long-forms":[[90,98],[109,120],[8,29],[66,80]]},{"text":"non-interactions   Y They are widely disseminated and mediate all of the known biologic effects of  angiotensin II (AngII) through a multitude of signal transduction systems, consisting activation of phospholipases C and A2, inhibition of adenylate cyc-","acronyms":[[115,120]],"long-forms":[[99,113]]},{"text":"Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al, 2005), and perform quite well on corresponding test sets.","acronyms":[[117,120]],"long-forms":[[99,115]]},{"text":"3 Paragon We introduce a topic-model based approach to declarative knowledge (DK) acquisition and describe how this expertise can be applied to two unsuper-","acronyms":[[76,78]],"long-forms":[[53,74]]},{"text":"operating system.  For production deployment we  used Message Driven Beans (MDBs)used IBM  Websphere Application Server? (","acronyms":[[76,80],[87,90]],"long-forms":[[54,74]]},{"text":" 3.1 Underspecified domains An underspecified domain (UD) represents a partially specified reference domain corresponding to the","acronyms":[[54,56]],"long-forms":[[31,52]]},{"text":"ers. In Proceedings of the 19th International Conference on Compuatational Language (COLING),  pages 556?562.","acronyms":[[88,94]],"long-forms":[[60,86]]},{"text":"perimented with three classifiers available in R?  logistic regression (LogR), decision tree (DTree) and support vector machines (SVM).","acronyms":[[72,76],[94,99],[130,133]],"long-forms":[[51,70],[79,92],[105,127]]},{"text":" 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of apply-","acronyms":[[13,16]],"long-forms":[[18,40]]},{"text":"5 Conclusion We have presented an efficient broadening of the subsequent regularization (PR) framework to a more general class of penalty functions.","acronyms":[[87,89]],"long-forms":[[61,85]]},{"text":"mark conception are sent to the kernel-based location belief tracker, while all other concepts are expedition to a Dynamic Probabilistic Ontology Trees (DPOT) semantic belief tracker, whose structure is illustrated in","acronyms":[[144,148]],"long-forms":[[106,142]]},{"text":" 1 Introduction Coreference resolution (CR) ? the task of determin-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"1 Introduction 1.1 Background Back in 2004, ETV (Eenadu Television), Hyderabad, felt a need for a text editor to prepare news","acronyms":[[44,47]],"long-forms":[[49,66]]},{"text":"based Translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING?90), pages 247?252.","acronyms":[[101,110]],"long-forms":[[60,99]]},{"text":"In Modern Norms Arabic (MSA), all nouns and adjectives have one of three lawsuits: nominative (NOM), accusative (ACC), or genitive (GEN). What","acronyms":[[113,116],[132,135],[27,30],[95,98]],"long-forms":[[101,111],[122,130],[3,25],[83,93]]},{"text":"wsj_1286)) In addition to the semantic roles described in the rolesets, verbs can take any of a set of general, adjunct-like arguments (ArgMs), distinguished by one of the function tags shown in Table 1.","acronyms":[[136,141]],"long-forms":[[125,134]]},{"text":"calizations.  Direct responses (DS) are essentially characterized by introductory markers like yes\/no\/this is pos-","acronyms":[[32,34]],"long-forms":[[14,30]]},{"text":"document is different from the question. Also, in  Information Extraction (IE), in which the system  tries to extract elements of some events (e.g. ","acronyms":[[75,77]],"long-forms":[[51,73]]},{"text":"Kanayama et al99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55 Haruno et al98 Probabilistic model (DT + Reinforce) EDR (50,000) 85.03 Fujio et al98 Probabilistic model (MILS) EDR (190,000) 86.67 Tabled 4: Comparison with the related collaboration","acronyms":[[174,176],[38,40],[43,47],[49,52],[105,107],[120,123],[178,181]],"long-forms":[[167,172]]},{"text":" In Proceedings of the 16th International Conference on Computational Linguistics (COLING), volume I, pages 466?471.","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"and opportunity. In Proceedings of the 1st International Temporal Web Analytics Workshop (TWAW), pages 1?8.","acronyms":[[92,96]],"long-forms":[[59,90]]},{"text":"noun adjustments? which generally is shown in the form of a Noun phrase (NP) [A DE B]. A in-","acronyms":[[74,76]],"long-forms":[[61,72]]},{"text":"lated by the original Penn Treebank grammar to total PCFG surprisal calculated by the Nguyen et al (2012) Generalized Categorial Grammar (GCG). ","acronyms":[[138,141],[53,57]],"long-forms":[[106,136]]},{"text":" 2.3 IR Similarity Measures (IR) The information retrieving?based characters (IR) were based on a dump of English Wikipedia from Novem-","acronyms":[[75,77],[5,7],[29,31]],"long-forms":[[37,58]]},{"text":"Symbol Descriptor Example  If. I Simple connection between (IMPEDANCE) GA TAF~I  I|.2 ","acronyms":[[60,69]],"long-forms":[[31,58]]},{"text":"#and (a probabilistic AND) is used. Otherwise, the  probabilistic passage operator #UWn (unordered window)  is used.","acronyms":[[84,87],[22,25]],"long-forms":[[89,105]]},{"text":"(section 24). Legend of models: ST=Divide Tags; EC=enhanced connectivity. ","acronyms":[[47,49]],"long-forms":[[50,71]]},{"text":"Free word associations are the words people spontaneously come up with in re-sponse to a stimulus word. Such informa-tion has been collected from test persons and stored in databases.  A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en-abling the computer to produce similar associative responses as people do.","acronyms":[[246,249]],"long-forms":[[213,244]]},{"text":"TECHNICS+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95 A0 - - - 0.85 - - - 0.93 - - - N\/A - - - 0.97 Table 5: Harmonize conclusions for all datasets and configurations: Use semantic ties (SR), monosemous bind (LM) or both (SR+LM).","acronyms":[[225,227],[120,123],[248,250],[261,266]],"long-forms":[[205,223],[230,246]]},{"text":"Most common approaches to language model adaptation, such as count merging and model interpolation, are special cases of maximum a posteriori (MAP) estimation (Bacchiani and Roark, 2003).","acronyms":[[143,146]],"long-forms":[[121,141]]},{"text":"In order to  make the notion of focusing more precise I will use the  notion of Reference Time (RT), adopted from Reichen-  bach 1947 but reinterpreted pragmatically: RT is to be ","acronyms":[[96,98],[167,169]],"long-forms":[[80,94]]},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is a difficult Natural Language Treat task which requires","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"ity, to validate Boosting NER supposition. We moreover use three Markov chain Monte Carlo (MCMC) algorithms for probabilistic inference in MLNs.","acronyms":[[86,90],[26,29],[134,138]],"long-forms":[[60,84]]},{"text":"passenger boat.  (5) Multiple mentions (MENTION): These alignments link one word to multiple occur-","acronyms":[[42,49]],"long-forms":[[32,40]]},{"text":"scheme that includes: (1) a pre-annotation that pieces the dialogue into turns which are further segmented into Elementary Discourse Units (EDUs) with the author of each turn automatically bestowed;","acronyms":[[142,146]],"long-forms":[[114,140]]},{"text":"guistics to guide the solution of locality of arguments. In particular, Maximal Projections (MP) which dominates","acronyms":[[92,94]],"long-forms":[[72,90]]},{"text":"In Proceedings of the Conference of the Tranquil Association for Computational Linguistics (PACLING), pages 120?128. ","acronyms":[[91,98]],"long-forms":[[40,89]]},{"text":"Beijing University of Posts and Telecommunications (BUPT) ? ?  Beijing Institute of Technician (BIT) ?  ","acronyms":[[96,99],[52,56]],"long-forms":[[63,94],[0,49]]},{"text":"clude examples such as Facebook AI Investigative?s challenge problems for AI-complete QA (Westin et al, 2015) and the Allen Institute for AI?s (AI2) Aristo project (Clark, 2015) along with its recent","acronyms":[[139,142],[81,83]],"long-forms":[[113,137]]},{"text":">60 ICE ICING ICE ICE 10C 1~ 10C I0C  >.Y0 ICE ICE ICE ICE ICE IOC I(E 1~  >40 IO(J ICE ICE lO0 ICE ICE 1~ FROSTING  >35 ICE lO(l ICE FROSTING lie lOC IOC lOC ","acronyms":[[77,79]],"long-forms":[[82,93]]},{"text":"MTCL = Machine Translation and Computational  Linguistics (1965-1968)  AJCL = AmericanJournal of Computational  Linguistics (1974-present) ","acronyms":[[71,75],[0,4]],"long-forms":[[78,110],[7,57]]},{"text":"domains: ? Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings","acronyms":[[36,38],[80,85]],"long-forms":[[11,34]]},{"text":"Their study with three different learner ? na??ve Bayes, maximum entropy (MaxEnt) and the support vector machine (SVM) ?","acronyms":[[75,81],[115,118]],"long-forms":[[58,73],[91,113]]},{"text":"by the name of its author or the author?s place of  work.  In computational linguistic (CL) terms thi s  exercise relies on proper noun extra ction.","acronyms":[[88,90]],"long-forms":[[62,86]]},{"text":"the previous section. We contrast this metric with Normalized Pointwise Reciprocal Information (NPMI) which usage only the events A = X+a and B = X","acronyms":[[92,96]],"long-forms":[[51,90]]},{"text":"We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolves 807","acronyms":[[77,79]],"long-forms":[[61,75]]},{"text":"from standards formats to OpenNLP specific ones. We represented standard formats with EMF (an Ecore models for each one) and we created specific transformations utilise Java Emitter Templates (JET) 16","acronyms":[[189,192],[29,32],[85,88]],"long-forms":[[165,187]]},{"text":"left AV and right AUDIOVISUAL. For a string s with length l, we identifies the left accessor variety (LAV) as the types of distinct characters preceding s in","acronyms":[[89,92],[5,7],[18,20]],"long-forms":[[66,87]]},{"text":"One means of achieving a fiat structure with extrinsic  ordering is by using the ID\/LP formalism, a subformalism of  GPSG that allows immediate dominance (ID) information to be  specified separately from linear precedence (LP) notions. (","acronyms":[[155,157],[81,86],[117,121],[223,225]],"long-forms":[[134,153],[204,221]]},{"text":"ACC mPUR + ACC The random baseline(BL) is calculated as follows: BL = 1\/number of categories","acronyms":[[35,37],[0,3],[4,8],[11,14],[65,67]],"long-forms":[[26,33]]},{"text":"In (Raymond and Riccardi, 2007), the SFST-based model is compared with Support Vector Machines (SVM) (Vapnik, 1998) and Conditional Random Fields (CRF) (Laf-","acronyms":[[96,99],[37,41],[147,150]],"long-forms":[[71,94],[120,145]]},{"text":"and intangible factors known to the engineer but unknown to the computer. For example, the engineer  may need to postpone the placement of equipment in a certain CSA (Carrier Serving Area) due to a fixed  cap on near-term expenditures, or she may decide to activate DLC (Digital Loop Carrier) equipment in ","acronyms":[[162,165]],"long-forms":[[167,187]]},{"text":" of the International Joint Conference on Artificial Intelligence (IJCAI). ","acronyms":[[67,72]],"long-forms":[[8,65]]},{"text":"As expected, the poor performance observed on the sniper text is mainly due to two reasons: the presence of out of vocabulary (OOV) words and the incorrect translations of terminological","acronyms":[[127,130]],"long-forms":[[108,125]]},{"text":"This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM.","acronyms":[[92,95],[116,120]],"long-forms":[[71,90]]},{"text":"Language expression can vanish and appear in translation. For example, the preposition (PP) in the source rule does not illustrates up in any of   Machine Translation Based on Constraint-Based Synchronous Grammar 615 ","acronyms":[[91,93]],"long-forms":[[78,89]]},{"text":"6 Scope Resolution One way of dealing with scope ambiguities is by utilizing underspecified representations (URs). A","acronyms":[[105,108]],"long-forms":[[73,103]]},{"text":"1998) as the reference. In Chinese FrameNet, the predicates, called lexical units (LU), evoke frames which roughly correspond to different","acronyms":[[83,85]],"long-forms":[[68,81]]},{"text":"sets used in the fast Tree Kernel.  2.3 A Prompt Tree Kernel (FTK) To computation the kernels defined in the previous","acronyms":[[60,63]],"long-forms":[[42,58]]},{"text":"the interpolated and adapted models are compared.  For the Estonian task, letter error rate (LER) is also reported, since it tends to be a more indicative","acronyms":[[93,96]],"long-forms":[[74,91]]},{"text":"Abstract We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual","acronyms":[[67,69]],"long-forms":[[52,65]]},{"text":"Schafer and Graham, 2002) discussed several approaches such as case deletion, mean substitution, and recommended maximum likelihood (ML) and Bayesian multiple imputation (MI).","acronyms":[[133,135],[171,173]],"long-forms":[[113,131],[150,169]]},{"text":"used as the seed to build a reliable hedging cue set. Maximum Entropy (MaxEnt) model is used as the learning technological to","acronyms":[[69,75]],"long-forms":[[52,67]]},{"text":"8  end if  Active attribute are deliberated in the section about identifying the SC (Section 8),  because the raison d'etre of the active-character component of an interpretation is that ","acronyms":[[80,82]],"long-forms":[[84,91]]},{"text":"based method is somewhat better. The two systems share the same topic relevance score (REL) and sentiment score, but the sentence-ranking manner","acronyms":[[87,90]],"long-forms":[[70,79]]},{"text":" The obtained Japanese scores as compared to the  scores from the original English experiment (E-E-E)  are displays in Figure 6.","acronyms":[[94,99]],"long-forms":[[74,92]]},{"text":"phases that are performed sequentially without feedback: Question Processing (QP), Passage Retrieval (PR) and Reply Extraction (AE). More","acronyms":[[129,131],[78,80],[102,104]],"long-forms":[[110,127],[57,76],[83,100]]},{"text":"from the remaining pool of data.  The intrinsic stopping criterion (ISC) we propose here focuses on the latter aspect of the ideal stop-","acronyms":[[68,71]],"long-forms":[[38,66]]},{"text":"4-gram + LSA usage linear interpolation  with ? LSA = 0.11 (LI). ","acronyms":[[60,62],[9,12]],"long-forms":[[48,51],[19,39]]},{"text":"overall 412 5298 1519 750 Table 1: Corpus stats: number of sentences (S), phrases (W), frame elements (FE) and alignments.","acronyms":[[106,108]],"long-forms":[[90,104],[64,73],[79,84]]},{"text":"RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. ","acronyms":[[78,81],[0,2],[30,33],[57,59]],"long-forms":[[84,95],[5,17],[36,55],[62,76]]},{"text":"Moreover, in (Hahn et al, 2008a) two more templates are applied to SLU: a Maximum Entropy (EM) model and a models coming from the Statistical Machine Translation (SMT) commu-","acronyms":[[88,90],[64,67],[159,162]],"long-forms":[[79,86],[126,157]]},{"text":"8 Completely Monitor Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6 Table 4: Results obtained using the MUC punctuation program for the Circulated News and Newswire data sets Broadcast News (BNEWS) Newswire (NWIRE) Real Mentions System Mentions True Mentions System Mentions","acronyms":[[203,208],[220,225],[121,124]],"long-forms":[[187,201],[210,218]]},{"text":" 5.1 Calculation of Emotion Tag weights  Sense_Tag_Weight (STW): The tag underweight has  been calculated using SentiWordNet.","acronyms":[[59,62]],"long-forms":[[41,57]]},{"text":"88 lated based on a co-occurrence relationship between i and w. Next, the semantic orientation (SO) of the phrase i is obtained by calculating the difference be-","acronyms":[[96,98]],"long-forms":[[74,94]]},{"text":"fidence informations about each system?s hypothesis. This feature class includes the mess network (CN) word confidence, CN slot entropy, and the number of alter-","acronyms":[[103,105],[124,126]],"long-forms":[[84,101]]},{"text":"represented in an n ? n matrix of objects by a  multidimensional scaling (MDS) of the distancing  between each object.","acronyms":[[74,77]],"long-forms":[[48,72]]},{"text":"simardm@iro.umontreal.ca Abstract The term translation spotting (TS) refers to the tasks of identifying the target-language (TL)","acronyms":[[65,67]],"long-forms":[[43,63],[107,122]]},{"text":"email: mal@aber.ac.uk Stephen Pulman University of Oxford (UK) email: sgp@clg.oxley.ac.uk","acronyms":[[59,61]],"long-forms":[[37,57]]},{"text":"                  Threshold is a function of Length(LR) and text  size. The basic idea is larger amount of length(LR)  or text size matches larger amount of Threshold.","acronyms":[[114,116]],"long-forms":[[90,96]]},{"text":"Such vectors can be used to fulfil all standard linear algebra operations applied in vector-based semantics: Measuring the cosine of the angle between vectors, implement singular value decomposition (SVD) to the generals matrix, and so on.","acronyms":[[200,203]],"long-forms":[[170,198]]},{"text":"Table 4: Entailment judgment in closed test  of mutual information (T=True, F=False,  MI=mutual information). ","acronyms":[[86,88]],"long-forms":[[89,107],[70,74],[78,83]]},{"text":"This model is a multinomial DP model. Under the Chinese restaurant processed (CRP) (Aldous, 1985) 394","acronyms":[[76,79],[28,30]],"long-forms":[[48,74]]},{"text":"validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training","acronyms":[[142,145],[99,102]],"long-forms":[[110,140],[69,97]]},{"text":"3.1 Graph-based parsing model The graph-based parsing model purpose to browsing for the maximum spanning tree (MST) in a graph (McDonald et al, 2005).","acronyms":[[106,109]],"long-forms":[[83,104]]},{"text":"The parameters ? are estimated through the optimization of a Maximum Likelihood (ML) criterion using the Expectation-Maximization (EM) al-","acronyms":[[81,83],[131,133]],"long-forms":[[61,79],[105,129]]},{"text":"Examples of failure of analysis  (i) JISSAI (in fact), CHOSHA-TACHI-WA (authors) {\\[KORE-WO (it) TSUKATTE  (using), JUURYOKU-SOUGO-SAYOU-GA (gravitationally interacting) SHIHAI-SURU (govern-  ing)\\] TENTAI-NO (astronomical) UNDOU-NI-TSUITE (about the motion), KOUSEIDO-DE (high- ","acronyms":[[137,139],[37,43],[55,70],[170,181],[199,208],[224,239]],"long-forms":[[141,156],[183,195],[210,222],[241,257]]},{"text":"Abstract  Since statistical machine translation (SMT)  and translation memory (TM) complement  each other in matched and unmatched regions, ","acronyms":[[79,81],[49,52]],"long-forms":[[59,77],[16,47]]},{"text":"al. ( 1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the buy of the lexical concept ","acronyms":[[47,49]],"long-forms":[[23,45]]},{"text":"(41a) PP = ~ Ctu i (PN)  i i>0  (41b) NP = N ~ Cati(P N) i  1 ","acronyms":[[38,40],[6,8],[20,22]],"long-forms":[[43,53]]},{"text":"restrictive relative clauses, and epithets ? trigger conventional implicatures (CI) whose truth is necessarily presupposed, even if the truth conditions","acronyms":[[80,82]],"long-forms":[[53,78]]},{"text":"Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (?","acronyms":[[71,77],[90,94],[117,120]],"long-forms":[[45,69]]},{"text":"The work described in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programs.  To promote more linguistically-oriented writing process research, Inputlog aggregates the logged process data from the character level (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) tools to further annotate the logged process data with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.","acronyms":[[382,385]],"long-forms":[[353,380]]},{"text":"(disharmonic) combinators to increase the expressive power of the model.  \u0001 KZGS10 (Kwiatkowski et al2010) uses a restricted higher-order unification procedure, which iteratively breaks up a logical form into","acronyms":[[76,82]],"long-forms":[[84,105]]},{"text":"number of correcVi'abeled-constituents in proposed parse  number of correct matched constituent inproposed parse  6) Sentence parsing ratio(SPg) =  number\" of sentences having a proposed parse by parser ","acronyms":[[140,143]],"long-forms":[[117,133]]},{"text":"Query key words and concepts Token and concept Indexing Knowledge Source Adapters (KSAs)   integrate and deliver content from ","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":"Table 3 presents the total number of training example extracted from SemCor (CASTE) and from the background documents (BG). As expected, by","acronyms":[[117,119],[78,80]],"long-forms":[[95,105],[70,76]]},{"text":"mfly@sky.ru Abstract YARN (Yet Another RussNet) project started in 2013 aims at engender a large","acronyms":[[21,25]],"long-forms":[[27,46]]},{"text":"i' SRI - text extraction  ~\" TRW - document detected output  i' University of Massachusetts (UMass) -  document detection ","acronyms":[[94,99],[29,32],[3,6]],"long-forms":[[65,92]]},{"text":"Abstract We present the outset provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":"tion), and thus mitigating the overfitting problem.  A Dirichlet process (DP) prior is typically used to achieve this interplay.","acronyms":[[74,76]],"long-forms":[[55,72]]},{"text":"C, N, X, Y } (V = verb, AV = auxiliary verb, EV = verb with Ersatzinfinitiv, Vfin = finite verb, Vinf","acronyms":[[24,26],[45,47],[77,81]],"long-forms":[[29,43],[50,75],[84,95],[18,22]]},{"text":"In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 1262?1267, Genoa, Italy, May.","acronyms":[[89,93]],"long-forms":[[54,87]]},{"text":"3. Formal descriptions about specific games which are classified as formal texts (FoT) 4.","acronyms":[[82,85]],"long-forms":[[68,80]]},{"text":" A statistical classification technique based  on the use of Hidden Markov Models (HMM) was  used as a language discriminator.","acronyms":[[83,86]],"long-forms":[[61,81]]},{"text":" ? Minimum Bayes Risk (MBR). We rescore the","acronyms":[[23,26]],"long-forms":[[3,21]]},{"text":" ? True Positive (TP), the predicted e was correctly referred to by s.   ","acronyms":[[18,20]],"long-forms":[[3,16]]},{"text":" 1 Introduction A Chinese natural language processing (NLP) platform always includes lexical analysis (word","acronyms":[[55,58]],"long-forms":[[26,53]]},{"text":" Acknowledgements  This work was supported by the Social Sciences and Humanities Research Council (SSHRC) of Canada. We ","acronyms":[[99,104]],"long-forms":[[50,97]]},{"text":"greater segments.  Speech Plans Incorporated (SSI) has been using a  version of this two-stage cascade of the MMI encoders in the ","acronyms":[[47,50],[111,114]],"long-forms":[[18,45]]},{"text":"ical relations may be at the leiter of multiple arcs.  For example, the surface themes (S-SBJ) of a passive verb is also the logical object (L-OBJ).","acronyms":[[87,92],[140,145]],"long-forms":[[70,85],[124,138]]},{"text":"  For example, the s t ruc ture  tn whtch  ad jec t ives  (ADJ) repeat  a rb i t ra ry  ttmes and a noun  (N) fo l lows  them tn Engl lsh ts expressed as ","acronyms":[[59,62]],"long-forms":[[43,56],[100,104]]},{"text":" (Markov) that impressively differed from the output of the noisy channel model (NoisyC), which assures our finding that Markovized models can pro-","acronyms":[[82,88]],"long-forms":[[61,74]]},{"text":"outlining in Section 3. We then trained linear SVMs (Supporting Vector Machine) using the LIBLINEAR software (Fan et al, 2008), using L1 loss","acronyms":[[47,51],[87,96]],"long-forms":[[53,75]]},{"text":"URL?, excluded utterances with the symbols that indicate the re-posting (RT) or quoting (QT) of others?","acronyms":[[89,91],[0,3],[73,75]],"long-forms":[[80,87],[61,71]]},{"text":"Scores for PK    Because Academies Sinica corpora (AS) are  provided by us, we are not allowed to participate ","acronyms":[[50,52],[11,13]],"long-forms":[[25,40]]},{"text":"Yoram Bachrach is a researcher in the Online Services and Advertising group at Microsoft Research Cambridge BRITONS. His research area is artificial intelligence (AI), emphasis on multi-agent scheme and computational game theory.","acronyms":[[158,160],[108,110]],"long-forms":[[133,156]]},{"text":"served as the founder Editor-in-Chief of ACM  Transactions on Knowledge Discovered from Data (TKDD). He has received ACM SIGKDD In-","acronyms":[[94,98],[42,45],[117,120],[121,127]],"long-forms":[[47,92]]},{"text":"dependency tree.  3.1 Naive Approaches (NA)  In this approach we first run a parser on the input ","acronyms":[[38,40]],"long-forms":[[22,36]]},{"text":"The general structure of D2S is represented in Figure 1. It comprises of two modules, the Language Gen-  eration Module (LGM), and the Speech Generation Module (SGM). The LGM takes data as inlet .and ","acronyms":[[163,166],[28,31],[123,126],[173,176]],"long-forms":[[137,161],[92,121]]},{"text":"English?German 45.59 43.72 Automatically aligned corpora average 47.99?4.20 45.75?3.64 Table 1: The grammar coverage (GC) of NF-ITG for varying corpora dependant on the interpretation of word alignments: neighbour Translation Equivalence or discontiguous Translation Equivalence","acronyms":[[122,124],[129,135]],"long-forms":[[100,120]]},{"text":"2.4 Dictionaries of  Bahasa Nusantara, Indonesian Linguistics Association (MLI)   Masyarakat Linguistik Indonesia (MLI) is a panel  of institutions, organizations and enterprise, ","acronyms":[[115,118],[75,78]],"long-forms":[[82,113]]},{"text":"(TK), i.e. any node of a tree along with all its descendants. una subset tree (SST) exploited by the SubSetTreeKernel is a more","acronyms":[[77,80],[1,3]],"long-forms":[[64,75],[99,109]]},{"text":"(Schubert 1987; Maxwell & Schubert 1989), and fig-  ure 1 shows the dependency trees for this example,  cross-coded for translation units (TUs). Each ellipse ","acronyms":[[139,142]],"long-forms":[[120,137]]},{"text":"II: irrelevant input due to ASR errors or noise.  We adopt logistic regression (LR)-based dialogue act tagging approach (Tur et al.,","acronyms":[[80,82],[28,31]],"long-forms":[[59,78]]},{"text":" The system includes four main stages: topic classification, named entity recognition (NER), disease\/location detection, and visualization.","acronyms":[[87,90]],"long-forms":[[61,85]]},{"text":" 4.1.7 Doctors? Prescriptions (PRESC) Some of our food-health relations are moreover men-","acronyms":[[31,36]],"long-forms":[[16,29]]},{"text":"In this section, we define pomsets as a modelling for contour concurrency. A labelled partial order (LPO) is a 4 tuple (V, ?,","acronyms":[[100,103]],"long-forms":[[76,98]]},{"text":"segmentable candidates, and selected a correct segmentation candidate from the list by using a value of LEF (Likelihood Valuation Function, Section 2.1) and so on.","acronyms":[[101,104]],"long-forms":[[106,136]]},{"text":" ? REL = relation + property; ARG = NP\/VP\/ADJ","acronyms":[[3,6],[30,33]],"long-forms":[[9,17],[36,45]]},{"text":"AFIPS Washington Off ice   GAO REPORTS ON FEDERAL MODELING  The General Accounting 0fEic.e (GAO') has released a repor t -  on \"Ways to  Improve  &mug-nt of Federally kcnded ~o@ute r i z ed  ~ o d e Z s I ~  (#-  enc luse $1.00) .","acronyms":[[92,96],[0,5],[27,30]],"long-forms":[[64,90]]},{"text":"colour histograms derived from images.  In the RGB (Red Green Blue) colour model, each pixel is represented as an integer in range of","acronyms":[[47,50]],"long-forms":[[52,66]]},{"text":"Table 1: Summary of distance values between the 380 observed A-N pairs and the prophecies from each modelled (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression).","acronyms":[[108,111],[122,125],[61,64],[142,146]],"long-forms":[[112,120],[126,140],[147,179]]},{"text":"tion task which involves acquired patterns in the distribution of opinion-bearing words and targets using machine learning (ML) techniques. Onto partic-","acronyms":[[125,127]],"long-forms":[[107,123]]},{"text":" ? Research Question 1 (RQ1): How do we determining suggestions in suggestion mining?","acronyms":[[24,27]],"long-forms":[[3,22]]},{"text":" The test set (Table 13) consisted of the beginnings of three short stories by  Ernest  Hemingway,  15 three articles f rom the New York Times (NYT), 16 the first three chapters of  a novel  by  Uwe JohnsonS the first two chapters of a short story by Heiner Mfiller, TM ","acronyms":[[144,147],[267,269]],"long-forms":[[128,142]]},{"text":"subcategorisation information in the form of a Y in col-  umn 8, then we can assign the tags for wh-pronou,  head (HWH). An exception listings is utilize to map to the ","acronyms":[[116,119]],"long-forms":[[110,114]]},{"text":"2https:\/\/www.mturk.com\/mturk\/. 3http:\/\/tatars.org\/martin\/PorterStemmer\/ 4Reviews with NDr = NTr are regarded as wrong classified by TopicSpam.","acronyms":[[94,97],[88,91]],"long-forms":[[98,125]]},{"text":"25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are additionally orthogonal to our jobs.","acronyms":[[65,70],[34,38]],"long-forms":[[51,63],[19,32]]},{"text":" 3.1 Classification Our evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine (SVM) classifiers.","acronyms":[[76,82],[112,115]],"long-forms":[[59,74],[88,110]]},{"text":"Central to the predictive dialogue is the topic representation for each scenario, which enables the population of a Predictive Dialogue Network (PDN). ","acronyms":[[145,148]],"long-forms":[[116,143]]},{"text":"system architecture  The data is stored in one central Resource  Repository (RR). As training data may change (for ","acronyms":[[77,79]],"long-forms":[[55,75]]},{"text":"The number of sentences with product features  ? Word level (WL)  ?","acronyms":[[61,63]],"long-forms":[[49,59]]},{"text":"Because of a scarcity of such corpora, most work has using the International Corpus of Learner English (ICLEv2) (Granger et al 2009) for training and appraise","acronyms":[[103,109]],"long-forms":[[62,101]]},{"text":"Linear-chain CRFs correspond to finite state machines, and can be roughly understood as conditionally-trained hidden Markov models (HMMs). This class of CRFs","acronyms":[[132,136]],"long-forms":[[110,130]]},{"text":"There are four goodness algorithms reviewed by Zhao and Kit (2008a). The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), Accessor Variety (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii","acronyms":[[120,123]],"long-forms":[[95,118]]},{"text":"there is a ties to the next node).  Prompts (PT) occur when the tutor attempts to elicit a meaningful contribution from the schoolchildren.","acronyms":[[45,47]],"long-forms":[[36,43]]},{"text":"velopment and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al, 2005); articles 001-270 and 440-","acronyms":[[90,93]],"long-forms":[[72,88]]},{"text":"In Proceedings of the 43rd Annual Meetings of the Association for Computational Linguistics (ACL), pages 75? ","acronyms":[[92,95]],"long-forms":[[49,89]]},{"text":"the current work, we uses a subset of that corpus consisted of examples whose question types were PRC (procedure), RSN (reason) or ATR (atrans). ","acronyms":[[99,102],[116,119],[132,135]],"long-forms":[[104,113],[121,127],[137,143]]},{"text":"Table 9 Number of times a core grammatical operation was annotated more than once in the treebank (TRBK) by the model utilise gold morphology (GOLD-M), and by the model using predict morphology (PRED-M).","acronyms":[[140,146],[98,102],[194,200]],"long-forms":[[123,138],[88,96],[172,192]]},{"text":"CrossT values). The regression model predicted  Figure 4: ST aligned crossings (CrossS), as generated  when verifying the ST against the TT ","acronyms":[[82,88],[0,6],[58,60],[123,125],[138,140]],"long-forms":[[71,80]]},{"text":" 1 Introduction Language identification (LangID) is the problem of deciding what natural language a document is written in.","acronyms":[[41,47]],"long-forms":[[16,39]]},{"text":"In Proc. of the Conference on Computational Natural Language Learning (CoNLL), 7.","acronyms":[[71,76]],"long-forms":[[30,69]]},{"text":"formation from non-expert bilingual speakers. The  Translation Corrected Tool (TCTool) is a userfriendly online tool that allows users to add, removal ","acronyms":[[80,86]],"long-forms":[[51,78]]},{"text":"LPM output using application knowledge  ? Function Generator Module (FGM) convert  SAM output into executable function calls ","acronyms":[[69,72],[86,89],[0,3]],"long-forms":[[42,67]]},{"text":"Note that for the objective of Figure 1: Example Babytalk Input Data: Sensors HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 = blood CO2 levels; SaO2 = oxygen saturation; T1 = chest","acronyms":[[77,79],[94,99],[118,124],[144,148],[170,172]],"long-forms":[[82,92],[102,110],[127,136],[151,168]]},{"text":"He et al (2010) measure the similarity between hypothetical and referencing translation in terms of the Lexical Functional Grammar (LFG) representation.","acronyms":[[128,131]],"long-forms":[[100,126]]},{"text":"lexicon tool, with a classifications phase founded on  Featured-Based kernel such as SL kernel and TreeBased kernel such as Dependency tree (DT) kernel  (Culotta and Sorensen, 2004) and Phrase Structures ","acronyms":[[138,140],[82,84]],"long-forms":[[121,136]]},{"text":"tial state for HMM, then experiment with different inference algorithms such as ExpectationMaximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare","acronyms":[[130,132],[15,18],[105,107],[153,155]],"long-forms":[[110,128],[80,103],[137,151]]},{"text":" In Proceedings of the Globally Conference on Language Resources and Evaluation (LREC). ","acronyms":[[86,90]],"long-forms":[[51,69]]},{"text":"3.2 Feature Space An essential aspect of our approach below is the word sense disambiguation (WSD) of the noun. Us-","acronyms":[[94,97]],"long-forms":[[67,92]]},{"text":"We therefore choices to perform ASR utilizes a statistical language model (LM) and employ CMU?s Sphinx to generate an n-best list of recogni-","acronyms":[[70,72],[30,33],[85,90]],"long-forms":[[54,68]]},{"text":"Figure 1(a), the node @VP indicates that a binarization has been performed on the subtree VP (VBD PRT PP). Totality remaining stipulations that","acronyms":[[90,92],[23,25]],"long-forms":[[94,101]]},{"text":"The projection  of the root noose on the active leaves is referred to  as the M-BDU (Leading BDU). Only syntactic infor-","acronyms":[[77,82]],"long-forms":[[84,92]]},{"text":"In this paper, we also implement type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).","acronyms":[[82,84],[112,114]],"long-forms":[[86,106],[116,132]]},{"text":" IV. RETROACTIVE SSL (R-SSL). After","acronyms":[[24,29]],"long-forms":[[5,22]]},{"text":"OWL DL is a logical language that combines the expressivity of OWL2 with the favourable computational properties of Description Logics (DL), notably decidability and monotonicity (Baader et al, 2003).","acronyms":[[136,138],[0,3],[4,6],[63,67]],"long-forms":[[116,134]]},{"text":" 4.4 SLU Features The SLU (Spoken Language Understanding) features are used to resolve implicit and explicit REs.","acronyms":[[22,25],[5,8],[109,112]],"long-forms":[[27,56]]},{"text":"168 Figure 1: Plots of concreteness vs. imageability scores for literal vs. nonliteral words in the VUAMC (Conc=concreteness, Imag=imageability, NL=nonliteral, L=literal) concrete than the dependent\/s; H","acronyms":[[145,147],[100,105],[107,111],[126,130]],"long-forms":[[148,158],[112,124],[131,143],[162,169]]},{"text":"where C is the concept that subsumes both C1 and C2 and has the biggest information content (i.e., it is the slightest common subsumer (LCS)). ","acronyms":[[132,135]],"long-forms":[[109,130]]},{"text":"V is the dictionary size.  The question difficulty estimation (QDE) task aims to automatically learn the question difficul-","acronyms":[[63,66]],"long-forms":[[31,61]]},{"text":"  Abbreviation: ACK=Acknowledgment; COMP=Completion;  SNU=Signalling Non Understanding; Sources abbreviated as: F  = Friendship; V = Visibility; Rte = Route ","acronyms":[[54,57],[16,19],[36,40],[141,144]],"long-forms":[[58,82],[20,33],[41,51],[113,123],[129,139],[147,152]]},{"text":"Language Weaver, Inc. This article shows that the structures of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistic machine translation (SMT) systems. ","acronyms":[[198,201]],"long-forms":[[165,196]]},{"text":"it can therefore mean ? prostrated on the threshold  and respectfully (AD) paid visits three times? or ","acronyms":[[71,73]],"long-forms":[[53,56]]},{"text":" 3 The V IT  Format   The VIT (short for Verbmobil Interface Term) was  destined as a common output format for the deux ","acronyms":[[26,29],[7,11]],"long-forms":[[41,65]]},{"text":"This paper proposes a method of sentencing extraction based on Support Vector Machines (SVMs). To","acronyms":[[86,90]],"long-forms":[[61,84]]},{"text":"chose a method from the second clusters. We use the Hierarchical Agglomerative Clustering (HAC) algorithm (Jain et al, 1999) for all experimenting reported","acronyms":[[88,91]],"long-forms":[[49,86]]},{"text":"The TRIANGLE application  (TRIANGLE) and a new Windows 95  Accessible Graphing Calculator (ACG) both  worded by the SAP employs tone plots to ","acronyms":[[91,94],[27,35],[119,122]],"long-forms":[[59,78],[4,12]]},{"text":"Evaluation (LREC?08), Marrakech, Morocco, may.  European Language Resources Association (ELRA). ","acronyms":[[89,93],[12,16]],"long-forms":[[48,87]]},{"text":"gorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very accurate to represent shallow syn-","acronyms":[[143,148],[9,16],[77,81]],"long-forms":[[111,141]]},{"text":" 1 Introduction  Statistical Machine Translation(SMT) is currently the state of the art solution to the machine ","acronyms":[[49,52]],"long-forms":[[17,47]]},{"text":"\/(cs + |N |?)  P (GR = gri|SCF = s) = (csgri +?) \/(cs + |G|?)","acronyms":[[18,20]],"long-forms":[[23,30]]},{"text":"While initial-state ECR provides a measure of the likelihood of a favorable result, it does not address how well a unique state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the paths to a successful outcome con-tributed to that results, or whether the second-best decision in a singular state ought have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much nicer a peculiar political is compared to its replacement. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two steps in that state and schism by the mean of the two values.","acronyms":[[485,487],[576,578],[20,23]],"long-forms":[[467,483]]},{"text":"stituent category labels expressing adverbials (RB), coordinations (CC), various types of interjections (UH, INTJ) and adverbial expression (ADVP). We maggio","acronyms":[[138,142],[48,50],[68,70],[105,107],[109,113]],"long-forms":[[119,136],[53,66],[25,46],[90,103]]},{"text":"5 Conclusions We have presented a sequential semantic role labeling system for the Semeval-2007 tasks 17 (SRL). ","acronyms":[[105,108]],"long-forms":[[68,95]]},{"text":"implicit (assuming a good adequate coverage of the marker resource). The Annodis corpus lists rhetorical relationships between elementary discourse units (EDUs), typically clauses, and complex discourse unit (sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a principal verb of","acronyms":[[149,153],[212,216],[257,261]],"long-forms":[[121,147]]},{"text":"(see, e.g., (Moschitti, 2006) for more details).  Syntactic Tree Kernel (STK), likewise known as a subset tree kernel (Collins and Duffy, 2002), mapping","acronyms":[[73,76]],"long-forms":[[50,71]]},{"text":"In: Ernst Buch-berger (ed.): Tagungsband deir 7. Konferenz zur Verarbeitung nats?rlicher Sprache (KONVENS), Universit?t Wien, 161?168. Strube, Gerhard (1984).","acronyms":[[96,103]],"long-forms":[[48,94]]},{"text":"capability [and t o  go] interregional without involving the private sector.  The General Services Administratioq (GSA) l a s t  month amended its M v a o y  Gu--&de-  Zines adding privacy a d  security considerations for use i n  ADP o r  tqlecom- ","acronyms":[[115,118],[231,234]],"long-forms":[[82,113]]},{"text":"Rule-Based Machine Translation(MT)(Hutchins and Somers, 1992) requires large-scale knowledge to analysed both source language(SL) sentences and purpose language(TL) sentences.","acronyms":[[125,127],[31,33],[159,161]],"long-forms":[[109,123],[11,30],[143,158]]},{"text":"assignment. Ours use a generative model based on a Dirichlet Process (DP) defined over composed rules.","acronyms":[[68,70]],"long-forms":[[49,66]]},{"text":"class of mildly context sensitive grsmmars which we  230  call \"Ranked Node Rewrite Grammaxs\" (RNRG's). ","acronyms":[[97,103]],"long-forms":[[64,94]]},{"text":" As with most modern simulators, DISs are controlled via  graphical user interfaces (GUIs). Still, the simulation ","acronyms":[[85,89],[33,37]],"long-forms":[[58,83]]},{"text":"emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA)  ~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD) ","acronyms":[[85,88],[8,11],[24,27],[41,45],[56,59],[70,73],[85,88],[95,98],[107,110],[117,120]],"long-forms":[[75,84],[0,7],[13,23],[30,40],[47,56],[63,69],[91,94],[100,106],[112,116]]},{"text":"unigram model (BoW)+HI, where in addition to representing what words occur in a text, we also represent what Harvard Inquirer (HI)3 word classes occur in it.","acronyms":[[127,129],[15,18],[20,23]],"long-forms":[[109,125]]},{"text":"The current state of the art within the comment summarisation sphere is to grouping comments used Latent Dirichlet Allocation (LDA) topic modelling (Khabiri et al, 2011; Ma et al, 2012;","acronyms":[[126,129]],"long-forms":[[97,124]]},{"text":"Economics neighborhood fbank  bank  Subject Cipher EC = Economies  account cheque money by ","acronyms":[[49,51]],"long-forms":[[54,63]]},{"text":"ing measure of the loss in modeling accuracy:     Probability Loss (PL):   )()()(),( vuvuvu +?","acronyms":[[68,70]],"long-forms":[[50,66]]},{"text":" 1 Introduction Synchronous contex free grammars (SCFGs) generalize traditions context-free grammars to generate","acronyms":[[50,55]],"long-forms":[[16,48]]},{"text":"the bird comes.  Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly","acronyms":[[92,98]],"long-forms":[[66,90]]},{"text":"constituents. For example, discussions the possible adaptation of Phillips' algorithm to incremental gener-  ation, Lager and Black (1994) point out that some versions of Categorial Grammar (CG) would make the  generators more talkative, by giving risen to \"a more generous notions of constituency\".","acronyms":[[190,192]],"long-forms":[[170,188]]},{"text":"experimenting can be listed as follows.  Head Word (HW.) The predicate?s head word as","acronyms":[[50,53]],"long-forms":[[39,48]]},{"text":"ian.fletcher, peter.maguire@cs.man.ac.uk Abstract Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent","acronyms":[[65,68]],"long-forms":[[50,63]]},{"text":" This is reflected in the query focused tasks run in the Document Comprehension Conference (DUC) and Text Analysis Conference (TAC) over the preceding","acronyms":[[92,95],[127,130]],"long-forms":[[57,90],[101,125]]},{"text":" 5 Conclusions Multiword expressions (MWEs) are a major obstacle that hinders precise natural language processing","acronyms":[[38,42]],"long-forms":[[15,36]]},{"text":"In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pages 581?588, Granada.","acronyms":[[91,95]],"long-forms":[[56,74]]},{"text":"written japanese. In Proceedings of the 6th  Workshop on Asian Language Resources (ALR),  pages 101?102.","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":"rejected by the space-saving algorithm.  Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely to relate to a burst of a certain topic.","acronyms":[[62,65]],"long-forms":[[41,60]]},{"text":"schematic representations of situations implicating  various participants, props, and other conceptual  roles, each of which is a frame aspect (FE). ","acronyms":[[143,145]],"long-forms":[[128,141]]},{"text":" 2. All the named agencies(NE) in the question are extracted as NE set.","acronyms":[[27,29],[64,66]],"long-forms":[[12,25]]},{"text":"61 Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP","acronyms":[[78,81],[98,101],[124,127],[141,144]],"long-forms":[[84,89],[72,76],[130,139],[104,113]]},{"text":" 1 Introduction  Generation of referring expression (GRE) is an  sizable task in the field of Natural Language ","acronyms":[[53,56]],"long-forms":[[17,51]]},{"text":"2. Definition of Stochastic Context-Free Grammars  We will now define stochastic ontext free grammars (SCFGs) and establish some  notation.","acronyms":[[103,108]],"long-forms":[[70,101]]},{"text":"The alignments produced by MEBA were  compare to the ones produced by YAWA and  evaluation against the Gold Standard (GS)1 annotations used in the Word Alignment Shared ","acronyms":[[118,120],[27,31],[71,75]],"long-forms":[[103,116]]},{"text":"ing via schema matching and lexicon extension. Across Association for Computational Linguistics (ACL). ","acronyms":[[93,96]],"long-forms":[[50,91]]},{"text":"though this is never the case.) The seconds such  peculiarities \"Theme as Chomeuf  (TAC) is the only  non-trinary-valued feature in our learner; it spec- ","acronyms":[[77,80]],"long-forms":[[58,74]]},{"text":"al., 2005) filters and summarizes the OmniPage output into Intermediate XML (IXML), as well as correcting certain characteristic errors from that stage.","acronyms":[[77,81]],"long-forms":[[59,75]]},{"text":" 1 Introduct ion  Many of the Natural Language Generation (NLG)  systems that produce flexible output, i.e. penalties ","acronyms":[[59,62]],"long-forms":[[30,57]]},{"text":"From our point of view, it  is very interesting to compare the results of Czech  stochastic POS (SPOS) tagger and a modified RB-  POS tagger for Czech.","acronyms":[[97,101]],"long-forms":[[81,95]]},{"text":"the verb that contained in a subordinated clause.  We use semantic role labeling (SRL) to aids  solve this problem in which the coordinated can ","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":" These features of Latin influenced the choice  of Dependency Grammars (DG)2 as the most  suitable grammar framework for building Latin ","acronyms":[[72,74]],"long-forms":[[51,70]]},{"text":"driver was measured in two ways. The driver performed a Tactile Detection Task (TDT) (van Winsum et al, 1999).","acronyms":[[80,83]],"long-forms":[[56,78]]},{"text":"226  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313, Seoul, South Korea, 5-6 July 2012.","acronyms":[[101,108]],"long-forms":[[51,99]]},{"text":"Table 4: Human expert evaluated accuracy (Acc.)  and full cluster accuracy (FAcc.) of models on","acronyms":[[76,81],[42,45]],"long-forms":[[53,74],[32,40]]},{"text":"The metrics Precision (P), Recall (R),  F-score (F) (F=2PR\/(P+R)), Recall of OOV  (ROOV) and Recall of IV (RIV) are used to  evaluate the results.","acronyms":[[107,110],[83,87]],"long-forms":[[93,105],[12,21],[27,33],[40,47],[67,80]]},{"text":" Dictionary-based methods rely on some dictionaries or lexical knowledge groundwork (LKB) such as WordNet (Fellbaum and Miller, 1998) that con-","acronyms":[[77,80]],"long-forms":[[53,75]]},{"text":"proach to automatically recognize predicate  heads of Chinese sentences based on a preprocessing step for maximal noun phrases 1(MNPs). ","acronyms":[[129,133]],"long-forms":[[106,126]]},{"text":"1.1 A System Exhibiting Reinforcement Learning The central motivation for building this dialogue system is as a platform for Reinforcement Learning (RL) experiments.","acronyms":[[149,151]],"long-forms":[[125,147]]},{"text":"Not Available?.  OmegaWiki (OW) is a freely editable online dictionary like WKT.","acronyms":[[28,30],[76,79]],"long-forms":[[17,26]]},{"text":"notation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using human-","acronyms":[[62,64],[86,88]],"long-forms":[[41,60],[66,84]]},{"text":"5) is clogged by (7), because of the  passinginto the subject'a story about es'; i.e., the specifier of  INFL (in the transformational ccount) or of VP (in theory  fond GPSG, etc.).","acronyms":[[105,109],[149,151],[171,175]],"long-forms":[[111,134]]},{"text":"line debates. In Proceedings of the 5th Workshop on Language Analyses for Social Media (LASM)@ EACL, pages 35?43.","acronyms":[[88,92],[95,99]],"long-forms":[[52,86]]},{"text":"Diacritization valuation of our experiments is  stated in terms of word error rate (WER), and  diacritization error rate (DER)5. ","acronyms":[[125,128],[87,90]],"long-forms":[[98,123],[70,85]]},{"text":"  This approach has been employed in Augmentative  and Alternative Communicative (AAC), in the  forms of multimodal vocabularies in assistive de-","acronyms":[[82,85]],"long-forms":[[37,80]]},{"text":"****I TRANSFORPlATICNS **SIW:  S C A N  C-ALLED AT 1 I  ANTFST CALLED FOR SrqSYLLA B (AACC) ,SD= 6 .  RES= 1 1 .","acronyms":[[70,73],[93,95],[86,90]],"long-forms":[[56,69]]},{"text":"In LREC 2006, Genoa Yes?im Aksan and Mustafa Aksan 2012. Construction of the Turkish National Corpus (TNC). In LREC 2012,","acronyms":[[102,105],[3,7]],"long-forms":[[77,100]]},{"text":"pairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al.,","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":" Like most of the successful AQUAINT QA systems,  LCC?s system uses an answer type (AT) ontology for  the classification of AT categories.","acronyms":[[84,86],[29,36],[37,39],[50,53],[124,126]],"long-forms":[[71,82]]},{"text":"the Tomita parser, which handled Japanese and Spanish as well as English . The parser output is grammatical structures called Functionally Tagged Templates (FLTs) which are construction using a linguistics formalism that modifies and extends the f-structure of Lexical-Functional Grammar (LFG) .","acronyms":[[159,163],[283,286]],"long-forms":[[126,157],[255,281]]},{"text":"fered to punched cards. Abbreviated alphabetical symbols  are used for the syntactic analysis (AP=adJective phrase)  because of the program's 24unlt search limitation.","acronyms":[[95,97]],"long-forms":[[98,114]]},{"text":"thematic structure, and defined well formedness conditions on the thematic structure and on the relation between thematic structure (TH) and syntactic dominance (ID) structure.","acronyms":[[133,135],[162,164]],"long-forms":[[113,121],[0,8]]},{"text":"UniProt the protein sequence database managed by the Swiss Institute of Bioinformatics (SIB), the European Bioinformatics Institute (EBI) and the Protein Information Resource (PIR)","acronyms":[[133,136],[88,91],[176,179]],"long-forms":[[98,131],[53,85],[146,174]]},{"text":"4 Supervised Named Entity Recognition In the first part of this work, we adopt a supervised named entity recognition (NER) framework for the attribute extraction problem from eBay listing titles.","acronyms":[[118,121]],"long-forms":[[92,116]]},{"text":"ing is combined with dictionary-based (e.g.,  WordNet) reranking, which leads to a 25% increase in mean reciprocal rank (MRR). Xu et al ","acronyms":[[121,124]],"long-forms":[[99,119]]},{"text":" 3.1 Classify Our evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine (SVM) classifiers.","acronyms":[[76,82],[112,115]],"long-forms":[[59,74],[88,110]]},{"text":"Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.","acronyms":[[94,97],[73,75],[138,143],[178,186],[204,206],[239,241]],"long-forms":[[98,125],[76,92],[144,169],[187,195],[242,244],[207,226]]},{"text":"1 Introduction Creating the annotated corpus needed for training a NER (named entity recognize) model is costly. ","acronyms":[[67,70]],"long-forms":[[72,96]]},{"text":"LDEP(NEXT) + Table 1: History-based featuring (TOP = token on superior of stack; NEXT = next token in input buffer; HEAD(w) = head of w; LDEP(w) = leftmost depen-","acronyms":[[75,79],[0,4],[46,49],[5,9],[110,117],[131,138]],"long-forms":[[82,92],[52,64],[120,129],[141,155]]},{"text":"This paper introduces a web application and a web service for the diagnostic valuation of Machine Translation (MT). These web-based","acronyms":[[110,112]],"long-forms":[[89,108]]},{"text":"specific right category before COO > (5) the common left coordination classes > (6) the other special right category > (7) the free cross-clause clausal category (IC) > (8) the common exited cross-clause category > (9) the free cross-clause punctuations (PUS). ","acronyms":[[253,256],[30,33],[163,165]],"long-forms":[[239,251],[56,68]]},{"text":"Internet: chris@lsi .com NLP OBJECTIVES LSI's overall natural language processing (NLP) objective is the development of a broad coverage, reusable system which is readily transportable to additional domains, applications, and sublanguages in English, as well as","acronyms":[[83,86],[25,28],[40,43]],"long-forms":[[54,81]]},{"text":"ining both BLEU and NIST scores? relationship to their Unlabeled Accuracy Notation(UAS). ","acronyms":[[80,83],[11,15],[20,24]],"long-forms":[[55,79]]},{"text":"375 that...). Extreme case formulations (ECF) are lexical patterns emphasizing extremeness (e.gram., This is","acronyms":[[41,44]],"long-forms":[[14,39]]},{"text":"vised taggers. One commonly-used unsupervised tagger is the Hidden Markov model (HMM), which models the joint distribution of a word se-","acronyms":[[81,84]],"long-forms":[[60,79]]},{"text":"More  technically, for each syntactic feature {sf1, sf2, ...,  sfn} of the set SF (Syntactic Features) represented  in the lexical typology, we define the goal of our ","acronyms":[[79,81]],"long-forms":[[83,101]]},{"text":"I .  INTRODUCTION  Preliminary research on machine translat ion (MT) started soon af ter  computers  became avai lable.","acronyms":[[65,67]],"long-forms":[[43,59]]},{"text":"1. Introduction  Named entity(NORTHEASTERN) recognition is important for recent  sophisticated information service such as question answering ","acronyms":[[30,32]],"long-forms":[[17,28]]},{"text":"06 33.3 125 33.1 73 Average: 35.4 216 36.1 125 Table 3: Directed dependency accuracies (DDA) and iteration counts for the 10 (of 23) train\/test splits affected by","acronyms":[[88,91]],"long-forms":[[56,86]]},{"text":"chose a method from the second group. We use the Hierarchical Agglomerative Clustering (HAC) algorithm (Jain et al, 1999) for all experiments reported","acronyms":[[88,91]],"long-forms":[[49,86]]},{"text":"2011) created a corpus of posts from several online forums about breast cancer, which later was used to extract potential adverse reactions from the most commonly used drugs to treat this disease: tamoxifen, anastrozole, letrozole and axemestane. The authors collected a lexicon of lay medical terms from websites and databases about drugs and adverse events. The lexicon was extended with the Consumer Health Vocabulary (CHV)5, a vocabulary closer to the lay terms, which patients usually use to describe their medical experiences. Then, pairs of terms co-occurring within a window of 20 tokens were considered.","acronyms":[[422,425]],"long-forms":[[394,420]]},{"text":"CN Bank(CNB): 200,000 samples  ? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples ","acronyms":[[42,46],[8,11],[74,78]],"long-forms":[[33,40],[0,7],[65,73]]},{"text":"1434  Trials of the 3rd Workshops on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"Ill the latter two  experimeuts a memory with the analysis of tile  most frequent word-forms (MFW) in Basque  was employs, so that only word-forms not found ","acronyms":[[92,95]],"long-forms":[[66,84]]},{"text":"scribe the relation R. Most previous systems perform these steps by first using named entity recognition (NER) to identify possible arguments and then using a simple string match, but this crude","acronyms":[[106,109]],"long-forms":[[80,104]]},{"text":"3 Polylingual Topic Model The polylingual topic model (PLTM) is an extension of latent Dirichlet alocation (LDA) (Blei et al.,","acronyms":[[108,111],[55,59]],"long-forms":[[80,106]]},{"text":"also recorded in the miscellaneous information field of the lexeme. Similarly, Gene Ontology (GO) (Consortium.,","acronyms":[[94,96]],"long-forms":[[79,92]]},{"text":" 2.3 Graphical User Interface The graphical user interface (GUI) is an important feature that has been recently added to Clairlib","acronyms":[[60,63]],"long-forms":[[34,58]]},{"text":"tence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark. ","acronyms":[[106,110],[29,32],[49,52],[75,78],[92,95]],"long-forms":[[113,124],[23,27],[35,40],[55,64],[81,90],[98,104]]},{"text":"ond, task B referred to as task of normalization involves the charting of each disarray mention to a  UMLS notions unique identifier (CUI).The mapping was limited to UMLS CUI of SNOMED clin-","acronyms":[[133,136]],"long-forms":[[106,131]]},{"text":"DC-10-30?s number 1 engine, a General Electric CF6-50C2, seasoned a casing breach when the 2nd-stage low pressure impeller (LPT) anti-rotation nozzle locks failed.?","acronyms":[[126,129],[47,55],[0,8]],"long-forms":[[104,124]]},{"text":" Take for example the word cliff which could be a  proper (NP) 1 or a common noun (NN) (ignoring ca-  pitalization of proper nouns for the moment).","acronyms":[[83,85],[59,61]],"long-forms":[[77,81]]},{"text":"c?2009 Association for Computational Linguistics Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE) Donna Byron","acronyms":[[135,139],[69,72]],"long-forms":[[86,133]]},{"text":"mance.  We investigated chopping criteria based on a fixed number of words (FIXED), at  speaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentence ","acronyms":[[76,81],[105,109],[123,128]],"long-forms":[[53,74],[115,121]]},{"text":"parsing. In Tenth International Conference on Parsing Technologies (IWPT), pages 121?132, Prague, Czech Republic.","acronyms":[[68,72]],"long-forms":[[18,66]]},{"text":"  We have also made a preliminary attempt to transfer a thesaurus entry from the Collins Thesaurus (CT) into  Italian by means of the English-Italian and Italian- ","acronyms":[[100,102]],"long-forms":[[81,98]]},{"text":"School of Computer Science, University of Manchester, UK ? National Centre for Text Mining (NaCTeM), UK ?","acronyms":[[92,98],[54,56],[101,103]],"long-forms":[[59,90]]},{"text":"1 Introduction Since the introduction of BLEU (Papineni et al, 2002), automatic machine translation (MT) evaluation has received a lot of research interest.","acronyms":[[101,103],[41,45]],"long-forms":[[80,99]]},{"text":"through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06-","acronyms":[[123,130],[8,11],[50,53],[167,170]],"long-forms":[[95,121],[20,48],[140,165]]},{"text":"2.3.1 KiF (Knowledge in Frame) The overall system, training and prediction, has been implemented in KiF (Knowledge in Frame), a script parlance that has been implemented into","acronyms":[[98,101],[6,9]],"long-forms":[[103,121],[11,29]]},{"text":"presidential nomination to seek the backing of the {{w|Libertarian Party (United States)|Libertarian Party}} (LP). ","acronyms":[[110,112]],"long-forms":[[89,107]]},{"text":"since it transpires in the corpus with the six differ-  ent tags: CD (cardinal), DT (determiner), JJ (ad-  jective), NN (noun). NNP (suitable noun) and VBP ","acronyms":[[114,116],[63,65],[78,80],[95,97],[125,128],[147,150]],"long-forms":[[118,122],[67,75],[82,92],[104,111],[130,141]]},{"text":"The project ? Reference Corpus Oriente Low German\/ Low Rhenish (1200?1650)?2 transliterates and grammatically annotates the Middle Low German (GML) texts from which we taking our examples. Be-","acronyms":[[142,145]],"long-forms":[[95,133]]},{"text":"all levels of syntactic represent-  ation, name\\].y, D-structure,  S-structure, and LF (logical form). ","acronyms":[[84,86]],"long-forms":[[88,100],[55,64],[69,78]]},{"text":"word lexicon optimized for a given task. We examine the task of out of vocabulary (OOV) word detecting, which relies on output from","acronyms":[[84,87]],"long-forms":[[65,82]]},{"text":"The availability of large scale data sets of manually annotated predicate?argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices","acronyms":[[205,208]],"long-forms":[[181,203]]},{"text":"These features capture the context of the adverb and aid in deciding the presence of the manner (MNR) component. ","acronyms":[[98,101]],"long-forms":[[90,96]]},{"text":"http:\/\/www.ukp.tu-darmstadt.de Abstract In this paper, we present a machine learned approach for word sense alignment (WSA) which combines distances between senses in the charts representations of lexical-semantic finances","acronyms":[[120,123]],"long-forms":[[98,118]]},{"text":"Harman D.K. 1983. Overview of the second Text Retrieval Conference (TREC-2). Information Processing","acronyms":[[68,74],[7,10]],"long-forms":[[34,66]]},{"text":"2.3 Approach BUAP-RUN-3: Random Indexing and Bag of Concepts The vector space model (VSM) for document representation supporting search is probably the most","acronyms":[[85,88],[13,23]],"long-forms":[[65,83]]},{"text":"or database .  Superficially, DEFT resembles a Natural language Understanding (NLUI) system ; however, there are key differences .","acronyms":[[79,83]],"long-forms":[[47,77]]},{"text":"telling. In Computers in Entertainment (CIE), Association for Reckoning Machinery (ACM), volume 5.","acronyms":[[83,86],[40,43]],"long-forms":[[46,81],[12,38]]},{"text":"tially freely-available fount: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinic Evidence (BMJ-CE)4 were used to design and develop the presented test","acronyms":[[137,143],[70,74],[104,106]],"long-forms":[[114,135],[33,68],[78,102]]},{"text":"It is encouraging that the results (after correcting the misaligned identifiers) for the patched system are approaching the Inter Tagger Agreement (ITA) level reported for OntoNotes sense tags by the task or-","acronyms":[[148,151]],"long-forms":[[124,146]]},{"text":"Xi, where Par(Xi) denotes the parents of Xi.  Conditional probability distributions (CPDs) can be defined in various ways, from look-up tables","acronyms":[[85,89],[10,13]],"long-forms":[[46,83],[30,37]]},{"text":"ory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchy structures called Discourse Trees (DTs), which can integrate several layers of other linguistic information, e.g.,","acronyms":[[114,117],[34,37]],"long-forms":[[97,112]]},{"text":" 3 Data The RST Discourse Treebank (RST-DT) (Carlson et al, 2002) was used for training and testing.","acronyms":[[36,42]],"long-forms":[[12,34]]},{"text":"following are the most frequently used ones: ? MS = Mel?c?uk style used in the MeaningText Theory (MTT): the first conjunct is the","acronyms":[[47,49]],"long-forms":[[52,66]]},{"text":" 1 Introduction  Open domain question answering (QA), as defined  by the TREC competitions (Voorhees, 2003), ","acronyms":[[49,51],[73,77]],"long-forms":[[29,47]]},{"text":"From our point of opinion, it  is very interesting to compare the results of Czech  stochastic POS (SPOS) tagger and a amendment RB-  POS tagger for Czech.","acronyms":[[97,101]],"long-forms":[[81,95]]},{"text":"Saccade Length (SL) Veritable Sum of saccade lengths (measured by number of phrase) divided by word count Simple Regression Count (REG) True Generals number of gaze regressions Gaze Skip count (SKIP) Real Number of words skipped divided by total word count","acronyms":[[125,128],[16,18],[185,189]],"long-forms":[[107,117],[0,14]]},{"text":" cs.uni-kassel.de\/bibsonomy\/dumps Content Relevance (CRM) model (Iwata et al, 2009) and Etiquette Allocation Modeling (TAM) (Si et al,","acronyms":[[53,56],[110,113]],"long-forms":[[34,51],[88,108]]},{"text":"episodes in lexical processing. In Proceedings of SWAP (Spoken Word Access Processes), A. Cutler, J. M McQueen, and R. Zondervan, ed.,","acronyms":[[50,54]],"long-forms":[[56,84]]},{"text":"KEY: Number of discussions and posts on the topic (Discs, Posts).  Number of authors (NumA). Posts per author (P\/A).","acronyms":[[86,90],[111,114]],"long-forms":[[67,84],[93,109]]},{"text":"2.2 Graphical Representation Recently, Ding et al (2008) utilizes skip-chain and 2D Conditional Random Spheres (CRFs) (Lafferty et al, 2001) to perform the relational learning for","acronyms":[[106,110],[76,78]],"long-forms":[[79,104]]},{"text":"The CoreSC scheme consists of three layers; the first layer corresponds to eleven concepts (Background (BAC), Hypothesis (HYP), Motivation (MOT), Goal (GOA), Object (OBJ), Method (MET), Model","acronyms":[[122,125],[140,143],[4,10],[104,107],[152,155],[166,169],[180,183]],"long-forms":[[110,120],[128,138],[92,102],[146,150],[158,164],[172,178]]},{"text":" Definitions  Default Unification (first version) AU!B = A ~ U B, where A ~ is the maximal (i.e. most  specific) ingredients in the subsumption order such that A' r- A and A ~ U B is defined.","acronyms":[[49,53]],"long-forms":[[56,63]]},{"text":"weiwei@cs.columbia.edu Recap In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence,","acronyms":[[64,67]],"long-forms":[[35,62]]},{"text":" ? Term Base share (TBX): XML Terminology Exchange Standard.","acronyms":[[23,26]],"long-forms":[[3,21]]},{"text":"Tipster (ADEPT) Program is a demonstration project  aimed at alleviating problems currently being faced by  the Office of Information Resources (OIR). OIR has ","acronyms":[[145,148],[9,14],[151,154]],"long-forms":[[112,143]]},{"text":"[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and H. Cheng, Bandera: Extracting finite-state models from java source code, in: Proceedings of the Internationally Conference on Software Genie (ICSE), 2000. ","acronyms":[[231,235]],"long-forms":[[181,229]]},{"text":"that to go from the head of the chunk to the objectives in the dependency graph (Figure 3), you traverse a SUB (topic) link upwards. ","acronyms":[[103,106]],"long-forms":[[108,115]]},{"text":"5 System Description The PEZ system encompass of three element, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest","acronyms":[[100,105],[25,28],[125,130]],"long-forms":[[76,98]]},{"text":"of the log-likelihood.     We usage the tempered EM (TEM) as described  by Hofmann (1999).","acronyms":[[52,55]],"long-forms":[[39,50]]},{"text":" Acknowledgements  This worked was supported by the Social Sciences and Humanities Research Council (SSHRC) of Canada. We ","acronyms":[[99,104]],"long-forms":[[50,97]]},{"text":"traction) consists of two steps: keyphrase candidate extraction and the genetic programming of keyphrase scoring measures (KSMs).1 3.1 Step 1: Keyphrase candidate extraction","acronyms":[[123,127]],"long-forms":[[95,121]]},{"text":" ? National Drug File7 (NDF): this ontology  contains information about a comprehensive ","acronyms":[[24,27]],"long-forms":[[3,22]]},{"text":" ? Modifier substitution (M-Sub) :  t2 is a substitution of t~ if and only if : ","acronyms":[[26,31]],"long-forms":[[3,24]]},{"text":" Type Short time members Long time members Abbreviations Husband My DD (Dear Daughter), Ton PS (Plastic Surgeon) Social networking Facebook, fb","acronyms":[[68,70],[93,95]],"long-forms":[[72,85],[97,112]]},{"text":"cos(d, c).  Candidate Rank (CR) The features described so far disambiguate entire surface form s ?","acronyms":[[28,30]],"long-forms":[[12,26]]},{"text":"step of segmentation is tabled in Section 3 with two variants: stochastic word alignment (GIZA) and integer linear programming (ILP). Then assessments","acronyms":[[131,134],[93,97]],"long-forms":[[103,129]]},{"text":" org\/wikipedia\/California?. It is distinct from named entity extracting (NEE) in that it identifies not the occurrence of names but their reference.","acronyms":[[68,71]],"long-forms":[[43,66]]},{"text":"associated with each. The next column indicates the percentage of the majority classe (MAJ.) and count","acronyms":[[86,90]],"long-forms":[[70,78]]},{"text":" 2004. The Automatic Content Extraction (ACE) Program?Tasks, Data, and Evaluation.","acronyms":[[41,44]],"long-forms":[[11,39]]},{"text":" ? REL = relationship; ARG = NP\/VP\/ADJ (6) ACADE?MIQUE = Qui manque d?originalite?,","acronyms":[[3,6],[19,22],[39,50]],"long-forms":[[9,17],[25,34],[53,78]]},{"text":"In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA).We do not assume the existence of any anno-","acronyms":[[86,88]],"long-forms":[[68,84]]},{"text":" 1 Introduction Information Extraction (IE) is a natural language processing task in which text documents are ana-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"ning of ORG (B-O). Various of them are mislabeled as O and beginning of location (B-L), resulting low remind and low precision for ORG.","acronyms":[[79,82],[8,11],[13,16],[128,131]],"long-forms":[[56,77]]},{"text":" In a second experiment, we used the original TGRD corpus but added the linguistics variety (LV) (i.e., MSA and DA) trait.","acronyms":[[90,92],[46,50],[101,104],[109,111]],"long-forms":[[72,88]]},{"text":"is a very laborious and costly treated. Silver standard corpus (SSC) annotation is a very recently direction of corpus development which","acronyms":[[64,67]],"long-forms":[[40,62]]},{"text":"ysis incorporating social networks. In Proceedings of Knowledge Discovering and Data Mining (KDD). ","acronyms":[[91,94]],"long-forms":[[54,82]]},{"text":"sible transliteration candidates. We measured performance using the Mean Reciprocal Rank (MRR) measure.","acronyms":[[90,93]],"long-forms":[[68,88]]},{"text":"  In the late 1970s a research team at USCInformation Sciences Institute (ISI) studied natural  dialogues with particular interest in applying the ","acronyms":[[74,77],[39,42]],"long-forms":[[42,72]]},{"text":"notation is illustrated in Figure 3.  3.3 Positional Unknown Model (PosUnk) The main weakness of the PosAll model is that","acronyms":[[68,74]],"long-forms":[[42,60]]},{"text":"with parts of the annotated logical forma.  \u0001 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra (disharmonic) combinators to rises the expressive power of the model.","acronyms":[[45,49],[89,93]],"long-forms":[[51,79]]},{"text":"model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919","acronyms":[[83,87],[108,112],[113,117]],"long-forms":[[54,81]]},{"text":"In Proceedings of the 11th Globally Joint Conference on Artificial Intelligence (IJCAI-89), volume 2, pages 1511?1517.","acronyms":[[86,94]],"long-forms":[[27,84]]},{"text":"end returnmodels [] Algorithm 1: Propitious Diversity Tuning (PDT) lectively produce very diverse translations.3","acronyms":[[60,63]],"long-forms":[[33,58]]},{"text":"Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien","acronyms":[[139,144],[64,67],[170,177]],"long-forms":[[126,137]]},{"text":"This work was partly supported by UK EPSRC project GR\/N36462\/93: ? Robust Accurate Statistical Parsing (RASP)?. ","acronyms":[[104,108],[34,36],[37,42]],"long-forms":[[67,102]]},{"text":"vantages while considering the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints","acronyms":[[123,126]],"long-forms":[[96,121]]},{"text":"mostly context-free, with some context-sensit ive and  some transformational  rules, written in a modif ied  Backus Normal Form (BNF). Each rule contains the ","acronyms":[[129,132]],"long-forms":[[109,127]]},{"text":"The Text Encoding Efforts (TEI) is a cooperate undertaking of the Association  for Machines and the Humanities (ACH), the Association for Computational Lin-  guistics (ACL), and the Association for Literary and Linguistic Computing (ALLC). ","acronyms":[[239,243],[30,33],[118,121],[174,177]],"long-forms":[[188,237],[4,28],[71,116],[128,172]]},{"text":"a major Department of Agriculture system, due to inadequate organ planning.  Complete sets of the Federal mfomtion treatments staodards (FIPS) are now avail-  able from the National Bureau of Norms at $46.00 each.","acronyms":[[138,142]],"long-forms":[[99,136]]},{"text":"CRF) A wide range of contextual information, such as surrounding words (GREF), dependency or case structure (GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.","acronyms":[[138,142],[0,3],[72,76]],"long-forms":[[109,114]]},{"text":" 1 Introduction Information extraction (IE) systems reclaiming structured information from text.","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"We use the following label set: S-O (not in maze); S-M (single word maze); B-M (beginning of multi-word 72","acronyms":[[75,78],[32,35],[51,54]],"long-forms":[[80,98],[56,72]]},{"text":"minimal set of defaults.  A Preferential Default Description Logic (PDDL)  based on weigthed defaults has been worded in ","acronyms":[[68,72]],"long-forms":[[28,66]]},{"text":"In practice we can find approximate solution using such algorithms as: Loopy Belief Propagation (BP), Mean Field (MF), Gibbs Sampling (Gibbs). ","acronyms":[[114,116],[97,99],[135,140]],"long-forms":[[102,112],[77,95],[119,124]]},{"text":"  1 Introduction  Sign Language (SL) is a visual-gestural parlance, using the whole upper body articulators ","acronyms":[[33,35]],"long-forms":[[18,31]]},{"text":"In sum, the text-to-text similarity measure combined with our sentence-level sen-timent analysis algorithm aids us identify the representative rationales of diverse opinions in an online discussion. An overview of our meth-od is illustrated in Silhouette 1.  We applied our means in analyzing Wikipe-dia Article for Deletion (AfD) deliberation con-tent. Next we discuss how this method is utilized to analyze the content.","acronyms":[[320,323]],"long-forms":[[298,318]]},{"text":" 5 Conclusions The spoken language understanding (SLU) system discussed in this paper is entirely statistically based.","acronyms":[[50,53]],"long-forms":[[19,48]]},{"text":"given for each environment. For example, Q appeared eight  times in the context EH--EN ( E - n ,  and a inspections of the  reference list illustrates that all these phenomena were in the ","acronyms":[[84,86],[80,82]],"long-forms":[[89,94]]},{"text":"7-3-1, Hongo, Bunkyo-ku  Tokyo 113, Japan  It is difficult for a natural language understanding system (NLUS) to deal  with ambiguities.","acronyms":[[104,108]],"long-forms":[[65,102]]},{"text":" 3 Results and Chat Chain frequency (CF) and chain length (CL) reflect the dyad?s tweeting behaviors.","acronyms":[[44,46],[66,68]],"long-forms":[[27,42],[52,64]]},{"text":"eigenvalues are not zero, then I? minimizes the multiway normalized cut(MNCut): MNCut(I) = K ??","acronyms":[[72,77],[80,85]],"long-forms":[[48,71]]},{"text":"Roget (RG) ablaze aglow alight argent auroral beaming blazing superb WordNet (WN) burnished sunny shiny lustrous undimmed sunshiny brilliant TransGraph (TG) nimble ringing fine aglow keen glad light colorful Lin (LN) red yellow orange pink blue brilliant archer white somber","acronyms":[[156,158],[219,221],[81,83],[7,9]],"long-forms":[[144,154],[214,217],[72,79],[0,5]]},{"text":"ticPhrase, Predicate, Argument, ? the MILE Data Classes (MDC) which constitute the attributes and values to adorn","acronyms":[[60,63]],"long-forms":[[38,58]]},{"text":" Recently, there have been increasing interests for dialogue act (DA) recognition in spoken and written conversations, which include meetings,","acronyms":[[66,68]],"long-forms":[[52,64]]},{"text":"cal Dirichlet Process (HDP) (Teh et al, 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically infer the number of topics.","acronyms":[[112,115],[23,26]],"long-forms":[[83,110],[0,21]]},{"text":"systematic way.  The MIME (Managing Information in Medical Emergencies)1 project is developing technology to","acronyms":[[21,25]],"long-forms":[[27,70]]},{"text":"transduction and matching phrases approximately.  Unicode (UTF8) is altogether supported and is in fact the only encoding accepted by Foma.","acronyms":[[57,61]],"long-forms":[[48,55]]},{"text":"The next-to-last  column showings the precision (PRE)--the true positives divided by all verbs that Lerner  judged to be +S. The last column shows the remembering (REC)--the true favorable divided  by all verbs that were judged +S by hand.","acronyms":[[157,160],[46,49]],"long-forms":[[149,155],[35,44]]},{"text":"an ASR system. The main idea was to design a vocabulary model (LM) to combine the trigram language model probability with the translate","acronyms":[[61,63],[3,6]],"long-forms":[[45,59]]},{"text":"Two criteria are used:  1. Overlapping Ambiguity Strings (OAS): the  reference segmentation and the segmenter ","acronyms":[[58,61]],"long-forms":[[27,56]]},{"text":"In t roduct ion   This paper discusses the relationstfip between Tree Adjoin-  ing Grammars (TAG's) and :Head Grammars (HG's). TAG's ","acronyms":[[120,124],[93,98],[127,132]],"long-forms":[[105,118],[65,91]]},{"text":" 2 Extended typed  A-ca lcu lus   CU(\\] (Categorial U,,ificAtion (l:ra,nma,r) \\[8\\] is a,d-  vantageous, compared to other phrase structure ","acronyms":[[34,36]],"long-forms":[[41,64]]},{"text":"Experimental results on Europarl with different translation directions (BLEU% on WMT08).  RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05).","acronyms":[[90,92],[72,77],[81,86]],"long-forms":[[93,104]]},{"text":"CP = Relative Pronoun  +  IP  PP = Preposition  + NP AdjP = Adjective + NP","acronyms":[[30,32]],"long-forms":[[35,46]]},{"text":"identification. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.","acronyms":[[98,104]],"long-forms":[[71,96]]},{"text":"in sentence level are not combined. That is why  most of the verb phrases(VP) are inside match  (53.28%).","acronyms":[[74,76]],"long-forms":[[61,72]]},{"text":" (d) The word?s position in the sentence (e) The word?s Part of speech (POS) tag, based on the Stanford POS tagger2","acronyms":[[72,75],[104,107]],"long-forms":[[56,70]]},{"text":"The lexical features used are word bigrams. The Part of Speech (PoS) of the target word and its neighbors make up the the syntactic","acronyms":[[64,67]],"long-forms":[[48,62]]},{"text":"2: boston sweeping colorado to win world series 3: rookies respond in first crack at the vast time C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 rankings summaries of the redsox cluster","acronyms":[[111,114],[95,99]],"long-forms":[[115,145],[100,109]]},{"text":"versational participants. This type of HMM is called a speaker HMM (SHMM) and has been successfully utilized to model two-party conversa-","acronyms":[[68,72],[39,42]],"long-forms":[[55,66]]},{"text":"vide a significant degree of control. Perhaps nowhere is this observation more keenly  felt than in weak lexical ontologies like Princeton WordNet (PWN). In PWN [1], ","acronyms":[[148,151],[157,160]],"long-forms":[[129,146]]},{"text":"of Electrical and Computers Genie Pohang University of Science and Technology (POSTECH) Advanced Information Technology Research Center (AITrc) San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784","acronyms":[[142,147],[84,91]],"long-forms":[[93,140],[39,82]]},{"text":"syntactic skeleton defined in Equalizer. 1, namely, Subject(S), Verb(V), Directly Object(DO), Indirect Object(IO), Preposition(P) and Noun(Object) of the Preposition(N).","acronyms":[[80,82],[101,103]],"long-forms":[[66,78],[85,99],[45,52],[57,61],[106,117]]},{"text":"participant Japanese systems were developed in a four-  mes schedules of time and output results similar to the Message Understanding Conference-6 (MUC-6) \\[1\\]  English language systems with F-Measures between 70 - ","acronyms":[[150,155]],"long-forms":[[114,148]]},{"text":"We use the same evaluated metrics as in (mcdonalds et al, 2005). Dependency accuracy (DA) is the proportion of non-root words that are assigned the","acronyms":[[86,88]],"long-forms":[[65,84]]},{"text":"monolingual applications and have been used in commercial grammar checkers.1 These parsers produce a logical form (LF) representation that is compatible across multiple languages (see","acronyms":[[115,117]],"long-forms":[[101,113]]},{"text":"We also mark conjunct clauses with the featuring nosubj if they are neither spearheaded by an vital nor contain a child node with the grammatical function SB (subject) or EP (expletive). This is useful in order to correctly parse","acronyms":[[153,155],[169,171]],"long-forms":[[157,164],[173,182]]},{"text":"in a Swedish Clinical Corpus Hercules Dalianis, Maria Skeppstedt Department of Computer and Systems Sciences (DSV) Stockholm University","acronyms":[[110,113]],"long-forms":[[65,108]]},{"text":"Adjoining Grammars as the compilation of a  more abstract and modular layer of linguistic  description : the  metagrammar (MILLIGRAM). MG ","acronyms":[[123,125],[128,130]],"long-forms":[[110,121]]},{"text":"extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff,","acronyms":[[121,123],[12,14]],"long-forms":[[97,119]]},{"text":"In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[92,95]],"long-forms":[[49,90]]},{"text":"systems. This shared task was partially supported by Shared Annotated Resources (ShARe) projects NIH 5R01GM090187 and Temporal His-","acronyms":[[81,86],[96,99]],"long-forms":[[53,79]]},{"text":"Natural Language Processing (NLP) techniques can be leveraged in detecting events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are mentioned in the text that defines an event. To perform Named Entity Recognition (NER) on tweets Ritter et. al. (","acronyms":[[302,305]],"long-forms":[[276,300]]},{"text":"t have also been used.  2.2 EasyAdapt (EA) For this section, we give a brief overview of","acronyms":[[39,41]],"long-forms":[[28,37]]},{"text":"Participants in this study included 39 children with typical development (TD) and 21 children with autism spectrum disorder (ASD). ASD was di-","acronyms":[[125,128],[74,76],[131,134]],"long-forms":[[99,123],[53,72]]},{"text":"la AR  TD  par PREP (prEposition)  main SUBS (substamif)  REC8 (rEcursif simpler) ","acronyms":[[40,44],[3,5],[7,9],[15,18],[58,62]],"long-forms":[[46,55],[21,32],[64,72]]},{"text":" This representation treats clitics as separate tokens and abstracts the orthographic rewrites they undergo when cliticized. See the handling of the l\/PREP+Al\/DET in word #6 in Table 5.  This representation is used by the LDC in the Penn Arabic Treebank (PATB) (Maamouri  et al., 2004) and tools such as MADAMIRA (Pasha et al.,","acronyms":[[255,259],[222,225],[149,155],[156,162],[304,312]],"long-forms":[[233,253]]},{"text":"ing and understanding convolutional networks. In European Conference on Computer Vision (ECCV). ","acronyms":[[89,93]],"long-forms":[[49,87]]},{"text":"the earliest in the passage is returned. We utilized the selective gain computation (SGC) algorithm (Zhou et al, 2003) to select features and estimation","acronyms":[[81,84]],"long-forms":[[53,79]]},{"text":" At our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both smithereens of text.","acronyms":[[41,48]],"long-forms":[[50,57]]},{"text":"we describe SCANMail, a system that employs automatic speech recognition (ASR), information retrieval (IR), information extraction (IE), and human computer interaction (HCI) technology to permit users to browse and search their voicemail messages by content","acronyms":[[169,172],[12,20],[74,77],[103,105],[132,134]],"long-forms":[[141,167],[44,72],[80,101],[108,130]]},{"text":"project.  Among section 2 we provide an overview of the automatic compound processing (AuCoPro) project, which forms the background of this researching.","acronyms":[[84,91]],"long-forms":[[53,82]]},{"text":"y) 7. Mutual dependency (MD) log P (xy)2P (x?)P (? y)","acronyms":[[25,27]],"long-forms":[[6,23]]},{"text":"validity of the large margin mode is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; testing error is related to training data errors, number of training","acronyms":[[142,145],[99,102]],"long-forms":[[110,140],[69,97]]},{"text":"Extraction Abbreviations NE = Named Entity CE = Correlated Entity","acronyms":[[25,27],[43,45]],"long-forms":[[30,42],[48,65]]},{"text":"constituent boundary prediction algorithm, the  followiug measures were used:  1) The cost time(CT) of the kernal  functions(CPU: Celeron TM 366, RAM: 64M).","acronyms":[[96,98],[125,128],[138,140],[146,149]],"long-forms":[[86,94]]},{"text":"certainty factor equals to 0.6 (3\/5). The general formula for the certainty factor (CF) is demonstrated as follow: CFi = Generals number of the answer component at leaf node i","acronyms":[[84,86],[108,110]],"long-forms":[[66,82]]},{"text":"Table 9 (Hindi). Here, precision measures the number of correct Named Entities (NEs) in the machine tagged file over the total number of NEs in the ma-","acronyms":[[80,83],[137,140]],"long-forms":[[64,78]]},{"text":"We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., ","acronyms":[[99,106],[68,73]],"long-forms":[[108,115]]},{"text":"was supported in part by JSPS Investigate Fellowships for Young Scientists and in part by CREST, JST (Japan Science and Technological Agency). ","acronyms":[[94,97],[25,29],[87,92]],"long-forms":[[99,127]]},{"text":" The motive for that collaborate is twofold: on the one hand it builds on the strength of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of selections themost commonly used sense of a word, irrespective of the background in which","acronyms":[[143,146]],"long-forms":[[116,141]]},{"text":"Artola Xo (1993)o \"ilIZTSUA: lIiztegi-sistema urDrt.lc  a(lime~dunaren sorkuntza eta eraikuntza \/Conccption  d'un syst~,me intelligent d'aide dictionnarialc (SIAl))\"  Ph.D. Thesis.","acronyms":[[158,162]],"long-forms":[[114,156]]},{"text":" Throughout Proceedings of the 24th International Conference on Computational Linguistics (COLING). ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":" 2.2 Hierarchical Softmax Hierarchical Softmax (HSM) organizes the output vocabulary into a tree where the leaves are","acronyms":[[48,51]],"long-forms":[[26,46]]},{"text":"ing decision can be introduced directly base on attention weights from the two query components or further rescored by the maximum spanning tree (MST) search algorithm.","acronyms":[[141,144]],"long-forms":[[118,139]]},{"text":"2011) generated a corpus of posts from several online fora about breast cancer, which later was used to extraction potential adverse reactions from the most fluently use medicated to treat this disease: tamoxifen, anastrozole, letrozole and axemestane. The authors collected a lexicon of lay medical terms from websites and databases about drugs and adverse events. The dictionary was extended with the Consumer Hygiene Vocabulary (CHV)5, a vocabulary closer to the lay terms, which patient usually use to describe their medical experiences. Then, pairs of terms co-occurring within a window of 20 tokens were considered.","acronyms":[[422,425]],"long-forms":[[394,420]]},{"text":"SVO = Subject-Verb-Object GE = General Event PE = Predefined Happenings Rule-based","acronyms":[[45,47],[0,3],[26,28]],"long-forms":[[50,66],[6,25],[31,44]]},{"text":"explanations of these metrics are described below.  Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition ","acronyms":[[83,88],[116,122]],"long-forms":[[65,81],[92,114]]},{"text":"framework. This algorithm uses grammars in the Chomsky Normal Form (CNF) so we employed the open source Natural Language Toolkit2","acronyms":[[68,71]],"long-forms":[[47,66]]},{"text":"for the task: a training dataset (TrainSet) with 9675 messages directly retrieved from Twitter; a development dataset (DevSet), with 1654 messages; the testing dataset from 2013 run, which","acronyms":[[119,125],[34,42]],"long-forms":[[98,117],[16,32]]},{"text":" 310 X. Fan et al  Causality Na?ve Bayesian Classifier (CNB): Onto a document constituted by a binary-valued vector d=(X1 ,X2 , ?,","acronyms":[[56,59]],"long-forms":[[19,43]]},{"text":"  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shares Chore, pages 120?125, Boulder, Colorado, June 2009.","acronyms":[[87,92]],"long-forms":[[46,85]]},{"text":"trol treaties principle.  Consider first the foot feature principle (FFP). ","acronyms":[[70,73]],"long-forms":[[46,68]]},{"text":"In Proceedings of the Fifth Mediterranean Morphology Meetings (MMM5), pages 269?290, Fr?jus.","acronyms":[[62,66]],"long-forms":[[42,60]]},{"text":"This data may be presented in various forms, e.g. as  dictionaries, transition networks for lexical analysis,  augmented transition networks (ATN) for syntactic analysis,  semantic networks,  re la t ions ,  end so on.","acronyms":[[142,145]],"long-forms":[[111,140]]},{"text":" 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4","acronyms":[[51,54],[88,91],[108,115]],"long-forms":[[26,49]]},{"text":"Branching quantification in DTS has partially been debated in [7] and [8], in which we compared DTS with First Order Logic (FOL). However, FOL is restrained in that it authorizes to","acronyms":[[126,129],[28,31],[98,101],[141,144]],"long-forms":[[107,124]]},{"text":" A dialogue act is defined as a pair consisting of a communicative function (CF) and a semantic content (SC): a =< CF,SC >.","acronyms":[[77,79],[105,107],[115,117],[118,120]],"long-forms":[[53,75],[87,103]]},{"text":"forward and backward application (FA and BA), implies a  standard notion of constituency, rules like type raising (TR)  and functional composition (FC) give rise to a more  generous notion of constituency (this is what makes 'non- ","acronyms":[[148,150]],"long-forms":[[124,146]]},{"text":"and BLANC (Recasens and Hovy, 2011). We uses the average scores (AVG) of these four metrics as the main comparative metric.10","acronyms":[[64,67],[4,9]],"long-forms":[[48,55]]},{"text":"matical relations on the arcs).  Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corre-","acronyms":[[55,57]],"long-forms":[[33,53]]},{"text":"   then  Cast_in_Chain(Antecedent, Anaphor)    then  Cast_in_Chain(Antecedent, Anaphor) RULE-1-Filter-1-Pronoun (R1F1Pron) RULE-1-Filter-1-Nominal (R1F1Nom)","acronyms":[[113,121],[148,155]],"long-forms":[[88,111],[123,146]]},{"text":"course structure. In Proceedings of the International Conference in Computational Linguistics (COLING), pages 43?49. ","acronyms":[[95,101]],"long-forms":[[68,93]]},{"text":"types through various features based on e.grammes., partof-speech tagging (POS) and named entity recognition (NER). ","acronyms":[[104,107],[69,72]],"long-forms":[[78,102],[46,59]]},{"text":"From raw text extract the temporal agency (events and timexes), identifying the pairs of temporal entities that have a temporal link (TLINK) and classify the temporal relation between them.","acronyms":[[133,138]],"long-forms":[[118,131]]},{"text":" Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks.","acronyms":[[84,86]],"long-forms":[[70,82]]},{"text":"Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets).","acronyms":[[271,275],[293,297],[315,319]],"long-forms":[[277,290],[299,312],[321,350]]},{"text":"entire web corpus. We use the API provided by the Measurement of Semantic Relatedness (MSR)4 engine for this aim.","acronyms":[[84,87],[30,33]],"long-forms":[[50,82]]},{"text":"tence is assigned a Sense_Weight_Score (SWS)  for each emotion labelled which is calculated by partition the total Sense_Tag_Weight (STW)of all  occurrences of an emotion tag in the condemnation by ","acronyms":[[127,130],[40,43]],"long-forms":[[109,125],[20,38]]},{"text":"pre-rendered animations.  The Natural Language Understanding (NLU) module needs to cope with both chat and solider","acronyms":[[62,65]],"long-forms":[[30,60]]},{"text":" ? LEAD (lead-based): n% sentences are chosen from the beginning of the text.","acronyms":[[3,7]],"long-forms":[[9,13]]},{"text":" 3.2 Convolutional Neural Network modeling The Convolutional Neural Network (CNN) model utilised raw audio as input is shown in Figure 1.","acronyms":[[74,77]],"long-forms":[[44,72]]},{"text":"tract syntactic idiosyncrasies for FB-LTAG.  We utilize with Sejong Treebank (SJTree) which  contains 32 054 eojeols (the unity of segmenta-","acronyms":[[68,74]],"long-forms":[[51,66]]},{"text":"{afader,soderlan,etzioni}@cs.washington.edu Abstract Open Information Extraction (IE) is the task of extracting assertions from massive corpora","acronyms":[[82,84]],"long-forms":[[58,80]]},{"text":" Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an","acronyms":[[71,74],[107,110],[140,143]],"long-forms":[[76,98],[112,131],[145,157]]},{"text":"partially completed subproof or function of the system. The implementation f this  was the IPSIM (Interruptible Prolog SIMulator) theorem prover, which can maintain  a set of partially completed proofs and jump to the appropriate one as dialog pro- ","acronyms":[[91,96]],"long-forms":[[98,128]]},{"text":"More recently, (Areces et al, 2008) explored GRE as a problem in Description Logic (DL), a formalism which, like Conceptual Graphs, is specially designed for","acronyms":[[84,86],[45,48]],"long-forms":[[65,82]]},{"text":"represents the editing cycles. Awarded a  Semantic Network (SN) in a knowledge base (KB),  the system generates a description of the SN in the ","acronyms":[[57,59],[82,84],[130,132]],"long-forms":[[39,55],[66,80]]},{"text":"for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF\/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data","acronyms":[[87,94],[4,7],[78,85],[102,109]],"long-forms":[]},{"text":"alignment. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 215?222.","acronyms":[[103,106]],"long-forms":[[60,101]]},{"text":"AVERAGE 70.8% 70.1% Table 2: Ten-fold cross-validation classification result, using a Na??ve Bayes (NB) or Helping Vector Machines (SVM) classifier","acronyms":[[101,103],[133,136]],"long-forms":[[87,99],[108,131]]},{"text":"ed AIC   a) Dialogue ActsOnly (DAONLY)    N (number of hides states) ","acronyms":[[31,37],[3,6]],"long-forms":[[12,29]]},{"text":"ing to its current beliefs concerning the state of the dialogues. Boosting Learning (RL) has been more and more used for the optimisa-","acronyms":[[89,91]],"long-forms":[[65,87]]},{"text":"guage varieties: the language of aborigines speakers (N), the language of advanced, enormously fluent nonnative speakers (NN), and translationese (T). We","acronyms":[[114,116],[50,52],[139,141]],"long-forms":[[97,103],[33,39],[123,137]]},{"text":"Table 1). CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB =","acronyms":[[41,43],[79,81],[10,12],[63,65],[91,93],[111,114],[130,134],[189,191],[198,200],[176,178],[157,160]],"long-forms":[[46,54],[84,89],[15,39],[68,77],[105,109],[117,128],[137,155],[163,174],[181,187]]},{"text":"We applying LDA on  the user-word matrix UW:  UW = HMM * MW  , where UM is the user-hidden matrix, MW is the ","acronyms":[[48,50],[9,12],[38,40],[43,45],[53,55],[65,67],[95,97]],"long-forms":[[51,52],[21,37]]},{"text":" We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al, 2005), for updating the weights.","acronyms":[[43,47]],"long-forms":[[49,69]]},{"text":"Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al, 1998).","acronyms":[[94,97]],"long-forms":[[66,92]]},{"text":" The attribute of a node is one of part of speech (POS), lexical value (LEX), or dependency label (DEP), as for instance LEX(QUEUE0)","acronyms":[[72,75],[51,54],[99,102],[121,124],[125,131]],"long-forms":[[57,64],[35,49],[81,91]]},{"text":"We evaluate the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000)","acronyms":[[74,76]],"long-forms":[[51,62]]},{"text":"1 Introduction Recent research exhibitions that it is possible, using current natural parlance processing (NLP) and machine learning technology, to automatically induce lex-","acronyms":[[101,104]],"long-forms":[[72,99]]},{"text":"on terrorism in a matter of weeks. Since the terrorism  domain knowledge bases (KB's) were developed for English  for MUC-4 already, and since the KB's can be shared across ","acronyms":[[80,84],[118,123],[147,151]],"long-forms":[[63,78]]},{"text":"middle value of 6.5, was tested.  The Lexile-like measure (LX) used the same two traits as the Lexile measure: mean logging frequency","acronyms":[[59,61]],"long-forms":[[38,44]]},{"text":"We review the hierarchical Dirichlet document (HDD) model in section 2, and present our proposed hierarchical Dirichlet tree (HDT) document model in section 3.","acronyms":[[126,129],[47,50]],"long-forms":[[97,124],[14,45]]},{"text":"1 Introduction We extend a popular model, latent Dirichlet al location (LDA), to unbounded streams of documents.","acronyms":[[72,75]],"long-forms":[[42,70]]},{"text":"as idiosyncrasies vectors. The model of our choice is the maximum entropy model (MaxEnt), also known as logistic regression (?).","acronyms":[[74,80]],"long-forms":[[51,66]]},{"text":"AVERAGE 3.31 316 72.58% 78.02% 84.65% Table 2: Word sense disambiguation results, including two baselines (MFS = most frequent sense; LeskC = Lesk-corpus) and the word sense disam-","acronyms":[[107,110],[134,139]],"long-forms":[[113,132],[142,153]]},{"text":" REF = obj123 SIZE = sizesensorreading85 FORMA = shapesensorreading62","acronyms":[[14,18],[1,4],[41,46]],"long-forms":[[21,40],[49,69]]},{"text":" 2. Character Error Rate (CER): Edit distance in terms of characters between the target sentence","acronyms":[[26,29]],"long-forms":[[4,24]]},{"text":" UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = England National Corpus (Aston and Burnard, 1998).","acronyms":[[44,46],[58,60],[1,3],[13,15],[28,30]],"long-forms":[[49,56],[63,79],[6,11],[18,26],[33,42]]},{"text":"? P2E1N3S2, C S D G ASD Simplified Technical English (ASD-STE) (ASD 2013), often abbreviated to Simplified Technical English (STE) or just Simplified English, is a CNL for the aerospace","acronyms":[[54,61],[0,10],[126,129],[164,167],[64,67]],"long-forms":[[20,52],[96,124]]},{"text":"We present experiments using counts of tre genre of ngrams: lemma ngrams (LN), POS ngrams (PN) and blended ngrams (MN).2 Mixed ngram is a restricted formulation of lemma ngram where open-","acronyms":[[115,117],[76,78],[93,95]],"long-forms":[[101,113],[62,74],[81,91]]},{"text":"In my semantic role tagging research, I used the Tilburg Memory Learner (TiMBL) for this purpose.","acronyms":[[74,79]],"long-forms":[[50,72]]},{"text":"\u0000 EFF (effect): We made her the secretary.  \u0000 ORIG (origin): She made a cake from apples. ","acronyms":[[46,50],[2,5]],"long-forms":[[52,58],[7,13]]},{"text":" In Proceedings of the 14th International Conference on World Wide Web (WWW), pages 342?351, Chiba.","acronyms":[[72,75]],"long-forms":[[56,70]]},{"text":"The outlined research is begun in the course of the development of human computer interfaces for natural interaction in Virtual Reality (VR). Conducting empirical inves-","acronyms":[[143,145]],"long-forms":[[126,141]]},{"text":"al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organi-","acronyms":[[42,45],[11,15],[82,85]],"long-forms":[[36,40],[74,80]]},{"text":" One example of this should be in supply support for the curation of the Genes Expression Database (GXD).4 This support could come in the form of a named entity recog-","acronyms":[[101,104]],"long-forms":[[75,99]]},{"text":"The IE results are stored in a database which  is the basis for IE-related applications like QA,  BR (Browsing, threading and visualization) and  AS (Automatic Summarization).","acronyms":[[98,100],[4,6],[64,66],[93,95],[146,148]],"long-forms":[[102,110],[150,173]]},{"text":"sociations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 cat-","acronyms":[[148,150]],"long-forms":[[130,146]]},{"text":" The implemented system has three main modules: the Feature Extractor (FE), the Generalized Iterative Scale (GIS), and the Classifica-","acronyms":[[71,73],[111,114]],"long-forms":[[52,69],[80,109]]},{"text":"To address this problem, Xiong et al (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual","acronyms":[[84,90],[56,59]],"long-forms":[[67,82]]},{"text":"Initials experimental data indicates that the approach is maybe to be successful. The significant benefit of the approach is that it makes extending pronunciation lexi-cons accessible to average users. 1 Intro Speech recognition (SR) systems utilize pronuncia-tion lexicons to map words into the phoneme-like units used for acoustic modeling. Text-to-speech (TTS) systems also make used of pronunciation lexicons, both internally and as ?","acronyms":[[235,237],[360,363]],"long-forms":[[215,233],[344,358]]},{"text":"and the speech. The SU detection task is appraisals on both the reference human transcriptions (REF) and rhetoric recognition outputs (STT).","acronyms":[[95,98],[20,22],[132,135]],"long-forms":[[63,72],[104,130]]},{"text":"The dataset utilised in the CIPS-ParsEval-2010 evaluation is transformations from the Tsinghua Chinese Treebank (TCT). There are two subtasks: (1)","acronyms":[[103,106],[24,42]],"long-forms":[[93,101]]},{"text":"them as \"LIG-equivalent formalisms\". LIG is a vari-  ant of index grammar (IG) (Aho, 1968). Like CFG, IG ","acronyms":[[75,77],[37,40],[9,12],[97,100],[102,104]],"long-forms":[[60,73]]},{"text":"Another dictionary device deals wlth  unrecognized elements - the so-called  transducing dictionary (TD). TD re- ","acronyms":[[101,103]],"long-forms":[[77,99]]},{"text":"Our objective is to study the effect of using bigram features against co?occurrences in first (PB) and second (SC) order context vectors while using relatively small amounts of training data per word.","acronyms":[[111,113],[95,97]],"long-forms":[[103,109]]},{"text":"Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space mannequins (VSMs) based on the brainchild of distributional similarity (Turney","acronyms":[[83,87]],"long-forms":[[62,81]]},{"text":" To adapt for Chinese phonetic rule, we divide the  continuous CLs into independent CLs(IC) and  divide structure of CL+VL+CL into CL+VL and ","acronyms":[[88,90],[63,66],[117,119],[120,122],[123,125],[131,133],[134,136]],"long-forms":[[72,86]]},{"text":" Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.","acronyms":[[94,97]],"long-forms":[[70,92]]},{"text":" 2001. Linguistic Inquiry and Word Counting (LIWC):  LIWC2001.","acronyms":[[42,46]],"long-forms":[[7,40]]},{"text":"To classify the NPs according to their type in biomedical terms, we have adopted the Sequence Ontology (SO)2 (Eilbeck and Lewis, 2004).","acronyms":[[104,106],[16,19]],"long-forms":[[85,102]]},{"text":"assumption of which is the stratificational  approach to sentence analysis pursued by  Functional Sentence Perspective (FSP), a  linguistic theory developed by Jan Firbas in the ","acronyms":[[120,123]],"long-forms":[[87,118]]},{"text":"In Modern Standard Arabic (MSA), all nouns and adjectives have one of three cases: nominative (NOM), accusative (ACC), or genitive (GEN). What","acronyms":[[113,116],[132,135],[27,30],[95,98]],"long-forms":[[101,111],[122,130],[3,25],[83,93]]},{"text":" One of the most competitive summarization methods is bases on Integer Linear Programming (ILP). ","acronyms":[[91,94]],"long-forms":[[63,89]]},{"text":"Abstract  This paper presents a new bootstrapping  approach to named entity (NE)  classification.","acronyms":[[77,79]],"long-forms":[[63,75]]},{"text":"In sum, the text-to-text similarity measure combined with our sentence-level sen-timent analysis algorithm helps us identify the representative rationales of diverse opinions in an online deliberation. An overview of our meth-od is shown in Figure 1.  We applied our method in analyzing Wikipe-dia Article for Deletion (AfD) deliberation con-tent. Next we discuss how this method is used to analyze the content.","acronyms":[[320,323]],"long-forms":[[298,318]]},{"text":"PROBES QUESTION (QP) Do you think that looks rightness? 4.99% 4.76% 0.731 QUESTION PROMPT (QQ) Any questions? 2.49% 2.24% 0.978","acronyms":[[18,20],[90,92]],"long-forms":[[73,88],[0,16]]},{"text":"data as described in Niehues and Vogel (2008).  The phrase table (PT) is built using the Moses toolkit (Koehn et al.,","acronyms":[[66,68]],"long-forms":[[52,64]]},{"text":"Semantic Computing Group Cognitive Interaction Technology ? Center of Excellency (CITEC), Bielefeld University, Germany","acronyms":[[82,87]],"long-forms":[[60,80]]},{"text":"Hyderabad, India Abstract Named Entity Acknowledged(NER) is the task of identifying and classifying tokens in a","acronyms":[[51,54]],"long-forms":[[26,49]]},{"text":"ciently or accurately than alternative approaches.  Constraint Programming (CP) is a field of research that develops algorithms and tools for","acronyms":[[76,78]],"long-forms":[[52,74]]},{"text":"navigation3  SCAN was developed initially for the TREC-6  Spoken Document Retrieval (SDR) task, which em-  ploys the NIST\/DARPA HUB4 Broadcast News cor- ","acronyms":[[85,88],[117,127],[128,132]],"long-forms":[[58,83],[50,54]]},{"text":"      Input provenance sentence (ISS)    ","acronyms":[[29,32]],"long-forms":[[6,27]]},{"text":"which fixes its results after a given time ? and report the corresponding word error rate (WER). This","acronyms":[[91,94]],"long-forms":[[74,89]]},{"text":"duction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original","acronyms":[[87,90]],"long-forms":[[57,85]]},{"text":"  For all the reasons listed above, a dictionary of  Turkish Language Association (TLA) is utilised in  this study.","acronyms":[[83,86]],"long-forms":[[53,81]]},{"text":"CoTrain v. SVM(EN) 2.15E-18 1.1E-17 3.46E-13 CoTrain versus. SVM(ENCN) 2.08E-13 8.13E-12 1.05E-12 CoTrain versus. TSVM(CN) 1.2E-19 1.07E-06 0.0624 CoTrain versus. TSVM(EN) 1.41E-05 0.00311 2.17E-08","acronyms":[[112,114],[12,15],[16,18],[58,61],[62,66],[152,156],[157,159],[107,111]],"long-forms":[[95,102]]},{"text":"ALO ( in to )   RO (OST LOC)  PO (PRE)  ON (VO)) ","acronyms":[[34,37],[0,3],[16,18],[20,23],[24,27],[44,46]],"long-forms":[[30,32]]},{"text":"Symbols Descriptor Example  If. I Simple connection between (IMPEDANCE) GA TAF~I  I|.2 ","acronyms":[[60,69]],"long-forms":[[31,58]]},{"text":"1992. 100 million words of  English: the British National Corpus (BNC). ","acronyms":[[66,69]],"long-forms":[[41,64]]},{"text":"(CPs) increases the number of Intricate Predicates (CPs) entries along with compound verbs  (CompVs) and conjunct verbs (ConjVs). The ","acronyms":[[119,125]],"long-forms":[[103,117]]},{"text":"questions estimation in TREC1. For example questions 1The Text recoup Conferences (TREC) are evaluation workshops in which Information Retrieval tasks are annually","acronyms":[[85,89],[23,27]],"long-forms":[[57,83]]},{"text":"The IE results are stored in a database which  is the foundation for IE-related applications like QA,  BR (Search, threading and visualization) and  AS (Automatic Summarization).","acronyms":[[98,100],[4,6],[64,66],[93,95],[146,148]],"long-forms":[[102,110],[150,173]]},{"text":"As shown in Table 3, using the transformation of dependency trees, the Dep2Str modelled implemented in Moses (D2S) is comparable with the standard execution","acronyms":[[107,110]],"long-forms":[[71,78]]},{"text":"Head and Obj3 and the counts f(gf, fe) of occurrences of the grammatical functions together with the roles DEGREE (DEG), THEME (THM), DEPICTIVE (DEP) and LOCATION (LOC).","acronyms":[[128,131],[9,13],[115,118],[145,148],[164,167]],"long-forms":[[121,126],[107,113],[134,143],[154,162]]},{"text":"related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al 2006) and the CDER measure (Leusch, Ueffing, and Ney 2006) are based on the edit","acronyms":[[153,156],[8,12],[50,53],[58,61],[186,190]],"long-forms":[[130,151]]},{"text":" 2.3 Named Entity Recognition Named Entity Recognition (NER) is the task of finding all instances of explicitly named entities","acronyms":[[56,59]],"long-forms":[[30,54]]},{"text":"follows:  1. If the topic is an individual constant (IC), establish  restrictions (using the Restricts link), if any, on the ","acronyms":[[53,55]],"long-forms":[[32,51]]},{"text":"an algorithm that combines the reference choice rules for  reason and the reference choice ordinances for methodology, to pro-  duce preverbal messages (PMs) from PCAs. As such, the ","acronyms":[[144,147]],"long-forms":[[124,142]]},{"text":" This representative deals clitics as separate tokens and summary the orthographic rewrites they undergo when cliticized. See the handling of the l\/PREP+Al\/DET in word #6 in Tabled 5.  This representation is using by the LDC in the Penn Arabic Treebank (PATB) (Maamouri  et al., 2004) and tools such as MADAMIRA (Pasha et al.,","acronyms":[[255,259],[222,225],[149,155],[156,162],[304,312]],"long-forms":[[233,253]]},{"text":"Initiative (ECI) plans to distribute similar material in a variety of languages. Even  greater esources are expected from the Linguistic Data Consortium (LDC). And the ","acronyms":[[154,157],[12,15]],"long-forms":[[126,152]]},{"text":" 2.1 Named Entity Recognition We regard named entity recognition (NER) as a standalone task, independent of language identification.","acronyms":[[66,69]],"long-forms":[[40,64]]},{"text":"(approx. 68,000 sentences, 1.4 million tokens), (2) Brown Corpus (BROWN) (approx. 60,000","acronyms":[[66,71]],"long-forms":[[52,57]]},{"text":" Shirai, K., T. Tokunaga, and H. Tanaka: Automatic Extraction of Japanese Grammar f om  a Bracketed Corpus, in Natural Language Treating Pacifist Rim Symposium(NLPRS'gs),  pp.","acronyms":[[161,169],[173,175]],"long-forms":[[111,160]]},{"text":" ? Unaligned word penalty feature (UWP): hUWP (Q,S,A), which is defined as the ratio","acronyms":[[35,38],[41,45]],"long-forms":[[3,25]]},{"text":"exposed through the feature HOOKS to facilitate further composition. These properties including pointers to the local top handle (LTOP), the constituent?s primordial index (INDEX), and the external argument, if any (XARG).","acronyms":[[127,131],[167,172],[210,214],[28,32]],"long-forms":[[109,118],[160,165],[183,199]]},{"text":"CDD, which comprises a total of 173 gold annotated cues, we find that Classifier I mislabels 11 false positives (FPs) and seven false negatives (FNs). ","acronyms":[[113,116],[145,148],[0,3]],"long-forms":[[96,111],[128,143]]},{"text":"Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract  In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands.","acronyms":[[584,587],[636,638],[278,283],[708,711],[729,732]],"long-forms":[[567,582],[616,634]]},{"text":"SELF = talk to oneself  TQ = terse question  TI = terse information  INT = interrupted ","acronyms":[[48,50],[0,4],[27,29],[72,75]],"long-forms":[[53,70],[7,25],[32,46],[78,89]]},{"text":" Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly optimum result.","acronyms":[[66,69]],"long-forms":[[34,64]]},{"text":"Clark, 2008). During constituent-based parsing usage the Chinese Treebank (CTB), Wang et al (2006) have shown that a shift-reduce parser can give","acronyms":[[72,75]],"long-forms":[[54,70]]},{"text":"insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995).","acronyms":[[120,123],[73,76]],"long-forms":[[95,118],[48,70]]},{"text":" 1 Introduction Machine translation (MT) by statistical model of bilingual phrases is one of the most successful ap-","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"Concerned about this inflation of the grammar constant, (DeNero et al, 2009) consider a superset of CNF called Lexical Normal Form (LNF). A rule is","acronyms":[[132,135],[100,103]],"long-forms":[[111,130]]},{"text":"Table 5 shows the results when experimenting with multiple tree structures (see columns 2-5): (i) the basic trees (BT), (ii) the basic tree augmented with part-of-speech information (BTP), (iii) shallow syntactic trees (ShT), and (iv) syntactic tree (ST). We","acronyms":[[217,220],[113,115],[181,184],[248,250]],"long-forms":[[193,215],[101,111],[127,179],[232,246]]},{"text":"Given these restrictions the DAR troubles becomes to find that valuing da of DA that maximises P ( DA = da | f1 = v1, . . . , fn = vn,","acronyms":[[96,98],[29,32],[74,76]],"long-forms":[[101,103]]},{"text":"research relies is that the failures of current automatic metrics are not algorithmic: BLEU, Meteor, TER (Translation Edit Rate), and other metrics efficiently and correctly compute informative distance","acronyms":[[101,104],[87,91]],"long-forms":[[106,127]]},{"text":"slightly simplified version of Webber's original rule  schema, says that for any formula that meets the  structural description (SD), a discourse ntity identified  by the ID formula is to be constructed.","acronyms":[[129,131],[171,173]],"long-forms":[[105,127]]},{"text":" 1 Introduction Named entity recognition (NER) is the task of identifying and classifying phrases that denote certain types of named","acronyms":[[42,45]],"long-forms":[[16,40]]},{"text":" CTexT. 2005. CKarma (C5 KompositumAnaliseerder vir Robuuste Morfologiese Analise). [ C5 Compound","acronyms":[[14,20],[1,6]],"long-forms":[[22,47]]},{"text":" We follow Curran (2004) and utilizes two performance measures: direct matches (DIRECT) and inverse rank (INVR).","acronyms":[[75,81],[101,105]],"long-forms":[[59,73],[87,99]]},{"text":"Researchers have ex-plored the subjects of CLI in the zones of lexical style (Jarvis et al 2012a), lexical n-grams (Jarvis & Paquot, 2012), personage n-grams (Tsur & Rappo-prot, 2007), uses variables relating to uniformity, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), errors patterns (Bestgen, et al 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al 2012b; Koppel et al 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).  Such studies have demonstrated relatively strong success rates for ranking an L2 writing sample bases on the L1 of the writer. Per examples, Jarvis and Paquot (2012), employs 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al 2009) obtained a 53.6% classification accuracy for 12 panels of L1s. Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge picked from the computational tool Coh-","acronyms":[[813,817],[40,43],[590,592],[621,623]],"long-forms":[[772,811]]},{"text":"response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Sch?olkopf, 2004), with the complexity parame-","acronyms":[[89,92]],"long-forms":[[62,87]]},{"text":"category. The following conveyance knowledge involves  sets of three common ouns (CNs):  3A' is the transferred expression of A ","acronyms":[[80,83]],"long-forms":[[67,78]]},{"text":"4.1 Syntactic template selection PERSONAGE?s input generation dictionary is made of 27 Deep Syntactic Structures (DSyntS): 9 for the recommendation claim, 12 for the comparison","acronyms":[[114,120]],"long-forms":[[87,112]]},{"text":"ing to its current beliefs concerning the state of the dialogue. Reinforcement Learning (RL) has been more and more used for the optimisa-","acronyms":[[89,91]],"long-forms":[[65,87]]},{"text":"[NP : [XNOUNS : MERINO'S (NOUN) DWELLINGS (NOUN)] ] [NP : [XNOUNS : MERINO'S (NOUN)] ] [VP : [VERB_GROUP : HOME (VERB)] ] [XPPS : [PP : IN (PREPOSITION )","acronyms":[[102,106],[48,50],[83,85],[126,128],[7,13],[54,60],[118,122]],"long-forms":[[89,99],[1,3]]},{"text":"knowledge about the structuf.e of the world into account.  - The Data Base Language ( DBL ) , which contains conatants that correspond  to data base primitives . (","acronyms":[[86,89]],"long-forms":[[65,83]]},{"text":" We apply both helped vector machine (SVM)  and Maximum Entropy (ME) algorithms with the  assistance of the SVM-light4 and Mallet5 tools.","acronyms":[[66,68],[39,42],[103,106]],"long-forms":[[49,64],[15,37]]},{"text":"Abstract We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using","acronyms":[[81,84]],"long-forms":[[56,79]]},{"text":"there are traditionally three kinds of named entities (NEs) to be dealt with: names of persons (PER) , location (LOC) and organizations (ORG).","acronyms":[[108,111],[49,52],[90,93],[132,135]],"long-forms":[[97,106],[33,47],[81,87],[117,129]]},{"text":"This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity (STS). The task is","acronyms":[[117,120],[24,30],[64,67]],"long-forms":[[88,115]]},{"text":"to prevent this class of mistakes. To put it another camino, we hoped to exploited the correlation between named-entities and noun phrase (NP) boundaries. A","acronyms":[[134,136]],"long-forms":[[121,132]]},{"text":"the Extraction of Potentials Avis Phrases. Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object.","acronyms":[[89,91],[55,57]],"long-forms":[[92,103],[58,75],[79,87],[107,114],[118,127],[131,137]]},{"text":"word if it exceeds a certain threshold.  4.2 LDA Graph Method (LDA-GM) The LDA-GM algorithm creates a similarity graph","acronyms":[[63,69],[75,81]],"long-forms":[[45,61]]},{"text":"ring Words Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz and Eric Kow; the two GREC Challenges, GREC Leading Topic Reference Generation (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organizing by Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in","acronyms":[[202,210],[33,41],[101,105],[118,122],[158,166]],"long-forms":[[172,200]]},{"text":"ing the following measures:   1. PrecisionCorrectTransliterations(PTrans)  2.","acronyms":[[66,72]],"long-forms":[[33,64]]},{"text":" We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al, 2013) and WordNet (Fell-","acronyms":[[79,83]],"long-forms":[[58,77]]},{"text":"<AbstractText Label=?RESULTS? NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in the CKC group (14\/36, 38.88%) than in control group (14\/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985); and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than","acronyms":[[188,190],[106,109],[268,270]],"long-forms":[[176,186]]},{"text":" enard Centers de Recherche Informatique de Montr?eal (CRIM) Montr?eal, QC, Canada","acronyms":[[54,58],[71,73]],"long-forms":[[7,52]]},{"text":"them, and finally generating and demonstrating them.  The Input Analyzer (IA) of the system is the  most stable end experimented components and it is ","acronyms":[[71,73]],"long-forms":[[55,69]]},{"text":"In Proc. IEEE Automatic Speech Recognised and Understanding (ASRU), Merano, Italy, December. ","acronyms":[[62,66],[9,13]],"long-forms":[[14,60]]},{"text":"tailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). ","acronyms":[[146,152],[123,128]],"long-forms":[[129,144]]},{"text":"Universities of Brighton There is croissant interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been using in NLG.","acronyms":[[154,157],[284,287]],"long-forms":[[125,152]]},{"text":"sHDP 0.162 0.046 0.442 0.102 Table 2: Averages topic coherence for different baselines (HDP, Gaussian LDA (G-LDA)) and sHDP. ","acronyms":[[104,109],[116,120],[85,88],[0,4]],"long-forms":[[90,102]]},{"text":"maries that are too particular. In this paper, we propose a natural language generation (NLG) model for the automatic inception of indicative multidoc-","acronyms":[[87,90]],"long-forms":[[58,85]]},{"text":"Note that for the purposes of Figure 1: Example Babytalk Input Data: Sensors HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 = blood CO2 level; SaO2 = oxygen saturation; T1 = chest","acronyms":[[77,79],[94,99],[118,124],[144,148],[170,172]],"long-forms":[[82,92],[102,110],[127,136],[151,168]]},{"text":"on the gender of the user.  User ID: The user ID (UID) labels are inspired by research on Arabic Twitter showing that a consid-","acronyms":[[50,53]],"long-forms":[[41,48]]},{"text":"Our work is most similar to the content selection method of the multimedia conversation system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004).","acronyms":[[95,98]],"long-forms":[[100,132]]},{"text":"6 BC = Broadcast Dialogues; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs.","acronyms":[[53,56],[92,94],[2,4],[32,34],[107,109],[135,137]],"long-forms":[[59,90],[97,105],[7,30],[37,51],[112,129],[140,147]]},{"text":"five tasks) representing fine-grained bio-IE.  2.1 Genia task (GE) The GE task (Kim et al, 2011) preserves the task","acronyms":[[63,65],[71,73],[38,45]],"long-forms":[[51,56]]},{"text":"No 0 12 23 292 Table 4: Confusion matrix (SVM) for argument component classification (MC = Major Claim; Cl = Claim; Pr = Premise; No = None)","acronyms":[[86,88],[42,46],[104,106],[116,118],[130,132]],"long-forms":[[91,102],[109,114],[121,128],[135,139]]},{"text":"They found that using a combination of all the features in a Support Vector Machine (SVM), they can obtain an accuracy of 80% in the classification of 5 differ-","acronyms":[[85,88]],"long-forms":[[61,83]]},{"text":"(ADV), cause (CAU), direction (DIR), extent (EXT), location (LOC), manner (MNR), and time (TMP), modal verbs (MOD), negative markers (NEG), and discourse connectives (DIS). ","acronyms":[[134,137],[1,4],[14,17],[31,34],[45,48],[61,64],[75,78],[91,94],[110,113],[167,170]],"long-forms":[[116,124],[7,12],[20,29],[37,43],[51,59],[67,73],[85,89],[97,102],[144,153]]},{"text":"(3) range, e.g., (age(value(0 100))).  The notion of IMPORTANCE VALUES (IVs) are  introduced here and are utilise to numerically expound ","acronyms":[[72,75]],"long-forms":[[53,70]]},{"text":" 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding jeopardize between the intuitive expressivity","acronyms":[[53,57]],"long-forms":[[28,51]]},{"text":"i' SRI - text extraction  ~\" TRW - document detection output  i' University of Massachusetts (UMass) -  document detection ","acronyms":[[94,99],[29,32],[3,6]],"long-forms":[[65,92]]},{"text":" 9 Here, we present the generation-oriented PG Workbench (PGW), which assists grammar developers, among other things, in testing whether the implemented syntactic and lexical knowledge allows all and only well-formed permutations. In Section 2, we describe PG?s topology-based linearizer implemented in the PGW gen-erator, whose software design is sketched in Section 3.","acronyms":[[58,61],[307,310],[257,261]],"long-forms":[[44,56]]},{"text":" FRANZISKA constraints have three kinds of literals, two kinds of elementary predications (EPs) in the outset two lines and handle constraints in the third line:","acronyms":[[85,88],[1,4]],"long-forms":[[60,83]]},{"text":"lem, and our system adopted a supervised learning approach with Maximum Entropy classifier, which is amply used in natural language treatment(NLP). ","acronyms":[[144,147]],"long-forms":[[116,142]]},{"text":"representation and transformation are necessary.  (2)  Latent Semantic Indexing (LSI) without  transformation (LSI-Com): we first merge the ","acronyms":[[81,84],[111,118]],"long-forms":[[55,79]]},{"text":"collected as follows: Positive Diccn: 3,730 Chinese positive terms (e.g., \/good-looking, \/ lucky) were collected from the Chinese Vocabulary for Sentiment Analysis (VSA)20 released by HOWNET.","acronyms":[[165,168],[184,190]],"long-forms":[[130,163]]},{"text":"n - c-dow British English American Francais Tables 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical type tagset (LTT).","acronyms":[[115,117],[157,160]],"long-forms":[[97,113],[136,155]]},{"text":"1.1 A System Exhibiting Reinforcement Taught The central motivation for building this dialogue system is as a platform for Boosting Learning (RL) experiments.","acronyms":[[149,151]],"long-forms":[[125,147]]},{"text":"provement against Naive Bayes was reported. Our suggested model, tag guided RNN (TG-RNN), is designed to use the syntactic tagging of the parent","acronyms":[[80,86]],"long-forms":[[64,78]]},{"text":"84  Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 144?151, Ann Arbor, July 2005.","acronyms":[[82,87]],"long-forms":[[41,80]]},{"text":" 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a comparatively","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":"Figure 1: Sample transcribed from a TD child 3 Narrative Topic Analysed Using LDA Latent Dirichlet Allocation (LDA) (Blei et al 2003) has been used in NLP to modelled topics within","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"Advanced Researches and Development Activity (ARDA)?s Advanced Question Answering for Intelligence (AQUAINT) Programs, a DOI grant under the Reflex","acronyms":[[98,105],[44,48],[118,121]],"long-forms":[[0,42],[52,96]]},{"text":"tences in the other part of the corpus. Consequently, we realized a language identification (LID)based filtering afterwards (performed only on the","acronyms":[[91,94]],"long-forms":[[66,89]]},{"text":"CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08 CoTrain vs. TSVM(ENCN) 1.37E-08 0.0113 0.0396 CoTrain vs. SelfTrain(CN) 7.07E-18 2.79E-11 6.53E-07 CoTrain vs. SelfTrain(EN) 1.01E-07 0.0192 1.35E-07","acronyms":[[115,117],[12,16],[17,19],[59,63],[64,68],[168,170]],"long-forms":[[93,100]]},{"text":"ferent levels. For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word","acronyms":[[85,88],[111,115],[134,138]],"long-forms":[[62,83],[91,109],[118,132]]},{"text":"Section 7 concludes this article.  2 Automatic Speech Recognition (ASR)  Tae ASR research focused on two major topics.","acronyms":[[67,70],[78,81]],"long-forms":[[37,65]]},{"text":"sisted of 2000 features, representing the most frequent (head, subject) and (head, object) pairs in the British National Corpus (BNC). The feature-","acronyms":[[129,132]],"long-forms":[[104,127]]},{"text":"In  Proceedings of the 16th International Conference on  Computational Linguistics (COLING-96), Copenhagen,  Denmark, pp 459-465.","acronyms":[[84,93]],"long-forms":[[57,82]]},{"text":"2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of","acronyms":[[75,77]],"long-forms":[[61,73]]},{"text":"For instance, a problem-tagged entity is represented as a first word tagged B-P (begin problem) and other 59","acronyms":[[76,79]],"long-forms":[[81,94]]},{"text":"fer to semantically similar words. We have applied the Markov Cluster algorithm (MCL) (van Dongen, 2000) to clusters semantically akin terms together.","acronyms":[[81,84]],"long-forms":[[55,79]]},{"text":"On Complexity of Word Decree. Traitement Automatique des Langues (TAL), 41(1):273?300. ","acronyms":[[65,68]],"long-forms":[[29,62]]},{"text":"The generator  utilize as its linguistic resource a lexicon encoded in a version  of Categorial Grammar (CG), the extension of which with  rules of function composition gives rose to a issues of ","acronyms":[[102,104]],"long-forms":[[82,100]]},{"text":"It is based on bilingual phrases, where a bilingual expression (PB ) is simply two monolingual phrases (MP ) in which each one is supposed to be the translation of each","acronyms":[[100,102],[60,62]],"long-forms":[[79,98],[42,58]]},{"text":"words. During Proceedings of the International Conference on Computational Linguistics (COLING). ","acronyms":[[84,90]],"long-forms":[[57,82]]},{"text":"| lexica: The relative frequency of categories, based on the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al, 2007).","acronyms":[[96,100]],"long-forms":[[61,94]]},{"text":"frequency pair, Erk et al(2010) drew (R, t)  pairs from each of fifth frequency bands in the  entire British National Corpus (BNC):  50-100  occurrences; 101-200; 201-500; 500-1000; and ","acronyms":[[126,129]],"long-forms":[[101,124]]},{"text":"Non-MonoClausal Verbs (NMCV), Passives  (Pass) and Auxiliary Construction (KT) that  are identified as compound verbs (CompVs). ","acronyms":[[119,125],[23,27],[41,45],[75,77]],"long-forms":[[103,117],[0,21],[30,38],[51,73]]},{"text":"categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition, CONJ = conjunction, DET = determiner, PRON = 1http:\/\/www.wiktionary.org\/","acronyms":[[94,98],[114,117],[31,34],[48,51],[62,65],[76,79],[132,136]],"long-forms":[[101,112],[120,130],[37,46],[54,60],[68,74],[82,92]]},{"text":" In this paper, we introduce a novel method called Haphazard Manhattan Indexing (RMI). RMI","acronyms":[[78,81],[84,87]],"long-forms":[[51,76]]},{"text":"AJCL = AmericanJournal of Computational  Linguistics (1974-present)  SNLP = Studies in Natural Language Processin~  (Cambridge University Press Monograph ","acronyms":[[69,73],[0,4]],"long-forms":[[76,114],[7,52]]},{"text":"5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs)","acronyms":[[95,99],[162,165]],"long-forms":[[70,93],[142,160]]},{"text":" ? Term Base eXchange (TBX): XML Terminology Exchange Standard.","acronyms":[[23,26]],"long-forms":[[3,21]]},{"text":"other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis","acronyms":[[124,127],[68,71]],"long-forms":[[96,122],[42,66]]},{"text":"the specified length limit. This idea is realized using the integer linear programming-based (ILP) optimization framework, with objective function set to","acronyms":[[94,97]],"long-forms":[[60,86]]},{"text":"monolingual applications and have been used in commercial grammar checkers.1 These parsers produce a logical form (LF) representation that is compatibility throughout multiple languages (see","acronyms":[[115,117]],"long-forms":[[101,113]]},{"text":"In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October. ","acronyms":[[117,120],[134,136]],"long-forms":[[104,115]]},{"text":"paring average precision values obtained 1) against the original, manual chronologies (APC), and 2) against the expert assessment (APE). These values","acronyms":[[131,134],[87,90]],"long-forms":[[100,129]]},{"text":"three stages. A source-language input string is rewritten to form an information retrieval (IR) query.","acronyms":[[92,94]],"long-forms":[[69,90]]},{"text":"Abstract At this paper we present results from the METER (MEasuring TExt Reuse) project whose aims is to explore issues","acronyms":[[51,56]],"long-forms":[[58,78]]},{"text":"  (a)  Support vector machine (SVM) (Vapnik,  1998) is a surveillance learning algorithm proposed ","acronyms":[[31,34]],"long-forms":[[7,29]]},{"text":"Linguistic expressions can vanish and appear in translation. For example, the preposition (PP) in the source rule does not show up in any of   Machine Translation Based on Constraint-Based Synchronous Grammar 615 ","acronyms":[[91,93]],"long-forms":[[78,89]]},{"text":"S i 3.3.3 Method 3: TrueSkill (TS) TrueSkill is an adaptive, online system that em-","acronyms":[[31,33]],"long-forms":[[20,29]]},{"text":"with the overall metric of error per response fill  (ERR), overgeneration (OVG) does not correlate  with it, and substitution (SUB) correlates with it  only to a limited extent.","acronyms":[[127,130],[53,56],[75,78]],"long-forms":[[113,125],[59,73]]},{"text":"3.1 The Information Extraction Pipeline  The extraction task we are addressing is that of the  Automatic Content Extraction (ACE)1 evaluations. ","acronyms":[[125,128]],"long-forms":[[95,123]]},{"text":"crimination information (MDI) assess criterion for speech recognise and notes an improvement in terms of perplexity and word error rate (WER). ","acronyms":[[143,146],[25,29]],"long-forms":[[126,141]]},{"text":" 4 . i .1  Bagging (BAGGIES)  From a training set of n examples, severaI sam- ","acronyms":[[20,23]],"long-forms":[[11,18]]},{"text":"sets for Chinese and English. For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted 2For replicability, a complete description of all features can","acronyms":[[82,86]],"long-forms":[[60,80]]},{"text":"it is the in f in i t ive  form of a verb. If SO, it is to be attached to the  parsing tree, and given the additionql feature MVB (main verb). The current ","acronyms":[[126,129],[46,48]],"long-forms":[[131,140]]},{"text":"An entity has  three types of mention: NAM (proper name), NOM  (nominal) or PRO (pronoun). A relation was ","acronyms":[[76,79],[39,42],[58,61]],"long-forms":[[81,88],[51,55],[64,71]]},{"text":" c?2015 Association for Computational Language Building a Scientific Concept Hierarchy Database (SCHBASE) Eytan Adar","acronyms":[[100,107]],"long-forms":[[61,98]]},{"text":"model. We tuned the parameters of these features with Minimal Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Estimation data set (919","acronyms":[[83,87],[108,112],[113,117]],"long-forms":[[54,81]]},{"text":"CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context There are four different kinds of rules that may be","acronyms":[[59,61],[78,80],[0,2],[3,5],[6,8],[9,11],[18,20],[44,46]],"long-forms":[[64,76],[83,96],[23,42],[49,57]]},{"text":"lined previously, in that its target is entity extraction from raw news wires from the news agency Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input","acronyms":[[121,124],[164,167]],"long-forms":[[99,119]]},{"text":"3http:\/\/www.noslang.com 370 are the New York Times (NYT),4 SMS,5 and Twitter.6 The results are presented in Figure 1.","acronyms":[[52,55]],"long-forms":[[36,50]]},{"text":"Upsala Academia  INTRODUCTION  A computerized conference (CC) is a form of co~znunica-  tion in which participants kinds into and read frc~ a ","acronyms":[[57,59]],"long-forms":[[32,55]]},{"text":"First, we investigate how laypeople intuitively recognize metaphor by conducting Amazon Mechanical Turk (MTurk) experiments.","acronyms":[[105,110]],"long-forms":[[88,103]]},{"text":"Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary UTurku VIBGhent ConcordU HCMUS","acronyms":[[187,189],[15,17],[36,39],[80,82],[124,130],[147,152],[222,226]],"long-forms":[[190,209],[18,34],[40,67],[83,99],[114,122],[131,137],[153,178],[227,237]]},{"text":"are apart kernel methods that directly take into account the multiclass nature of the troubles such as the kernel partial least squares regression (KPLS). ","acronyms":[[146,150]],"long-forms":[[105,144]]},{"text":"06 33.3 125 33.1 73 Medium: 35.4 216 36.1 125 Table 3: Oriented dependency accuracies (DDA) and iteration counts for the 10 (of 23) forming\/test splits affected by","acronyms":[[88,91]],"long-forms":[[56,86]]},{"text":"distinct verbs, not occurrences. The seventh column showing the number of verbs that  were misclassified (MC)--the sum of erroneous positives and false negatives. The eighth ","acronyms":[[104,106]],"long-forms":[[89,102]]},{"text":"oracle? which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and","acronyms":[[72,75],[98,101]],"long-forms":[[51,70]]},{"text":"resent horizontal movement of the eyebrows.  2.2 Continuous Profile Models (CPM) Continuous Profile Model (CPM) aligns a set","acronyms":[[76,79],[107,110]],"long-forms":[[49,74],[81,105]]},{"text":"tent both in their living rooms and on their mobile devices. Digital video recorders (DVRs) allow people to record TV programs from hundreds of chan-","acronyms":[[86,90],[115,117]],"long-forms":[[61,84]]},{"text":"for candidate summary sentence selection  by standard page rank algorithms used in  Information Retrieval (IR). As Bengali is ","acronyms":[[107,109]],"long-forms":[[84,105]]},{"text":"(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP,  &MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). ","acronyms":[[235,240],[26,30],[49,54],[108,111],[69,78],[138,144],[158,164],[188,192],[212,216],[256,262]],"long-forms":[[243,252],[5,14],[20,24],[33,47],[57,67],[115,123],[81,107],[128,136],[147,156],[167,186],[195,210],[219,231],[265,292]]},{"text":"Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3. Student's Priming Ratio aggregated by task set (TS = Task Set)  Several significant relationships emerged within the models. We discuss a subset of these here.","acronyms":[[123,125],[46,48]],"long-forms":[[128,136],[51,59]]},{"text":"Both Russian and Czech have relatively free word order, so it may seem an odd choice to use a Markov model (MM) tagger. Why should second order","acronyms":[[108,110]],"long-forms":[[101,106]]},{"text":"tell verb base (VB), tell VB tell told verb past tense (VBD), tell VBD,VBN tell verb past participle (VBN), tell tells verb present 3rd person sing (VBZ), tell VBZ tell","acronyms":[[102,105],[16,18],[26,28],[56,59],[67,70],[71,74],[149,152],[160,163]],"long-forms":[[80,100],[5,14],[39,54],[119,142]]},{"text":"pointing and lacks systematic evaluation.  This paper employs a label propagation (LP)  algorithm for global learning of NP anaphoricity.","acronyms":[[83,85],[121,123]],"long-forms":[[64,81]]},{"text":">60 ICE ICE ICE ICE 10C 1~ 10C I0C  >.Y0 ICE ICE ICE ICE ICE IOC I(E 1~  >40 IO(J ICE ICE lO0 ICE ICE 1~ ICE  >35 ICE lO(l ICE ICE lie lOC IOC lOC ","acronyms":[[77,79]],"long-forms":[[82,93]]},{"text":"is still room for improvement.  2 The Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough,","acronyms":[[66,71]],"long-forms":[[38,64]]},{"text":"the boys) or part of a complex coordinating conjunction (both boys and girls), the Penn  Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter-  miner), RB (adverb), NNS (plural common noun) and coordinating conjunction (CC),  respectively.","acronyms":[[251,253],[159,162],[183,185],[196,199]],"long-forms":[[225,249],[164,173]]},{"text":"cognitive system (Wilkes, 1997). We refer to this  subset of knowledge shop (KS) in operation for and in  a given discourse as discourse paragon (DM) and hold ","acronyms":[[78,80],[145,147]],"long-forms":[[61,76],[128,143]]},{"text":" 2004. Document understanding conferences (DOK). ","acronyms":[[43,46]],"long-forms":[[7,41]]},{"text":"performs two essential functions in the case of our Japanese  processing:  1 CN = common noun; SN = sa-inflection noun (  nominalized .sneezing); VB = verb; VSUF = verb suffix; CM = ","acronyms":[[73,75],[138,140],[149,153],[169,171]],"long-forms":[[78,89],[143,147],[156,167]]},{"text":"For comparison, the graphs in Figures 1 and 2 also show the curves corresponding to the evaluation of Pointwise Mutual Information (PMI).8 The cooccurrence statistics of the expressions in Disco-En-","acronyms":[[132,135]],"long-forms":[[102,130]]},{"text":"constructed utilised only surface Alterf patterns; for the GLM and text versions, we can used either surface patterns, logical form (LF) patterns, or both. ","acronyms":[[129,131],[56,59]],"long-forms":[[115,127]]},{"text":" Each verb-noun pair was presented to turkers via Amazon Mechanical Turk (AMT) and they were asked to described (by text) the amendment of","acronyms":[[74,77]],"long-forms":[[50,72]]},{"text":"Free word associations are the words people spontaneously arrived up with in re-sponse to a inducement word. Such informa-tion has been collections from test persons and stored in databases.  A well known example is the Edinburgh Associative Thesaurus (CONSUMED). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en-abling the computer to producing similar associative reply as people do.","acronyms":[[246,249]],"long-forms":[[213,244]]},{"text":"2.2 Natural Language Processing  2.2.1 Woods' Augmented Transition Networks  Thc Augmented Transition Network (ATN) of Woods [Woods 501 is a  powerful formalisnl for representing grammars.","acronyms":[[111,114]],"long-forms":[[81,109]]},{"text":"Knowledge-free induction of morphology using latent semantic analysis. In Proceedings of the Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67?72, Lisbon, Portugal.","acronyms":[[139,149]],"long-forms":[[93,137]]},{"text":"P2E5N5S1, C W D I FAA Air Traffic Control Phraseology (FAA 2010) is a controlled language defined by the Federal Aviation Administration (FAA) and used for the communication in air traffic 135","acronyms":[[138,141],[0,8],[55,58],[18,21]],"long-forms":[[105,136]]},{"text":"adv Adverbial words(RB, RBR, RBS)  adj Adjunct word(JJ,JJR,JJS)  advP Adverb phrase(ADVP)  punct Rating(,) ","acronyms":[[84,88]],"long-forms":[[70,82]]},{"text":" We split annotated data into two parts: the BLOB (Binary Grandes OBject) and the XML annotations that refer to specific region of the BLOB.","acronyms":[[45,49],[80,83],[134,138]],"long-forms":[[51,70]]},{"text":"~eather(WEA) ingestion(ING)  utilise(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR) ","acronyms":[[74,77],[95,98],[8,11],[23,26],[33,36],[45,48],[55,58],[118,121]],"long-forms":[[61,72],[79,93],[1,7],[13,22],[29,32],[38,44],[50,54],[101,116]]},{"text":"z and a segmental grammar g, compute the sur-  face form y : g(z) of z.  The phonological recognition problem (PRP) is:  Given a (partially specified) surface form y, a dic- ","acronyms":[[111,114]],"long-forms":[[77,109]]},{"text":"8. Strong forms of pronouns not preceded by preposition (unless they carry IC) t Table 1: Annotation guidelines; CI = Intonation Center 4.2 Evaluation frame","acronyms":[[113,115],[75,77]],"long-forms":[[118,135]]},{"text":"~ ~ . ~  5  Object S t r i n grammes  (OBJL1,ST) Refercn'ce Guide. .............. 9 ","acronyms":[[33,41]],"long-forms":[[12,30]]},{"text":" A baseline system was also implemented using  the principle of most frequent sense (MFS),  where each word sense distribution was retrieved ","acronyms":[[85,88]],"long-forms":[[64,83]]},{"text":"The following are  typlcal relation names: NT (narrower term); PT (part); FUN (function);  SYN (syntax); EG (example). ","acronyms":[[91,94],[43,45],[47,60],[63,65],[74,77],[105,107]],"long-forms":[[96,102],[67,71],[79,87],[109,116]]},{"text":"3.2 Formalizing Paradigmatic Relations with Lexical Functions Lexical functions (LF) are a formal tool designed to describe all types of genuine lexical relations","acronyms":[[81,83]],"long-forms":[[62,79]]},{"text":"training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation.","acronyms":[[93,96],[53,56]],"long-forms":[[69,91],[35,51]]},{"text":"Abstract This paper suggests two ways of improving semantic role labeling (SRL). First, we intro-","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"1Note: LM(language model); ME(maximum entropy).  Brand Name(BRA), Product Type(TYP), Product Name(PRO), and BRA and TYP are often embed-","acronyms":[[60,63],[7,9],[27,29],[79,82],[98,101],[108,111],[116,119]],"long-forms":[[49,54],[10,24],[30,45],[66,78],[85,92]]},{"text":"P2E5N5S1, C W D I FAA Air Traffic Checking Phraseology (FAA 2010) is a controlled language defined by the Federal Airline Governance (FAA) and used for the communication in air traffic 135","acronyms":[[138,141],[0,8],[55,58],[18,21]],"long-forms":[[105,136]]},{"text":"Typically, the weights of the log-linear conjunction in Equation 3 are optimised by means of Minimum Mistake Rate Training (MERT) (Och, 2003).","acronyms":[[122,126]],"long-forms":[[93,120]]},{"text":"overlap relations. In Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[91,94]],"long-forms":[[48,88]]},{"text":"2007). The practical NLP application based evaluations are automatic speech recognition (ASR), information recapture (IR) and statistical machine","acronyms":[[89,92],[21,24],[118,120]],"long-forms":[[59,87],[95,116]]},{"text":"ing is how to identify a temporal relation between a pair of temporal entities such as events (EVENT) and time expressions (TIMEX) in a narrative. Af-","acronyms":[[124,129]],"long-forms":[[106,122]]},{"text":"  We have also made a preliminary attempt to transfer a thesaurus inlet from the Collins Thesaurus (CT) into  Italy by means of the English-Italian and Italian- ","acronyms":[[100,102]],"long-forms":[[81,98]]},{"text":"work first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) (Katagiri et al, 1991).","acronyms":[[128,131]],"long-forms":[[93,126]]},{"text":"are assigned the correct chief and dependency type ? and unlabeled attachment score (UAS) ? the per-","acronyms":[[84,87]],"long-forms":[[56,82]]},{"text":"linear chaining, CRFs make a first-order Markov independence assumption, and thus can be understanding as conditionally-trained finite state machines(FSMs). ","acronyms":[[144,148],[14,18]],"long-forms":[[122,142]]},{"text":"representation, a Partial-Lattice Markov Haphazard Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (UH) for language modeling.","acronyms":[[131,134],[55,61]],"long-forms":[[110,129],[18,53]]},{"text":" This paper proposes an approach for implicit feature detection based on SVM and Subjects Model(TM). ","acronyms":[[93,95],[73,76]],"long-forms":[[81,91]]},{"text":"where P is the Macro Precision and R is the Macro Recall. We also use tree induced error (TIE) in the experiments.","acronyms":[[90,93]],"long-forms":[[70,88],[21,30],[50,56]]},{"text":"the left hand side (LHS) of the rule, and ? the right hand side (RHS) of the rule.","acronyms":[[65,68],[20,23]],"long-forms":[[48,63],[4,18]]},{"text":"7-3-1, Hongo, Bunkyo-ku  Tokyo 113, Japan  It is complex for a natural language fathom system (NLUS) to deal  with ambiguities.","acronyms":[[104,108]],"long-forms":[[65,102]]},{"text":"Since it is costly to computations the partition function over the whole vocabulary, we utilized noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and","acronyms":[[118,121]],"long-forms":[[87,116]]},{"text":"means of |P (G)|. This guarantees that neither trees of maximum height (MHT) nor of maximum degree (MDT), i.e. trees which trivially","acronyms":[[72,75],[100,103]],"long-forms":[[56,70],[84,98]]},{"text":"Computing Center ,  Oscars of Sc iences ,  Hosoow, USSR  1.  Personal  Computer Plan (POS) represent  nowadays a  s ign i f teaut  t rend  in  the professiona~, and amateur uses of  ","acronyms":[[90,93],[52,56]],"long-forms":[]},{"text":"fcn@dsic.upv.es Abstract State-of-the-art Machine Translation (MT) systems are still far from being perfect.","acronyms":[[63,65]],"long-forms":[[42,61]]},{"text":"\\[Harman, 1996\\] D. Harman. Overview of the Forth  Text RetrievalConference (TREC-4). In Proceedings ","acronyms":[[77,83]],"long-forms":[[44,75]]},{"text":"vided within scientific articles. In addition, image Regions of Interest (ROIs) are commonly referred to within the image caption.","acronyms":[[74,78]],"long-forms":[[53,72]]},{"text":"(1992). Grammars  are defined over typed fea-  twre .structures (TFSs) which can be viewed as  generalizations of first-order terms (Carpenter, ","acronyms":[[65,69]],"long-forms":[[35,63]]},{"text":"The dependency inductions were evaluated on 3 metrics: directed accuracy, undirected accuracy and Neutral Edge Detection (NED) (Schwartz et al.,","acronyms":[[122,125]],"long-forms":[[98,120]]},{"text":"For POS tagging, the three main error categories are the confusion between adverbs (AD) and verbs with an  adverbial force, between measure words (M) and ","acronyms":[[84,86],[4,7]],"long-forms":[[75,82],[132,139]]},{"text":"i );3 MFS = Maximal Freq Sequence(d1 i","acronyms":[[6,9]],"long-forms":[[12,37]]},{"text":"2011a.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.","acronyms":[[45,47],[57,63]],"long-forms":[[24,43]]},{"text":"  Secondly, as for the word decree of prepositional  phrases (PP), Arabic and English are similar in  that PPs typically appear at the end of the sen-","acronyms":[[61,63],[106,109]],"long-forms":[[52,59]]},{"text":"also republish the baseline results of Schnabel and Sch?utze (2014) using the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger. ","acronyms":[[131,135],[87,90]],"long-forms":[[101,129]]},{"text":" University of Texas at Austin Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators.","acronyms":[[58,61]],"long-forms":[[31,56]]},{"text":"2013 Association for Computational Linguistics FCG offers a comparable grammar engineering framework that follows the principles of Build Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG","acronyms":[[151,154],[47,50],[202,205]],"long-forms":[[129,149]]},{"text":"from Si.  Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a  given document, to have a Feature Causality Diagram (FCD).","acronyms":[[37,40],[43,46],[142,145]],"long-forms":[[10,35],[115,140]]},{"text":"The major categories of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc.","acronyms":[[190,193],[137,140]],"long-forms":[[166,188],[121,135]]},{"text":"? similar count? score (SC) is calculated as the number of characters that match between two","acronyms":[[24,26]],"long-forms":[[17,22]]},{"text":" 2 American National Corpus The American National Corpus (ANC) project (Ide and Mcleod, 2001; Ide and Suderman, 2004) has","acronyms":[[58,61]],"long-forms":[[32,56]]},{"text":"\u0000 EFF (effect): We made her the secretary.  \u0000 ORIG (genesis): She made a cake from apples. ","acronyms":[[46,50],[2,5]],"long-forms":[[52,58],[7,13]]},{"text":"(41a) PP = ~ Cat i (PN)  i i>0  (41b) NP = N ~ Cati(P N) i  1 ","acronyms":[[38,40],[6,8],[20,22]],"long-forms":[[43,53]]},{"text":"Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al, 2011) and apply maximum entropy (MaxEnt) in the MALLET","acronyms":[[119,125],[134,140]],"long-forms":[[102,117]]},{"text":"CoTrain versus. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain versus. LEX(EN) 1.87E-05 1.64E-05 8.92E-07 CoTrain v. SVM(CN) 5.7E-08 2.91E-09 2.27E-11 CoTrain v. SVM(EN) 3.74E-15 5.77E-17 1.18E-20","acronyms":[[110,112],[12,15],[16,18],[59,62],[63,65],[152,155],[156,158]],"long-forms":[[94,101]]},{"text":"terms encountered.  Shallow Syntactic (SSyn) features consider the number and ratios of commons part-of-speech","acronyms":[[39,43]],"long-forms":[[20,37]]},{"text":"2.4 Optimisation and Sampling from a WCFG Optimisation in a weights CFG (WCFG)3, that is, finding the maximum shunt, is well stud-","acronyms":[[74,78],[37,41]],"long-forms":[[60,72]]},{"text":"Many different algorithms  have been used for this task, including some  machine learning (ML) algorithms, such as  Na?ve Bayesian model, decision trees, and ","acronyms":[[91,93]],"long-forms":[[73,89]]},{"text":"Tests Data Method Accuracy leave-one-out Minnen et al 83.58% Parlance Model (LM) 86.74% tenfold on development LM 84.72%","acronyms":[[76,78],[110,112]],"long-forms":[[60,74]]},{"text":"not be effective due to the shortness of contribution.  Barzilay and Lee?s algorithm (B&L) did not  generalize well to either dialogue corpus.","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":"knowledge resource (e.g., WordNet), and (ii) corpus-based that do not required any foreign knowledge source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954).","acronyms":[[181,185]],"long-forms":[[149,179]]},{"text":"there are usually three kinds of named entities (NEs) to be dealt with: names of persons (PER) , locations (LOC) and organizations (ORG).","acronyms":[[108,111],[49,52],[90,93],[132,135]],"long-forms":[[97,106],[33,47],[81,87],[117,129]]},{"text":"ing is combined with dictionary-based (e.g.,  WordNet) reranking, which leads to a 25% widened in mean reciprocal rank (MRR). Hsu et al ","acronyms":[[121,124]],"long-forms":[[99,119]]},{"text":"Abstract Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising","acronyms":[[153,156]],"long-forms":[[124,151]]},{"text":"  TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l  ","acronyms":[[24,26],[2,6],[19,23],[46,50],[51,53]],"long-forms":[[27,29],[55,58]]},{"text":"2007. CRFsuite: a fast implementation of conditional haphazard fields (CRFs). ","acronyms":[[68,72],[6,14]],"long-forms":[[41,66]]},{"text":"Albeit, it is observed that the identification of  lexical scopes of compound verbs (CompVs)  and conjunct verbs (ConjVs) from prolonged sequence of successive Complex Predicates ","acronyms":[[111,117],[82,88]],"long-forms":[[95,109],[66,80]]},{"text":"shown by Kusner and colleagues (2015), semantic representations such as Latent Semantic Indexing and Latent Dirichlet Allocation (LDA) can outperform a BoW representation.","acronyms":[[130,133],[152,155]],"long-forms":[[101,128]]},{"text":"topic distributions for all phrase torque in the expressions table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underly-","acronyms":[[137,140]],"long-forms":[[108,135]]},{"text":"call this set of feature Feat-IV.  Table 3 demonstrates the word error rate(WER) improvements enabled by our binary subsampling","acronyms":[[77,80]],"long-forms":[[61,75]]},{"text":" ? Generat ionCoverage(GC).Thepercentageofcoml) lete  and correct iuterlingna expressions R}r which the gener- ","acronyms":[[23,25]],"long-forms":[[3,21]]},{"text":"SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95 A0 - - - 0.85 - - - 0.93 - - - N\/A - - - 0.97 Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links (LM) or both (SR+LM).","acronyms":[[225,227],[120,123],[248,250],[261,266]],"long-forms":[[205,223],[230,246]]},{"text":"Stephanie Strassel and Zhiyi Song and Joe Ellis University of Pennsylvania Linguistic Data Consortium (LDC) Philadelphia, PA, USA","acronyms":[[103,106],[126,129],[122,124]],"long-forms":[[75,101]]},{"text":"BlogSum-generated summary content using ROUGE and compared the results with the original contestant list (OList). The t-test re-","acronyms":[[105,110],[40,45]],"long-forms":[[80,103]]},{"text":"noted in Dutch. Dutch shows a pattern in which  an arbitrary number of noun phrases (NP's) maggio be  followed by a finite verb and an arbitrary number ","acronyms":[[88,92]],"long-forms":[[74,86]]},{"text":"that of Visweswariah et al(2011) ? hereby called Travelling Salesman Problem (TSP) model ? with","acronyms":[[78,81]],"long-forms":[[49,76]]},{"text":"tions. Following Bahdanau et al (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its competency in caught long-distance depen-","acronyms":[[76,79]],"long-forms":[[54,74]]},{"text":"16 Production rule that expands n?s parent * * 17 Parse tree path from n to the nearest support verb * 18 Last portions of speech (POS) subsumed by n * 19 Production rule that enlarging n?s left sisterly *","acronyms":[[127,130]],"long-forms":[[111,125]]},{"text":"morpheme omitted.   We adopt Support Vector Machines(SVM) as  the device by which a conferred adnoun clause is ","acronyms":[[53,56]],"long-forms":[[29,51]]},{"text":"5 23   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69, Sofia, Bulgaria, August 9, 2013.","acronyms":[[72,79]],"long-forms":[[38,70]]},{"text":"(i) string kernels applied to sentences, or PTK applied to structural depictions with and without embedded relational informations (REL). This","acronyms":[[136,139],[44,47]],"long-forms":[[112,122]]},{"text":"Into Proc. 6th Canadian Conf on AI (CSCSI-86), pp. 78?83.","acronyms":[[34,42],[45,47]],"long-forms":[[13,32]]},{"text":"Many researchers have attempted several tech-  niques to deal with extragrammatical sentences  such as Augmented Transition Network(ATN)  (Kwasny and Sondheimer, 1981), network-based ","acronyms":[[132,135]],"long-forms":[[103,130]]},{"text":"vides brief details of each annotation dimension.   2.1 Knowledge Type (KT)  This dimension is responsible for seizing the ","acronyms":[[72,74]],"long-forms":[[56,70]]},{"text":"approaches. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 419?424. ","acronyms":[[82,86]],"long-forms":[[57,80]]},{"text":"1985) in which Tree Adjoining Grammars fall. 3 Since the  set of Tree Adjoining Languages (TALs) is a strict super-  set of the set of Context Free Languages, in order to define ","acronyms":[[91,95]],"long-forms":[[65,89]]},{"text":"joining grammars. In Proceedings of the 12 th International  Conference on Computational Linguistics (COLING'88),  Budapest, Hungary, August 1988.","acronyms":[[102,111]],"long-forms":[[61,100]]},{"text":"each other.  Normalized common neighbourhood (NCN). Nor-","acronyms":[[42,45]],"long-forms":[[13,40]]},{"text":"CoTrain v. BaseCN2 1.8E-07 0.00257 0.000182 CoTrain versus. BaseCN3 1.27E-06 0.00922 0.000765 CoTrain versus. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329","acronyms":[[107,109],[103,106],[150,153],[154,156]],"long-forms":[[91,98]]},{"text":"DO: parent:number := node:number; parent:gender := node:genders; 3.1.5 Preposition without child (PrepNoCh) In our dependency trees, the preposition is the","acronyms":[[100,108],[0,2]],"long-forms":[[70,98]]},{"text":"The straight case is the one mentioned above, treating all elements on the PARTS list equally (EQUAL). As a second op-","acronyms":[[95,100]],"long-forms":[[86,93]]},{"text":"Snow follows that of (Escudero et al, 2000c).  2.4 LazyBoost ing  (KG)  The main idea of improve algorithms is to ","acronyms":[[67,69]],"long-forms":[[51,60]]},{"text":" In the medical domain, Castan?o et al (2002) used UMLS (Unified Medical Language System)7 as their knowledge source.","acronyms":[[51,55]],"long-forms":[[57,90]]},{"text":"CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13 CoTrain vs. SVM(ENCN) 2.08E-13 8.13E-12 1.05E-12 CoTrain vs. TSVM(CN) 1.2E-19 1.07E-06 0.0624 CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08","acronyms":[[112,114],[12,15],[16,18],[58,61],[62,66],[152,156],[157,159],[107,111]],"long-forms":[[95,102]]},{"text":"Internet: chris@lsi .com NLP OBJECTIVE LSI's overall natural language treat (NLP) objective is the development of a broad coverage, reusable system which is readily transportable to additional domains, application, and sublanguages in English, as well as","acronyms":[[83,86],[25,28],[40,43]],"long-forms":[[54,81]]},{"text":" Definition 1  A lexical conceptual structure (LCS) is a modified version of the representation proposed  by Jackendoff (1983, 1990) that conforms to the following structural form: ","acronyms":[[47,50]],"long-forms":[[17,45]]},{"text":"its nondecomposability, as well as a cross between B??? and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be","acronyms":[[77,80]],"long-forms":[[60,75]]},{"text":" 50 missed OG events were labeled as Elapsed (PA) while FU events were commonly mislabeled as both PA","acronyms":[[43,45],[11,13],[53,55],[96,98]],"long-forms":[[37,41]]},{"text":"Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information","acronyms":[[132,134]],"long-forms":[[112,130]]},{"text":"Han, C-H., Han, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean  Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on  Language Resources and Evaluation (LREC).(2002)  5.","acronyms":[[212,216]],"long-forms":[[177,195]]},{"text":"context dependency and mutual information. Yamamoto and Church (2001) experiment with both mutual informational and residual inverse document frequency (RIDF)1 as criteria for deciding Japanese words, and their primary contribution is in affording","acronyms":[[151,155]],"long-forms":[[114,149]]},{"text":" 1 During t roduct ion   Vocabulary Understanding (LU) has been the focus of much research work in the last twenty ears. ","acronyms":[[45,47]],"long-forms":[[21,43]]},{"text":"object (J), which is bad for the Jews (the  event is marked by minus), and after that (>) a  Jewish bearer of Sacred Power (J*SP) miracu-  lously (MIR) protects the (above) Jewish object, ","acronyms":[[124,128],[147,150]],"long-forms":[[93,122],[130,145]]},{"text":"1 Introduction A fairly novel area of retrieval called topic detection and tracking (TDT) attempts to design methods to automatically (1) spot new, previously unreported events, and (2)","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":"Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee*  *Department of Computer Science and Engineering  Pohang University of Science & Technology (POSTECH)  San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea ","acronyms":[[142,149]],"long-forms":[[99,140]]},{"text":"NEDcost = EDcost\/lifespan (4) ? The Match Number(MN): The match number is the number of words","acronyms":[[47,49],[0,7],[10,16]],"long-forms":[[34,45]]},{"text":"176 Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human monocytes.","acronyms":[[201,206],[140,145]],"long-forms":[[185,199]]},{"text":"Fazly and Stevenson, 2007), our approach is to compare the context vector of a VNC with the composed vector of the verb and noun (V-N) component units of the VNC when they occur in iso-","acronyms":[[130,133]],"long-forms":[[115,128]]},{"text":"word lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from","acronyms":[[84,87]],"long-forms":[[65,82]]},{"text":"Semantic Feature Performance  The semantic features include Named Entity  (NE), Noun Hypernym (NHype) and Head Verb  Synset (HVSyn).","acronyms":[[95,100],[75,77],[125,130]],"long-forms":[[80,93],[60,72],[106,123]]},{"text":"EOPAS (PARADISEC tool) for text interlinear text and media analysis  2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009)  3.","acronyms":[[72,76],[0,5],[7,16]],"long-forms":[[78,102]]},{"text":"approach and extraction features from the names.  They utilise Maximum Entropy (MaxEnt) model and a number of features based on n-grams,","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":"outer: the perceived external frame or point of reference for  the action, event, or state as a whole  Means (MNS):  inner: the perceived immediate affeetor or effeetor of the ","acronyms":[[110,113]],"long-forms":[[103,108]]},{"text":" Examples of lexical and contextual ordinance learns by  the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb, ","acronyms":[[92,94],[73,76],[115,117],[146,148],[162,165]],"long-forms":[[97,105],[79,90],[120,144],[151,160],[168,172]]},{"text":"lar, at the level of additional information (CR), we observed some differences in judgement in particular between restrictions (AR) and warnings (AA), and a few others between CSFH and CSFC whose","acronyms":[[146,148],[45,47],[128,130],[176,180],[185,189]],"long-forms":[]},{"text":"Learning attitudes and attributes from multi-aspect exams. At Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020?1025.","acronyms":[[77,81],[89,93]],"long-forms":[[61,75]]},{"text":"1 Introduction  Syntax parsing is one of the most fundamental  tasks in natural language processing (NLP) and  has attracted extensive attention during the past ","acronyms":[[101,104]],"long-forms":[[72,99]]},{"text":"wards a Shared Task for Multiword Expressions. ACL Particular Interest Group on the Lexicon (SIGLEX), Marrakech.","acronyms":[[90,96],[47,50]],"long-forms":[[51,88]]},{"text":"a topic model are compared. Latent Dirichlet Allocation (LDA) (Blei et al 2003) is a amply used type of subjects model in which documents can be","acronyms":[[57,60]],"long-forms":[[28,55]]},{"text":" 2004. The Automatic Content Extraction (ACE) Programmes?Tasks, Data, and Evaluation.","acronyms":[[41,44]],"long-forms":[[11,39]]},{"text":"fields: ? Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings","acronyms":[[36,38],[80,85]],"long-forms":[[11,34]]},{"text":"1434  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"4 = 1, 440 items) (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression, OBS=observed vectors) . ","acronyms":[[92,95],[19,22],[33,36],[53,57]],"long-forms":[[96,104],[23,31],[37,51],[58,90]]},{"text":"higher indicating better.  We used Amazon?s Mechanical Turk (MTurk)5 to collect the human judgements.","acronyms":[[61,66]],"long-forms":[[44,59]]},{"text":"model organism databases (e.gram., for mouse3 and  yeast4) as well as various protein databases (e.g.,  Protein Info Resource5 (PIR) or SWISS-                                                                                           tor), a paragon organism for genetics research: ","acronyms":[[132,135]],"long-forms":[[101,130]]},{"text":"For attribute selection on the composed vector, we use two methods we found to perform best in Hartung and Frank (2010): Entropy Selection (ESel) and Most Prominent Component (MPC).","acronyms":[[140,144],[176,179]],"long-forms":[[121,138],[150,174]]},{"text":"2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical","acronyms":[[81,83],[51,55]],"long-forms":[[60,79],[8,49]]},{"text":"on the guidance of domain experts, who can devise pedagogically valuable reading lists that order docAutomatic Speech Recognition (ASR) with HMMs Loud Channel Model Viterbi Unscramble for ASR","acronyms":[[131,134],[187,190]],"long-forms":[[101,129]]},{"text":"press first-order logic. This requirement motivates our use of Inductive Logic Programming (ILP), a learning algorithm capable of inferring logic pro-","acronyms":[[92,95]],"long-forms":[[63,90]]},{"text":"ARG1, ARG2, MOD-BENEFICIARY, and MOD-TIME. To identifying which slot has the most similarity  amongst its elements, we calculate the number of distinct elements (NDE) in each slot across the  propose.","acronyms":[[157,160],[0,4],[6,10],[12,27],[33,41]],"long-forms":[[128,155]]},{"text":"We use multiple epochs of minibatch stochastic gradient descent and update all parameters to minimize the negative log likelihood (NLL) of our training set.","acronyms":[[131,134]],"long-forms":[[106,129]]},{"text":"ZC05 (Zettlemoyer and Collins 2005) 79.3 ?  ZC07 (Zettlemoyer and Collins 2007) 86.1 ? ","acronyms":[[44,48],[0,4]],"long-forms":[[50,78]]},{"text":"1 Automatic Discovery of Phone(me)s Statistical models learnt from data are extensively used in modern automatic speech recognition (ASR) systems.","acronyms":[[133,136]],"long-forms":[[103,131]]},{"text":"functions to SGML-mark the input.  Fast Partial Parser (FPP) .  The ultimate ","acronyms":[[56,59],[13,17]],"long-forms":[[35,54]]},{"text":"12  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142, October 25, 2014, Doha, Qatar.","acronyms":[[82,86]],"long-forms":[[46,80]]},{"text":"tion of precise probability scores for partial hypotheses contain-  ing islands, in the context of  a Stochastic-Context-Free-Grammar  (SCFG) for Language Modelled (LM). The seconds issue is the ","acronyms":[[165,167],[136,140]],"long-forms":[[146,163],[102,133]]},{"text":"3.5 Classification   For our task of classifying ALI patients, we  picked the Maximum Entropy (MaxEnt) algorithm due to its good performance in text classi-","acronyms":[[95,101],[49,52]],"long-forms":[[78,93]]},{"text":"on a questionnaire provided to them. And a Mean Opinion Notation(MoS) of 62.27% was achieved.","acronyms":[[62,65]],"long-forms":[[43,61]]},{"text":"mance. The query-based selection model utilizes Support Vector Regression (SVR) models to predict the mean average precision (MAP) of each query","acronyms":[[75,78],[126,129]],"long-forms":[[48,73],[102,124]]},{"text":"For example, in (2) we find frames identifying baseform verbs (VB) (2a) and frames identifying cardinale numbers (CD) (2b), though having a variety of context words.","acronyms":[[113,115],[63,65]],"long-forms":[[95,103],[56,60]]},{"text":" The first principle of a search engine is based  on shallow Natural Language Processing (NLP)  techniques, for instance, string matching, while ","acronyms":[[90,93]],"long-forms":[[61,88]]},{"text":"proaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic","acronyms":[[129,132]],"long-forms":[[106,114]]},{"text":" 3.2 Duluth Systems Evaluation The FScore (F-10), F-Measure (F-SC), and Jaccard Coefficient result in a comparable and consistent","acronyms":[[61,65]],"long-forms":[[35,41]]},{"text":"3 Framework We model the information extraction task as a markov decision process (MDP), where the model learns to utilize external sources to improve upon","acronyms":[[83,86]],"long-forms":[[58,81]]},{"text":"and reranking for the statistical parsing of spanish. In Proceedings of Human Language Technology (HLT) and the Conference on Empirical Methods in Natural","acronyms":[[99,102]],"long-forms":[[72,97]]},{"text":"each cross level text pair, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (SPh), Phrase to Word (Ph-W) and Word to Sense (W-Se).","acronyms":[[105,109],[57,60],[83,86],[130,134]],"long-forms":[[89,103],[34,55],[63,81],[115,128]]},{"text":"To tackle this  problem, we propose to employ the  Support Vector Machines(SVM) in  determining the grammatical functions.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"knowledge in building the target realms classifier, we suggest a novel optimization method based on the Na??ve Bayesian (NB) framework and stochastic gradient descent.","acronyms":[[121,123]],"long-forms":[[104,119]]},{"text":"Academics of South California  Los Angeles, California 90089  Abstract Tiffs paper describes Kind Types (KT), a system which uses  commonsense knowledge to reason about natural anguage text.","acronyms":[[109,111]],"long-forms":[[97,107]]},{"text":"the text. Our proposed metric is called Sentiment Annotation Complexity (BAG). ","acronyms":[[73,76]],"long-forms":[[40,71]]},{"text":"We gratefully acknowledge the support of Turkish  Scientific and  Technological Research Council of  Turkey  (TUBITAK)  and  METU  Scientifically  Researches  Fund  (no.","acronyms":[[110,117],[125,129]],"long-forms":[]},{"text":" 110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP) ","acronyms":[[32,36],[13,16],[49,52],[66,70]],"long-forms":[[18,30],[6,12],[39,48],[54,65]]},{"text":"2 Background 2.1 Surface Realization with Combinatory Categorial Grammar (CCG) CCG (Steedman, 2000) is a unification-based cat-","acronyms":[[74,77],[79,82]],"long-forms":[[42,72]]},{"text":"pathway representation formats: Systems Biology Dialing Language (SBML)3 (Hucka et al, 2003) and Biological Pathway Interchange (BioPAX)4 (Demir et al, 2010).","acronyms":[[125,131],[65,69]],"long-forms":[[96,123],[32,63]]},{"text":"edge mining.  Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news realm.","acronyms":[[30,37]],"long-forms":[[14,28]]},{"text":"proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of","acronyms":[[83,86]],"long-forms":[[55,81]]},{"text":"2. We propose a new topic model, Topic Sentiment Latent Dirichlet Allocation (TSLDA), which can capture the topic and sentiment si-","acronyms":[[78,83]],"long-forms":[[33,76]]},{"text":"4 Hierarchic Autoepistemic  Logic  Autoepistemic (AE) logic was developed by Moore  \\[I0\\] as a reconstruction of McDermott's nonmono- ","acronyms":[[50,52]],"long-forms":[[35,48]]},{"text":"Headline Generation. In the Proceedings of the  Document Understand Conference (DUC). ","acronyms":[[83,86]],"long-forms":[[48,81]]},{"text":"siderable progress. The bakeoff series hosted by  the Chinese Information Treatments Society (CIPS)  and ACL SIGHAN denotes that an F measure of ","acronyms":[[94,98],[105,108],[109,115]],"long-forms":[[54,92]]},{"text":"the divergence of their distributions in the targets and backgrounds. A support vector machine (SVM) was used to learn to classify between the targets and","acronyms":[[96,99]],"long-forms":[[72,94]]},{"text":"= Europarl). TOOL = grammatical words, PCT\/NB = punctuation and numbers, ADJ\/ADV = adjectives and adverbs, NAM = proper name, NOM = noun,","acronyms":[[39,45],[73,80],[107,110],[126,129]],"long-forms":[[48,71],[83,105],[120,124],[132,136]]},{"text":"Table 4. Comparative results in AIMed. The number of positive instances (POS) and negative instances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.","acronyms":[[73,76],[32,37],[102,105],[137,141],[152,156],[172,176]],"long-forms":[[53,71],[82,100],[111,135],[144,150],[162,170]]},{"text":"2.2 Natural Language Processing  2.2.1 Woods' Grow Transition Networks  Thc Rising Transition Network (ATN) of Woods [Woods 501 is a  powerful formalisnl for representing grammars.","acronyms":[[111,114]],"long-forms":[[81,109]]},{"text":"77 (a) README.txt file (d) RPM Spec PACKAGE section (metadata) Bean Scripting Framework (BSF) is a set of Java classes which gives an easy to utilize scripting language help","acronyms":[[89,92],[27,30]],"long-forms":[[63,87]]},{"text":" Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was adds after the valuation pe-","acronyms":[[85,88],[9,12]],"long-forms":[[63,83]]},{"text":"  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 67?68, Gothenburg, Sweden, April 27, 2014.","acronyms":[[71,76]],"long-forms":[[37,69]]},{"text":"Approach for Arabic-English Named Entity Translation, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages (ACL),  University of Michigan, Ann Arbor","acronyms":[[136,139],[73,76]],"long-forms":[[103,134]]},{"text":"a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers.","acronyms":[[121,124]],"long-forms":[[95,119]]},{"text":" MRS constraints have three kinds of literals, two kinds of elementary predications (EPs) in the first two lines and handle constraints in the third line:","acronyms":[[85,88],[1,4]],"long-forms":[[60,83]]},{"text":"  Abbreviation: ACK=Acknowledgment; COMP=Completion;  SNU=Signal Non Understanding; Sources abbreviated as: F  = Friendship; V = Visibility; Rte = Route ","acronyms":[[54,57],[16,19],[36,40],[141,144]],"long-forms":[[58,82],[20,33],[41,51],[113,123],[129,139],[147,152]]},{"text":"mathematical models from experimental data. The  algorithm is similar to Genetic Programming (GP),  but uses fixed-length character strings (called chro-","acronyms":[[94,96]],"long-forms":[[73,92]]},{"text":" 1 Introduction Mental State Verbs (MSVs), such as think, know, and want, are very frequent in child-directed lan-","acronyms":[[36,40]],"long-forms":[[16,34]]},{"text":"vv@iiit.ac.in Abstract Recognition of Named Entities (NEs) is a difficult process in Indian languages like Hindi,","acronyms":[[54,57]],"long-forms":[[38,52]]},{"text":"Schafer and Graham, 2002) discussed variety approaches such as lawsuit deletion, mean substitution, and recommended maximum likelihood (ML) and Bayesian multiple imputation (MI).","acronyms":[[133,135],[171,173]],"long-forms":[[113,131],[150,169]]},{"text":"to be relevant: 1. Subject (SS) 2.","acronyms":[[28,30]],"long-forms":[[19,26]]},{"text":"computation of distributional thesauri (Hayashi, 1998) has been about for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications.","acronyms":[[154,157]],"long-forms":[[125,152]]},{"text":"Finance and economics (FAE) 100  Teaching (EDU) 100  Entertainment (ENT) 100  Computer (COM) 100 ","acronyms":[[69,72],[23,26],[44,47],[89,92]],"long-forms":[[54,67],[0,20],[33,42],[79,87]]},{"text":"FA8750-09-C-0181. The second author also thanks the Vietnam Education Cornerstones (VEF) for its sponsorship.","acronyms":[[82,85]],"long-forms":[[52,80]]},{"text":" ? Generat ionCoverage(GC).Thepercentageofcoml) lete  and exact iuterlingna expressions R}r which the gener- ","acronyms":[[23,25]],"long-forms":[[3,21]]},{"text":"In this section, we define pomsets as a model for describing concurrency. A labelled partial order (LPO) is a 4 tuple (V, ?,","acronyms":[[100,103]],"long-forms":[[76,98]]},{"text":"the influence of several kinds of physical degradation that pages may endure before they are scanned and processed using optical character recognition (OCR) software. ","acronyms":[[149,152]],"long-forms":[[118,147]]},{"text":"tences in the other part of the corpus. Therefore, we performed a language identification (LID)based filtering afterwards (performed only on the","acronyms":[[91,94]],"long-forms":[[66,89]]},{"text":"phrase structure grammar (PSG) as thc tagging  formalisms(Lecch & Garside 1991), and some  adopt dependency grammar(DG) 1993, Komatsu,  Jin, & Yasuhara, 1993).","acronyms":[[116,118],[26,29]],"long-forms":[[97,114],[0,24]]},{"text":"The application of the program is demonstrated using the Aberdeen Report Judgment Scales (ARJS; Sporer, 2004) with a set of 72 deceptive and true accounts of a driving examination. Data on different types of inter-coder reliabilities are presented and implications for future research with computer-assisted qualitative coding procedures as well as training of coders are outlined. Credits This research has been supported by a grant from the German Science Foundation (Deutsche Forschungsgemeinschaft (DFG): Sp262\/3-2) to the present author. The author would like to thank Edda Niederstadt and Nina F. Petermann for the coding of the data, and to Jaume Masip, Valerie Hauch, and Sarah Treiber for comments on an earlier version of this manuscript.","acronyms":[[503,506],[90,94]],"long-forms":[[470,501],[57,88]]},{"text":"We show that hierarchies of this genus can be  automatical!y constructions, by employs the semantic ategory codes and the subject codes of the  Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in  noun definitkms.","acronyms":[[183,188]],"long-forms":[[139,181]]},{"text":"139  Computational Linguistics Volume 18, Number 2  (18a) sense of JUMP = jumping. ","acronyms":[[67,71]],"long-forms":[[74,81]]},{"text":"16 Production rule that expands n?s parent * * 17 Parse tree path from n to the nearest support verb * 18 Last part of speech (POS) subsumed by n * 19 Production rule that expands n?s left sibling *","acronyms":[[127,130]],"long-forms":[[111,125]]},{"text":" In table 1, we presented the accuracy of the model qualified on the output of the joint inference (JOINT) against that of the self-training baseline (SELF).","acronyms":[[96,101],[147,151]],"long-forms":[[79,84],[123,136]]},{"text":"3 The  S imulat ion  Mode l   The computational simulation supports the evolu-  tion of a population of Language Agents (LAgts),  similar to Holland's (1993) Echo agents.","acronyms":[[121,126]],"long-forms":[[104,119]]},{"text":"71 tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for","acronyms":[[79,82],[18,23]],"long-forms":[[52,77]]},{"text":"complementary features ( Section 3.3).  3.2 Mining Labeled Sequential Patterns ( LSP ) Labeled Sequential Patterns (LSP).","acronyms":[[81,84],[115,119]],"long-forms":[[51,78],[87,114]]},{"text":"Following are the two broad types of social events that were annotated: Interaction event (INR): When both entities participating in an event are aware of each other and of","acronyms":[[91,94]],"long-forms":[[72,83]]},{"text":"(Figure 2). Argviz is a web-based application, built using Google Web Toolkit (GWT),4 which allows users to visualize and manipulate SITS?s outputs en-","acronyms":[[79,82],[133,138]],"long-forms":[[59,77]]},{"text":"for the task: a training dataset (TrainSet) with 9675 messages directly recover from Twitter; a development dataset (DevSet), with 1654 messages; the essays dataset from 2013 run, which","acronyms":[[119,125],[34,42]],"long-forms":[[98,117],[16,32]]},{"text":"syntactically correct) to 1.0 (completely wrong).  ISER (information item semantic error rate): The test sentences are segmented into information items; for each of these items, the translation candidates","acronyms":[[51,55]],"long-forms":[[57,93]]},{"text":"namely person name, location name, organizational name and miscellaneous name to apply  Aid Vector Machine (SVM) based machine  learning technique.","acronyms":[[110,113]],"long-forms":[[86,108]]},{"text":"2.1 Description of the procedure Two specialized topics In this study MA student interpreters were invited to prepare for simultaneous interpreting tasks on two specialised topics: fast reactors (FR) and Seabed minerals (SM). They","acronyms":[[196,198],[221,223],[70,72]],"long-forms":[[181,194],[204,219]]},{"text":"response- and reference-based scoring methods. All models utilised helping vector regression (SVR) (Smola and Sch?olkopf, 2004), with the complexity parame-","acronyms":[[89,92]],"long-forms":[[62,87]]},{"text":"3.1 Feature Structure To implement the twin model, we adopt the log linear or maximum entropy (MaxEnt) model (Berger et al, 1996) for its flexibility of combining diverse sources of","acronyms":[[95,101]],"long-forms":[[78,93]]},{"text":"twelve NE tags. The NE tagged corpus has been  utilizes to develop Named Entity Acknowledgment (NER)  system in Bengali using pattern directed shallow ","acronyms":[[89,92],[7,9],[20,22]],"long-forms":[[63,87]]},{"text":"the interpolated and adapted models are comparisons.  For the Estonian task, letter error rate (LER) is also reported, because it tends to be a more indicative","acronyms":[[93,96]],"long-forms":[[74,91]]},{"text":"sets for Chino and English. For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), convert 2For replicability, a complete description of all features can","acronyms":[[82,86]],"long-forms":[[60,80]]},{"text":"tions.  3.1 Simple Ranks (SC) Given an expression x consisting of n words x1,","acronyms":[[35,37]],"long-forms":[[12,33]]},{"text":"For attribute choice on the composed vector, we use two methods we found to perform best in Hartung and Frank (2010): Entropy Selection (ESel) and Most Prominent Elements (MPC).","acronyms":[[140,144],[176,179]],"long-forms":[[121,138],[150,174]]},{"text":"tions of words. In Proceedings of the International Conference on Computational Linguistics (COLING), Bombay, India, December.","acronyms":[[93,99]],"long-forms":[[66,91]]},{"text":"translation architecture, was developed under sponsorship from  Swedish Telecom by a collaboration between SRI International,  the Swedish Institute of Computer Science (SICS), and Telia  Research.","acronyms":[[170,174],[107,110]],"long-forms":[[131,168]]},{"text":"lowing three metrics are used in this experiment.  (a) EPN in total (EPN-T): The number of the expanded problems which are generated in the","acronyms":[[69,74]],"long-forms":[[55,67]]},{"text":"be intuitively characterized as a way of trigger-  ing semantically associated concepts which define for  each role the projective concluded space (PCS). ","acronyms":[[146,149]],"long-forms":[[117,144]]},{"text":"s), correlation with error counts (re and ? e), average precision (AP) and pairwise accuracy.","acronyms":[[67,69]],"long-forms":[[48,65]]},{"text":"= Europarl). TOOL = grammatical words, PCT\/NB = punctuation and numbers, ADJ\/ADV = adjectives and adverbs, NAM = appropriate name, NOM = noun,","acronyms":[[39,45],[73,80],[107,110],[126,129]],"long-forms":[[48,71],[83,105],[120,124],[132,136]]},{"text":"Norm = Normalisation of input beforehand to tagging. SUC = Subset of Stockholm-Umea? ","acronyms":[[48,51]],"long-forms":[[54,73]]},{"text":"restriction to do top-down filtering.  1986 Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood.","acronyms":[[117,120]],"long-forms":[[102,115]]},{"text":"roles they typically enter: ACT (Actor), PAT (Patient), ADDR (Addressee), ORIG (Origin) and EFF (Effect). Syntactic criteria are used to identify","acronyms":[[92,95],[28,31],[41,44],[56,60],[74,78]],"long-forms":[[97,103],[33,38],[46,53],[62,71],[80,86]]},{"text":"extrinsic and linguistic independent features.  The student response analysis (SRA) task (Dzikovska et al 2013) addresses the fol-","acronyms":[[77,80]],"long-forms":[[50,75]]},{"text":"Among the NEs we select six of them as the recognized objects, that is, personal name (PN), date or time (DT), location name (LN), team name (TN), competition title (CT) and per-","acronyms":[[106,108],[126,128]],"long-forms":[[92,104],[111,124]]},{"text":" 3.2 Formal basis: Lexical Resource Semantics Lexical Resource Semantics (LRS) (Richter and Sailer, 2003) is an underspecified semantic formal-","acronyms":[[74,77]],"long-forms":[[46,72]]},{"text":"we use the DUC2002 and DUC2004 data sets, both of which are open benchmark data sets from Document Understanding Conference (DUC) for generic automatic summarization evaluation.","acronyms":[[125,128],[11,14],[23,26]],"long-forms":[[90,123]]},{"text":"The contextual information about social status and  sentence-external individuals can bc included in the  attribute CONTEXT (CONX). Sick ordering to see values the ","acronyms":[[125,129]],"long-forms":[[116,123]]},{"text":"ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information.","acronyms":[[63,66],[0,4],[13,18]],"long-forms":[[39,61]]},{"text":" As an example consider the multiple alignments in Figure 4, with the gold standard alignment (GS) on the left and the generated alignment (GA) on","acronyms":[[95,97]],"long-forms":[[70,83]]},{"text":"slot-def SN subslot-of SN1 . . . SNn (SN v SN1) . . . ( SN vs SNn)","acronyms":[[33,36],[9,11]],"long-forms":[[38,46]]},{"text":"This paper explores the using of the homotopy method for training a semi-supervised Disguising Markov Model (HMM) used for sequence labeling.","acronyms":[[103,106]],"long-forms":[[82,101]]},{"text":"vided for training data.  the utilize of well-motivated Lexical Structures (LS's)  to capture the presuppositional nd anaphoric as- ","acronyms":[[72,76]],"long-forms":[[52,70]]},{"text":" 3.3 Question Classification We look next at question classification (QC). ","acronyms":[[70,72]],"long-forms":[[45,68]]},{"text":"and above 95% for determiners (DT). In addition, items (SS) have a score above 90%. In all these","acronyms":[[59,61],[31,33]],"long-forms":[[49,57],[18,28]]},{"text":"more, 1976; Fillmore, 1982). The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the","acronyms":[[97,100]],"long-forms":[[82,95]]},{"text":"automatique, GDR I3 ATALA, Paris, November 1999.  Tang E.K., Natural languages Analysis in machine translation (MT) based on the STCG, PhD thesis, Sains Malaysia University, Penang, March 1994","acronyms":[[112,114],[13,16],[20,25],[129,133],[135,138],[55,58]],"long-forms":[[91,110]]},{"text":"tions and keying in on student language to promote self-explanation of concepts, and its curriculum is based on the Full Option Scientifically System (FOSS) 1 a proven system for investigate based learn.","acronyms":[[144,148]],"long-forms":[[116,142]]},{"text":"This paper is an at tempt  to provide part  of the basis for a general theory of robust  process ing in Machine Trans lat ion  (MT) wi th  relevance to other areas of Natural  Language ","acronyms":[[128,130]],"long-forms":[[104,125]]},{"text":"neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":" 2 Graphical Model Framework una Graphical Modelled (GM) represents a factorization of a family of joint probability distributions over a","acronyms":[[48,50]],"long-forms":[[31,46]]},{"text":"DUC 2007. In Proceedings of the Seven Document Understanding Conference (DUC), Rochester, NY.","acronyms":[[75,78],[0,3],[92,94]],"long-forms":[[40,73]]},{"text":"We present experiments using counts of three types of ngrams: lemma ngrams (LN), POS ngrams (PN) and mixed ngrams (MN).2 Mixed ngram is a restricted formulation of lemma ngram where open-","acronyms":[[115,117],[76,78],[93,95]],"long-forms":[[101,113],[62,74],[81,91]]},{"text":" In this definition, WHEN has two official parameters x and y; each of them refers to a  situation that transpires at the same point in time (PTIM). Every occurrence of the relation ","acronyms":[[136,140]],"long-forms":[[121,134]]},{"text":"HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge.","acronyms":[[441,444],[0,3]],"long-forms":[[412,439]]},{"text":"from Si.  Traits Causality Diagram (FCD): CNB entitles each feature Y, which occurs in a  given document, to have a Feature Causality Diagram (FCD).","acronyms":[[37,40],[43,46],[142,145]],"long-forms":[[10,35],[115,140]]},{"text":"Performing proper Arabic dialect identification may positively impact many Natural Language Processing (NLP) application.","acronyms":[[104,107]],"long-forms":[[75,102]]},{"text":" ? Cutback(RE): Pop the stack. ","acronyms":[[10,12]],"long-forms":[[3,9]]},{"text":"  Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2  N?cleo Interinstitucional de Ling??stica Computacional (NILC)  http:\/\/www.nilc.icmc.usp.br ","acronyms":[[144,148]],"long-forms":[[88,142]]},{"text":"248   Trials of the 2014 Conference on Empirical Techniques in Natural Language Processing (EMNLP), pages 1284?1295, October 25-29, 2014, Doha, Qatar.","acronyms":[[94,99]],"long-forms":[[44,92]]},{"text":"Abstract This paper proposing two ways of improving semantic role labeling (SRL). First, we intro-","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"built such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT)  (Carlson et al, 2001) is an accessible resource to ","acronyms":[[77,83]],"long-forms":[[53,75]]},{"text":"large and is often simplified.  Because we use belief propagation (BP) as baseline to compare to, and as a subroutine in our pro-","acronyms":[[67,69]],"long-forms":[[47,65]]},{"text":"words, respectively. We include two versions of this general model; Continuous Luggage of Phrase (CBOW) that predicts a word based on the context, and Skip-","acronyms":[[93,97]],"long-forms":[[68,91]]},{"text":"Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0 to 100, indicating the level of compositionality (the","acronyms":[[108,113],[86,90],[65,71]],"long-forms":[[97,106],[74,84],[42,63]]},{"text":"(pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (echelon uni), independently encoding the parts of discourses (POS) of the words preceding and following the adjective, respec-","acronyms":[[142,145]],"long-forms":[[125,140]]},{"text":"3 Architecture of SCQA As shown in Figure 2, SCQA consists of a pair of deep convolutional neural networks (CNN) with convolution, max pooling and rectified lin-","acronyms":[[108,111],[18,22],[45,49]],"long-forms":[[77,106]]},{"text":"lar, at the level of supplemental information (CR), we remarked some differences in judgement in particular between restrictions (AR) and alerted (AA), and a few others between CSFH and CSFC whose","acronyms":[[146,148],[45,47],[128,130],[176,180],[185,189]],"long-forms":[]},{"text":"project.  In section 2 we provide an overview of the automatic compound processing (AuCoPro) project, which forms the background of this research.","acronyms":[[84,91]],"long-forms":[[53,82]]},{"text":"(1987) presents in his Case grid.  Case Analysis (CA) extracts the acts and Lawsuit  constellations almost them from the structure ","acronyms":[[50,52]],"long-forms":[[35,48]]},{"text":"1. Grider, T., Mosley, H., Snow, L, and Wilson, W., \"Users Manual for the  Momentum Analytical Replanning Tool (DRAFT)\", prepared for BBN by  Systems Researching and Applications Companies, 9 November 1990.","acronyms":[[111,116]],"long-forms":[[75,109]]},{"text":"Fortunately, learning the reward function using IRL means have already been recommendation for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al.,","acronyms":[[104,106],[48,51],[107,110]],"long-forms":[[78,90]]},{"text":"navigation instruction to guide the IF to a convenient location at which she can then utilizing a simple referring phrases (RE). That is, there is an inter-","acronyms":[[121,123],[36,38]],"long-forms":[[99,119]]},{"text":"Ravi and Knight (2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while Corlett and Penn (2010) solve the problem using","acronyms":[[134,137]],"long-forms":[[110,132]]},{"text":"not vacillate, vacillate is, line vacillate?  English Context(EC): shakes\/vacillate  Putting on Search Engine and getting counts:  ","acronyms":[[62,64]],"long-forms":[[46,60]]},{"text":"1 Intro For the past three decades, there has been a great deal of work on the automatic identification (ID) of languages from the speech signaling alone.","acronyms":[[112,114]],"long-forms":[[96,110]]},{"text":"The word nchi is dis-  ambiguated with a ordinance relying on the Ncl of the  following genitive connector (GEN-CON). ","acronyms":[[103,110],[61,64]],"long-forms":[[83,101]]},{"text":"P i jPMI P i P j=   Equation 2: Pointwise Mutual Information (PMI)  between deux terms i and j. ","acronyms":[[62,65],[5,8]],"long-forms":[[32,60]]},{"text":"The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). ","acronyms":[[129,131],[28,31],[104,107]],"long-forms":[[113,127],[4,26],[74,102]]},{"text":"Tables 6: DEFICIENCIES scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston","acronyms":[[43,45],[55,57],[70,72],[9,12],[86,88],[100,102]],"long-forms":[[48,53],[60,68],[75,84],[91,98],[105,121]]},{"text":"swer sequence tagging.  bels: B-ANSWER (beginnings of answer), I-ANSWER (inside of answer), O (outside of answer).","acronyms":[[30,38],[62,70]],"long-forms":[[40,59],[94,111]]},{"text":"processing. This paper explores grammatical issues in Scottish Gaelic by means of dependency tagging and combinatory categorial grammar (CCG), which we see as complementary approaches. As such it","acronyms":[[137,140]],"long-forms":[[105,135]]},{"text":"namely, the set {? ( D)d |ad = a}, where ad is the author of documented d. An alternative approach is LDA-S (LDA with a single document per author), where each author?s documents are concatenated into a single documented in a preprocessing steps, LDA is running","acronyms":[[100,105],[242,245]],"long-forms":[[107,124]]},{"text":"2005; Wieling et al, 2007) for string similarity  estimation, and is based on the notion of string  Edit Distance (ED). String ED is defined here as ","acronyms":[[115,117],[127,129]],"long-forms":[[100,113]]},{"text":"features: O-SEM ('ordinary semantics') and  I,'-SKEL (F-skeleton) of the type of a semantic ob-  ject, tile set-valued IS-CSTR (IS constraints) and  the binary MAX-F (for potential maximal focus).","acronyms":[[119,126],[10,15],[44,52],[160,165]],"long-forms":[[128,142],[18,36],[54,64]]},{"text":"to be pertinent: 1. Subject (SS) 2.","acronyms":[[28,30]],"long-forms":[[19,26]]},{"text":"Our data happens from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). ","acronyms":[[85,88],[29,32],[44,47],[62,65]],"long-forms":[[72,83],[20,27],[35,42],[50,60]]},{"text":"This examination can be held by plotting values of recall, precise and F-measure during each step  of merging process. Figure 5 shows the fluctuation of positive recalls(PR), positive preclsion(AP),  averaged recall(AR), averaged exact and F-measure (FM).","acronyms":[[171,173]],"long-forms":[[155,169]]},{"text":"the MDL methods.  ConVote (CONVOTE) Our second dataset is taken from segments of speech from United States","acronyms":[[27,34],[4,7]],"long-forms":[[18,25]]},{"text":"3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously.","acronyms":[[400,403]],"long-forms":[[376,398]]},{"text":"We applied 3 MCMC algorithms: Gibbs sampling (GS), MCSAT and Simulated Tempering (ST) for inference and the comparative NER results are shown in Table 1.","acronyms":[[82,84]],"long-forms":[[61,80]]},{"text":"  MTI. The original Physician Text Indexer (MTI)  system, shown in Figure 1, consists of an infra-","acronyms":[[42,45],[2,5]],"long-forms":[[20,40]]},{"text":" Definition 2.5  Given a grammatical, G, define MCL(G) (Maximum Amendment in Length) as:  MCL(G) = max { m \\] A (.. q\/1. . .","acronyms":[[44,47],[83,86],[92,95]],"long-forms":[[52,76],[25,32]]},{"text":"translations. Although initially intended for  trainees of English as Foreign Linguistic (EFL)  in Taiwan, it is a gold mine of texts in English ","acronyms":[[88,91]],"long-forms":[[59,86]]},{"text":"(FW), SeekBw (BW), ScrollFw (FS), ScrollBw (BS), Ratechange Increase (RCI), Ratechange Decrease (RCD). ","acronyms":[[97,100],[1,3],[14,16],[29,31],[44,46],[70,73]],"long-forms":[[76,95],[6,12],[19,27],[34,42],[49,68]]},{"text":"semantic F1 of 85.63 for English.  Time Expression Identification (TEI) and Normalization (TEN): We use the period module","acronyms":[[67,70]],"long-forms":[[35,65]]},{"text":"are defined:    (1)  VSM-based (Vector Space Model based)  trigger word similarity: the trigger words ","acronyms":[[21,30]],"long-forms":[[32,50]]},{"text":"We show each sentence to three unique workers on Amazon Mechanical Turk (MTurk) and ask each to judge how well the paraphrase retains the mean-","acronyms":[[73,78]],"long-forms":[[56,71]]},{"text":"While much of the focus in developing a statistical machine translation (SMT) system revolves around the translation model (TM), most systems do not emphasize the role of the language model (LM).","acronyms":[[124,126],[73,76],[191,193]],"long-forms":[[105,122],[40,71],[175,189]]},{"text":"based classifier is employs to chooses the most informative examples for training an another type of  classifier based on multinomial na?ve Bayes (NB)  model (McCallum and Nigam, 1998b).","acronyms":[[143,145]],"long-forms":[[130,141]]},{"text":"4.1 The NIST evaluation scheme The National Institute of Science and Technology (NIST) proposed an evaluation scheme that looks at the following properties when","acronyms":[[81,85],[8,12]],"long-forms":[[35,79]]},{"text":"Memo: in genera\\], the resultant segments,  such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrases(INF), and Verb Phrase(VP), ","acronyms":[[93,95],[73,77],[119,122],[141,143]],"long-forms":[[81,91],[52,72],[98,110],[129,140]]},{"text":"successful method submitted by Bendersky and Croft (2008) for choose and weighting of query noun phrases (NPs). It also extends work for deter-","acronyms":[[109,112]],"long-forms":[[95,107]]},{"text":"2 Description of the France Telecom 3000 Voice Agency corpus The France Telecom 3000 (FT3000) Voice Agency service, the first deployed vocal service at France","acronyms":[[86,92]],"long-forms":[[65,84]]},{"text":"We conducted experiment on a number of different datasets: (1) the English Wall Rue Journal (WSJ) part of the Penn Treebank (Marcus et al, 1993) with standard POS","acronyms":[[97,100],[163,166]],"long-forms":[[76,95]]},{"text":" As the noun or adjective occur in the first slot  of conjunct verbs (ConjVs) construction, the  search starts from the point of noun or adjec-","acronyms":[[70,76]],"long-forms":[[54,68]]},{"text":"conversions: ovc~iflow in the Data Treatment  category (DPR,) and out-o\\[-.\/lushnc.ss in the Air-  m'M't Structure category (STR). As shown in the ","acronyms":[[127,130],[58,61]],"long-forms":[[107,125],[31,56]]},{"text":"ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase Substitution","acronyms":[[88,93]],"long-forms":[[60,86]]},{"text":"5http:\/\/wordnet.princeton.deu\/. 58 that, as in Informtion Recoveries (IR), multiple occurrences in the same document count as just one","acronyms":[[69,71]],"long-forms":[[47,67]]},{"text":"First, we rewrite equation 1 in a more detailed fashion as: A?R = argmax A","acronyms":[[60,63]],"long-forms":[[66,72]]},{"text":"?? ( sE) one or more times are considered to be locative MWEs (LOC). In contrast, bigrams","acronyms":[[63,66],[57,61]],"long-forms":[[48,56]]},{"text":"3 CLaC Methodology Preprocessing consists of tokenizing, lemmatizing, sentence splitting, and part of speech (POS) tagging. ","acronyms":[[110,113],[2,6]],"long-forms":[[94,108]]},{"text":"3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task.","acronyms":[[105,108],[31,40]],"long-forms":[[80,103]]},{"text":"A Robust Algorithm for the Tree Edit Distance.  Proceedings of the VLDB Endowment (PVLDB), 5(4):334?345.","acronyms":[[83,88]],"long-forms":[[48,71]]},{"text":"3.2 Data Normalization The ACL interchange task is very close in form and content to the Final Text Editions (FTE) task of the TCSTAR (TC-STAR, 2004) assessments.","acronyms":[[105,108],[27,30],[130,137]],"long-forms":[[84,103],[122,128]]},{"text":"processing. Throughout Proceedings of the Conference on  Knowledge Capture (K-CAP), pages 70-77, 2003. ","acronyms":[[68,73]],"long-forms":[[49,66]]},{"text":"Berwick and Weinberg (1982). Gazdar noted that if  transformational grammars (TG's) were stripped of  all their transformations, they became CFL- ","acronyms":[[78,82],[141,144]],"long-forms":[[51,76]]},{"text":"? This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No.","acronyms":[[96,99],[62,66]],"long-forms":[[67,94]]},{"text":"aKemp|es  of  t. raL~uct lon  du  russe  eD f ranca ls  rea l  i s le  par   le  grams~TA a Grenob le ,   MUTS-CLES:  TAO ( t raduct ion  butt i s t ,  co  par  o rd lnateur )f   naa lyse  morpno l  og ique~ t rans fer t  lex lca l~ 0enerat  1o~ ","acronyms":[[114,117],[81,85],[102,111]],"long-forms":[[120,168]]},{"text":"compiled two datasets consisting of research papers from two top-tier machine learning conferences: World Wide Web (WWW) and Knowledge Discovery and Data Mining (KDD).","acronyms":[[116,119],[162,165]],"long-forms":[[100,114],[125,153]]},{"text":"Table 10: A=audio, P=psycholinguistic, POS=part-of-speech, C=complexity, F=fluency, VR=vocabulary richness, CFG=CFG production rule featuring.","acronyms":[[87,89],[42,45],[111,114],[115,118]],"long-forms":[[90,109],[12,20],[24,40],[46,60],[64,74],[78,85]]},{"text":"ogy (GO), Cell Type Ontology (CTO), BRENDA Tissue Ontology (BTO), Foundational Model of Anatomy (FMA), Cell Cycle Ontology (CCO), and Sequence Ontology (SO)?and a small number of","acronyms":[[124,127],[5,7],[30,33],[60,63],[97,100],[153,155]],"long-forms":[[103,122],[10,28],[36,58],[66,95],[134,151]]},{"text":"translators with the help of computer-aided translation tools (CAT), (3) rule-based MT systems (RBMT) and (4) statistical MT systems (SMT). ","acronyms":[[134,137],[63,66],[96,100]],"long-forms":[[110,132],[29,55],[73,86]]},{"text":"sures for each portion of the result. One is a relevance score (RS) with the target document \u0001 \u0002","acronyms":[[65,67]],"long-forms":[[48,63]]},{"text":"review  ? Product feature level (PFL)  ?","acronyms":[[33,36]],"long-forms":[[10,31]]},{"text":"duction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effectively mode to approximate the original","acronyms":[[87,90]],"long-forms":[[57,85]]},{"text":"4 = 1, 440 subjects) (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression, OBS=observed vectors) . ","acronyms":[[92,95],[19,22],[33,36],[53,57]],"long-forms":[[96,104],[23,31],[37,51],[58,90]]},{"text":"HLT\/EMNLP, 2005  http:\/\/www.nist.gov\/speech\/tests\/ace\/ace07\/doc, The  ACE 2007 (ACE07) Evaluation Plan, Evaluation of  the Detection and Recognition of ACE Entities, Val-","acronyms":[[80,85],[0,9],[152,155]],"long-forms":[[70,78]]},{"text":"ple sentences. Such knowledge is also important for Textual Entailment (TE), a generic framework for modeling semantic inference.","acronyms":[[72,74]],"long-forms":[[52,70]]},{"text":"jective U-shaped is an example of gesture enriching an adjectival meaning through the interface defaulted Adjectives meaning extended (AdjMExt) AdjMExt: Adjective(u), sem(u) is ?","acronyms":[[132,139],[141,148]],"long-forms":[[104,130]]},{"text":"H A V E  CSEXCH F O R  MOVEF IN 11  C H A N G E ,  CALL EL3MOP P O R  E R A S E  0 I 2   ANTEST CALLED FOR 14'IGLOT \" (AACC) S D =  15. RES= '3.","acronyms":[[103,106],[119,123],[125,128],[9,15],[16,21],[23,28],[136,139]],"long-forms":[[89,102],[107,115]]},{"text":"2 As a matter of facto, Figure 1 only shows 8 columns, notwithstanding  the CoNLL-X format includes two additional columns for the  projective head (PHEAD) and projective addiction relation  (PDEPREL), which have not been used in our work.","acronyms":[[141,146],[68,75],[185,192]],"long-forms":[[124,139],[152,182]]},{"text":"given topic.  The Maximal Marginal Relevance (MMR) summarization method, which is based on a","acronyms":[[46,49]],"long-forms":[[18,44]]},{"text":" 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the ordering in","acronyms":[[52,55]],"long-forms":[[19,50]]},{"text":" 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or","acronyms":[[64,68]],"long-forms":[[37,62]]},{"text":"syntactic skeleton defined in Eq. 1, namely, Subject(S), Verb(V), Direct Object(DO), Indirect Object(IO), Preposition(P) and Noun(Object) of the Preposition(N).","acronyms":[[80,82],[101,103]],"long-forms":[[66,78],[85,99],[45,52],[57,61],[106,117]]},{"text":"We demonstrating that languageindependent features can be used for regression with Support Vector Machines (SVMs) and the Margin-Infused Relaxing Algorithm (MIRA), and","acronyms":[[99,103],[147,151]],"long-forms":[[74,97],[113,145]]},{"text":"res. Onto Section 2 we present a structural reformulation of Support Vector Machines (SVMs) that can take similarities between multiple genres into ac-","acronyms":[[84,88]],"long-forms":[[59,82]]},{"text":" 250 Support Vector Machines (SVMs) construct a hyperplane in a multi-dimensional space which yields a good separation between positive and negative training examples, represented as data points.","acronyms":[[30,34]],"long-forms":[[5,28]]},{"text":"rank verb pairs with respect to the strength of their association with a unique discourse relation. We adapted versions of standard lexical association action loves PMI (pointwise mutual information) and their variants, as well as some measurements specific to the association of a causal relation between items (Do","acronyms":[[170,173]],"long-forms":[[175,203]]},{"text":"(2) works7. This algorithm is evaluated in (Jin and Tanaka-Ishii, 2006) using Peking University (PKU) 7Three segmentation criteria are given in (Jin and Tanaka-","acronyms":[[97,100]],"long-forms":[[78,95]]},{"text":"Proc. ACM Multimedia (MILLIMETERS), ACM, Florence, Italy. pp.","acronyms":[[22,24]],"long-forms":[[15,20]]},{"text":"man::n a::d Figure 2: Lexical Only Centering Tree (LOCT) be::v","acronyms":[[50,54]],"long-forms":[[22,48]]},{"text":"This method is much simpler than the ILP method, while it can achieve comparable result on the CLANG (Coach Language) and Query corpus. ","acronyms":[[95,100],[37,40]],"long-forms":[[102,116]]},{"text":"To this end, we employ the  27 Adaptive Hierarchical Density Histograms (AHDH) as visual feature vectors, due to the fact that they  have shown discriminative power between binary complex drawings (Sidiropoulos et al.,","acronyms":[[73,77]],"long-forms":[[31,71]]},{"text":" 3.1 The Beta Process and the Bernoulli process The beta process(BP) (Thibaux and Jordan, 2007; Paisley and Carin, 2009) and the related Indian buf-","acronyms":[[65,67]],"long-forms":[[52,64]]},{"text":"Traditional readability measures for L1 Swedish at the text level include LIX (L?asbarthetsindex, ? Readability index?)","acronyms":[[74,77],[37,39]],"long-forms":[[79,96]]},{"text":"1 Automatic Discovery of Telephoned(me)s Statistical models learnt from data are extensively used in modern automatic speech acknowledging (ASR) systems.","acronyms":[[133,136]],"long-forms":[[103,131]]},{"text":"In Proceedings of the Second Internationale Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 1?5. ","acronyms":[[101,111]],"long-forms":[[22,99]]},{"text":"pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377?392. ","acronyms":[[70,74]],"long-forms":[[7,68]]},{"text":"of two kinds of data: One is the hand built seg-  mentation dictionary (HBSD)  and the other is the  simple noun dictionary for segmentation (SND). ","acronyms":[[142,145],[72,76]],"long-forms":[[101,123],[33,70]]},{"text":"An potency semi-supervised extractor will have good performance over a range of extraction missions and corpora. However, various of the learning methods just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcoming of the prior bootstrapping method, propose a more efficiently and stable rankings method, and consider the effect of different corpora and evaluated metrics.","acronyms":[[336,339],[396,399]],"long-forms":[[341,373],[401,429]]},{"text":"819 location (LO) of the incident (e.g. airport name), and the country (CO) where the incident occurred. ","acronyms":[[72,74],[14,16]],"long-forms":[[63,70],[4,12]]},{"text":"sures for each portion of the results. One is a relevance score (RS) with the target document \u0001 \u0002","acronyms":[[65,67]],"long-forms":[[48,63]]},{"text":"work first arose out of a broader family of approaches to pattern classifier designed familiar as Generalized Probabilistic Descent (GPD) (Katagiri et al, 1991).","acronyms":[[128,131]],"long-forms":[[93,126]]},{"text":"The notation measures the maximum overlap between a hypothesized cluster (HYP) and a corresponding gold standard cluster (GOLD), and computes a weighted averaging across all the HYP clus-","acronyms":[[119,123],[71,74],[173,176]],"long-forms":[[96,109],[49,61]]},{"text":"fast method to forming SVM. SMO breaks the considerable  quadratic programming (QP) optimization problem needed to be resolved in SVM into a series ","acronyms":[[71,73],[21,24],[26,29],[121,124]],"long-forms":[[48,69]]},{"text":"on Chinese FrameNet is divided into the subtasks of boundary identification(BI) and semantic role classification(SRC). ","acronyms":[[113,116],[76,79]],"long-forms":[[84,112],[52,75]]},{"text":"Abstract Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tryouts, which measure human?s verbal ability includ-","acronyms":[[89,91]],"long-forms":[[66,87]]},{"text":"2 valued (ARG2-4, ARGM-DIS (discourse), ARGM-LOC (locative), ARGM-MNR (manner), and ARGM-TMP (temporal)), but given the sizeable number of degrees of free-","acronyms":[[84,92],[40,48],[61,69]],"long-forms":[[94,102],[50,58],[71,77]]},{"text":"notation for real-world applications. In Machine Translate (MT) Quality Estimation (QE), for instance, using human-","acronyms":[[62,64],[86,88]],"long-forms":[[41,60],[66,84]]},{"text":"writes japanese. In Proceedings of the 6th  Workshop on Asian Language Resources (ALR),  pages 101?102.","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":"tasks yet to be tackled by TM but identified as an essential potential application for it (Lewin et al 2008): Cancer Risk Assessment (CRA). Over the","acronyms":[[133,136],[26,28]],"long-forms":[[109,131]]},{"text":"2010). Alternatively, iteratively optimized embeddings such as Skip Gram (SG) modelled (Mikolov et al.,","acronyms":[[74,76]],"long-forms":[[63,72]]},{"text":" 1 Introduction As Machine Translation (MT) systems become widely adopted both for gisting purposes and to","acronyms":[[40,42]],"long-forms":[[19,38]]},{"text":"prove SRL performance.  Template Generation (TG)  Our template generation (TG) algorithm extracts ","acronyms":[[45,47],[6,9],[75,77]],"long-forms":[[24,43],[54,73]]},{"text":"Centro de Sondi f Imagen S.L. (Spain)  - Lead Industrial User  University of Sunderland (UK)  - Academic Research ","acronyms":[[89,91],[25,28]],"long-forms":[[63,87],[31,36]]},{"text":"eling the sequential nature of the output. The constraint satisfaction inference (CSInf) approach (Bosch and Canisius, 2006) improves the","acronyms":[[82,87]],"long-forms":[[47,80]]},{"text":"maries C are overall better for answering questions than summaries B. Comparison between B and C (B-C) precision reminded","acronyms":[[98,101]],"long-forms":[[89,96]]},{"text":"user?s state in the given session. In this research, the support vector machine (SVM) is used as a classifier.","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"suffix and prefix information, as well as information about the sorrounding words and their tags are used to developing a Maximum Entropy (MaxEnt) base Hindi NER system.","acronyms":[[136,142],[156,159]],"long-forms":[[119,134]]},{"text":"tags and word.  Rich Morphological Specifics (Rich-MF): In addition to the elementary features we use the am-","acronyms":[[45,52]],"long-forms":[[16,43]]},{"text":"      Input source sentence (ISS)    ","acronyms":[[29,32]],"long-forms":[[6,27]]},{"text":"Maximum Entropy Markov Model (MEMM)-based word segmenter with Conditional Random Fields (CRF)based chunking; 3.","acronyms":[[89,92]],"long-forms":[[62,87]]},{"text":"of Excellence and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.","acronyms":[[133,137],[65,70]],"long-forms":[[102,131],[22,63]]},{"text":"on Chinese FrameNet is divided into the subtasks of boundary identify(BI) and semantic role classification(SRC). ","acronyms":[[113,116],[76,79]],"long-forms":[[84,112],[52,75]]},{"text":"assumption of which is the stratificational  approach to sentencing analysis pursued by  Functional Sentence Viewpoint (FSP), a  linguistic theory developed by Jan Firbas in the ","acronyms":[[120,123]],"long-forms":[[87,118]]},{"text":"We requiring that the language I has an available Wordnet linked to the Princeton Wordnet (PWN) (Fellbaum, 1998). ","acronyms":[[89,92]],"long-forms":[[70,87]]},{"text":"dleware architecture (Scha?fer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in","acronyms":[[83,86]],"long-forms":[[54,71]]},{"text":"characterize the AAV roles mediating this effect, clooney AAV kind 2 wild-type or mutant genomes were transfected into simian virus 40 (SV40)-transformed hamster cells together with the six HSV replication genes","acronyms":[[139,143],[17,20],[61,64],[193,196]],"long-forms":[[122,137]]},{"text":"As suggested from the tables, the accuracy values of the component classifiers (Ccn and Cen) in CoTrain are almost always higher than those of the corresponding TSVM(CN) and TSVM(EN), based on any machine translation service.","acronyms":[[166,168],[161,165],[174,178],[179,181],[80,83],[88,91]],"long-forms":[]},{"text":"the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on Building and Using Analogue Corpora (BUCC), Nunez, Iceland. ","acronyms":[[145,149]],"long-forms":[[106,143]]},{"text":"extraction (RE) in a bootstrapping framework are regarded as very effective methods for building informations extraction (IE) systems and for readjustment them to new domains (e. g., (Riloff,","acronyms":[[121,123],[12,14]],"long-forms":[[97,119]]},{"text":" 110  ehange(CHA) communication(COMM)  cognition(COG) competition(ED) ","acronyms":[[32,36],[13,16],[49,52],[66,70]],"long-forms":[[18,30],[6,12],[39,48],[54,65]]},{"text":"Figure 8: Alternative English lexical entry for *WORK-  FUNCIONAR  (GENDER and NUMBER) count\/mass (COUNT) and  a trinary distinction of ANIMACY (human, animal, ","acronyms":[[79,85],[56,65],[99,104],[136,143],[68,74]],"long-forms":[[87,92]]},{"text":"recording  information pertinent to treatment of a patient that consists of a number of subsections such as Chief Complaints (CC), History of Present Malady (HPI),","acronyms":[[125,127],[158,161]],"long-forms":[[108,123],[130,156]]},{"text":"PROBING QUESTION (QP) Do you think that looks correct? 4.99% 4.76% 0.731 QUESTION PROMPT (QQ) Any questions? 2.49% 2.24% 0.978","acronyms":[[18,20],[90,92]],"long-forms":[[73,88],[0,16]]},{"text":"uation test sets. Equal Error Rates (EER), where FA = FR, are given in Table 5. Results on EVAL","acronyms":[[54,56],[37,40],[49,51],[91,95]],"long-forms":[[18,35]]},{"text":" 1 Introduction Word sense disambiguation (WSD) is the task of assigning sense tags to foggy lexical items","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":" 2 Data Kinyarwanda (KIN) and Malagasy (MLG) are lowresource, KIN is morphologically rich, and Englishman","acronyms":[[21,24],[40,43],[62,65]],"long-forms":[[8,19],[30,38]]},{"text":"figure 1. To avoid confusion, we refer to this basic  unit throughout as a Temporal Unit (TU). ","acronyms":[[90,92]],"long-forms":[[75,88]]},{"text":" In order to procure labeled instances for training, we decompose the gold standard (GS) events into multiple events with single controversies.","acronyms":[[85,87]],"long-forms":[[70,83]]},{"text":"Due to the existence of CTB-I, we were able to train new automatic Chinese language treatments (CLP) paraphernalia, which crucially use annotated corpora as training","acronyms":[[96,99],[24,29]],"long-forms":[[67,94]]},{"text":" ? Named entity (NE) representation in KBs poses another NED challenge.","acronyms":[[17,19],[39,42],[57,60]],"long-forms":[[3,15]]},{"text":"using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They","acronyms":[[131,134],[10,13],[102,104]],"long-forms":[[110,129],[88,100]]},{"text":"every node be covered by some lexeme.  Partial SemSpec (PSemSpec): The contribution that the lexeme can  make to a sentence SemSpec.","acronyms":[[56,64]],"long-forms":[[39,54]]},{"text":"a t tachment  (PP): the attachment ofa PP  in the sequence VP hip PP (VP = verb  phrase, 51P = noun phrase, PP = prepo-  sitional phrase).","acronyms":[[108,110],[15,17],[39,41],[59,61],[66,68],[70,72]],"long-forms":[[113,119],[75,87]]},{"text":"each frame from each camera view.  3.3 Dyadic Features (DF)  All of the features discussed above are low-level ","acronyms":[[56,58]],"long-forms":[[39,54]]},{"text":"pairs. Transactions of the Association for Computational Linguistic (TACL), 2(10):377?392. ","acronyms":[[70,74]],"long-forms":[[7,68]]},{"text":"pondence between proofs and dependency structures.  Dependency grammar (DG) takes as fundamental  ~This approach of 'normal form parsing' has been ","acronyms":[[72,74]],"long-forms":[[52,70]]},{"text":"Norm = Normalisation of input prior to tagging. SUC = Subset of Stockholm-Umea? ","acronyms":[[48,51]],"long-forms":[[54,73]]},{"text":"1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). At exam-","acronyms":[[97,100]],"long-forms":[[68,95]]},{"text":"mance. The query-based select model utilizes Support Vector Regression (SVR) models to foretold the mean average precision (MAP) of each query","acronyms":[[75,78],[126,129]],"long-forms":[[48,73],[102,124]]},{"text":"Extended Markup Language (XML) is a pro-  posed standard (XML, 1997) specified by the World  Wide Web Consortium (W3C). In XML, tags and ","acronyms":[[114,117]],"long-forms":[[93,112]]},{"text":"Adverb Variants (AdvV) ? Modifier Variation (ModV) ?","acronyms":[[46,50]],"long-forms":[[26,44]]},{"text":"Intersection (I), Union (U), (Koehn et al, 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Mean (PMn) alignment sets where n = 5.","acronyms":[[133,136],[70,73]],"long-forms":[[121,131],[0,12],[18,23],[49,68]]},{"text":"In a spoken dialog system that can handle natural dialogues between a human and a machine, spoken language understanding (SLU) is a crucial component aims at capturing","acronyms":[[125,128]],"long-forms":[[94,123]]},{"text":" 1 Introduction The Semantic Textual Similarity (STS) shared task consists of several data sets of paired passages of","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":"date translations. Notwithstanding the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise lin-","acronyms":[[63,66]],"long-forms":[[41,61]]},{"text":"of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 aljosav@gmail.com     Abstract  We report on a series of experiments aiming at improving the appliance translation of ambig-uous lexical items by utilizing wordnet-based unattended Word Sense Disambiguation (WSD) and comparing its results to three MT scheme. Our experiments are performed for the English-Slovene language pairs using UKB, a freely available graph-based word sense disambiguation system.","acronyms":[[259,262],[299,301],[385,388]],"long-forms":[[232,257]]},{"text":"cal patterns and the impacts of different  constraints that are used to identify the  Complex Predicates (CPs). System ","acronyms":[[106,109]],"long-forms":[[86,104]]},{"text":"The DM is bracketed between two other components, the Input Manager (IM) and the Output Manager (OM). The","acronyms":[[97,99],[4,6],[69,71]],"long-forms":[[81,95],[54,67]]},{"text":"Telecast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). ","acronyms":[[104,106],[122,124]],"long-forms":[[94,102],[112,120]]},{"text":"Abstract  We present a novel method for evaluations  the output of Machine Translation (MT),  based on comparing the dependency ","acronyms":[[87,89]],"long-forms":[[66,85]]},{"text":"and include them in the training data for the SMT.  Corpus Combination (CComb) The easiest method is to use these n newly created paral-","acronyms":[[72,77]],"long-forms":[[52,70]]},{"text":"tent both in their living salas and on their mobile devices. Digital video recorders (DVRs) authorize people to record TV programs from hundreds of chan-","acronyms":[[86,90],[115,117]],"long-forms":[[61,84]]},{"text":"face form via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), HeadDriven Phrase Structure Grammar (HPSG), Com-","acronyms":[[130,133],[173,177]],"long-forms":[[102,128],[136,171]]},{"text":"various evidential features are proposed and  integrated effectively and efficiently through a  Hidden Markov Model (HMM). At addition, a ","acronyms":[[117,120]],"long-forms":[[96,115]]},{"text":"Using the alignments from HIER, we created phrase tables using model odds (MOD), and heuristic extraction on words (HEUR-W), blocking (HEUR-B), and minimal phrases (HEUR-P) as de-","acronyms":[[125,131],[26,30],[84,87],[142,148],[172,178]],"long-forms":[[94,123]]},{"text":"121 domain adjustments algorithm mentioned in (Daume, 2007) based on Maximum Entropy model (MaxEnt) (Ratnaparkhi, 1996).","acronyms":[[91,97]],"long-forms":[[68,83]]},{"text":"nique was used in The MAYO Clinic Vocabulary  Server (MCVS)5, which encodes clinical expressions into medical ontology (SNOMED-CT) and  identifies whether the event is positive or negative.","acronyms":[[120,129],[22,26],[54,58]],"long-forms":[]},{"text":"augment by a set of PRIDES-specific Common  Gateway Interfaces (CGIs), communicates with the  clientele via Hypertext Transport Protocol (HTTP). A ","acronyms":[[137,141],[22,37],[66,70]],"long-forms":[[107,135],[38,64]]},{"text":"67  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"?  Figure 2: Hierarchical Dirichlet Processes (HDP) for WSI. ","acronyms":[[45,48],[54,57]],"long-forms":[[13,43]]},{"text":"The effectiveness of customer care in the email channeling is measured utilise two competing metrics: Average Handling Time (AHT) and Customer Experience Evaluation (CEE).","acronyms":[[120,123],[161,164]],"long-forms":[[97,118],[129,159]]},{"text":"Topic 1 Other modules Salutes(GR) Keep silence(KS) Figure 2: Overview of the information navigation","acronyms":[[31,33],[48,50]],"long-forms":[[22,29],[35,46]]},{"text":"a user interface for the production of wordlevel annotations for an opinion-mining task in the information technology (IT) domain. ","acronyms":[[119,121]],"long-forms":[[95,117]]},{"text":"OWL DL is a logical language that combine the expressivity of OWL2 with the favourable computational properties of Description Logics (DL), specially decidability and monotonicity (Baader et al, 2003).","acronyms":[[136,138],[0,3],[4,6],[63,67]],"long-forms":[[116,134]]},{"text":"Building on the error analysis of the rule-based approach, we replace the rule-based component with support vector machine (SVM) classifiers trained on partial event annotation in the form of","acronyms":[[124,127]],"long-forms":[[100,122]]},{"text":" X '1\"~,. \\[-1  Def in i t ionic 2.2 una N:M sur face coerc ion  (SC)  ru le ix a quadruple (\/,c\/,c~,r) where l and rs ","acronyms":[[62,64]],"long-forms":[[41,59]]},{"text":"(2) works7. This algorithm is assessed in (Jin and Tanaka-Ishii, 2006) using Peking University (PKU) 7Three segmentation criteria are bestowed in (Jin and Tanaka-","acronyms":[[97,100]],"long-forms":[[78,95]]},{"text":" In addition, the user can supply relevance judgements on  any document by clicking Rel (relevant), NRel (not rel-  evant), or PRel (probably relevant).","acronyms":[[100,104],[84,87],[127,131]],"long-forms":[[106,121],[89,97],[133,150]]},{"text":"we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon?s Mechanical Turk (MTurk) to annotate word sense on the English side.","acronyms":[[112,117]],"long-forms":[[95,110]]},{"text":"true supportive (TP) (i.e., a correct match), and an appropriate NNS triple not found in the gold standard set a fraudulent negative (FN) (i.e., an fallacious nonmatch), as shown in Table 4.","acronyms":[[127,129],[15,17],[63,66]],"long-forms":[[111,125],[0,13]]},{"text":" The single product opinion summarizer we consider is the Sentiment Aspect Match model (SAM) described and evaluated in (Lerman et al, 2009).","acronyms":[[88,91]],"long-forms":[[58,80]]},{"text":"With respect to the EUROTRA MT system this has  key implications for the translation between the syntactic  dependency levels - the EUROTRA Relational Structure (ERS)  and the semant ic  level  - the in ter facing  St ructure  (IS).","acronyms":[[167,170],[20,27],[28,30],[231,233]],"long-forms":[[137,165],[205,228]]},{"text":"University of Brighton There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG.","acronyms":[[154,157],[284,287]],"long-forms":[[125,152]]},{"text":"The English supervised NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-alignments sug-","acronyms":[[86,90],[23,25]],"long-forms":[[92,105]]},{"text":"This paper is an at tempt  to provide part  of the basis for a generals theory of forceful  process ing in Machine Trans lat ion  (MT) wi th  relevance to other scopes of Natural  Language ","acronyms":[[128,130]],"long-forms":[[104,125]]},{"text":"Ney discounting and interpolation. The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words.","acronyms":[[90,93]],"long-forms":[[78,88]]},{"text":"source yes better Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer.","acronyms":[[75,78],[53,56],[104,108],[143,145]],"long-forms":[[79,102],[109,141],[146,161]]},{"text":"AT&T Labs-Research Abstract Statistic Machine Translation (SMT) systems are heavily dependent on the qual-","acronyms":[[61,64],[0,4]],"long-forms":[[28,59]]},{"text":"featured to argument ranks models, but also  represent full parsing information as constraints in  integer linear programs (ILP) to resolve label inconsistencies.","acronyms":[[133,136]],"long-forms":[[108,131]]},{"text":"PCFG = Probabilistic  Context-Free Grammar, LM = Bigram Model with Witten-Bell smoothing,  PM = Priority Model. ","acronyms":[[91,93],[0,4],[44,46]],"long-forms":[[96,110],[7,42],[49,61]]},{"text":" More of lexical networks, as networks extracted from real world, are small worlds (SW) networks.","acronyms":[[84,86]],"long-forms":[[70,82]]},{"text":"Figure 3: The system architecture.   CA = communicative acts. ","acronyms":[[37,39]],"long-forms":[[42,59]]},{"text":"conceptualizing the relation of coincidence or proximity with the together  Landmark (LM) when it is conceived as a  point.","acronyms":[[83,85]],"long-forms":[[73,81]]},{"text":" Introduction  The DARPA ATIS Spoken Language System (SLS) task  represents ignificant new challenges for speech and natural ","acronyms":[[54,57],[19,24],[25,29]],"long-forms":[[30,52]]},{"text":"****I TRANSFORPlATICNS **SIW:  S C A N  C-ALLED AT 1 I  ANTFST DREW ONTO SrqSYLLA B (AACC) ,SD= 6 .  RES= 1 1 .","acronyms":[[70,73],[93,95],[86,90]],"long-forms":[[56,69]]},{"text":"cause de la limite des outils informatiques li?s ? yarns traitement  automatique,  ce  qui  rend  difcile  yarns  adh?sion  ?  ses  schmucks?urs  dans  le domaine des nouvelles technique de l'information et de la impart (NTIC). Par cons?quent, un ensemble de recherches scientifques et linguistiques sont lanc?es pour rem?dier ?","acronyms":[[223,227]],"long-forms":[[159,221]]},{"text":" 1 Introduction  Generation of referring expression (GRE) is an  important task in the field of Natural Language ","acronyms":[[53,56]],"long-forms":[[17,51]]},{"text":"In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). ","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":"The overall idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we firstly construct a directed candidate aligning graph (CAG). Section","acronyms":[[190,193]],"long-forms":[[163,188]]},{"text":" ? Reverse Gap (RG), if (i2 + 1) < i3 for OL and if (i6 + 1) < i1 for ODER. (","acronyms":[[16,18]],"long-forms":[[3,14]]},{"text":"  Abstract  Our previous work focuses on combining translation memory (TM) and statistical machine translation  (SMT) when the TM database and the SMT training set are the same.","acronyms":[[71,73],[113,116],[127,129],[147,150]],"long-forms":[[51,69],[79,110]]},{"text":"translation quality include the ridge regression (RR) and support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Scho?lkopf, 2004).","acronyms":[[95,98],[50,52],[85,88]],"long-forms":[[100,122],[32,48],[58,83]]},{"text":"srlrr(r** TRANSPORMATIONS IC*S**  SCAN TELEPHONED FOR 1 I  ANTEST CALLED F O R  5\"SYLLAB \" (AACC) ,SD= 6. RES= 11.","acronyms":[[87,91],[94,96],[101,104]],"long-forms":[[54,67]]},{"text":" 7.1 Supports vector machines  Support vector machines (SVMs) were introduced by (Vapnik, 1995) as an instantiation ","acronyms":[[55,59]],"long-forms":[[30,53]]},{"text":"eling approaches. Table 3 shows the corresponding letter error rates (LER). LERs are more compa-","acronyms":[[70,73],[76,80]],"long-forms":[[50,68]]},{"text":"model in their study is the relative height by range model, where (in our notation): (13) relative height by range (RH-R): ? ","acronyms":[[116,120]],"long-forms":[[90,114]]},{"text":"must be stored at each step of the decode algorithm.  This information includes: the current score (SCORE),  the pointer to the previous lexical item (BPO) on the finest ","acronyms":[[102,107],[153,156]],"long-forms":[[95,100]]},{"text":"301 3. Incorrect Attributes (FA) - these are situations where a value for an attribute has been spec-","acronyms":[[25,27]],"long-forms":[[7,23]]},{"text":"sions to identify stylistic shifts in paraphrase, allowing us to differentiate stylistic properties in the Paraphrase Database (PPDB) with high accuracy. Sec-","acronyms":[[128,132]],"long-forms":[[89,126]]},{"text":"Networks A more similar model to the proposed larger-context recurrent language model is a hierarchical recurrent encoder decoder (HRED) proposed recently by Serban et al (2015).","acronyms":[[131,135]],"long-forms":[[91,129]]},{"text":" A project that is based on a almost similar notion of text meaning representation (TMR) concepts is the ?","acronyms":[[85,88]],"long-forms":[[56,83]]},{"text":"ical analyzer for English (Sekine, 2001)are selected and translating candidates having POS tags other than NN (noun) are discarded. Choices translation","acronyms":[[107,109],[87,90]],"long-forms":[[111,115]]},{"text":"New in three aspects. First, the basic units of their model are elementary discourse units (EDUs) from Rhetorical Structure Theory (RST) (Mann","acronyms":[[92,96],[132,135]],"long-forms":[[64,90],[103,130]]},{"text":"System and Datasets We use the Moses phrasebased MT system (Koehn et al, 2007) and consider Urdu?English (UR?EN), Chinese?English (ZH?EN) translation, and Arabic?English","acronyms":[[106,111]],"long-forms":[[92,104]]},{"text":"Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion annotation process introduced by Pyysalo et al","acronyms":[[177,179],[15,22],[41,47],[64,68],[84,92],[111,116],[151,159]],"long-forms":[[180,199],[32,39],[48,54],[69,82],[93,101],[117,142],[160,168]]},{"text":" ? Self-training Segmenters (STS): two variant models were defined by the approach re-","acronyms":[[29,32]],"long-forms":[[17,27]]},{"text":"1 Introduction ? Language Model (LM) Grow? refers to adding","acronyms":[[33,35]],"long-forms":[[17,31]]},{"text":"The starting point for the approach followed here was a dissatisfaction with certain  aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and  implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as ra- ","acronyms":[[216,219],[188,192],[229,236]],"long-forms":[[194,214]]},{"text":"Utilised either method of uncertainty sampling,  the computational cost of selecting an example from  T candidates is: O(TD) where D is the number of  model parameters.","acronyms":[[116,118]],"long-forms":[[97,109]]},{"text":"Brighton, BN1 9QN, England  Abstract  Generalised phrase structure grammars (GPSG's)  appear to furnishes a means by which the syntactic ","acronyms":[[77,83],[14,17],[10,13]],"long-forms":[[38,75]]},{"text":"notator. For brevity, we only considered PubMed as the source DB, and named entity recognition (NER)type annotations, which may be simply represented","acronyms":[[96,99]],"long-forms":[[70,94]]},{"text":"coding-related concepts that appear in the EHR.  We use General Equivalence Mappings (GEMs) between ICD-9 and ICD-10 codes (CMS, 2014)","acronyms":[[86,90],[43,46],[100,105],[110,116],[124,127]],"long-forms":[[56,84]]},{"text":"The \u0002 grams in an utterance SSG can be extracted by converting it to a finis state transducer (FST), \f\u000e  .","acronyms":[[96,99],[28,31]],"long-forms":[[71,94]]},{"text":" For this reason, NIST assessors not only marked  the segments shared between system units (SU)  and model units (MU), they also indicated the ","acronyms":[[92,94],[18,22],[114,116]],"long-forms":[[78,90],[101,112]]},{"text":"LDEP(NEXT) + Table 1: History-based features (TOP = token on top of stack; NEXT = next token in input buffer; HEAD(w) = head of w; LDEP(w) = leftmost depen-","acronyms":[[75,79],[0,4],[46,49],[5,9],[110,117],[131,138]],"long-forms":[[82,92],[52,64],[120,129],[141,155]]},{"text":"a wordbreak (WB). In other words, we model Chinese word segmentation as wordbreak (WB) identification which takes all CB?s as candidates and","acronyms":[[83,85],[13,15],[118,122]],"long-forms":[[72,81],[2,11]]},{"text":"face form via the application of a set of grammar rules basis on singular linguistic theories, e.g. Lexical Functional Grammar (LFG), HeadDriven Phrase Structure Grammar (HPSG), Com-","acronyms":[[130,133],[173,177]],"long-forms":[[102,128],[136,171]]},{"text":"ging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4)6. Ninety percent of the","acronyms":[[72,77]],"long-forms":[[52,70]]},{"text":"tures. Since the Parallel Align TreeBank is a subset of the Chinamen TreeBank (CTB) 8.0, we automatically parsed the CTB 8.0 by doing a 10-","acronyms":[[82,85],[120,123]],"long-forms":[[64,80]]},{"text":"give an indication as to what the vibhakti\/TAM are.  Words with PSP (postposition) and NST (noun with spatial and temporal properties) labels are typically","acronyms":[[64,67],[34,46],[87,90]],"long-forms":[[69,81]]},{"text":"ity. Across Proceedings of Treebanks and Linguistic Theories (TLT) 2003, Vaxjo, Sweden. ","acronyms":[[58,61],[71,74]],"long-forms":[[23,56]]},{"text":" Figure 5 also gives a speed comparisons of our methodology to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0?","acronyms":[[79,81]],"long-forms":[[59,77]]},{"text":" Data set We appraisal segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtains","acronyms":[[76,79]],"long-forms":[[59,74]]},{"text":"maries that are too specific. In this paper, we propose a natural language generation (NLG) model for the automatic creation of indicative multidoc-","acronyms":[[87,90]],"long-forms":[[58,85]]},{"text":"editor, Proceedings of the Sixth International Lectures on Language Resources and Appraisal (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http:\/\/www.lrec-","acronyms":[[172,176],[96,103]],"long-forms":[[131,170],[61,94]]},{"text":" 1 Introduction Mental State Verbs (MSVs), such as think, savoir, and want, are very frequent in child-directed lan-","acronyms":[[36,40]],"long-forms":[[16,34]]},{"text":"In this paper we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank, with about 1 mil-","acronyms":[[100,103]],"long-forms":[[79,98]]},{"text":"set we formed on both glosses and statistic MT data, for the OnWN and FNWN test sets we qualified on glosses only (OnWN), and for the SMT test set we trained on statistical MT data only (MTnews and","acronyms":[[116,120],[47,49],[64,68],[73,77],[135,138],[174,176],[188,194]],"long-forms":[[99,114]]},{"text":"+), and (?) appoints Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the discrepancy in system","acronyms":[[91,93],[104,106],[125,127]],"long-forms":[[94,102],[107,123],[128,143]]},{"text":" 2 Related Work Sentiment analysis (SA) and related topics have been extensively studied in recent years.","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":" Unknown  1976 \"The Lexicography Informati on Sys tern (LEXIS) o f  the Bundeswher  Language Service,\" i n  Machine Assisted \"h-ansl ation i n  West ","acronyms":[[56,61]],"long-forms":[[20,49]]},{"text":"the source and target sides are lexicons (terminals) 2) Unlexicalized (ULex): all leaf nodes in both the 46","acronyms":[[71,75]],"long-forms":[[56,69]]},{"text":"2.2 Conditional Random Fields Conditional random field (CRF) was an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs) that was firstly","acronyms":[[109,113],[56,59],[141,145]],"long-forms":[[86,107],[30,54],[119,139]]},{"text":"TDP (target only) 62.60 33.04 Table 2: Conclusions Generalized average precision (GAP) is a more precise measures than P","acronyms":[[78,81],[0,3]],"long-forms":[[47,76]]},{"text":"former networks for image recognise. Bulletin of the International Statistical Institute (ISI). ","acronyms":[[92,95]],"long-forms":[[55,90]]},{"text":" 1997) has led us to employ, among other param-  eters, mutual information (MI) bits of individ-  ual characters derived from large hierarchically ","acronyms":[[76,78]],"long-forms":[[56,74]]},{"text":"This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To","acronyms":[[86,90]],"long-forms":[[61,84]]},{"text":"(adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and","acronyms":[[102,105],[118,122],[14,17],[29,33],[46,49],[65,68],[139,142],[156,160]],"long-forms":[[107,115],[124,136],[19,26],[35,43],[51,62],[70,99],[144,153],[162,173]]},{"text":" 1 In t roduct ion   For most natural language processing (NLP) systems,  thesauri comprise indispensable linguistic knowledge.","acronyms":[[59,62]],"long-forms":[[30,57]]},{"text":"and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj). For each","acronyms":[[100,103],[122,127],[31,34],[82,87]],"long-forms":[[90,98],[109,120],[8,29],[66,80]]},{"text":"108   Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 1, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1?47.","acronyms":[[74,78],[51,54]],"long-forms":[[55,72]]},{"text":"The development of efficient estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al.,","acronyms":[[147,150]],"long-forms":[[117,145]]},{"text":"followed by a verbal suffix\". This is to cover general  verb inflection, for both auxiliaries (AUX +) and main  verbs (AUX -).","acronyms":[[95,100],[119,122]],"long-forms":[[82,93]]},{"text":"the preferred word attachments.  EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents","acronyms":[[49,52]],"long-forms":[[33,47]]},{"text":"we used two other error-annotated learner corpora.  The NUS Corpus of Learner English (NUCLE) contains one million words of academic writing","acronyms":[[87,92]],"long-forms":[[56,85]]},{"text":"and embedded phrase levels: ? Object reordering (ObjR), in which the object and their dependents are moved in front","acronyms":[[49,53]],"long-forms":[[30,47]]},{"text":"  4.1 Speech recognition   The automatic speech recognition module (ASR)  is based on the Sphinx 4 system (Lamere et al, ","acronyms":[[68,71]],"long-forms":[[31,59]]},{"text":"Center for Language Technology. Subsequently accomplishing the task concerning named entity (NE)  identification, we go on studying identification ","acronyms":[[86,88]],"long-forms":[[72,84]]},{"text":"     1 Introduction  Most of the natural language generation (NLG)  components in current dialog plan are imple-","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"In Proc. 6th Canadian Conf on AI (CSCSI-86), pp. 78?83.","acronyms":[[34,42],[45,47]],"long-forms":[[13,32]]},{"text":"analog to how Iida et al (2011) computed features to present to their classifier: namely Ling (linguistic idiosyncrasies), TaskSp (task specific features), and Gaze (from SV only).","acronyms":[[118,124],[166,168]],"long-forms":[[126,139]]},{"text":"siderable progress. The bakeoff series hosted by  the Chinese Information Processing Society (CIPS)  and ACL SIGHAN shows that an F measure of ","acronyms":[[94,98],[105,108],[109,115]],"long-forms":[[54,92]]},{"text":" Another syntactic phenomena crucial to the parser is known as the complex NP  Constraint (CNPC) (Radford 1981); i.e., no transformation rule can move any element  out of a complex NP, where a complex NP (CNP) is an NP containing a relative clause.","acronyms":[[91,95],[181,183],[201,203],[205,208],[216,218],[75,77]],"long-forms":[[79,89]]},{"text":"tions and keying in on student language to promote self-explanation of concepts, and its curriculum is based on the Full Option Science System (FOSS) 1 a proven system for inquiry based learning.","acronyms":[[144,148]],"long-forms":[[116,142]]},{"text":" 3.1 Sentence Splitting (SS) Sentence Splitting (SS) is the rewriting of a sentence by rupture it into two or more penalties,","acronyms":[[49,51],[25,27]],"long-forms":[[29,47],[5,23]]},{"text":"and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these","acronyms":[[59,61],[31,33]],"long-forms":[[49,57],[18,28]]},{"text":"(MBF).  4 Multilingual PRF (MultiPRF) The schematic of the MultiPRF approach is exhibited","acronyms":[[28,36],[1,4],[59,67]],"long-forms":[[10,26]]},{"text":"measure specifically their performance, a selection of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the","acronyms":[[127,132],[87,90],[104,107],[152,157]],"long-forms":[[110,125],[75,85],[93,102],[135,150]]},{"text":"vide a significant degree of monitoring. Perhaps nowhere is this observation more keenly  felt than in weak lexical ontologies iike Princeton WordNet (PWN). In PWN [1], ","acronyms":[[148,151],[157,160]],"long-forms":[[129,146]]},{"text":"ing diverse methods: The methods respectively sans prediction(NP), with prediction(P),  with prediction and feedback(PF) only using term  frequency (TM), and with prediction and feed-","acronyms":[[122,124],[67,69],[154,156]],"long-forms":[[98,120],[77,87],[137,152]]},{"text":"A similar embedding means, called ? Global Vector (GloVe)?, was","acronyms":[[52,57]],"long-forms":[[37,50]]},{"text":"tive of the gold standard data.  Finally, the alignment error rate (AER) is lower (and hence better) for English?French than Romanian?","acronyms":[[68,71]],"long-forms":[[46,66]]},{"text":"Discovery of ambiguous and unambiguous discourse connectives via annotation projection. Into Proceedings of Seminars on Annotation and Exploitation of Parallel Corpora (AEPC), pages 83?82, Tartu, Estonia.","acronyms":[[167,171]],"long-forms":[[118,165]]},{"text":"clude examples such as Facebook AI Research?s challenge problems for AI-complete QA (Weston et al, 2015) and the Allen Institute for AI?s (AI2) Aristo project (Clark, 2015) along with its recently","acronyms":[[139,142],[81,83]],"long-forms":[[113,137]]},{"text":"  Figure 1: Example of a BDT sentence in the CoNLL-X format  (V = main verb, AUXV = auxiliary sneezing, CONJ = cooperating, REL = subordinated clause, CMP = completive, ccomp_obj =  clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd persons sing.,","acronyms":[[77,81],[100,104],[147,150]],"long-forms":[[84,98],[107,118],[153,163]]},{"text":"by using RBM to implement the middle layers,  since RBM can be learned very quickly by the  Contrastive Divergence (CD) approach. ","acronyms":[[116,118],[9,12],[52,55]],"long-forms":[[92,114]]},{"text":"(SBAR-TMP (IN after) (S (NP (DT the) (NN sale)) (VP (AUX is) (VP (VBN completed))) ))))))))","acronyms":[[66,69],[1,9],[25,27],[29,31],[38,40],[49,51],[53,56],[62,64]],"long-forms":[]},{"text":"ment. In Lawsuits of the 34th Annual Meeting of the Cognitive Science Society (CogSci), Sapporo. ","acronyms":[[82,88]],"long-forms":[[55,80]]},{"text":"Name Discriminating by Clustering Similar Contexts, Proceedings of the World Wide Web Conference (WWW). ","acronyms":[[98,101]],"long-forms":[[71,85]]},{"text":" NB-B utilized full Bayesian inference and NB-M uses Maximum a posteriori (MAP). ","acronyms":[[71,74]],"long-forms":[[49,69]]},{"text":"In Proc. of the Association for Computational Linguistics (ACL), page 523?530.","acronyms":[[59,62]],"long-forms":[[16,57]]},{"text":"Pr(f |e) Pr(e) (2) where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This ap-","acronyms":[[99,101]],"long-forms":[[83,97]]},{"text":" Unknowns  1976 \"The Lexicography Informati on Sys tern (LEXIS) o f  the Bundeswher  Language Service,\" i n  Machine Assisting \"h-ansl ation i n  West ","acronyms":[[56,61]],"long-forms":[[20,49]]},{"text":"Bases on these two conditions we have come up with a linear combination of two expense components, similar to Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998).","acronyms":[[135,138]],"long-forms":[[107,133]]},{"text":"imated conversational characteristics from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506?513.","acronyms":[[101,104],[71,74]],"long-forms":[[75,99]]},{"text":"for Reply Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) inside the Hamad Bin Khalifa University and","acronyms":[[131,135],[19,23],[81,84]],"long-forms":[[95,129],[51,79],[0,17]]},{"text":"=   where,   ij  is the term frequency(TF) of the j-th word  in the vocabulary in the document , i.e. the ","acronyms":[[39,41]],"long-forms":[[24,37]]},{"text":"2 Textual Entailment for MT Evaluation 2.1 Textual Entailment v. MT Evaluated Textual entailment (TE) was introduced by Dagan et al. (","acronyms":[[100,102],[25,27],[66,68]],"long-forms":[[80,98]]},{"text":"cal browsing. We examine this by using three different parsers in phase 0: (i) MS (Marecek and Straka, 657","acronyms":[[77,79]],"long-forms":[[81,99]]},{"text":"Statistical machine translation based on LDA.  In Universal Communication Symposium (IUCS), 2010 4th International, pages 286?290.","acronyms":[[85,89],[41,44]],"long-forms":[[47,83]]},{"text":"exp (14)    Short word difference punishments (SWDP): a  good translation should have roughly the same ","acronyms":[[43,47]],"long-forms":[[12,41]]},{"text":"of spoken dialogue systems is database recapture.  Some IVR (interactive voice response) systems using the speech acknowledge technology are being put","acronyms":[[56,59]],"long-forms":[[61,87]]},{"text":"In Proc. of Seventh Text REtrieval Conference (TREC-7). ","acronyms":[[47,53]],"long-forms":[[12,45]]},{"text":"ple need access to information anywhere, anytime. The  Adaptive Information Management (AIM) service in the  FASiL VPA seeks to automatically prioritise and pre-","acronyms":[[88,91],[109,114],[115,118]],"long-forms":[[55,86]]},{"text":"In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), page 581?588, Granada.","acronyms":[[91,95]],"long-forms":[[56,74]]},{"text":"construct the desired model in a way that allows efficient inference, even for large datasets, using determinantal point processes (DPPs). We begin","acronyms":[[132,136]],"long-forms":[[101,130]]},{"text":"Then, a supervised machine learning algorithm (e.grammes., Helps Vector Machines  (SVM), na?ve Bayesian classifier (NB)) is applied  to the training examples to build a classifier that is ","acronyms":[[79,82],[112,114]],"long-forms":[[85,99],[53,76]]},{"text":"of Data-to-Speech systems have been and are be-  ing developed on the basis of D2S. Examples are  the Dial Your Disc (DYD)-system, which presents  information in English about Mozart compositions ","acronyms":[[118,121],[79,82]],"long-forms":[[102,116]]},{"text":"]} (7) This optimization can be performed using the wait maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).","acronyms":[[78,80]],"long-forms":[[52,76]]},{"text":"11  Trials of the 2014 Conference on Empirical Methods in Natural Parlance Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw  Figure 26 ","acronyms":[[62,64],[0,4],[5,7],[27,31],[32,34],[57,61]],"long-forms":[[65,67],[8,10],[36,39]]},{"text":"This investigative has been backed in part by DARPA (under contract number FA8750-13-2-0005), NIH (NICHD award 1R01HD07532801), Keck Foundation (DT123107), NSF (IIS0835797), and","acronyms":[[92,95],[44,49],[73,75],[143,145],[154,157]],"long-forms":[[97,102]]},{"text":"e-mail: lynet te@goldilocks.lcs.mit.edu  ABSTRACT  The Air Travel Information System (ATIS) domain serves as  the common task for DARPA spoken language system re- ","acronyms":[[86,90],[130,135]],"long-forms":[[55,84]]},{"text":"\\[ Extra~:~nouns I IE~rac~'~ n?unsl i Computation f requ~ vectors (FreqVa) I ICalculating frequency vectors (FreqVe)l  ._1 Calculated similarity I ","acronyms":[[109,115],[67,73]],"long-forms":[[90,107],[50,65]]},{"text":"it is the in f in i t ive  forme of a verb. Though SO, it is to be attached to the  parsing tree, and given the additionql feature MVB (main verb). The ongoing ","acronyms":[[126,129],[46,48]],"long-forms":[[131,140]]},{"text":"Constituents are tagged with IsA class labels from a large, automatically extracted lexicon, using a probabilistic context free grammar (PCFG). ","acronyms":[[137,141]],"long-forms":[[101,135]]},{"text":"We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven","acronyms":[[60,62]],"long-forms":[[35,58]]},{"text":"Table 6: GAP scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston","acronyms":[[43,45],[55,57],[70,72],[9,12],[86,88],[100,102]],"long-forms":[[48,53],[60,68],[75,84],[91,98],[105,121]]},{"text":"2   TASK A: Matter Generation from Paragraphs  1.1   Task Definition  The Question Generation from Paragraphs (QGP) task difficulties participants to  engenders a list of 6 questions from a given input paragraph.","acronyms":[[113,116]],"long-forms":[[76,111]]},{"text":"utterance. The interface default allowing this is called Noun meaning extended (NMExt)13 NMExt: Noun(u), sem(u) is ?","acronyms":[[80,85],[89,94]],"long-forms":[[57,78]]},{"text":"The described research is undertaken in the course of the development of human computer interfaces for natural interaction in Virtual Reality (VR). Conducting empirical inves-","acronyms":[[143,145]],"long-forms":[[126,141]]},{"text":"rameter set to decode WSMT 2006 Europal test set (TEST1) and used the second on WSMT news remarks proof set 2007 (TEST2)6. Table 6 shows the","acronyms":[[116,121],[80,84],[22,26],[50,55]],"long-forms":[[101,114],[40,48]]},{"text":"harnessed in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that utilize ranking","acronyms":[[112,117]],"long-forms":[[90,110]]},{"text":"ability and to give analytical ideas into the  idiosyncrasies. Classification Accuracy (CAcc), the  percentage of the correctly labeled instances over ","acronyms":[[85,89]],"long-forms":[[60,83]]},{"text":"Abstract This paper explores the use of set expanding (SE) to improve question answering (QA) when the projected answer is a list of entities","acronyms":[[90,92],[55,57]],"long-forms":[[70,88],[40,53]]},{"text":"5 Conclusions We have presented a sequential semantic role labeling system for the Semeval-2007 task 17 (SRL). ","acronyms":[[105,108]],"long-forms":[[68,95]]},{"text":"mentation in (Zhang et al, 2006) 2.2 OOV Recognition with Accessor Variety Accessor variety (AV) (Feng et al, 2004) is a mere and effectiveness unsupervised method for extrac-","acronyms":[[93,95],[37,40]],"long-forms":[[75,91]]},{"text":"2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR),41(2):10. ","acronyms":[[66,70],[43,46]],"long-forms":[[47,64]]},{"text":"l (CAUSER sally-l)  (OBJECT paint-l)  (PATH (path-1 (DESTINATION wall-l))))  Sally sprayed the wall with paint.","acronyms":[[39,43],[10,17],[28,35]],"long-forms":[[45,51]]},{"text":"NEARING A PROJECT PROPOSAL TO OTTA BOARD  As the  Urns,,. C6ngress Office of Techdblogy Assessment (OTA) planning  sttidy on te lecomunica t ions ,  computers and information p o l i c i e s  approaches ","acronyms":[[100,103],[30,34]],"long-forms":[[67,98]]},{"text":" 1 I n t roduct ion   The development of Natural Language (NL) systems  for data retrieval has been a central issue in NL Pro- ","acronyms":[[59,61],[119,121]],"long-forms":[[41,57]]},{"text":"gathered training data from randomness texts for the set of most frequently occurring noun, adjective, and verb genus in the Brown Corpus (BC). These word","acronyms":[[137,139]],"long-forms":[[123,135]]},{"text":"crafted features, lexicons, and grammars.  Meanwhile, recurrent neural networks (RNNs) what are the momentous cities in utah ?","acronyms":[[81,85]],"long-forms":[[54,79]]},{"text":"We trained the UKP machine learning classifier originally developed for the Semantic Textual Similarity (STS) task at SemEval-2012 (B?r et al 2012) on the averaged binary and senary judge-","acronyms":[[105,108],[15,18],[118,130]],"long-forms":[[76,103]]},{"text":"by rank the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Recovery (RUR). In particular,","acronyms":[[129,132]],"long-forms":[[101,127]]},{"text":"In this paper, we further execute type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).","acronyms":[[82,84],[112,114]],"long-forms":[[86,106],[116,132]]},{"text":" This named-entity tagger program is based on a first order Maximum Entropy Markov Model (MEMM) and is described in Yoshida and Tsujii (2007).","acronyms":[[90,94]],"long-forms":[[60,88]]},{"text":"Here, we assume :  P(t i I grammes~ )  IP(tt I Pi-i PiWi) Pi-I piwi f dp  \\[ P(ti \/ Pi ) Pi-I Pi Wi ~ 1I) ","acronyms":[[33,35]],"long-forms":[[39,43]]},{"text":" 2. All the named entities(NE) in the question are extracted as NE set.","acronyms":[[27,29],[64,66]],"long-forms":[[12,25]]},{"text":"Several different learning algorithms have been explored for text classification (Dumais et al 1998) and support vector machines (SVMs) (Vapnik, 1995) were found to be the most computationally ef-","acronyms":[[130,134]],"long-forms":[[105,128]]},{"text":"pute probability scores of word sequences. The general conversational language model (LM) is bases on data from the SWITCHBOARD corpus and a tiny","acronyms":[[86,88]],"long-forms":[[70,84]]},{"text":" ? Reduce Left - X (RL) : Pops the top two nodes from the stack, combines them into a new node","acronyms":[[20,22]],"long-forms":[[3,14]]},{"text":" MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information.","acronyms":[[68,71],[1,4],[22,25],[43,46]],"long-forms":[[52,60],[11,20],[28,41]]},{"text":" The third is the confusion between adjective  (JJ) and noun (NN), when the word in question  modifies a noun that immediately follows.","acronyms":[[62,64],[48,50]],"long-forms":[[56,60],[36,45]]},{"text":"We evaluated our parsers using standard labeled accuracy scores (LAS) and unlabeled accuracy scores (UAS) excluding punctuation.","acronyms":[[101,104],[64,68]],"long-forms":[[74,99],[40,63]]},{"text":"words, from this the subscript b (bag-of-words).  Subtree Kernel (SbtK) is one of the simplest tree kernels as it only generates complete subtrees, i.e.,","acronyms":[[66,70]],"long-forms":[[50,64]]},{"text":"to) Peet a friend?  Figure 10: An argument post particle phrase (PP) (upper) and an adjunct PP (lower).","acronyms":[[65,67],[92,94]],"long-forms":[[48,63]]},{"text":"Table 5: Sample Hindi complex predicates   5 Corpus and pre-processing  Basic Travel Expressions Corpus (BTEC)  containing travel conversations is used for ","acronyms":[[105,109]],"long-forms":[[72,103]]},{"text":"2 Background  2.1 Gene Expression Programming  Gene Words Emissions (GEP), first introduced by (Ferreira 2001), is an evolutionary algo-","acronyms":[[76,79]],"long-forms":[[47,74]]},{"text":"I first  describe the language used to characterize the semantics of  lexical items, SEL (for Simple Episodic Logic), then the  syntax and interpretation f logical forms.","acronyms":[[85,88]],"long-forms":[[94,115]]},{"text":"dictionary (viz. \/hu:pana-taNa\/ and \/Fi:tiki-RaNa\/), I also searched the Ma?ori Broadcast Corpus (MBC) for words ending as if they had gerundial suffixes","acronyms":[[98,101]],"long-forms":[[73,96]]},{"text":"  2 NII-Speech Resources Consortium  The National Institute of Informatics (NII) was  founded in Tokyo, Japan in April 2000 as an in-","acronyms":[[76,79],[4,7]],"long-forms":[[41,74]]},{"text":"knowledge powered model to several baselines.  Random Guessing Model (RG). Random guess is","acronyms":[[67,69]],"long-forms":[[47,59]]},{"text":"tf iD j  is the inverse documented frequency(IDF)  of the j-th word calculated as below: ","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"      The system integrates both dependency parse  tree pattern and semantic role labeler (SRL) results  of each inlet sentences when extracting the triples.","acronyms":[[91,94]],"long-forms":[[68,89]]},{"text":"3.2 Data Normalization The ACL shared task is very close in form and content to the Final Text Editions (FTE) task of the TCSTAR (TC-STAR, 2004) evaluation.","acronyms":[[105,108],[27,30],[130,137]],"long-forms":[[84,103],[122,128]]},{"text":"Section 2  introduces some relevant work in IR and  question answering (QA). Section 3 discussions about ","acronyms":[[72,74],[44,46]],"long-forms":[[52,70]]},{"text":"NST = Noun Stem V-FLEX = Verbally Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Strained ","acronyms":[[98,100],[131,133],[0,3],[16,22],[43,46],[57,63],[88,90],[119,121],[142,144],[162,164],[174,176]],"long-forms":[[93,97],[136,140],[6,15],[25,41],[49,56],[66,86],[113,118],[124,130],[147,161],[167,173],[179,184]]},{"text":"1979). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.","acronyms":[[75,78],[107,110]],"long-forms":[[46,73]]},{"text":"capture all the types of entities. Typical structures  of Chino person behalf (CN), location name (LN)  and organization name (ON) are as follows: ","acronyms":[[79,81],[99,101]],"long-forms":[[58,77],[84,97]]},{"text":"We tested two differing algorithms on text from the Wall Street  Diaries (WSJ). Using BBN's party of speech tagger (POST), tagged  text was parsed using the full reunification grammar of Delphi to fred ","acronyms":[[115,119],[74,77]],"long-forms":[[92,113],[52,72]]},{"text":"3 Bayes ian  network   A Bayes ian  networking  (Pearl, 1988), or  Bayesian 1)el|el nel;cooperation (BBN),  eonsisi;s of a sol;  of var iab les  and a sel; of d i rec ted  edges  (:on- ","acronyms":[[92,95]],"long-forms":[[65,90]]},{"text":"computational devices for natural language processing.  called active production networks (APNs), and explore  how certain kinds of movement are handled.","acronyms":[[91,95]],"long-forms":[[63,89]]},{"text":"search tool, but it is insufficient for domain-  specific text, such as that faced in  the MUCs (Message Fathom Confer-  ences).","acronyms":[[97,101]],"long-forms":[[103,132]]},{"text":"Overall recall and precision were 0.80 and 0.87 for drugs, and 0.56 and 0.85 for adverse events. 1 Introduction  It is well-known that adverse drug reactions (ADRs) are an important health problem. Indeed, ADRs are the 4th cause of death in hospitalized patients (Wester et al.,","acronyms":[[159,163],[206,210]],"long-forms":[[135,157]]},{"text":" ? Modifier substitutes (M-Sub) :  t2 is a substitution of t~ if and only if : ","acronyms":[[26,31]],"long-forms":[[3,24]]},{"text":" The actual performance of a system is measured in terms of detection error tradeoff (DET) curves and the minimalist normalized expenditures.","acronyms":[[86,89]],"long-forms":[[60,84]]},{"text":"approaches (Gasic and Young, 2011; Lee and  Eskenazi, 2012; Williams, 2010; Young et al  2010) and Bayesian network (BN)-based  methods (Raux and Ma, 2011; Thomson and ","acronyms":[[117,119]],"long-forms":[[99,115]]},{"text":"z that charting sentences x to logical expressions z. We learns this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xi, zi)|i = 1 . . .","acronyms":[[106,110]],"long-forms":[[87,104]]},{"text":"form (FFC); from this decision he\/she formulates a natural language utterance with certain features including the sentence type (SeTp) the subject type (SuTp) and punctuation (Punct).","acronyms":[[129,133],[6,9],[153,157],[176,181]],"long-forms":[[114,127],[139,151],[163,174]]},{"text":"It predicts four type of reordering patterns, namely MA (monotone adjacent), MG (monotone gap), RA (reverse adjacent), and RG (reverse gap).","acronyms":[[96,98],[53,55],[77,79],[123,125]],"long-forms":[[100,116],[57,74],[81,93],[127,138]]},{"text":"Vector Machines (SVM) with radial basis kernel, Na??ve Bayes (NB), J48 Decision Trees (DT), and Neural Networks (NN) with back propagation. In","acronyms":[[113,115],[17,20],[62,64],[87,89]],"long-forms":[[96,111],[0,15],[48,60],[71,85]]},{"text":"factoid ones - as well as new elements ? such as  expected polarity type (EPT). Though, opi-","acronyms":[[74,77]],"long-forms":[[50,72]]},{"text":" 6. Decision Tree (DT) - with 12,782 MWEs of D5.","acronyms":[[19,21]],"long-forms":[[4,17]]},{"text":"TV programs they watch.  Collaborate filtering (CF) (Resnick et al, 1994; Breese et al, 1998) and content-based (or","acronyms":[[50,52],[0,2]],"long-forms":[[25,48]]},{"text":"  Proceedings of the Thirteen Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54, Boulder, Colorado, Juni 2009.","acronyms":[[87,92]],"long-forms":[[46,85]]},{"text":"KEY: Number of discussions and posts on the topic (Disk, Posts).  Number of authors (NumA). Posts per author (P\/una).","acronyms":[[86,90],[111,114]],"long-forms":[[67,84],[93,109]]},{"text":"of units. '  i Note that unlike RST, Veins Theory (VT) is not  concerned with the type of relations which hold ","acronyms":[[51,53],[32,35]],"long-forms":[[37,49]]},{"text":"ticular, this includes a model of the grounding process (Clark, 1996) that involves recognition and construction of common ground units (CGUs) (see (Traum, 2003)). ","acronyms":[[137,141]],"long-forms":[[116,135]]},{"text":" 5 Conclusions and Futuristic Work Distributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Modelled (CDSM) that effectively encode structure information and distributional semantics in tractable 2-","acronyms":[[59,62],[132,136]],"long-forms":[[31,57],[85,129]]},{"text":"not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP)?based search algorithm for statistical MT that monotonically translates the input sentence from left to right.","acronyms":[[108,110],[151,153]],"long-forms":[[87,106]]},{"text":"Therefore, we refined our reference performance level by combining the ME models (MEM) and handcrafted models (HCM). Suppose the score of a","acronyms":[[111,114],[82,85]],"long-forms":[[91,109],[71,79]]},{"text":"found in each row, as well as the level of granularity of analysis in each row.3 2KEY: ABS=abstract, COM=completive, CL=classifier, DEM=demonstrative, E=ergative, EV=evidential, S=singular,","acronyms":[[87,90],[101,104],[117,119]],"long-forms":[[91,99],[105,115],[120,130]]},{"text":"Based on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their com-","acronyms":[[52,58],[9,17]],"long-forms":[[60,75]]},{"text":"3.2 Result of Chinese NER We evaluated our named entity recognizer on the SIGHAN Microsoft Research Asia(MSRA) corpus in both closed and open track.","acronyms":[[105,109],[22,25],[74,80]],"long-forms":[[81,104]]},{"text":"11  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"Semantic Specialization (ISS) (Girju, Badulescu,  and Moldovan, 2006), Na?ve Bayes (NB) 3  and  Maximum Entropy (ME)4.  ","acronyms":[[113,115],[25,28],[84,86]],"long-forms":[[96,111],[0,23],[71,82]]},{"text":"~ '1  procedural component  Q data edifice  SSP = SemanticJSyntacticJPhonological  Figl~e 1: The SYNPHONICS Formulator ","acronyms":[[46,49],[99,109]],"long-forms":[[52,83]]},{"text":"Such vectors can be used to perform all standard linear algebra operations applied in vector-based semantics: Measuring the cosine of the angle between vectors, applying singular value decomposition (SVD) to the whole matrix, and so on.","acronyms":[[200,203]],"long-forms":[[170,198]]},{"text":"First, we investigate how laypeople intuitively accepted metaphor by conducting Amazon Mechanical Turk (MTurk) experiments.","acronyms":[[105,110]],"long-forms":[[88,103]]},{"text":"tools for the Indian Languages. For Telugu, though a Part Of Speech(POS) Tagger for Telugu, is available, the accuracy is less when compared to English","acronyms":[[68,71]],"long-forms":[[53,66]]},{"text":"They used a statistical finite-state transducer (SFST) as a generative model and a support vector machines (SVM) and conditional random spheres (CRF) as discrim-","acronyms":[[107,110],[49,53],[143,146]],"long-forms":[[83,105],[12,47],[116,141]]},{"text":" 2.1 Named Entity Recognition We regard named entity recognising (NER) as a standalone task, independent of language identification.","acronyms":[[66,69]],"long-forms":[[40,64]]},{"text":" 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meanings of a sentence is repre-","acronyms":[[52,55]],"long-forms":[[23,50]]},{"text":"Nincc NIA ~ \\]laS started moving from toy  problems to ,'eal applications one of the biggest  difficully has been Knowledge Acquisition (KA)  of different lypes (lexical, grammatical, domain ","acronyms":[[137,139]],"long-forms":[[114,135]]},{"text":" At the shallowest level of attachment we find the conjunctions (CONJ+) +\u0000 w+ ? and?","acronyms":[[65,70]],"long-forms":[[51,63]]},{"text":"Endowment (2014) baseline. These are: the full stacks LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (?","acronyms":[[71,77],[90,94],[117,120]],"long-forms":[[45,69]]},{"text":"for more general audience of users.  Aided our own (HOO) is an initiative that could in forthcoming spark a new interest in the re-","acronyms":[[54,57]],"long-forms":[[37,52]]},{"text":"cessing information from a structured database ? a natural language interface to a database (NLIDB) (Kapetanios et al, 2010).","acronyms":[[93,98]],"long-forms":[[51,91]]},{"text":"the 8th International Conference on Language Resources and Evaluating (LREC 2012), Istanbul, Turkey, may.  European Language Resources Association (ELRA). ","acronyms":[[148,152],[71,75]],"long-forms":[[107,146],[36,53]]},{"text":"ning of ORG (B-O). Many of them are mislabeled as O and beginning of location (B-L), resulting low recall and low precision for ORG.","acronyms":[[79,82],[8,11],[13,16],[128,131]],"long-forms":[[56,77]]},{"text":"4.3 Nested Expressions No nested expressions will be marked. Even in cases where LOCATION (ENAMEX) expressions occur within  TIMEX and NUMEX expressions, they are not to be tagged.","acronyms":[[91,97],[125,130],[135,140]],"long-forms":[[61,89]]},{"text":"3.3 Assessments Metric We use both Root Mean Square (RMS) mistakes and Correlation Coefficient (CRCoef) to evaluate our model, since the two metrics evaluate different as-","acronyms":[[92,98]],"long-forms":[[67,90]]},{"text":"Winnows software package.  Maximum Entropy Model (MEM) is  especially tailored for integrating evidences from ","acronyms":[[50,53]],"long-forms":[[27,48]]},{"text":"ordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = unequalled or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; MHM = interjection; VB =","acronyms":[[116,119],[24,26],[40,43],[69,71],[97,100],[141,143],[154,157],[180,182],[195,197],[214,216]],"long-forms":[[122,132],[29,38],[46,67],[74,95],[103,114],[146,152],[160,178],[185,193],[200,212]]},{"text":"in collaborated between HU Berlin, U Frankfurt and U Jena, waged in the wider context of the Deutsch Diachron Digital (DDD) initiative. The","acronyms":[[122,125],[23,25]],"long-forms":[[96,120]]},{"text":"resent horizontal movement of the eyebrows.  2.2 Continuous Profile Models (CPM) Unceasing Profile Model (CPM) aligns a set","acronyms":[[76,79],[107,110]],"long-forms":[[49,74],[81,105]]},{"text":"in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007).","acronyms":[[131,133],[30,32],[79,81]],"long-forms":[[108,122],[17,28],[56,77]]},{"text":"abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380?404. ","acronyms":[[56,60],[15,18]],"long-forms":[[19,54]]},{"text":"Figure 1 shows the example of the input format of ACABIT in XML makes use of which conforms to Document Type Definition (DTD) in Figure 2.","acronyms":[[121,124],[50,56],[60,63]],"long-forms":[[95,119]]},{"text":" The obtained Japanese scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in Figure 6.","acronyms":[[94,99]],"long-forms":[[74,92]]},{"text":"The generator operates from a declarative know-  ledge groundwork of linguistic acquaintances, common to that used  by PHRAN (PHRasal ANalyzer; Wilensky and Arens,  1980).","acronyms":[[109,114]],"long-forms":[[116,132]]},{"text":" The set of all homonyms built for a sentence is  called its morphological structure (MorphS). ","acronyms":[[86,92]],"long-forms":[[61,84]]},{"text":"CIMA is an online information center retained by the Spanish Agency for Medicines and Health Products (AEMPS). CIMA provides","acronyms":[[105,110],[0,4],[113,117]],"long-forms":[[84,103]]},{"text":"In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. ","acronyms":[[131,134],[144,146]],"long-forms":[[107,129]]},{"text":"tion access tasks. Current approaches to AZ rely on supervised machine learning (ML). ","acronyms":[[81,83],[41,43]],"long-forms":[[63,79]]},{"text":"The methods for scoring the Template Element, Template Relation, Scenario Template, and Named Entity tasks are  very equivalent. From the standpoint of computed scores, The template element (TE) task is the basic chore of these four. ","acronyms":[[191,193]],"long-forms":[[173,189]]},{"text":"computational devices for natural language processing.  telephoned active production networking (APNs), and explore  how certain kinds of movement are handled.","acronyms":[[91,95]],"long-forms":[[63,89]]},{"text":"In Lawsuits of the NAACL\/AMTA Workshop on Syntax and Structure in Statistical Translate (SSST), pages 33?40, Rochester, NY.","acronyms":[[94,98],[22,32],[125,127]],"long-forms":[[45,92]]},{"text":"No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1) Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores within brackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline. ","acronyms":[[260,262],[277,280]],"long-forms":[[265,275],[283,306]]},{"text":"Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.","acronyms":[[115,120],[23,25],[82,84]],"long-forms":[[94,113],[0,21]]},{"text":"A Machine Learning predicated Approach to Evaluating Retrieval Plans Huyen-Trang Vu and Patrick Gallinari Laboratory of Computer Science (LIP6) University of Pierre and Marie Curie","acronyms":[[135,139]],"long-forms":[[103,133]]},{"text":"srlrr(r** TRANSPORMATIONS IC*S**  SCAN CALLED AT 1 I  ANTEST CALLED F O R  5\"SYLLAB \" (AACC) ,SD= 6. RES= 11.","acronyms":[[87,91],[94,96],[101,104]],"long-forms":[[54,67]]},{"text":", VP ...... Others (OTHER): The remaining cases of comma receive the OTHER label, indicating they do","acronyms":[[20,25]],"long-forms":[[12,18]]},{"text":"Recall all the methods rely on the wrapped classifier. We selection two classic but very different classifiers: the Highest Entropy model (MaxEnt) and the Decision Tree C4.5 (Shue). We implement these","acronyms":[[136,142]],"long-forms":[[113,128]]},{"text":" Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is com-","acronyms":[[88,91]],"long-forms":[[59,86]]},{"text":"regression model. For our regression task we use a Generalised Linear Model (GLM) via penalized maximum likelihood (Friedman et al, 2010).","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":"manual mapping. In the stays of the paper, we shall first  outlines the Switchboard Dialogue Act (SWBD-DA)  Corpus and its annotation scheme (i.e. SWBD-DAMSL).","acronyms":[[97,104],[146,156]],"long-forms":[[71,95]]},{"text":"23-28, 1992   Trials of NAACL-HLT 2015 Student Research Workshop (SRW), pages 110?117, Denver, Colorado, June 1, 2015.","acronyms":[[71,74],[29,38]],"long-forms":[[44,69]]},{"text":"Morphological alterations of a browsing term have a negative impact on the recall performance of an information recover (IR) system (Choueka, 1990; Ja?ppinen and Niemisto?,","acronyms":[[121,123]],"long-forms":[[98,119]]},{"text":"vided within scientific articles. In addition, photographed Regions of Interest (ROIs) are commonly referred to within the image caption.","acronyms":[[74,78]],"long-forms":[[53,72]]},{"text":" To overcome this problems, Gliozzo et al (2005) introduced the domain model (DM) and showing how to define a domain VSM in which texts and terms","acronyms":[[77,79],[113,116]],"long-forms":[[63,75]]},{"text":"This representation uses the logical formulation of traits structure  as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach  to the logical formulation of Functional Unification Grammar (FUG) conferred by Rounds  and Manaster-Ramer (1987).","acronyms":[[218,221]],"long-forms":[[186,216]]},{"text":"Best-Scoring-Choice Realization Pablo Gerva?s, Raquel Herva?s, Carlos Leo?n Natural Interaction based on Language (NIL) Universidad Complutense de Madrid","acronyms":[[115,118]],"long-forms":[[76,113]]},{"text":"We have adapted the list from Rambow et al(2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle","acronyms":[[149,151],[162,164],[180,182],[194,197],[218,221],[78,84]],"long-forms":[[138,147],[118,122],[128,132],[154,160],[167,178],[185,192],[200,216],[224,235],[241,252]]},{"text":"The processing of BADGER is not dramatically different than that of the CIRCUS system used i n previous MUC evaluations [4, 5, 6] . Concept knot (CN) definitions are still use to create case frame instantiation s and diverse CN definitions can apply to the same text fragment .","acronyms":[[147,149],[105,108],[228,230]],"long-forms":[[133,145]]},{"text":"AB to letter l ji ? A where A is a regular letter alphabet and AB=A?{B} includes B as an abstract morpheme start symbol","acronyms":[[63,65]],"long-forms":[[66,70]]},{"text":" 1 Introduction Question answering (QA) has emerged as a practical research problem for pushing the boundaries","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":"attributes of the comment vectors and a  specific label).we can represent the input-output  peers via joint feature map (JFM)  1","acronyms":[[125,128]],"long-forms":[[106,123]]},{"text":"following are the most frequently used ones: ? MRS = Mel?c?uk style used in the MeaningText Theory (MTT): the frst conjunct is the","acronyms":[[47,49]],"long-forms":[[52,66]]},{"text":"4 Crowd-sourcing Multiple-choice Questions from Tables We usage Amazon?s Machinery Turk (MTurk) service to generate MCQs by imposing constraints","acronyms":[[88,93],[115,119]],"long-forms":[[71,86]]},{"text":"unified into one model. We refer to this modelling as the Unified Transition(UT) model. ","acronyms":[[73,75]],"long-forms":[[54,72]]},{"text":"PP ??  ( VP ( VBP practising ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ? ","acronyms":[[9,11],[0,2],[31,33],[50,52]],"long-forms":[[14,17]]},{"text":"Silhouette 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammatical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Disappearance","acronyms":[[126,128],[108,110],[142,144],[158,160],[175,177]],"long-forms":[[131,140],[113,124],[147,156],[163,173],[180,187]]},{"text":" 1 Introduction  Statistical Machines Translation (SMT) is attracting more attentions than rule-based and example-","acronyms":[[50,53]],"long-forms":[[17,48]]},{"text":" 1 Introduction Semantic Role Labeling (SRL), independently of the approach embraced, comprehends two steps be-","acronyms":[[40,43]],"long-forms":[[16,38]]},{"text":"(Marcus et al., 1993) whereas IN = preposition or conjunction, subordinating; CC = Coordinates Cooperate; VBN = Verb, past participle; VBG = verb, gerund or present partici-","acronyms":[[78,80],[30,32],[109,112],[138,141]],"long-forms":[[83,107],[115,136],[144,175]]},{"text":"semantic tree setups are depicted as follows:  TP2TP1 (a) Bag Of Features(BOF) ENT","acronyms":[[74,77],[47,53],[79,82]],"long-forms":[[58,72]]},{"text":" 3 The TMop framework TMop (Translation Memory open-source purifier) is an open-source TM cleaning software written","acronyms":[[22,26],[87,89]],"long-forms":[[28,46]]},{"text":"data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or recap (AB), and DP surveys are evaluated utilised nuggets from","acronyms":[[76,78],[95,97],[10,12],[104,106]],"long-forms":[[60,74],[84,93]]},{"text":"(:all it as word accuracy(W.A.). We use one  more measure, called character accuracy(C.A.)  that measures the character edit distance be- ","acronyms":[[85,89],[26,29]],"long-forms":[[66,83],[12,25]]},{"text":" 1 Introduction The Semantic Textual Similarity (STS) shared task consists of various data sets of paired passages of","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":" 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in","acronyms":[[52,55]],"long-forms":[[19,50]]},{"text":"data are classified manually (Human) into three  stability classes. Decision Tree (DT) automatic  algorithm C4.5 (Quinlan, 1993; Weiss & Kulikowski, ","acronyms":[[83,85]],"long-forms":[[68,81]]},{"text":"is varied from the annotation scheme (Abbas, 2012; Abbas, 2014) of phrase structure (PS) and the hyper dependency structures (HDS) of the URDU.KON-TB treebank along with the different","acronyms":[[128,131],[88,90],[140,151]],"long-forms":[[100,126],[70,86]]},{"text":"FA8750-09-C-0181. The first author similarly thanks the Vietnam Education Foundation (VEF) for its sponsorship.","acronyms":[[81,84]],"long-forms":[[51,79]]},{"text":"a question written in natural language is called ? Question Answering?(QA), and has gotten a lots of attention recently.","acronyms":[[71,73]],"long-forms":[[51,69]]},{"text":"EXPERIMENTS 3.1. Speaker Identification (SID) In order to investigate robust speaker identification under","acronyms":[[41,44]],"long-forms":[[17,39]]},{"text":"We use the publicly available runs of the two best schemes from the CoNLL 2003 shared chore, namely FLORIAN (Florian et al., ","acronyms":[[99,106],[68,73]],"long-forms":[[108,115]]},{"text":" 1 Overv iew o f  the  IPS  pro jec t   The IPS (Interactive Parsing System) research  project, at the Linguistics Departement of the ","acronyms":[[44,47],[23,26]],"long-forms":[[49,75]]},{"text":" 1 Introducing Predicate argument structure (PAS) analysis is a shallow semantic parsing task that identifies ba-","acronyms":[[46,49]],"long-forms":[[16,44]]},{"text":"Section 2 discusses related collaborate. Section 3 introduces the Condition Random Fields(CRFs)  and the defined Long-Dependency CRFs ","acronyms":[[83,87],[122,126]],"long-forms":[[59,82]]},{"text":"baseline (BAS) system which is nearer to the system described in (Hu et al 2009), and three variants of our novel divide and vanquishing (DAC) system. Fea-","acronyms":[[133,136],[10,13]],"long-forms":[[113,131],[0,8]]},{"text":" 5.2 Corpus Benchmark Tools The Corpus Benchmark Tool(CBT) is one of the components in WEARS which enables automatic evaluation of an","acronyms":[[53,56]],"long-forms":[[31,51]]},{"text":" ADEPT tags documents in a uniform fashion, using  Standard Generalized Markup (SGML) according to  OIR standards.","acronyms":[[80,84],[1,6],[100,103]],"long-forms":[[51,78]]},{"text":"input format. For instance, if the input is annotated with word and PoS (WP), so needs be the translation modelling.","acronyms":[[73,75]],"long-forms":[[59,71]]},{"text":" After detecting a new indefinite description (as ETA(x) : unlversity(x)) ReP  creates a new \"referential object'\" (RefO). During the discours6 (after the ","acronyms":[[116,120],[50,56],[74,77]],"long-forms":[[94,112]]},{"text":" 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random NLPBA","acronyms":[[60,62],[83,86],[107,110],[118,123]],"long-forms":[[39,58],[63,69]]},{"text":"the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of massive data: volume, velocity, and variety1. Natural Language Processing (NLP) processes, in especial, face further difficulties arising from the short, noisy, and roundly contextualised nature of social media. In order to address the 3Vs of societal media, new linguistic technologies have emerged, such as the identification and definition of users' vocabulary varieties and the translation to a different language, than the source.","acronyms":[[187,190]],"long-forms":[[158,185]]},{"text":"is the DIAGRAM grammar \\ [9 \\ ] .   It is un for tunate ly  the very power .of APSGs (and ATNs)  that  makes it  d i f f i cu l t  to capture l inguist ic  general izat ions ","acronyms":[[79,84],[90,94]],"long-forms":[]},{"text":"Vector Computers (SVM) with radial basis kernel, Na??ve Bayes (NB), J48 Decision Trees (DT), and Neural Networks (NN) with back propagation. Across","acronyms":[[113,115],[17,20],[62,64],[87,89]],"long-forms":[[96,111],[0,15],[48,60],[71,85]]},{"text":"and get close to the top echelon in several other tracks.  Lately, Maximum Entropy model(ME) and CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai","acronyms":[[89,91],[97,101]],"long-forms":[[67,82]]},{"text":"We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).","acronyms":[[132,135]],"long-forms":[[108,130]]},{"text":"AVERAGE 3.31 316 72.58% 78.02% 84.65% Tabled 2: Word sense disambiguation consequences, including two baselines (MFS = most frequent sense; LeskC = Lesk-corpus) and the word sensing disam-","acronyms":[[107,110],[134,139]],"long-forms":[[113,132],[142,153]]},{"text":"course Connectives. Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT). ","acronyms":[[93,96]],"long-forms":[[58,91]]},{"text":"11 Figure 4: Variation of Averaging Information Processing Indices(IPI) for Video 4-6 Figure 5: Variant of Average Information Processing Indices(IPI) for the full course the last zhou, i.e, students who do not finish the","acronyms":[[146,149],[65,68]],"long-forms":[[115,144],[34,64]]},{"text":"S = Spanish  JV = Collective Venture ?????????? ME = Microelectronics  Brief History of the Message Understanding Conferences","acronyms":[[43,45],[13,15]],"long-forms":[[48,64],[4,11],[18,31]]},{"text":"ing mobile devices. Such an application typically uses a speech recognizer (ASR) for transforming the user?s speech input to text and a search component","acronyms":[[76,79]],"long-forms":[[55,74]]},{"text":"For the time being,  the object of the testing is the generative component (GC) of this  description enumerating semantic representations (SR's) of sentences. ","acronyms":[[139,143],[76,78]],"long-forms":[[113,137],[54,74]]},{"text":"of Machine Translation and present an implemetation of a morphological analyser for Amharic utilise Xerox Finis State Tools (XFST). The different","acronyms":[[124,128]],"long-forms":[[98,122]]},{"text":"The idea is merely to count the mmlber of newest  words introduced ow'x a moving interval and 1)roduce  what he calls a vocabulary managemenl profiled (VMI'),  or lneasurements at intervals.","acronyms":[[148,152]],"long-forms":[[117,146]]},{"text":"ping observed sequences to doable ground truth sequences.  We do not use the Personages Error Rate (CER) metric, since for almost all NLP applications the unit of","acronyms":[[101,104],[135,138]],"long-forms":[[79,99]]},{"text":"  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95, Gothenburg, Sweden, April 27, 2014.","acronyms":[[71,76],[80,84]],"long-forms":[[37,69]]},{"text":"evaluation metrics. Legend: d = dependency f-score, _pr =  predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy ","acronyms":[[95,97],[53,55],[110,114],[138,142]],"long-forms":[[100,107],[32,42],[59,73],[87,93],[117,136],[145,159]]},{"text":"     - French\/English (fra-eng)    Seven Recognizing Textual Entailment (RTE)  evaluation tracks have already been held: RTE-1 ","acronyms":[[73,76]],"long-forms":[[41,71]]},{"text":"We would expect this to be the case in general, but as always, cases exist where a conflict between a contrast (CoCo) and a change to a method (PModi) occur:","acronyms":[[112,116],[144,149]],"long-forms":[[83,110]]},{"text":"sider this sentence is correctly tag.  Test data set 1 (TDS 1): contains about 10%  of the sentences from the completed emotion-","acronyms":[[59,64]],"long-forms":[[42,57]]},{"text":"tation for the joint learning process. Specifically, we make use of the latent structure SVM (LS-SVM) (Yu and Joachims, 2009) articulation.","acronyms":[[95,101]],"long-forms":[[72,93]]},{"text":"= argmaxjP (zi = j|saskatchewan).  4.2 Lexical Chain Segmenter (LCSeg) Our second model is the lexical chain based seg-","acronyms":[[54,59]],"long-forms":[[29,52]]},{"text":" 1 Introduction Amazon?s MechanicalTurk (AMT) is frequently used to evaluated experiments and annotate data in","acronyms":[[41,44]],"long-forms":[[16,39]]},{"text":"4.3 Classifiers All evaluation tests were performed using two classifiers, Maximum Entropy (MaxEnt) and Support Vector Appliances (SVM).","acronyms":[[92,98],[129,132]],"long-forms":[[75,90],[104,127]]},{"text":"a mistake when we engender the final output that upshot in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers.","acronyms":[[121,124]],"long-forms":[[95,119]]},{"text":"Frustratingly easy domain adaptation.  In Association for Computational Linguistics (ACL). ","acronyms":[[85,88]],"long-forms":[[42,83]]},{"text":"occurs from different production sources, we  propose an extension to this genre of technique  in the form of a Group Sparse Model (GSM)  which enforces sparsity with a L2,1 norm instead ","acronyms":[[132,135]],"long-forms":[[112,130]]},{"text":"edge in Y .  The Wall Street Journal Penn Treebank (PTB) (Marcus et al, 1993) contains parsed constituency","acronyms":[[52,55]],"long-forms":[[37,50]]},{"text":"to be explained by a set of unobserved (latent) theme. Hidden Markov Model LDA (HMM-LDA) (Griffiths et al, 2005) is a topic modeling that simul-","acronyms":[[81,88]],"long-forms":[[56,79]]},{"text":"only curtailed discontinuities in each tree.  Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive","acronyms":[[75,79]],"long-forms":[[44,73]]},{"text":"The parameters ? are estimated through the optimization of a Highest Likelihood (ML) criterion using the Expectation-Maximization (EM) al-","acronyms":[[81,83],[131,133]],"long-forms":[[61,79],[105,129]]},{"text":"the ratio of system?s moves stating that the calls informational is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the proportion of the information-providing games","acronyms":[[115,118]],"long-forms":[[85,113]]},{"text":" 6 Experiments and Results We use the Wall Rue Diary (WSJ) section of the Penn Treebank as our labeled source domain","acronyms":[[59,62]],"long-forms":[[38,57]]},{"text":" Results (in percentages) are for per-logical-predication (PR) and per-whole-graph (GRPH) tagging accurcies. ","acronyms":[[84,88],[59,61]],"long-forms":[[77,82],[46,57]]},{"text":"Each corpus uses a different set of entity labels.  MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numeri-","acronyms":[[73,76],[94,97],[52,55],[119,122]],"long-forms":[[62,71],[79,92],[103,111]]},{"text":"The current   representat ion fo r  t h a t  sentence i n  pur system would be:  Z V l  =Ncorn(elephant,X1) PI =P.P(size,X1 ,small)  & =Ncom(animal,X1) P2 =P(size ,XI ,large) ","acronyms":[[108,110],[89,94],[81,86],[136,140],[112,115],[152,154],[164,166],[104,106]],"long-forms":[]},{"text":"11 Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6 Figure 5: Variation of Average Information Processing Indices(IPI) for the full course the last week, i.e, students who do not finish the","acronyms":[[146,149],[65,68]],"long-forms":[[115,144],[34,64]]},{"text":"by each of these strategies.  The four Word Sense (WS) disambiguation  strategies resolve sense ambiguity errors.","acronyms":[[51,53]],"long-forms":[[39,49]]},{"text":"an example of a low-pass filter. The concept of recursion is next introduced in order to pave the way for a discussion of IIR (Infinite Impulse Response) filters. High-, low-, and","acronyms":[[122,125]],"long-forms":[[127,152]]},{"text":"tagging, lemmatization, etc.). For corpus query, we employ the Corpus Query Processor (CQP) (CWB; Evert, 2004) which works on the basis of","acronyms":[[87,90]],"long-forms":[[63,85]]},{"text":"cabbage 9 chou 1 chou blossoming 25 flowers 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bains 178 bath 1 bain mariposa 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond chalet 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 kid 1 infantile bed 806 illuminated 2 tabled beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 city 1 ville chica 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bruise hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bibles 1 Bible foot 23548 pied 8 siffler wheelchairs 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the vocabulary pair English ? French. The meaning of the columns is as follows: ESW = English origin word; CF = corpus frequency of Anglais source word; ET = anticipated translation according to gold standard; RE = computed rankings of expected translation; CT = computed translation. ","acronyms":[[925,927],[1069,1071],[898,901],[971,973],[1025,1027]],"long-forms":[[930,946],[1074,1094],[904,923],[976,996],[1039,1055]]},{"text":"On the foundations of these specifications, a mapping between VAML and Concrete AML (CAML) can be made. CAML","acronyms":[[79,83],[56,60],[98,102]],"long-forms":[[65,77]]},{"text":"tion chaining in speech: PER, GPE, ORG, LOC,  FAC, NPER (NOMINAL PER), NGPE, NORG,  NLOC, NFAC, PPER (PROUNOUN PER),  PGPE, PORG, PLOC, and PFAC.","acronyms":[[97,101],[26,29],[31,34],[36,39],[41,44],[47,50],[52,56],[72,76],[78,82],[85,89],[91,95],[119,123],[125,129],[131,135],[141,145]],"long-forms":[[103,115],[58,69]]},{"text":"The different resources used to constructed ArSenL.       The English WordNet (EWN) (Miller et al., ","acronyms":[[73,76]],"long-forms":[[56,71]]},{"text":"approach and extract features from the names.  They use Maximum Entropy (MaxEnt) model and a number of features based on n-grams,","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":"experiments can be listed as follows.  Head Word (HW.) The predicate?s head word as","acronyms":[[50,53]],"long-forms":[[39,48]]},{"text":"making procedures.  Latent semantic analysis (LSA) (Deerwester et al.,","acronyms":[[46,49]],"long-forms":[[20,44]]},{"text":"corpora (section 2.2).  The workflow for named entity (NE) and  terminology extraction and map from ","acronyms":[[55,57]],"long-forms":[[41,53]]},{"text":"J08b 97.74 93.37 N07 97.83 93.32 SF = segmentation F-score; JF = collective segmentation and POS-tagging F-score. ","acronyms":[[33,35],[60,62],[88,91]],"long-forms":[[38,52],[65,70]]},{"text":"Source Material (SMaterial) e.g., As, InGaAs   ? Source Material Characteristic  (SMChar) e.g. ,(111)B  ","acronyms":[[82,88],[17,26],[38,44]],"long-forms":[[49,79],[0,15]]},{"text":"also accessible through a phrase internal reordering. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li","acronyms":[[94,96]],"long-forms":[[80,92]]},{"text":"ing in the non-realizable case. In Advances in Neural Information Processing Regimes (NIPS), 2010.","acronyms":[[86,90]],"long-forms":[[47,84]]},{"text":"ing instance is created as during training, and then present to the decision tree, which returned a confidence value (CF)2 explaining the likelihood that NPi is coreferential to NPj .","acronyms":[[119,121],[155,158],[179,182]],"long-forms":[[101,111]]},{"text":"surveys of QA and DP data. The surveys are evaluated using nuggets drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).","acronyms":[[97,102],[11,13],[18,20],[119,124],[150,155]],"long-forms":[[78,95],[105,117],[131,148]]},{"text":"The TRIPS system here uses a wide range of statistically driven preprocessing, including part of speech tagging, constituent bracketing, inter-pretation of unknown words using WordNet, and named-entity recognition (Allen et al 2008). All these are generic off-the-shelf resources that ex-tend and help guide the deep parsing process.  The TRIPS LF (logical form) ontology1 is de-signed to be linguistically motivated and domain independent. The semantic types and selectional restrictions are driven by linguistic considera-tions rather than requirements from reasoning components in the system (Dzikovska et al 2003).","acronyms":[[345,347],[339,344],[4,9]],"long-forms":[[349,361]]},{"text":" ? System incorporate, through SGML (the Standard Generalized Markup Language), both at the leve l of meaning analysis and at the holistic application level .","acronyms":[[31,35]],"long-forms":[[41,77]]},{"text":"WordNet Spheres (Magnini and Cavagli a`, 2000).  Theoretical Density (CD) is a measure of the correlation among the sense of a given word and its","acronyms":[[69,71]],"long-forms":[[49,67]]},{"text":"We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps:","acronyms":[[122,124],[43,47]],"long-forms":[[105,120],[83,89]]},{"text":"We use two ways to measure contribution in terms of graphemes: contseq(w, b) is the length of the longest prefix\/suffix of word w which blend b begins\/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and b. This yields four features:","acronyms":[[215,218]],"long-forms":[[187,213]]},{"text":"Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindustani(HI) scrambling mixing. ","acronyms":[[97,99],[107,109],[86,88]],"long-forms":[[89,95],[101,106],[78,85]]},{"text":"nority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substan-","acronyms":[[118,120]],"long-forms":[[98,116]]},{"text":"document is zilch.  Agglomerative Hierarchical Clustering (AHC)  AHC is a bottom-up hierarchical clustering ","acronyms":[[58,61],[64,67]],"long-forms":[[19,56]]},{"text":"UniProt the protein sequencing database managed by the Swiss Institute of Bioinformatics (SIB), the European Bioinformatics Institute (EBI) and the Protein Information Remedies (PIR)","acronyms":[[133,136],[88,91],[176,179]],"long-forms":[[98,131],[53,85],[146,174]]},{"text":"Instead to measure topic cohesive we follow (Newman et al, 2009) to compute the Pointwise Bilateral Information (PMI) of topic words w.r.t wikipedia articles.","acronyms":[[111,114]],"long-forms":[[81,109]]},{"text":"processing beyond keyword indexing, typically supported by Natural Language Processing (NLP)  and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li ","acronyms":[[122,124],[88,91]],"long-forms":[[98,120],[59,86]]},{"text":"Two types of initial parameter configured were tried for BFGS; original parameters have the same fixed value, or were chosen randomly. Steepest Descent (SD) was used as online training where some portion (i.e. chunk) of the training data were using during an iteration.","acronyms":[[157,159],[61,65]],"long-forms":[[139,155]]},{"text":"search has encore'aged the FI{UM1 ) api)roach. The  SUMMONS (SUMMarizing Online News artMes)  system (McKeown and Radev, 1999) takes tem- ","acronyms":[[52,59]],"long-forms":[[61,84]]},{"text":"verbs. Albeit, the second Light Verb (LV) may be  a part of another Complex Predicate (CP). This ","acronyms":[[84,86],[35,37]],"long-forms":[[65,82],[23,33]]},{"text":"stituent category labels expressing adverbials (RB), coordinations (CC), various types of interjections (UH, INTJ) and adverbial phrases (ADVP). We may","acronyms":[[138,142],[48,50],[68,70],[105,107],[109,113]],"long-forms":[[119,136],[53,66],[25,46],[90,103]]},{"text":"the vertex of the Abox which it expresses. Ours current model using the Tree Adjoining Grammar (TAG) formalism, see Joshi (1987), and the Atree acts as a","acronyms":[[94,97]],"long-forms":[[70,92]]},{"text":"1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue man-","acronyms":[[107,109]],"long-forms":[[83,105]]},{"text":"rithm to handle this setting. To do so, we use dynamic programming (DP) together with greedy search.","acronyms":[[68,70]],"long-forms":[[47,66]]},{"text":" 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al.,","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"adapted to a new domain.  Word sense disambiguation (WSD), on the other hand, is the closely related task of assigning a sense","acronyms":[[53,56]],"long-forms":[[26,51]]},{"text":"When using the classifiers to predict the class of a test example, there are four possible outcomes; true affirmative (TP), true negative (TN), counterfeit positive (FP), and false nega-","acronyms":[[116,118],[136,138],[157,159]],"long-forms":[[101,114],[121,134],[141,155]]},{"text":"Markov Model (HHMM) word), all the corresponding TYP candidates triggered by ranking word features(CWF) should be removed.","acronyms":[[103,106],[14,18],[49,52]],"long-forms":[[77,101],[0,12]]},{"text":" 1 Introduction Coreference resolution (CoRe) is the process of finding markables (noun phrases) referring to the same","acronyms":[[40,44]],"long-forms":[[16,38]]},{"text":"Onto this paper, we examined the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. ","acronyms":[[131,134],[144,146]],"long-forms":[[107,129]]},{"text":"the parameters ? = (s,W,b,x) via backpropagation with stochastic gradient descent (SGD). ","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"The results can be seen at:  http:\/ \/c lwww.essex.ac.uk\/w3c\/.  The project was  funded by JISC (the Joint Information Systems Com-  mittee of the UK Higher Education Funding Coun- ","acronyms":[[90,94],[146,148]],"long-forms":[[100,138]]},{"text":"a set of features in the Sentence Scoring phase.  The Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering","acronyms":[[82,85]],"long-forms":[[54,80]]},{"text":"The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al.,","acronyms":[[114,116]],"long-forms":[[94,112]]},{"text":"z and a segmental grammatical grammes, calculator the sur-  face form y : g(z) of z.  The phonological recognition problem (PRP) is:  Given a (partially specified) surface form y, a dic- ","acronyms":[[111,114]],"long-forms":[[77,109]]},{"text":" 2 Changes to the ARIZONA Regimen Argumentative Zoning II (AZ-II) is a new annotation scheme, which is an elaboration of the orig-","acronyms":[[53,58],[18,20]],"long-forms":[[28,51]]},{"text":"2.3 Approaches BUAP-RUN-3: Random Indexing and Satchel of Concepts The vector space model (VSM) for document representation supporting search is probably the most","acronyms":[[85,88],[13,23]],"long-forms":[[65,83]]},{"text":"Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological unrest.  Generic Union Posttraumatic stress chaos (PTSD) is a psychological unrest, which is classified as an worry disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that","acronyms":[[146,150],[21,25],[231,237]],"long-forms":[[115,144]]},{"text":"NP and selecting of the head NP by the relative 1The following abbreviations are using in glosses: NOM = nominative, ACC = accusative, PRES = non-past and POT = potential. ( )","acronyms":[[116,119],[29,31],[0,2],[98,101],[134,138],[154,157]],"long-forms":[[122,132],[104,114],[160,169]]},{"text":"Verbs can also involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), etc.","acronyms":[[81,84],[60,67],[97,105]],"long-forms":[[86,94],[69,73],[107,112]]},{"text":"Afterward we name the TD composed of words from gold training set and tagged test set and as Na??ve TD (NTD) for its unbalanced coverage in training and test set.","acronyms":[[104,107],[22,24]],"long-forms":[[93,102]]},{"text":"ters instead of Y or N as labels. The character-level machine translation (MT) approach (Pennell and Liu, 2011) was modified in (Li and Liu, 2012a)","acronyms":[[75,77]],"long-forms":[[54,73]]},{"text":"cuisine cabinet and will hardly be able to won the elections]. The parse tree contains phrase labels NP (Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Penalties), and CS (Coordinated Sentence).","acronyms":[[119,121],[146,148]],"long-forms":[[123,143],[150,161]]},{"text":"native (see \\[2\\]).  Task  Manager  (TM)   In our previous experience, speech recognition systems ","acronyms":[[37,39]],"long-forms":[[21,34]]},{"text":"up in the bracketed terminal string as insertion o f \"  (* *) \" surround-  ing \" 1970 \".) Inverse noun expressions preposing (NPPREPOS), which  relates such surface torque as \" GM's sales \" and \" the selling of GM \", ","acronyms":[[121,129],[171,173],[203,205]],"long-forms":[[98,119]]},{"text":" NONHUMAN (animals), DNA, RNA, PROTEIN, CONTROL (control measures to contain the disease), BACTERIA, CHEMICAL and SYMPTOM.","acronyms":[[40,47],[21,24],[26,29],[31,38],[91,99],[101,109],[114,121],[1,9]],"long-forms":[[49,56],[11,18]]},{"text":"2.3.3 Designation List Generated using Double Propagation We implement the Twice Propagation (DP) algorithm described in Qiu et al. (","acronyms":[[88,90]],"long-forms":[[68,86]]},{"text":"t have also been used.  2.2 EasyAdapt (EA) In this section, we give a brief overview of","acronyms":[[39,41]],"long-forms":[[28,37]]},{"text":"et al, 2000b). A more complex application targets the Aircraft Maintenance Manual (AMM) of the Airbus A320 (Rinaldi et al, 2002b).","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"MusicalArtist:album or Book:genre. Therefore, in addition to recognising entities with Stanford NERC, we also implement our own named entity recogniser (NER), which only recognises entity boundaries, but does not classify them.","acronyms":[[153,156],[96,100]],"long-forms":[[128,151]]},{"text":"unfavorable has an incorrect, but credible, stress pattern, u. We adopt a Support Vector Machine (SVM) solution to these ranking constraints as described by","acronyms":[[96,99]],"long-forms":[[72,94]]},{"text":"gls: the definition of the verb  They also defined two alternate search protocols: rich hierarchy exploration (RHE) with no  more than six links and shallow hierarchy explo-","acronyms":[[111,114]],"long-forms":[[83,109]]},{"text":"and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10","acronyms":[[64,67],[4,9]],"long-forms":[[48,55]]},{"text":"appropriate.  Terminology Data banks (TD) are the least ambitious  plan because access frequently is not made during a ","acronyms":[[38,40]],"long-forms":[[14,30]]},{"text":"than three thousand five hundred days of imprisonment .  Figure 4: Example translation using the back-off and the continuous space language model (CSLM). ","acronyms":[[147,151]],"long-forms":[[114,145]]},{"text":"tions (Abe et al, 1996).  1.1 Question answering (QA) Unlike IR systems which return a list of documents","acronyms":[[50,52]],"long-forms":[[30,48]]},{"text":"lem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate","acronyms":[[128,130]],"long-forms":[[102,126]]},{"text":"5.1 Alternative Models Aux test LUX?s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels squarely","acronyms":[[94,96],[31,36],[115,118]],"long-forms":[[77,92]]},{"text":"4.1 Selection of PPs in the Lexicon Our parser makes use of the computational lexicon HaGenLex (Hagen German Lexicon, see (Hartrumpf et al, 2003)), which is a general do-","acronyms":[[86,94],[17,20]],"long-forms":[[96,116]]},{"text":"Networks A more similar model to the proposed larger-context recurrent language model is a hierarchical recurrent encryption decoder (HRED) proposed freshly by Serban et al (2015).","acronyms":[[131,135]],"long-forms":[[91,129]]},{"text":"six different domains: Newswire (NW), Broadcast News (BN), Broadcast Conversations (BC), Webblog (WL), Usenet (UN), and Conversational Telephone Speech (CTS).","acronyms":[[111,113],[33,35],[54,56],[84,86],[98,100],[153,156]],"long-forms":[[103,109],[23,31],[38,52],[59,82],[89,96],[120,151]]},{"text":"(http:\/\/ieee.rkbexplorer.com\/) repository7. The corpus of cancer research (COCR) therein 3334 domain specific abstracts of scientific publica-","acronyms":[[75,79]],"long-forms":[[48,73]]},{"text":"Comprehension in 100 days,? published by  Chung Hwa Book Co., (H.K.) Ltd.  The  ChungHwa training set includes 100 English ","acronyms":[[63,67]],"long-forms":[[48,56]]},{"text":"stem of LEAPING = <jump>.   sense of JUMP = jumping. ","acronyms":[[34,38],[8,12]],"long-forms":[[41,48],[16,20]]},{"text":"92  NAACL-HLT 2012 Workshop on Speech and Linguistic Processing for Assistive Technical (SLPAT), pages 28?36, Montre?al, Canada, June 7?8, 2012.","acronyms":[[90,95]],"long-forms":[[31,88]]},{"text":"The classification step now supports three machine learning algorithms from the Python scikit-learn4 packages: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector","acronyms":[[130,132],[152,158]],"long-forms":[[116,128],[135,150]]},{"text":"One of  the most commonly used methods is the  Latent Semantic Analysis (LSA). In this ","acronyms":[[73,76]],"long-forms":[[47,71]]},{"text":" The algorithm was first proposed by the Institute for Computer Scientific  and Technology (ICST) of the National Bureau of Norms (NBS') in 1973. ","acronyms":[[132,136],[89,93]],"long-forms":[[102,130],[41,87]]},{"text":"for unsupervised learning. In this paper, we are interested in partitioning words into several clusters without any label priori using unsupervised LP (Un-LP) algorithm. Firstly we randomly select K (K ?","acronyms":[[152,157]],"long-forms":[[135,150]]},{"text":"rank verb pairs with respect to the strength of their association with a particular discourse relation. We adapted versions of standard lexical association measures like PMI (pointwise mutual information) and their variants, as well as some measures specific to the association of a causal relation between items (Do","acronyms":[[170,173]],"long-forms":[[175,203]]},{"text":" The structure of a Idea is completed by its set of  Structural Descriptions (SD's). These express how the ","acronyms":[[81,85]],"long-forms":[[56,79]]},{"text":" ? Bigram Predictability (BP): Defined as the predictability of a word given a previous word, it","acronyms":[[26,28]],"long-forms":[[3,24]]},{"text":"   then  Cast_in_Chain(Antecedent, Anaphor)    then  Cast_in_Chain(Antecedent, Anaphor) RULE-1-Filter-1-Pronoun (R1F1Pron) RULE-1-Filter-1-Nominal (R1F1Nom)","acronyms":[[113,121],[148,155]],"long-forms":[[88,111],[123,146]]},{"text":" IHMM-based: He et al (2008) propose an  indirect hide Markov model (IHMM) for hypothesis alignment.","acronyms":[[71,75],[1,5]],"long-forms":[[41,69]]},{"text":"and Causal Relations. In proceedings of the Association for Computational Linguistics (ACL). ","acronyms":[[87,90]],"long-forms":[[50,85]]},{"text":"tures. Since the Parallel Alignment TreeBank is a subset of the Chinese TreeBank (CTB) 8.0, we automatically parsed the CTB 8.0 by doing a 10-","acronyms":[[82,85],[120,123]],"long-forms":[[64,80]]},{"text":"other animate and inanimate), which cover ten categories of noun phrases, with categories loves ORG (organization), ANIM (animal) and MAC (intelligent computers such as robots) categorised as other","acronyms":[[115,119],[95,98],[133,136]],"long-forms":[[121,127],[100,112],[150,158]]},{"text":"Rule-Based Machine Translation(MT)(Hutchins and Somers, 1992) requires large-scale knowledge to analyze both source language(SL) sentences and target language(TL) sentences.","acronyms":[[125,127],[31,33],[159,161]],"long-forms":[[109,123],[11,30],[143,158]]},{"text":"roles they popularly enter: ACT (Actor), PAT (Patient), ADDR (Addressee), ORIG (Origin) and EFF (Effect). Syntactic criteria are used to identifying","acronyms":[[92,95],[28,31],[41,44],[56,60],[74,78]],"long-forms":[[97,103],[33,38],[46,53],[62,71],[80,86]]},{"text":"46 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic mem-","acronyms":[[62,64]],"long-forms":[[40,60]]},{"text":"8. THE SAIL INTERFACING SYSTEM  The SAIL Interfacing System (S.I.S.) is the f ramework   where a user  can interacting with SAIL in formulated NL ","acronyms":[[61,67],[140,142],[121,125]],"long-forms":[[36,59]]},{"text":"We use two different windows to define a triggering environment: one for morpheme and another for its part of speech (POS) tag. Figure 2 shows ","acronyms":[[118,121]],"long-forms":[[102,116]]},{"text":"for topic models. In Trials of the 25th Conference on Uncertainty in Artificial Intelligence (UAI), pages 27?34.","acronyms":[[99,102]],"long-forms":[[59,97]]},{"text":"course structure. In Lawsuits of the International Conference in Computational Linguistics (COLING), pages 43?49. ","acronyms":[[95,101]],"long-forms":[[68,93]]},{"text":"and tests 2 and 3 are open tests performed on different test data. DM (i.e., Default Model) assigns all  incoming cases with the most likely class and it is ","acronyms":[[67,69]],"long-forms":[[77,90]]},{"text":"PrecisionCorrectTransliterations (PTrans)  2. RecallCorrectTransliteration (RTrans)  3.","acronyms":[[76,82],[34,40]],"long-forms":[[46,74],[0,32]]},{"text":"pus Linguistics 2001 Conference, pages 274?280. Lancaster University (UK). ","acronyms":[[70,72]],"long-forms":[[58,68]]},{"text":"A workaround is to restrict the possible tag candidates per position by using either morphological analyzers (MAs), dictionaries or heuristics (Hajic?,","acronyms":[[110,113]],"long-forms":[[85,108]]},{"text":" REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62","acronyms":[[14,18],[1,4],[41,46]],"long-forms":[[21,40],[49,69]]},{"text":"notation n-grams in the sentence.  Mean logprob (ML) This score is the logprob of the entire sentences divided by the length of the","acronyms":[[48,50]],"long-forms":[[34,46]]},{"text":"summarization. During these intervening decades,  advance in Natural Language Processing (NLP), coupled  with great increases of computer memory and promptness, ","acronyms":[[91,94]],"long-forms":[[62,89]]},{"text":"3.4 Algorithm The algorithm first splits the data into appropriate units (SL=source language, TL=targeting language): 1.","acronyms":[[74,76],[94,96]],"long-forms":[[77,92],[97,112]]},{"text":"We experiment with multiple ways to select a snippet: the first 50 words of the summary (START), the last 50 words (END) and 50 words starting at a randomly chosen sentence","acronyms":[[89,94],[116,119]],"long-forms":[[80,87]]},{"text":"  Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Marija das Gra?as V. Nunes2  N?cleopatra Interinstitucional de Leng??stica Computacional (NILC)  http:\/\/www.nilc.icmc.usp.br ","acronyms":[[144,148]],"long-forms":[[88,142]]},{"text":"Recently, McDonald et al (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative","acronyms":[[91,94]],"long-forms":[[68,89]]},{"text":"as director of Pentagon telecom~~nicntions and commc~nd :~nd control systcms.  Requests for coments on a proposed Federal information processing -t.lndard (PII'S)  for the National Communications System have been requested by .Innuor).","acronyms":[[156,161]],"long-forms":[[105,144]]},{"text":"set of principally two algorithms. One algorithm is a  variant of Alignment Based Learning (ABL), as  outline in Van Zaanen (2001).","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":"We present an open sources, freely available Java realization of Align, Disambiguate, and Walk (ADW), a state-of-the-art approach for measuring semantic similarity based on","acronyms":[[98,101]],"long-forms":[[67,96]]},{"text":"ance improve in our system.  The OOV recall rates (RRoov) showed in  Table 4 demonstrate that the OOV recognition ","acronyms":[[56,61],[38,41],[103,106]],"long-forms":[[42,54]]},{"text":"User Group (WON).4 Research and development  is carried out in close collaboration with user  groups and intellectual property (IP) professionals to ensure solutions and software are delivered ","acronyms":[[128,130],[12,15]],"long-forms":[[105,126]]},{"text":"Given an OOV word a and its IV version b we have extracted character transformation rules from a to b using the longest common substring (LCS) algorithm (See Table 5).","acronyms":[[138,141],[9,12],[28,30]],"long-forms":[[112,136]]},{"text":"They used a statistical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discrim-","acronyms":[[107,110],[49,53],[143,146]],"long-forms":[[83,105],[12,47],[116,141]]},{"text":"pense of introducing sonora.  We propose Embedded base phrase(EBP) detection as algorithm.2.","acronyms":[[61,64]],"long-forms":[[40,60]]},{"text":"tance Metric from Relative Comparisons. Advances in Neural Information Processing Systems (NAPA).. J. Weeds, D. Weir and D. McCarthy.","acronyms":[[91,95]],"long-forms":[[52,89]]},{"text":"studied three activated functions: 1. Rectified linear units (ReLUs) (Nair and Hinton, 2010):","acronyms":[[63,68]],"long-forms":[[39,61]]},{"text":". However, a method based on singular value decomposition (SVD) provides an efficient and exact solutions to this prob-","acronyms":[[59,62]],"long-forms":[[29,57]]},{"text":" Shirai, K., T. Tokunaga, and H. Tanaka: Automatic Extraction of Japanese Grammar f om  a Bracketed Corpus, in Natural Language Processing Pacific Rim Symposium(NLPRS'gs),  pp.","acronyms":[[161,169],[173,175]],"long-forms":[[111,160]]},{"text":"den Markov Models (HMMs), Conditional Random Fields (CRFs), Maximum Entropy Markov  Models (MEMMs), etc. ","acronyms":[[92,97],[19,23],[53,57]],"long-forms":[[60,90],[4,17],[26,51]]},{"text":"Transcutaneous Oxygen (TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7. ","acronyms":[[58,64],[23,28]],"long-forms":[[38,56],[0,21]]},{"text":"(2) dobj? det:DT NN prep:IN 7DT\/det=determiner, NN=noun, IN\/prep=preposition, dobj=direct object","acronyms":[[48,50],[14,16],[17,19],[60,64],[78,82],[28,35],[4,8],[20,24]],"long-forms":[[51,55],[65,76],[83,96],[36,46]]},{"text":"a PP (Mirroshandel and Ghassem-Sani, 2011), the prepositional lexeme of the PP if e2 is governed by a PP, the POS of the head of the verb phrase (VP) if e1 is governed by a VP, the POS of the head of the","acronyms":[[146,148],[2,4],[76,78],[102,104],[110,113],[173,175],[181,184]],"long-forms":[[133,144]]},{"text":"tion of predicate signs. Ramchand divides events into a maximum of three hierarchical phrases: an initiation phrase (InitP), a process phrase (ProcP), and a result phrase (ResP).","acronyms":[[117,122],[143,148]],"long-forms":[[98,115],[127,141]]},{"text":"ings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the","acronyms":[[68,73]],"long-forms":[[43,56]]},{"text":"ity? have been designed so far for Frenchmen L1 and only one for French as a strangers language (FFL) (see Section 2).","acronyms":[[92,95],[42,44]],"long-forms":[[62,90]]},{"text":"The classification step currently supports three machine learning algorithms from the Python scikit-learn4 package: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector","acronyms":[[130,132],[152,158]],"long-forms":[[116,128],[135,150]]},{"text":" The third thread is the sponsorship of the international  Message Understanding Conferences (MUC's) and Text  Retrieval Conferences (TREC's).","acronyms":[[94,99],[134,140]],"long-forms":[[59,92],[105,132]]},{"text":"of a sentence or phrase, in turn, is necessary for automatic purchased of sizeable lexical knowledge, such as subcategorization frames and argument structures, which is used in many natural language treatment (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;","acronyms":[[217,220]],"long-forms":[[188,215]]},{"text":" Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manuals assigned dozens from 0","acronyms":[[87,91],[109,114],[66,72]],"long-forms":[[75,85],[98,107],[43,64]]},{"text":"method of |P (grams)|. This guarantees that neither trees of maximum height (MHT) nor of maximum degree (MDT), i.e. trees which trivially","acronyms":[[72,75],[100,103]],"long-forms":[[56,70],[84,98]]},{"text":"entries in an existing knowledge bases is called entity tying and has been proposed and studied in the Knowledge Base Populace (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang,","acronyms":[[131,134],[175,178]],"long-forms":[[104,129],[149,173]]},{"text":"using the distributional similarity metric described by Lin (1998). We use WordNet (WN) as our sense inventory.","acronyms":[[84,86]],"long-forms":[[75,82]]},{"text":"ID full papers domain (TCS) 10 BB web pages domain (BB) 2 BI summarized domain (BS) 10 Table 1: Characteristics of BioNLP-ST 2011 principal tasks.","acronyms":[[79,81],[0,2],[23,26],[31,33],[52,54],[114,123]],"long-forms":[[58,70]]},{"text":"In Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In Discovery Proceedings (DESI-4). ","acronyms":[[122,128]],"long-forms":[[99,120]]},{"text":"helpful in identifying the embedded words. However  some ambiguous segmentation strings(ASSs) and  unregistered words (i.e. the word that is not registered ","acronyms":[[88,92]],"long-forms":[[57,87]]},{"text":"(domain specific) region. The upper region of the on-  tology is called the Ontology Base (OB) and contains  approximately 400 items that represent generalizations ","acronyms":[[91,93]],"long-forms":[[76,89]]},{"text":"sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. ","acronyms":[[131,133]],"long-forms":[[110,129]]},{"text":"discussed in section 2, are represented.  Evaluation of machine translation (MT) systems has to consider the pre-processing of input and","acronyms":[[77,79]],"long-forms":[[56,75]]},{"text":"irrespective of word order.  Longest Common Substring (LCS): This measures the longest sequence of words interchange between","acronyms":[[55,58]],"long-forms":[[29,53]]},{"text":"on a 1,000 TU, EN-IT test set. B=Basic, LI=vocabulary identification, QE=quality estimation, WE=word embedding. ","acronyms":[[68,70],[91,93],[11,13],[15,20],[33,38],[40,42]],"long-forms":[[71,89],[94,108],[43,66]]},{"text":"77 .73  Problem 2nd 1.45 3.00  GS = Group significant cannot pool by individual  DISCUSSION OF THE RESULTS ","acronyms":[[31,33]],"long-forms":[[36,53]]},{"text":" UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston and Burnard, 1998).","acronyms":[[44,46],[58,60],[1,3],[13,15],[28,30]],"long-forms":[[49,56],[63,79],[6,11],[18,26],[33,42]]},{"text":"ical analyzer for English (Sekine, 2001)are selected and translation candidates having POS tags other than NN (noun) are discarded. Selected translation","acronyms":[[107,109],[87,90]],"long-forms":[[111,115]]},{"text":"words in similar context have similar meanings ?  distributed semantic models (DSM)s build vector representations based on corpus-extracted context.","acronyms":[[79,82]],"long-forms":[[50,77]]},{"text":"2.4 Longest Common Substring  Given two strings, T of length n and H of length m,  the Longest Common Sub-string (LCS) problem  (Dan, 1999) will find the longest string that is a ","acronyms":[[114,117]],"long-forms":[[87,105]]},{"text":"For case, a problem-tagged entity is represented as a first word tagged B-P (begin problem) and other 59","acronyms":[[76,79]],"long-forms":[[81,94]]},{"text":"inference procedures?Markov Chain Monte Carlo (MCMC) (Johnson et al 2012), Expectation Maximization (EM), and Variational Bayes (VB)10?as good as the discourse-level model outline above.","acronyms":[[129,131],[47,51],[101,103]],"long-forms":[[110,127],[21,45],[75,99]]},{"text":"The Chinese text is segmented with a segmenter trained on CTB data using conditional random realms (CRF). ","acronyms":[[100,103],[58,61]],"long-forms":[[73,98]]},{"text":"parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method","acronyms":[[114,120]],"long-forms":[[83,112]]},{"text":"Recall all the methods rely on the wrapped classifier. We select two classic but very different classifiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree). We implement these","acronyms":[[136,142]],"long-forms":[[113,128]]},{"text":"single underweight matrixW . Onto contrast, the CVG uses a syntactically untied RNN (SU-RNN) which has a set of such weights.","acronyms":[[78,84]],"long-forms":[[52,76]]},{"text":"The  suggestions made here for the organization of space are only a working  set for which l i t t le  justification can be providing: location (LOC)--a  neutral statement of poste, contact--in corporeal contact, and -* near ","acronyms":[[143,146]],"long-forms":[[133,141]]},{"text":"ously capture the semantics of words and verdicts, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA). Yet, our","acronyms":[[122,125],[86,89]],"long-forms":[[94,120],[60,84]]},{"text":"2004). Accordingly, we define in-NE probability  to help eliminate and create named entities (NE). ","acronyms":[[91,93],[30,35]],"long-forms":[[75,89]]},{"text":"This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate re-","acronyms":[[97,103],[123,126]],"long-forms":[[80,95]]},{"text":"end 1 <= V I ;   SPAN (SPANS, 'CONSTITUENTS ' ) = <TUP , CONSTITUENTS>;  TODO = TUP -t TODO;  r e t u r n ;  ","acronyms":[[80,83],[51,54]],"long-forms":[[84,86]]},{"text":"Abstract In this paper, we address the problem of converting Dialectal Arabic (DA) text that is typed in the Latino script (called","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"ANTEST R E T U R N S  ** 1**  CHANGT, HAVE CSEXCE1 FOR HESGEF IN  ARTEST CALLEC FOR 18\"REGVO I C E u  (AACC)  ANTEST E T U E N S  ** l** ","acronyms":[[103,107],[43,50],[51,54],[55,61],[0,6]],"long-forms":[[66,96]]},{"text":"the vertex of the Abox which it expresses. Our current model uses the Tree Adjoining Grammar (TAG) formalism, see Joshi (1987), and the Atree acts as a","acronyms":[[94,97]],"long-forms":[[70,92]]},{"text":"In second method we compare CLIR performance of the two systems using Crossing Parlance Evaluation Forum (CLEF) 2007 ad-hoc bilingual track (Hindi-English) docu-","acronyms":[[103,107],[28,32]],"long-forms":[[70,101]]},{"text":" 1 Introduction The goals of statistical machine translation (SMT) is to producing a target sentence e from a source sen-","acronyms":[[61,64]],"long-forms":[[28,59]]},{"text":"VNMT 32.25 34.50++ 33.78++ 36.72?++ 30.92?++ 24.41?++ 32.07 Table 1: BLEU dozens on the NIST Chinese-English translation chore. AVG = average BLEU scores on testing sets. We","acronyms":[[141,145],[0,4],[69,73],[88,92],[127,130]],"long-forms":[[133,140]]},{"text":" 207  Informations Contribution (IC) function for each element  in a pair.","acronyms":[[34,36]],"long-forms":[[6,32]]},{"text":"at a supermarket 2 1 1 0 able unable 1 0 0 1 Table 1: Word-based Levenshtein distance (LD) feature and separated edit operations (D = deletions, I","acronyms":[[87,89]],"long-forms":[[65,85]]},{"text":"man::n a::d Figure 2: Lexical Only Centered Tree (LOCT) be::v","acronyms":[[50,54]],"long-forms":[[22,48]]},{"text":"In the impending section we will explain the concep-  tual paragon of ILMs by means of the KADS Domain  Description Language (DDL) proposed in Schreiber  (Schreiber et al, 1993).","acronyms":[[119,122],[63,67],[84,88]],"long-forms":[[97,117]]},{"text":"Daniele Vannella, 2013). The two system typing are WSI (Word Sense Induction) and WSD (Word Sense Disambiguation).","acronyms":[[50,53],[81,84]],"long-forms":[[55,75],[86,111]]},{"text":"Queue to for user intervention.  4.2,  Document Processor (DP)   The DP determines and extracts all SGML tags de- ","acronyms":[[59,61],[100,104]],"long-forms":[[39,57]]},{"text":"and only these. The possible binary tree structures of ABCD are covered by  ABCD = A(BCD) U (AB)(CD) U (ABC)D.  Since we are to handle binary concatenations only, we consider two concatenations ","acronyms":[[76,80],[55,59]],"long-forms":[[83,88]]},{"text":"Ney discounting and interpolation. The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, including of 1.1 mln words.","acronyms":[[90,93]],"long-forms":[[78,88]]},{"text":"4 Discussion We supplying a brief introduction to the framework of Linear Categorial Grammar (LinCG). One of","acronyms":[[92,97]],"long-forms":[[65,90]]},{"text":"1093  Proceedings of the EACL 2014 Workshop on Genera Theory and Natural Linguistic Semantics (TTNLS), pages 1?9, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[91,96],[25,29]],"long-forms":[[47,89]]},{"text":"AVERAGE 70.8% 70.1% Table 2: Ten-fold cross-validation classification results, using a Na??ve Bayes (NB) or Support Vector Machines (SVM) classifier","acronyms":[[101,103],[133,136]],"long-forms":[[87,99],[108,131]]},{"text":"Recently, a number of deep-learning based models have been proposes for the task of Visual Question Answer (VQA). The per-","acronyms":[[111,114]],"long-forms":[[84,109]]},{"text":" Association for Computational Linguistics.                       ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,                   Unsupervised Lexical Acquisition: Proceedings of the Workshop of the","acronyms":[[109,115],[66,69]],"long-forms":[[70,107]]},{"text":" Across Proceedings of the International Conference on Computational Linguistics (COLING-04). ","acronyms":[[78,87]],"long-forms":[[51,76]]},{"text":"In order to make it man-  ageable and intuitive, it employs yntactic constructs  called Typed Feature Structures (TFSs). The ,~vocab- ","acronyms":[[114,118]],"long-forms":[[88,112]]},{"text":"3 HCMUS 6L OpenNLP OpenNLP Dict Rules - Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,","acronyms":[[113,115]],"long-forms":[[116,132]]},{"text":"Hyderabad, India Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper","acronyms":[[52,55]],"long-forms":[[26,50]]},{"text":"get node and another node on the dependency parsed tree: ANC (ancestor), DES (descendant), SIB (sibling), and TARGET (target word). Figure 5 shows","acronyms":[[110,116],[57,60],[73,76],[91,94]],"long-forms":[[118,124],[62,70],[78,88],[96,103]]},{"text":"Succinct   This paper describes two algorithms which construct two differ-  ent kinds of generators for lexical functional grammars (LFGs). The ","acronyms":[[133,137]],"long-forms":[[104,131]]},{"text":"match with a hypothesis. These weights are  confidence measures: Logical Sufficiency (LS)  and Logical Necessity (LN).","acronyms":[[86,88],[114,116]],"long-forms":[[65,84],[95,112]]},{"text":"every lost arc translates to a set of lost parts, we can avoid repeating computations by storing the partial loss of every arc in a data structure (DS): e ?? ","acronyms":[[148,150]],"long-forms":[[132,146]]},{"text":"set of basically two algorithms. One algorithm is a  variant of Alignment Based Learning (ABL), as  described in Van Zaanen (2001).","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":"Linear-chain CRFs correspond to finite sate machine, and can be roughly understood as conditionally-trained hidden Markov models (HMMs). This class of CRFs","acronyms":[[132,136]],"long-forms":[[110,130]]},{"text":"The proceedings from CoNLL2004 and  CoNLL2005 detail a wide assortment of approaches  to Semantic Role Labeling (SRL).  Many re-","acronyms":[[110,113],[21,30],[36,45]],"long-forms":[[86,108]]},{"text":"In addition, adding the soft joint-inference formula results in further gain, and our full system (FULL) attained an F1 of 55.5. ","acronyms":[[99,103]],"long-forms":[[86,90]]},{"text":"study on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) expressions almost in news do-","acronyms":[[116,119],[9,12],[82,85],[97,100],[127,130],[144,147]],"long-forms":[[103,114],[75,81],[88,96],[122,126],[136,143]]},{"text":"2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (deter-","acronyms":[[69,72],[86,89],[100,104],[116,119]],"long-forms":[[74,83],[91,97],[106,113],[121,126]]},{"text":"340   Proceedings of the EACL 2014 Workshop on Kinds Theory and Natural Vocabulary Semantics (TTNLS), pages 37?45, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[91,96],[25,29]],"long-forms":[[47,89]]},{"text":"logical structure of the text is a boundary between two logical segments (see Figure 1).  The mode is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorithms for topic shifts detect (Hearst, 1997).","acronyms":[[131,134]],"long-forms":[[111,129]]},{"text":" We compared the resultant ranked lists of bigrams with a list of intents MWEs extracted from the British National Corpus (BNC)3. The target list was pro-","acronyms":[[122,125],[73,77]],"long-forms":[[97,120]]},{"text":" LTP. LTP (Language Technology Platform  established by HIT) is a package of tools to ","acronyms":[[6,9],[1,4],[54,57]],"long-forms":[[11,39]]},{"text":" 1 Introduction Referring expressions (REs) are expressions intended by loudspeakers to identify entities to hearers.","acronyms":[[39,42]],"long-forms":[[16,37]]},{"text":" Each verb-noun pair was presented to turkers via Amazon Mechanical Turk (AMT) and they were asked to describe (by text) the changes of","acronyms":[[74,77]],"long-forms":[[50,72]]},{"text":"Yoram Bachrach is a researcher in the Online Services and Advertising group at Microsoft Research Cambridge UK. His research area is artificial intelligence (AI), focusing on multi-agent systems and computational game theory.","acronyms":[[158,160],[108,110]],"long-forms":[[133,156]]},{"text":"nodes  where  the re lat ion cor responds  to a sneeze .   Verb  nodes  (VERBSTR)  conta in  a po in ter  to the  RE-  LATION constituted  by  the  verb ?","acronyms":[[71,78]],"long-forms":[[57,61]]},{"text":"meaning of the other. Examples are:  * Senior researches assistant at the Belgian National Fund for Science Research (F.N.R.S.). ","acronyms":[[119,127]],"long-forms":[[89,117]]},{"text":"fier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring","acronyms":[[95,98]],"long-forms":[[65,93]]},{"text":"Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total","acronyms":[[130,133]],"long-forms":[[108,128]]},{"text":" 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle","acronyms":[[28,30]],"long-forms":[[16,26]]},{"text":"NE translation.   Nevertheless NE translation is lesser sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT.","acronyms":[[90,92],[0,2],[27,29],[136,138],[175,177]],"long-forms":[[69,88]]},{"text":"The project ? Reference Corpus Middle Low German\/ Low Rhenish (1200?1650)?2 transliterates and grammatically annotates the Middle Low German (GML) texts from which we take our examples. Be-","acronyms":[[142,145]],"long-forms":[[95,133]]},{"text":"latent topic space. Values within vectors are the TF-ITF (term frequency - inverse topic frequency) scores which are calculated in a completely ana-","acronyms":[[50,56]],"long-forms":[[58,98]]},{"text":"sentences. The third, following (Yates et al, 2006), is maximum recall (MR). MR simply predicting that all","acronyms":[[72,74],[77,79]],"long-forms":[[56,70]]},{"text":"The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. ","acronyms":[[107,110]],"long-forms":[[78,105]]},{"text":"or more precedents turns in the dialogue. The third column shows the mean error and standard error (SE) predictions for the model specified by the first two columns. When","acronyms":[[98,100]],"long-forms":[[82,96]]},{"text":"Learning attitudes and attributes from multi-aspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020?1025.","acronyms":[[77,81],[89,93]],"long-forms":[[61,75]]},{"text":" Snow et al (2008) analysed the use of the Amazon Mechanistic Turk (MTurk) web service for gathering annotations for a variety of natural lan-","acronyms":[[67,72]],"long-forms":[[50,65]]},{"text":"system components goes through GDM, thereby insu-  lating parts from each other and providing a uniform  API (applications programmer interface) for manip-  ulating the data produced by the system.","acronyms":[[105,108],[31,34]],"long-forms":[[110,143]]},{"text":"Not Availability?.  OmegaWiki (OW) is a freely editable online dictionary like WKT.","acronyms":[[28,30],[76,79]],"long-forms":[[17,26]]},{"text":"Then the lexicon Chinese Semantic  Dictionary (CSD) containing sense descriptions  and the corpus Chinese Senses Pool (CSP) annotated with senses are built interactively, simulta-","acronyms":[[119,122],[47,50]],"long-forms":[[98,117],[17,45]]},{"text":"BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Discourses; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs. ","acronyms":[[105,107],[133,135],[0,2],[30,32],[51,54],[90,92]],"long-forms":[[110,127],[138,145],[5,28],[35,49],[57,88],[95,103]]},{"text":"tl,...,tM i=2 i=N+I   (7)  which is a Nth-order Markovian chain for the language model (MLM). ","acronyms":[[88,91]],"long-forms":[[48,86]]},{"text":"The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality; DP = Discriminatory Power; Train = Trainability; Typing = Hardwired Type Choosing; Hum = Humanity Preference Modelling; FB = Full Brevity .","acronyms":[[219,221],[103,105],[130,135],[185,188],[169,173]],"long-forms":[[224,236],[108,128],[138,150],[191,196],[152,156]]},{"text":"{kmpark, rim}@nlp.korea.ac.kr 1 Intro The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Ill, etc.)","acronyms":[[73,76]],"long-forms":[[49,71]]},{"text":"l (CAUSER sally-l)  (OBJECT paint-l)  (PATH (path-1 (DESTINATION wall-l))))  Solly sprayed the wall with paint.","acronyms":[[39,43],[10,17],[28,35]],"long-forms":[[45,51]]},{"text":"On the other hand, multi-lingual  ontology is very important for naturel language  processing, such as machine translation (MT), web  mining (Oyama et al 2004) and rist language ","acronyms":[[124,126]],"long-forms":[[103,122]]},{"text":"  2. PrecisionCorrectTransliteration  (PTrans)  The precision is going to be computed using the ","acronyms":[[39,45]],"long-forms":[[5,36]]},{"text":"We have been concentrating on the Hong  Kong Hansard, which are the parliamentary pro-  ceedings of the Legislative Council (LegCo). Anal- ","acronyms":[[125,130]],"long-forms":[[104,123]]},{"text":"posterior distribution.  We utilize maximum entropy (MaxEnt) model  (Birger et al, 1996) to design the basic classifier ","acronyms":[[53,59]],"long-forms":[[36,51]]},{"text":"The projection  of the root node on the active leaves is referred to  as the M-BDU (Main BDU). Only syntactic infor-","acronyms":[[77,82]],"long-forms":[[84,92]]},{"text":"Table 2: Overall scores of whole tasks as well as separately for each annotation format in terms of labeled precision (LP), reminded (LR) and F 1","acronyms":[[118,120],[131,133]],"long-forms":[[99,116],[123,129]]},{"text":" 1 Introduction Approaches to Machine Translation (MT) usage Data-Oriented Parsing (DOP: (Bod, 1998; Bod et","acronyms":[[51,53],[84,87]],"long-forms":[[30,49],[61,82]]},{"text":"source yes nicer Table 8: A comparison of syntactic SMT procedures (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer.","acronyms":[[75,78],[53,56],[104,108],[143,145]],"long-forms":[[79,102],[109,141],[146,161]]},{"text":"To scrutinize the impact of the quality of annotation resources, we furthermore use a Chinese language analysis tool: Language Technology Platform (LTP) (Che et al, 2010).","acronyms":[[138,141]],"long-forms":[[108,136]]},{"text":"Context-Free Grammars 2.1 Minimalist Grammars A Minimalist Grammar (MG) (Stabler and Keenan, 2003)1 is a five-tuple","acronyms":[[68,70]],"long-forms":[[48,66]]},{"text":"acoustic equivocation. We measure performance in  terms of character error rate (CER), which is the  number of characters wrongly conversions from the ","acronyms":[[78,81]],"long-forms":[[56,76]]},{"text":"The knowledge about actions and plans is stored in  a scheme librarians structured on the basis of two main hier-  archies: the Disintegration Hierarchy (DH) and the  Generalization Hierarchy (GH) \\[Kautz and Allen, 86\\].","acronyms":[[148,150],[187,189]],"long-forms":[[123,146],[161,185]]},{"text":"rently, a pure statistical MT system based on Pharaoh is developed by BPPT and National News  Agency (ANTARA) using 500K sentences pair,  expected to have better accuracy and robustness ","acronyms":[[102,108],[27,29],[70,74]],"long-forms":[[79,100]]},{"text":" Indoors this framework, in this paper we describe our tries to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for","acronyms":[[96,99],[105,107]],"long-forms":[[72,94]]},{"text":"For example, in (2) we find frames identifying baseform verbs (VB) (2a) and frames identifying cardinal numbers (CD) (2b), despite having a variety of context words.","acronyms":[[113,115],[63,65]],"long-forms":[[95,103],[56,60]]},{"text":"corpora (section 2.2).  The workflow for named entity (NE) and  terminology extraction and mapping from ","acronyms":[[55,57]],"long-forms":[[41,53]]},{"text":"the sentences produced. Additionally, automated machine translation (MT) metrics are explored to quantify the amount of information missing from","acronyms":[[69,71]],"long-forms":[[48,67]]},{"text":"In particular, we use transition probability and emission probability in Hidden Markov Model (HMM) (Leek, 1997) to capture this dependency.","acronyms":[[94,97]],"long-forms":[[73,92]]},{"text":" (d) The word?s position in the sentence (e) The word?s Part of speech (POS) labeling, groundwork on the Stanford POS tagger2","acronyms":[[72,75],[104,107]],"long-forms":[[56,70]]},{"text":" Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text","acronyms":[[49,53]],"long-forms":[[24,47]]},{"text":"on a 1,000 TU, EN-IT test set. B=Basic, LI=language identification, QE=quality estimation, WE=word embedding. ","acronyms":[[68,70],[91,93],[11,13],[15,20],[33,38],[40,42]],"long-forms":[[71,89],[94,108],[43,66]]},{"text":" 1 Introduction  Natural Language Generation (NLG) systems gotta of  course be evaluated, like all NLP systems.","acronyms":[[46,49],[98,101]],"long-forms":[[17,44]]},{"text":"NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective  Table h Habits for partOf(cava,building) ","acronyms":[[70,73],[0,2],[11,16],[32,35],[50,54],[88,91]],"long-forms":[[76,86],[5,9],[19,30],[38,48],[57,68],[94,103]]},{"text":"152 Kuhn A Investigated and Classification of Monitored Natural Languages Controlled Language for Crisis Governance (CLCM) (Temnikova 2010, 2011, 2012) is a language for writing instructions about how to deal with crisis situations.","acronyms":[[112,116]],"long-forms":[[69,110]]},{"text":"Abbreviations POS = Part of Speech NE = Named Entity CE = Correlated Entity","acronyms":[[35,37]],"long-forms":[[40,52]]},{"text":"We show that hierarchies of this type can be  automatical!y constructed, by using the semantic ategory codes and the subject codes of the  Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in  noun definitkms.","acronyms":[[183,188]],"long-forms":[[139,181]]},{"text":"Two criteria are used:  1. Overlapping Ambiguity Chaining (OAS): the  reference segmentation and the segmenter ","acronyms":[[58,61]],"long-forms":[[27,56]]},{"text":"Re-moving PHI from these free text portions requires application of techniques from natural language processing that are capable of identifying phrases of specific types based on the lexical content (the words that make up the phrases) and the surround-ing words.  3 Current Methods and Metrics Fortunately, the problem of identifying types of information in free text is a well-studied problem in the natural language processing community. We can leverage several decades of research on infor-mation extraction and the named entity identifica-tion problem in particular, including multiple community evaluations such as the Message Un-derstanding Conferences (MUC) (Grishman & Sundheim, 1996) and the subsequent Automated Content Extraction (ACE) evaluations1 ? both fo-cused on extraction from newswire -- as well as evaluations of biomedical entity extraction from the published literature e.g., in the BioCreative evaluations (Krallinger, et al, 2008).","acronyms":[[743,746]],"long-forms":[[713,741]]},{"text":"the better between all other symmetrization heuristics. The other was a Tree Edit Distance (TED) model, popularly used in a series of NLP appli-","acronyms":[[88,91],[130,133]],"long-forms":[[68,86]]},{"text":"To extract the verb?noun combinations that have been used by non-native speakers in practice, we use the Cambridge Learner Corpus (CLC), which is a 52.5 million-word corpus of learner En-","acronyms":[[131,134]],"long-forms":[[105,129]]},{"text":" ? Reduce(RE): Pop the stack. ","acronyms":[[10,12]],"long-forms":[[3,9]]},{"text":"implicit (assuming a good enough coverage of the marker resource). The Annodis corpus lists rhetorical relations between elementary discourse units (EDUs), typically clauses, and complex discourse units (sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb of","acronyms":[[149,153],[212,216],[257,261]],"long-forms":[[121,147]]},{"text":"Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney","acronyms":[[83,87]],"long-forms":[[62,81]]},{"text":" BBLT Input Screen      We originally developed BBLT for ourselves as machine translating (MT) developers and evaluators, to promptly see the meanings of Arabic strings","acronyms":[[91,93],[1,5],[48,52]],"long-forms":[[70,89]]},{"text":" (Choudhury et al, 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisy-","acronyms":[[86,89]],"long-forms":[[65,84]]},{"text":"In Table 1, we show the precision, recall, and F1 measures of our domain-aware method (DOM) and the baseline method (BL) in all three sets of experiments.","acronyms":[[117,119],[87,90]],"long-forms":[[100,108],[66,85]]},{"text":"5 Conclusion We have presented an efficient extension of the posterior regularization (PR) framework to a more general class of penalty functions.","acronyms":[[87,89]],"long-forms":[[61,85]]},{"text":" We excluded only punctuation; we did no filtering for part of speech (POS). Each word was genuinely","acronyms":[[71,74]],"long-forms":[[55,69]]},{"text":"the two probability distributions is calculated used a norms information-theoretic measure, the Kullback Leibler (KL-)divergence: K\u0002L^M_NQP\u0012R`NQS S54%6\u001a7","acronyms":[[118,121],[146,149],[134,143]],"long-forms":[[100,116]]},{"text":"most blogged about articles? of the New York Times (NYT)1. ","acronyms":[[52,55]],"long-forms":[[36,50]]},{"text":"Currently a large proportion of languageindependent MT approaches are based on the  statistical machine translation (SMT) paradigm  (Koehn, 2010).","acronyms":[[117,120],[52,54]],"long-forms":[[84,115]]},{"text":"Theoretically, the expressive power of transformed the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have qualified Hyper Lexicalized Shue Vicinity Grammars (HTAGs) (Pustejovsky[13]) . ","acronyms":[[257,262],[67,70],[76,79]],"long-forms":[[214,255]]},{"text":"tion results in a better AER. Running ALP with eradicated (TD) templates followed by multi-word (TMW ) template results in a lower AER than running ALP","acronyms":[[57,59],[25,28],[38,41],[130,133],[147,150],[95,98]],"long-forms":[[47,55],[61,93]]},{"text":"ments. We experimented with several classifiers including: SVM, Logistic Regression (LR), and Naive Bayes.","acronyms":[[85,87],[59,62]],"long-forms":[[64,83]]},{"text":"In Proc. of the 38th Annum Meeting of the Association for Computational Language (ACL), pages 440?447, Hong Kong, October.","acronyms":[[86,89]],"long-forms":[[43,84]]},{"text":" (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.","acronyms":[[81,85],[38,44],[2,5]],"long-forms":[[50,79],[21,36]]},{"text":"(? 2.2), we propose an initiating algorithm based on Integer Linear Programming (ILP). Figure 2","acronyms":[[80,83]],"long-forms":[[52,78]]},{"text":" 3.1 Annotating message-level data Amazon?s Mechanical Turk (MTurk) was used to annotate the 5,000 random English (American)","acronyms":[[61,66]],"long-forms":[[44,59]]},{"text":"a new father node. The following simple rule forms a  noun phrase (NP). ","acronyms":[[67,69]],"long-forms":[[54,65]]},{"text":" The structure of a Concept is completed by its set of  Structural Descriptions (SD's). These express how the ","acronyms":[[81,85]],"long-forms":[[56,79]]},{"text":"would also be possible.  Personalized PageRank similarity (PPR) (Agirre and Soroa, 2009) measures the semantic relatedness between two word senses s","acronyms":[[59,62]],"long-forms":[[25,57]]},{"text":"In Proceedings of the Third International Conference on Web Search and Web Data Mining (WSDM), pages 101?110, New York, NY, USA, 2010.","acronyms":[[88,92],[120,122],[124,127]],"long-forms":[[56,86]]},{"text":" won out on F-measure while giza++ syllabized attained better alignment error rate (AER). Refer to Table 3 for","acronyms":[[84,87]],"long-forms":[[62,82]]},{"text":"http:\/\/www.cs.ualberta.ca\/?yx2\/research.html 59 class-based hidden Markov paragon (HMM) model (Zhang et al, 2003).","acronyms":[[81,84]],"long-forms":[[60,79]]},{"text":"given the precorrected sentence. Each Noun Phrase (NP) in  the essays sentence will be pre-corrected as correc-","acronyms":[[51,53]],"long-forms":[[38,49]]},{"text":"forma is  employed to denote funct aeon  i t se l f .   b) Conceptual  Expression St ructure  (CPS) is  a data  s t ruc ture  in which syntact i c  and semant ic  in fo rma-  ","acronyms":[[89,92]],"long-forms":[[57,86]]},{"text":"1 Introduction Since the introduction of BLEU (Papineni et al, 2002), automatic appliance translation (MT) evaluating has received a lot of research interest.","acronyms":[[101,103],[41,45]],"long-forms":[[80,99]]},{"text":"number, fifteen predefined verbs (Paulo, 2010)  are opting as Light Verbs (LVs) for framing  the compound verbs (CompVs).  A manually ","acronyms":[[112,118],[74,77]],"long-forms":[[96,110],[61,72]]},{"text":"We also used two Korean discourse recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Techs) and a Korean commercial speech recog-","acronyms":[[72,80]],"long-forms":[[82,106]]},{"text":"ly correct. An n-gram (n= 2 and 3) language model was then construct from the Sinica corpus released  by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) utilized  the SRILM toolkit (Stolcke, 2002).","acronyms":[[182,188],[201,206]],"long-forms":[[107,180]]},{"text":"2 BP Neural Network  In the moment, there are about more than 30 kinds of  artificial neural network (ANN) in the domain of  research and asks.","acronyms":[[102,105],[2,4]],"long-forms":[[75,100]]},{"text":" 2003. Voice extensible markup language (VoiceXML) version 2.0.","acronyms":[[41,49]],"long-forms":[[7,39]]},{"text":" 3.3 Issue Classification We look next at question classification (QC). ","acronyms":[[70,72]],"long-forms":[[45,68]]},{"text":" 1 Introduction Information Extract (IE) is a natural language processing task in which text documents are ana-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"tion), and thus mitigating the overfitting problem.  A Dirichlet process (DP) prior is typically used to attaining this interplay.","acronyms":[[74,76]],"long-forms":[[55,72]]},{"text":"we use three categories for the identically spelled words: (a) we use the term true equivalents (TE) to refer to the pairs that have the same","acronyms":[[97,99]],"long-forms":[[79,95]]},{"text":" 5 Conclusions and Future Work Distributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Models (CDSM) that effectively encode structural information and distributional semantics in tractable 2-","acronyms":[[59,62],[132,136]],"long-forms":[[31,57],[85,129]]},{"text":"many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Covert Markov Model (UM) (Vogel et al, 1996).","acronyms":[[116,119],[50,53]],"long-forms":[[95,114]]},{"text":"and Chinese (ZH). The second section gives the result for the English (EN) test set, PTB Section 23. ","acronyms":[[71,73],[13,15],[85,88]],"long-forms":[[62,69],[4,11]]},{"text":"ded systems. It is widely used by software certification authorities such as FAA (Federal Aviation Association), and it establishes some guidelines and","acronyms":[[77,80]],"long-forms":[[82,110]]},{"text":"hierarchical phrase-based model, but constrained so that the English part of the right-hand side is restricted to a Greibach Normal Form (GNF)like structure: A contiguous sequence of termi-","acronyms":[[138,141]],"long-forms":[[116,136]]},{"text":"alternatively as uniform as possible (Berger et al, 1996). maximum entropy model (MaxEnt) is known to easily combining diverse features and","acronyms":[[78,84]],"long-forms":[[55,70]]},{"text":"4.2 Results Table 3 shows the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL).","acronyms":[[85,90],[126,129],[140,147]],"long-forms":[[67,83]]},{"text":"cal machine translation. The 41th Annual meeting of the Association for Computational Language (ACL), 311-318.","acronyms":[[99,102]],"long-forms":[[56,97]]},{"text":" 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner","acronyms":[[76,79],[152,155]],"long-forms":[[50,74],[126,150]]},{"text":"answers accordingly. Specially, we develop a supervised Maximum Entropy (MaxEnt) based modelled to rescore the answers from the pipelines,","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":"The proceedings from CoNLL2004 and  CoNLL2005 detail a wide variety of approaches  to Semantic Role Labeling (SRL).  Many re-","acronyms":[[110,113],[21,30],[36,45]],"long-forms":[[86,108]]},{"text":"Context-Free Grammars 2.1 Minimalist Grammars una Minimalist Grammar (MG) (Stabler and Keenan, 2003)1 is a five-tuple","acronyms":[[68,70]],"long-forms":[[48,66]]},{"text":"7 8  Figure 2: The Sense Distribution  the helps of a graphical user intefface(GUI) scans a  parsed sample article and indicate a series of se- ","acronyms":[[78,81]],"long-forms":[[53,76]]},{"text":"TH + DR + EG + LC 56.51 TH + EG + LC 56.50 Personage Genera (CT) 51.96 Word Familiarity (WF) 51.50","acronyms":[[59,61],[0,2],[5,7],[10,12],[15,17],[24,26],[29,31],[34,36],[87,89]],"long-forms":[[43,57],[69,85]]},{"text":" 4.1.7 Doctors? Prescriptions (PRESC) Some of our food-health relations are also men-","acronyms":[[31,36]],"long-forms":[[16,29]]},{"text":" 4.3 The Limited-Memory BFGS Algorithm The limited memory BFGS (L-BFGS) algorithm is a general purposes numerical optimization algorithms (Nocedal and Wright 1999).","acronyms":[[64,70],[24,28]],"long-forms":[[43,62]]},{"text":")( bwN We utilize the corpus provided by IR task of  NTCIR2 (NTCIR 2002) as the training set to  compute the mutual information of mots.","acronyms":[[49,55],[37,39]],"long-forms":[[57,67]]},{"text":" First we used the C&C Combinatory Categorial Grammar (CCG) parser5 (C&C) by Clark and Curran (2004) using the biomedical model described in","acronyms":[[69,72],[19,22],[55,58]],"long-forms":[[23,53],[77,93]]},{"text":" 8. Adjacent Variety(AV) of the candidate. Ours","acronyms":[[21,23]],"long-forms":[[4,19]]},{"text":"pseudo-terms?. We also discuss the use of Disguised Markov Models (HMMs) to capture contextual information.","acronyms":[[64,68]],"long-forms":[[42,62]]},{"text":"back. We define a currency for annotation cost as Annotation cost Units (AUs). Onto an annotation bud-","acronyms":[[73,76]],"long-forms":[[50,71]]},{"text":"ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Generation (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in","acronyms":[[202,210],[33,41],[101,105],[118,122],[158,166]],"long-forms":[[172,200]]},{"text":"workbench (Hall et al, 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for","acronyms":[[83,86],[34,37]],"long-forms":[[53,74]]},{"text":" 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mostly","acronyms":[[41,44],[7,10]],"long-forms":[[11,39]]},{"text":"2010). Domain event extracting has been advanced in singular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have","acronyms":[[90,92],[70,76]],"long-forms":[[77,88]]},{"text":"PP Fu?r diese Behauptung has the grammatical function OAMOD, which indicates that it is a modifier (MOD) of a direct object (OA) elsewhere in the structure (in this case keinen","acronyms":[[100,103],[125,127],[0,2],[54,59]],"long-forms":[[90,98],[105,109]]},{"text":"analysis. To include more of the corpus, parameters are relaxed: the high groups (HH) involves anyone whose score is above .5 SD","acronyms":[[81,83],[125,127]],"long-forms":[[69,73]]},{"text":"would have higher perplexity.  Syntactic Score (SC) Some erroneous sentences often contain words and concept that are locally cor-","acronyms":[[48,50]],"long-forms":[[41,46]]},{"text":"3.1 Graph-based parsing model The graph-based parsing model aims to search for the maximum spanning tree (MST) in a graph (McDonald et al, 2005).","acronyms":[[106,109]],"long-forms":[[83,104]]},{"text":"processing applications, su ch as larg e-vocab u lary speech recog nition (L V CS R), statistical machine translation (S M T ) and information retrieval (IR), is the morpholog ical analy sis of w ords.","acronyms":[[154,156],[119,124],[75,83]],"long-forms":[[131,152],[86,117],[34,73]]},{"text":"EXPERIMENTS 3.1. Speaker Identification (SID) Across order to investigate robust speaker identification under","acronyms":[[41,44]],"long-forms":[[17,39]]},{"text":" 3. Constructions a character list (CH)3, in which the characters are top 20 frequency in training","acronyms":[[32,34]],"long-forms":[[16,25]]},{"text":"lem, and our system adopted a supervised learning approach with Maximum Entropy classifier, which is widely used in natural language processing(NLP). ","acronyms":[[144,147]],"long-forms":[[116,142]]},{"text":"it as a model for gathering their own corpora.  The Russian National Corpus (RNC) has been released by the group of specialists from various organi-","acronyms":[[77,80]],"long-forms":[[52,75]]},{"text":"4-gram + LSA using linear interpolation  with ? LSA = 0.11 (LI). ","acronyms":[[60,62],[9,12]],"long-forms":[[48,51],[19,39]]},{"text":"on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed","acronyms":[[77,80],[37,42],[101,104]],"long-forms":[[55,75]]},{"text":"We labeled the Reddit dataset use crowdworkers on Amazon Mechanical Turk (AMT), creating the firstly public corpus annotated with levels of dogmatism.","acronyms":[[76,79]],"long-forms":[[52,74]]},{"text":"The third is end position (EP), after a predi-  cate. Pre position (PreP) and post position (PostP)  are provided for adverbs as modifiers.","acronyms":[[68,72],[93,98],[27,29]],"long-forms":[[54,66],[78,91],[13,25]]},{"text":"88 lated bases on a co-occurrence relationship between i and w. Next, the semantic orientation (SO) of the phrase i is obtained by calculating the divergence be-","acronyms":[[96,98]],"long-forms":[[74,94]]},{"text":"  2.1. The behavior of manner adverbs (MA) and sentence  adverbs (SA) in the sentence is widely different: ","acronyms":[[39,41],[66,68]],"long-forms":[[23,37],[47,64]]},{"text":" First we uses the C&C Combinatory Categorial Grammar (CCG) parser5 (C&C) by Clark and Curran (2004) using the biomedical model outline in","acronyms":[[69,72],[19,22],[55,58]],"long-forms":[[23,53],[77,93]]},{"text":"cf. Webber 1987b), representing the narrative's unfold-  ing contents, and the l inear text structure (LTS), whom  ingredient are linked by rhetorical relations such as ","acronyms":[[103,106]],"long-forms":[[79,101]]},{"text":"CFG. The proof is based on a lexicalization procedure connected to the lexicalization  procedure utilized to create Greibach normal form (GNF) as presented in Harrison 1978. ","acronyms":[[132,135],[0,3]],"long-forms":[[110,130]]},{"text":"such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP),  are big structures with some.k~y ","acronyms":[[97,99],[29,33],[49,51],[75,78]],"long-forms":[[85,95],[8,28],[37,48],[54,74]]},{"text":"2011.  Overview of the Infectious Diseases (ID) task of BioNLP Share Task 2011.","acronyms":[[44,46],[56,62]],"long-forms":[[23,42]]},{"text":" Because we don't have any other useful resources except ChineseGigaword(CGW),We first computed mutual information for all 3-character words in two","acronyms":[[73,76]],"long-forms":[[57,72]]},{"text":" 3.4.1 Arabic Named Entity Recognising Named Entity Recognition (NER) is a subtask of information extraction, where each proper designation in","acronyms":[[65,68]],"long-forms":[[39,63]]},{"text":"In Proceedings of the 2012 ACM Interntional Conference on Intelligent User Interfaces (IUI), pages 189?198. ","acronyms":[[87,90],[27,30]],"long-forms":[[58,85]]},{"text":"where cLen is the length of a pattern; cMatch is  the number of matched tags; cPtn is the number of  protein names tag (PTN) skipped by the alignment  in the penalties;  cVb is the number of skip ","acronyms":[[119,122],[168,171],[78,82],[6,10],[39,45]],"long-forms":[[101,113],[18,24],[64,71]]},{"text":"the Association for Computational Linguistics (ACL) and 17th World Conference on Computational Language (COLING) 1998, pages 41?47, Montre?al.","acronyms":[[116,122],[47,50]],"long-forms":[[89,114],[4,45]]},{"text":" BIBLIOGRAPHY  ALPAC (Automatic Language Processing Advisory Committee)  1966 Lanquage and Machines - Computers i n  Translations and Linguistics ","acronyms":[[15,20]],"long-forms":[[22,70]]},{"text":"our system; where we achieved 0.627 top1 accuracy for Japanese transliterated to Japanese Kanji(JJ), 0.713 for English-toChinese(E2C) and 0.510 for English-to-","acronyms":[[96,98],[129,132]],"long-forms":[[81,94],[111,128]]},{"text":"Lioma C. and Ounis I., A Syntactically-Based Query  Reformulation Technique for Information Retrieval,  Information Processing and Management (IPM), Elsevier Science, 2007 ","acronyms":[[143,146]],"long-forms":[[104,141]]},{"text":"37  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 97, Gothenburg, Sweden, April 27, 2014.","acronyms":[[73,78]],"long-forms":[[39,71]]},{"text":"tions of words. In Proceedings of the International Conference on Computational Linguistics (COLING), Bombay, India, Aug.","acronyms":[[93,99]],"long-forms":[[66,91]]},{"text":" 3 In this work, we use HL (Hu and Liu, 2004), MPQA (Wilson et al.,","acronyms":[[24,26],[47,51]],"long-forms":[[28,38]]},{"text":" From the results shown in Table 3, we could find the proposal semantic word embedding (SWE) model can continually achieve 0.8% (or more) ab-","acronyms":[[88,91]],"long-forms":[[63,86]]},{"text":"     - French\/English (fra-eng)    Seven Recognising Textual Entailment (RTE)  evaluation tracks have already been held: RTE-1 ","acronyms":[[73,76]],"long-forms":[[41,71]]},{"text":"taught is straightforward.  Very Reduced Regular Expression (VRRE):  Given a finite alphabet E,  the set of very ","acronyms":[[63,67]],"long-forms":[[30,61]]},{"text":" 1 Introduction Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context","acronyms":[[42,45]],"long-forms":[[16,41]]},{"text":" 3.1 Sentence Splitting (SS) Sentence Splitting (SS) is the rewriting of a sentence by breaking it into two or more sentences,","acronyms":[[49,51],[25,27]],"long-forms":[[29,47],[5,23]]},{"text":"than have them specified in a tag dictionary.  The Lexicon HMM (Lex-HMM) extends the Pitman-Yor HMM (PYP-HMM) described by","acronyms":[[64,71],[101,108]],"long-forms":[[51,62],[85,99]]},{"text":"Versa Gap 0.072 0.033 Table 1: Percentage of reordering patterns ` invert gap (RG): The two source phrases are not adjacent, and are in the reverse orders as","acronyms":[[82,84]],"long-forms":[[69,80]]},{"text":"In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 50?57, Stroudsburg, PA.","acronyms":[[100,103],[132,134]],"long-forms":[]},{"text":" 4. Template Relation(TR) recognise: Finding the relation between TEs and a question","acronyms":[[22,24]],"long-forms":[[4,20]]},{"text":"approach to structuring knowledge is based  on:  z automatic term gratitude (ATR)  z automatic term clustering (ATC) as an ","acronyms":[[79,82],[114,117]],"long-forms":[[51,77],[87,112]]},{"text":"This system tags, lemmatizes and parses corpus data using the current version of the RASP (Robust Accurate Statistical Parsing) toolkit (Briscoe et al, 2006), and on the basis of resulting","acronyms":[[85,89]],"long-forms":[[91,126]]},{"text":" Figure 5 also gives a speed comparison of our method to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0?","acronyms":[[79,81]],"long-forms":[[59,77]]},{"text":"three stages. A source-language input string is rewritten to form an information retrieval (ROE) query.","acronyms":[[92,94]],"long-forms":[[69,90]]},{"text":" Introduction  The DARPA ATIS Spoken Language System (SLS) task  represents ignificant new challenges for speech and naturel ","acronyms":[[54,57],[19,24],[25,29]],"long-forms":[[30,52]]},{"text":"5.2 Results We evaluate CONSEQUENTLY of words on three varying sized corpora: Gigaword (GW) 6.2GB, GigaWord + 50% of web data (GW+WB1) 21.2GB and Gi-","acronyms":[[80,82],[24,26],[119,121],[122,125]],"long-forms":[[70,78]]},{"text":" 5.3   Learning Algorithm: Conditional Random Field  Conditional Random Fields (CRF) is a formalism well-suited for learning and prediction on sequential data.","acronyms":[[80,83]],"long-forms":[[53,78]]},{"text":"Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). ( Section 2.1) ","acronyms":[[100,102]],"long-forms":[[85,96]]},{"text":"Table 1: Number of routes, directions, and tokens for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor.","acronyms":[[78,80],[96,98],[116,118]],"long-forms":[[83,94],[101,114],[121,135]]},{"text":"mance figures are the standard measures used for this task: F-measure (harmonic mean of recall and precision) and slit error rate (SER), where separate type, extent and content error measures are averaged to get the briefed outcomes.","acronyms":[[131,134]],"long-forms":[[114,129]]},{"text":"representative popular heterogeneous corpora, i.e. 232 Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD).","acronyms":[[78,81],[109,112]],"long-forms":[[60,76],[87,107]]},{"text":"in the clustering.  Cumulative Micro Precision (CMP). As pointed","acronyms":[[48,51]],"long-forms":[[20,46]]},{"text":" Opennlp maxent1, an implementing of Maximum Entropy (ME) modeling, is used as the classification tool.","acronyms":[[56,58]],"long-forms":[[39,54]]},{"text":"For each sense s i of the target words, we place a Hierarchical Dirichlet process (HDP) prior on the mixture proportion to latent concepts shown as follows:","acronyms":[[83,86]],"long-forms":[[51,81]]},{"text":"Averaged Reca l l  (AR)  : 2  PP.~.NP ? Averaged Prec is ion  (AP)  : 2  (fl3-1-1)?PPxPR ?","acronyms":[[63,65],[20,22],[30,32],[35,37],[83,85],[86,88]],"long-forms":[[40,60],[0,17]]},{"text":"Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008).","acronyms":[[121,125]],"long-forms":[[101,119]]},{"text":"all characteristics are included as specifics; full eliminating (P4), where all special Twitter features like user names, URLs, hashtags, retweet (RT ) tags, and emoticons are stripped; and replacing Twitter fea-","acronyms":[[136,138],[54,56],[111,115]],"long-forms":[[127,134]]},{"text":"2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd.","acronyms":[[162,167],[178,180]],"long-forms":[[121,160]]},{"text":"email: allan.ramsay@manchester.ac.uk Debora Field University of Sheffield (UK) email: D.Field@sheffield.ac.uk","acronyms":[[75,77]],"long-forms":[[50,73]]},{"text":"  Abstract  The Colorado Literacy Tutor (CLT) is a  technology-based literacy program, designed ","acronyms":[[41,44]],"long-forms":[[16,39]]},{"text":"These are selected from the LDC English Gigaword corpus. AFP = Agence France-Presse; AFW = Associated Press Worldstream; NYT = New York Times; XIN = Xinhua News Organizations; and CNA = Central News Agency of Taipei denote the sections of the LDC Englishman Gigaword","acronyms":[[121,124],[143,146],[28,31],[57,60],[85,88],[173,176],[236,239]],"long-forms":[[127,141],[149,160],[63,83],[91,119],[179,198]]},{"text":"person names, organisation names, location names, etc. The template element (TE) task extracts information centered approximately an entity, like the acronym,","acronyms":[[77,79]],"long-forms":[[59,75]]},{"text":"lar expressions. So the detection of factoid words  can be achieved by Finite State Automaton(FSA). ","acronyms":[[94,97]],"long-forms":[[71,92]]},{"text":" We first examined tre randomly selected part of the listings  in the American Heritage Word Frequency Book (AHWFB).side by side  with the corresponding entry lists of the American Heritage School Dic- ","acronyms":[[114,119]],"long-forms":[[75,112]]},{"text":"for the three sponsoring agencies. The TIPSTER  Research and Evaluation Committee (REC) was  charged with oversight responsibility of the 15 ","acronyms":[[83,86]],"long-forms":[[48,81]]},{"text":"PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense  NU = Number ","acronyms":[[131,133],[0,3],[14,20],[45,47],[55,57],[76,78],[88,90],[99,101],[119,121],[143,145]],"long-forms":[[136,141],[6,13],[23,43],[50,54],[70,75],[81,87],[93,97],[104,118],[124,130],[148,154]]},{"text":"scoring the candidate set of which classroom of anaphoric expression (DNOM = definite  NP, PER{I,2,3} = outset\/second\/third person pronouns, POS{1,2,3} = first\/second\/third  person possessives, RELA = relative pronouns, REFL = reflexive\/reciprocal pronoun). ","acronyms":[[189,193],[215,219],[66,70],[83,85],[87,90],[136,139]],"long-forms":[[196,204],[222,231],[73,81]]},{"text":"Genius neighborhood of bank  bank  Subject Code EG = Engineering  river wall flood thick ","acronyms":[[53,55]],"long-forms":[[58,69]]},{"text":"sided brevity penalty (C = 0.01). Table 2 demonstrating the average compression rates (CompR) for McDonald (2006) and our modelling (STSG) as well as their perfor-","acronyms":[[79,84],[121,125]],"long-forms":[[60,77]]},{"text":" Definition 2.5  Given a grammar, G, define MCL(G) (Maximum Change in Length) as:  MCL(G) = max { m \\] A (.. q\/1. . .","acronyms":[[44,47],[83,86],[92,95]],"long-forms":[[52,76],[25,32]]},{"text":"each frame from each camera view.  3.3 Dyadic Features (DF)  All of the characteristic discussed above are low-level ","acronyms":[[56,58]],"long-forms":[[39,54]]},{"text":" Association for Computational Linguistics.          ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,        Morphological and Phonological Learning: Proceedings of the 6th Workshop of the","acronyms":[[108,115]],"long-forms":[[57,106]]},{"text":"For WSD evaluation, three measures are used: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold","acronyms":[[64,66],[4,7]],"long-forms":[[49,62]]},{"text":"identification. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 2585?2602, Hyderabad, India.","acronyms":[[98,104]],"long-forms":[[71,96]]},{"text":"tive two-step model. We compare models based on the Akaike Information Criterion (AIC). ","acronyms":[[82,85]],"long-forms":[[52,80]]},{"text":"class of mildly context sensitive grsmmars which we  230  call \"Ranked Node Rewriting Grammaxs\" (RNRG's). ","acronyms":[[97,103]],"long-forms":[[64,94]]},{"text":"(AvgToRecipients) in emails sends by p, the ratio of emails p received in which he\/she was in the To list (InToList%), boolean specifics denoting whether p added or removed people when","acronyms":[[111,120],[1,16]],"long-forms":[[95,109],[43,53]]},{"text":"They are the One-error Loss (O-Loss) function, the Symmetric Loss (S-Loss) function, and the Hierarchical Loss (H-Loss) function: ?","acronyms":[[112,118],[29,35],[67,73]],"long-forms":[[93,110],[13,27],[51,65]]},{"text":"(Roelofs, 2004), experiments on priming (Schvaneveldt et al., 1976) or the tip of the tongue problem (TOT) (Brown and McNeill, 1996).","acronyms":[[102,105]],"long-forms":[[75,92]]},{"text":"transcripts produced with Automatic Speech Recognition  (ASR) systems tend to contain many recognition errors,  leading to low Information Retrieval (IR) performance  (Oard et al, 2007).","acronyms":[[150,152],[57,60]],"long-forms":[[127,148],[26,54]]},{"text":"F is a frame name, E a frame elements behalf, and t and s are sequences of word indices (t is for the target (FEE)) Utilizes this measure of partial agreement, we now","acronyms":[[107,110]],"long-forms":[[91,105],[7,17],[29,41]]},{"text":" Bidirectional CLSTM Graves and Schmidhuber (2005) proposed a Bidirectional LSTM (B-LSTM) model, which utilizes additional backward informa-","acronyms":[[82,88],[15,20]],"long-forms":[[62,80]]},{"text":"represents the editing cycle. Given a  Semantic Network (SN) in a knowledge base (KB),  the system generates a description of the SN in the ","acronyms":[[57,59],[82,84],[130,132]],"long-forms":[[39,55],[66,80]]},{"text":" i=1:n P (GR = gri|SCF = s) The three terms, given the hyper-parameters and","acronyms":[[10,12]],"long-forms":[[15,22]]},{"text":"Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a challenging topic in Natural Language Processing","acronyms":[[110,112],[77,79]],"long-forms":[[85,108],[56,75]]},{"text":"prove SRL performance.  Template Generation (TG)  Our templates generation (TG) algorithm extracts ","acronyms":[[45,47],[6,9],[75,77]],"long-forms":[[24,43],[54,73]]},{"text":"1 Introduction  Turkic Discourse Banque (TDB) is the outset discourse-annotated corpus of Turkish, which follows the  principles of Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) and includes annotations for dis-","acronyms":[[156,160],[40,43]],"long-forms":[[130,154],[16,38]]},{"text":"Certain schemes have been targeted at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), authorizes the username to query facts using conclusions, results, etc. For full papers, the most notable collaborating has orientated on argumentative zones (ARIZONA) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ implies seizing the attribution of knowledge claims and citation functions, and the scheme has been tested on information extraction and summarisation task with Computational Linguistics papers.","acronyms":[[387,389],[494,496]],"long-forms":[[365,385]]},{"text":"176 Figure 2: The embedding graph for the sentence Our earlier consequence illustrates that recombinant gp41 (aa565-647), the extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human monocytes.","acronyms":[[201,206],[140,145]],"long-forms":[[185,199]]},{"text":"Association measurement Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood percentages (LL) and chi-square (?","acronyms":[[127,130],[155,157]],"long-forms":[[97,125],[133,147]]},{"text":"presented at the 16th Worldwide Conference on Computational Linguistics, Copenhagen. Gupta, D., Saul, M., & Gilbertson, J. (2004). Appraise of a deidentification (DE-ID) software engine to exchanges morbid reports and clinical documents for research. Am J Clin Pathol, 121(2), 176-186.","acronyms":[[169,174]],"long-forms":[[151,167]]},{"text":"Abstract This paper introduces a tensor-based approaches to semantic role labeling (SRL). The motiva-","acronyms":[[82,85]],"long-forms":[[58,80]]},{"text":"of which are limited in scope. We here restrict inferables to the particular subset de-  fined by Hahn, Markert, and Strube (1996), which we call functional anaphora (FA). ","acronyms":[[167,169]],"long-forms":[[146,165]]},{"text":" ? Domain communicat ion knowledge (DCK). This is knowledge about how to communi- ","acronyms":[[36,39]],"long-forms":[[3,34]]},{"text":"Natural Language Processing (NLP) technical can be leveraged in detect events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are talked in the text that defines an events. To perform Named Entity Recognizing (NER) on tweets Ritter et. al. (","acronyms":[[302,305]],"long-forms":[[276,300]]},{"text":" 2 Hidden Markov Models Hidden Markov models (HMMs) are commonly used to constituted a wide range of linguistic phe-","acronyms":[[46,50]],"long-forms":[[24,44]]},{"text":"and normalizing it with a view to discriminate the importance of words across documents and then approximating it using singular value decomposition(SVD) in R dimensions (Bellegarda, 2000).","acronyms":[[149,152]],"long-forms":[[120,147]]},{"text":"the toolkits. Sizes are given for the resulting transducers (VM = Verbmobil). ","acronyms":[[61,63]],"long-forms":[[66,75]]},{"text":"cjlin\/libsvm\/ System P R F1 Schwartz & Hearst (SH) .978 .940 .959 SaRAD .891 .919 .905","acronyms":[[47,49]],"long-forms":[[28,45]]},{"text":"Nigh 2,500 sets of related words in  LLOCE are organized according to 14 theme and 129 topics (TOP). Rist references (REF) between sets,  topics, and subjects are also given to show many inter-sense r lations not captured within the same topic.","acronyms":[[124,127]],"long-forms":[[112,122]]},{"text":"As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments.","acronyms":[[352,355],[232,234],[256,258],[373,376]],"long-forms":[[318,346]]},{"text":" Classifier models. We used a first-order linear chain conditional random fields (CRF) model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a","acronyms":[[82,85],[138,144]],"long-forms":[[55,80],[121,136]]},{"text":"In order to solve idiomatic expressions as well as  collocations and frozen compound nouns, we  have developed the compound unit(CU)  recognizer (Jung et.","acronyms":[[129,131]],"long-forms":[[115,127]]},{"text":"include: 1) candidate frequency and its distribution in different Web pages, 2) length ratio between source terms and target candidates (S-T), 3)  distance between S-T, and 4) keywords, key ","acronyms":[[137,140],[164,167]],"long-forms":[[101,135]]},{"text":" ? Max Similarity (MaxSim): At tuple ? in an","acronyms":[[19,25]],"long-forms":[[3,17]]},{"text":" We evaluate performance use 3 measures:  exact match (EM), head match (HMM), and partial  match (PM), similar to Choi et al (2006).","acronyms":[[57,59],[74,76],[99,101]],"long-forms":[[44,55],[62,72],[83,97]]},{"text":"Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an Idiomatic chunk), I-I (Inside an Idiomatic chunk),","acronyms":[[106,109],[139,142],[70,73],[175,178]],"long-forms":[[111,130],[144,166],[75,97],[180,199]]},{"text":"F-measure between precision and recall over the sure and possibles. They argue that it is a better alternating to the commonly utilised Alignment Error Rate (AER), which does not suitably sanction unbalanced precision and recall.9 As our corpus is mono-","acronyms":[[154,157]],"long-forms":[[132,152]]},{"text":"For exam-  ple, an analysis of the texts using Mann and Thomp-  son's (1987) Rhetorical Structure Theory (RST) would  result primarily in the relations sequence  and jo in t  ","acronyms":[[106,109]],"long-forms":[[77,104]]},{"text":" 1 Introduction Medical relation (MR) classification, an information extraction task in the clinical domain that was recently defined in the 2010 i2b2\/VA Challenge (Uzuner et al.,","acronyms":[[34,36]],"long-forms":[[16,32]]},{"text":"well. The functionary framework for analysis will be the Discourse Representation Theory (DRT). ","acronyms":[[85,88]],"long-forms":[[52,83]]},{"text":"logical structure of the text is a boundary between two logical segments (see Figure 1).  The method is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorithm for topic shifts detection (Hearst, 1997).","acronyms":[[131,134]],"long-forms":[[111,129]]},{"text":"Harman D.K. 1983. Overview of the second Text Salvaging Conference (TREC-2). Information Processing","acronyms":[[68,74],[7,10]],"long-forms":[[34,66]]},{"text":"Pos i t i ve  Recal l  (PR)  :  ? Pos i t ive  Prec is ion  (PP)  :  d ?","acronyms":[[61,63],[24,26]],"long-forms":[[34,58],[0,21]]},{"text":"Table 3: Experimental Results (Microsoft?s Provided Train and Test Set) kind the sentences pairs of the MSRP corpus pursuant to the length difference ratio (LDR) defined in Section 3, and partitioned the graded cor-","acronyms":[[160,163],[106,110]],"long-forms":[[135,158]]},{"text":"summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) peculiarities. ","acronyms":[[103,106]],"long-forms":[[89,101]]},{"text":"On the standard view in transformational theory (Chomsky, 1981) both topic raising and object raising, or Wonderful Case Marking (ECM), cases are explained by the same principles.","acronyms":[[134,137]],"long-forms":[[108,132]]},{"text":" 50 missed OG events were labeled as Past (PA) while FU events were commonly mislabeled as both PA","acronyms":[[43,45],[11,13],[53,55],[96,98]],"long-forms":[[37,41]]},{"text":"3.2 Featuring Space An vital aspect of our approach below is the word sense disambiguation (WSD) of the noun. Us-","acronyms":[[94,97]],"long-forms":[[67,92]]},{"text":"associated with each. The next column indicates the percentage of the majority class (MAJ.) and count","acronyms":[[86,90]],"long-forms":[[70,78]]},{"text":"ping observed sequences to possible ground truth sequences.  We do not use the Character Error Rate (CER) metric, since for almost all NLP applications the unit of","acronyms":[[101,104],[135,138]],"long-forms":[[79,99]]},{"text":"100 Another measure of accuracy that is frequently usage is the so called Out Of Vocabulary (OOV) measurement, which represents the percentage of words that was not recog-","acronyms":[[92,95]],"long-forms":[[73,90]]},{"text":"(AvgToRecipients) in emails sent by p, the percentage of emails p received in which he\/she was in the To list (InToList%), boolean features denoting whether p added or removed people when","acronyms":[[111,120],[1,16]],"long-forms":[[95,109],[43,53]]},{"text":"Abstract   In recent years, statistical approaches on  ATR (Automatic Term Recognition) have  achieved good results.","acronyms":[[55,58]],"long-forms":[[60,86]]},{"text":"to the Markov Reasoning system. At each step, we compute both the maximum a posteriori (MAP) allocation of coreference relationships as well","acronyms":[[84,87]],"long-forms":[[62,82]]},{"text":"al., 2005) filtering and summarizes the OmniPage output into Intermediate XML (IXML), as well as correcting certain attribute errors from that stage.","acronyms":[[77,81]],"long-forms":[[59,75]]},{"text":"110  ehange(CHA) communication(COMM)  cognition(COG) competing(COMP)  contact(CeNT) motion(MOT) ","acronyms":[[48,51],[65,69]],"long-forms":[[38,46],[53,63]]},{"text":"Furthermore,  the dependency framework is arguably nearer to  semantics than the phrase structure grammar (PSG)  if the dependency relations are judiciously chosen.","acronyms":[[100,103]],"long-forms":[[74,98]]},{"text":"mantic content Ks, sending a messaging m to R and R  interprets m as meaning CR. CS = cn is a neces-  sary condition for this turn of communication to ","acronyms":[[77,79],[15,17],[73,75]],"long-forms":[[82,87]]},{"text":"subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links between phrases, and the entire taxonomy is genus of fragmented into a many-rooted DAG (oriented acyclic graphics). More-","acronyms":[[168,171]],"long-forms":[[173,195]]},{"text":"rule in a CFG. It can  therefore populate the ACity (ArrivalCity) or the  DCity (DepartureCity) slot, and instantiate a ","acronyms":[[42,47],[10,13],[70,75]],"long-forms":[[49,60],[77,90]]},{"text":"for candidate summary sentence chooses  by standard page rank algorithms used in  Information Retrieval (IR). As Bengali is ","acronyms":[[107,109]],"long-forms":[[84,105]]},{"text":" In Proceedings of the 17th International Conference on Computational Linguistic (COLING), Montreal, Canada.","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":" CTexT. 2005. CKarma (C5 KompositumAnaliseerder vir Robuuste Morfologiese Analise). [ C5 Compounds","acronyms":[[14,20],[1,6]],"long-forms":[[22,47]]},{"text":"schematic representations of situations involving  various participants, props, and other conceptual  roles, each of which is a frame element (FE). ","acronyms":[[143,145]],"long-forms":[[128,141]]},{"text":"S# for the count in non-speculative ones)  3 Means  Conditional random fields (CRF) model was  firstly lodged by Lafferty et al (2001).","acronyms":[[81,84]],"long-forms":[[54,79]]},{"text":"data with which to test research hypotheses. We de-  scribe the Aeroplane Travel Information System (ATIS) pilot  corpus, a corpus designed to measuring progress in Spo- ","acronyms":[[95,99]],"long-forms":[[64,93]]},{"text":"matical relations on the arcs).  Grammatical Relation (GR) trees: If we substitution the words at the nodes by their relation to their corre-","acronyms":[[55,57]],"long-forms":[[33,53]]},{"text":"Instead to measure topic coherence we follow (Newman et al, 2009) to compute the Pointwise Mutual Information (PMI) of topic words w.r.t wikipedia articles.","acronyms":[[111,114]],"long-forms":[[81,109]]},{"text":"vestigate four factors: text length (TL), sentence length (SL), average number of phrase per sentence (WS), and average number of traits per word (CW). Since","acronyms":[[150,152],[37,39],[59,61],[102,104]],"long-forms":[[129,148],[24,35],[42,57],[82,100]]},{"text":"Electronic Text Encoding and Interchange were first published in April 1994 and were initially based on Standard Generalized Markup Language (SGML).  The","acronyms":[[142,146]],"long-forms":[[104,140]]},{"text":"The most common and observable route to  deal with disjunctive constraints i to expanding the grammat-  ical description to disjunctive normal form (DNF) during a  pre-processing step, thereby eliminating disjunction from the ","acronyms":[[141,144]],"long-forms":[[116,139]]},{"text":"based on such formalisms include Generalized Phrase  Structure Grammar (GPSG) \\[Gazdar et al 1985\\],  Lexical Functional Grammar (LFG) \\[Bresnan 1982\\],  Functional Unification Grammar (bUG) \\[Kay 1984\\], ","acronyms":[[130,133],[72,76],[186,189]],"long-forms":[[102,128],[33,70],[165,184]]},{"text":"Averaged Reca l l  (AR)  : 2  PP.~.NP ? Averaged Prec is ions  (AP)  : 2  (fl3-1-1)?PPxPR ?","acronyms":[[63,65],[20,22],[30,32],[35,37],[83,85],[86,88]],"long-forms":[[40,60],[0,17]]},{"text":" 2 Story Segmentation using Modified Kmeans (MKM) Clustering The first step in multi-document summarization is","acronyms":[[45,48]],"long-forms":[[28,43]]},{"text":"dependency analyzer, respectively.  (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, ","acronyms":[[62,66],[85,90],[144,147],[224,228],[248,252]],"long-forms":[[69,83],[93,103],[56,60],[41,50],[151,159],[164,172],[231,246],[255,267]]},{"text":" Bidirectional CLSTM Graves and Schmidhuber (2005) proposed a Bidirectional LSTM (B-LSTM) model, which utilizing additional backward informa-","acronyms":[[82,88],[15,20]],"long-forms":[[62,80]]},{"text":"4 Crowd-sourcing Multiple-choice Questions from Tables We use Amazon?s Mechanical Turk (MTurk) service to generate MCQs by imposing constraints","acronyms":[[88,93],[115,119]],"long-forms":[[71,86]]},{"text":"As a starting point, the classes for complements  and features developed by the New York Univer-  sity Linguistic String Project (LSP) (Fitzpatrick,  1981), were selected sin(x, the coverage is very ","acronyms":[[130,133]],"long-forms":[[103,128]]},{"text":"JOPER (enclitic personally) DEMON (demonstrative) 1 SING (singular) INTG (interogative) 2 PLUR (plural) CREFX (common reflexive) 3","acronyms":[[66,70],[88,92],[0,5],[26,31],[50,54],[102,107]],"long-forms":[[72,84],[94,100],[33,46],[7,24],[56,64],[109,125]]},{"text":"This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output.","acronyms":[[105,107]],"long-forms":[[84,103]]},{"text":"In this example, only Midas can be choosing for the role of dumb, but any member of the class PN (proper names) having the attributes male, valiant and handsome can be","acronyms":[[92,94]],"long-forms":[[96,108]]},{"text":"lowing rules are using in the detailed instance.  if tense of el and of e2 is simple past (SP)  with perfective AP tben there is justification for ","acronyms":[[89,91],[110,112]],"long-forms":[[76,87]]},{"text":"2.4 Optimisation and Sampling from a WCFG Optimisation in a weighted CFG (WCFG)3, that is, finding the maximum derivation, is well stud-","acronyms":[[74,78],[37,41]],"long-forms":[[60,72]]},{"text":"the percentages of phrases that were placed in a segment totally identical to that in the reference. The dialogue act based metric (DER) was proposed in Zimmermann et al (2005). In this metric a word is","acronyms":[[131,134]],"long-forms":[[104,129]]},{"text":"f ~ r l r ~ f  TRANSFORIATICNS *llrllrt*  SCAN C A L L E D  AT 1 I  ANTEST CALLED FOR 1 '*REDVOW \" (AACC) ,SD= 2. RES= 6.","acronyms":[[82,85],[100,104],[107,109],[114,117]],"long-forms":[[68,81]]},{"text":"targeted text. This genre of question has been studied extensively in the Text Retrieval Conference Question Responses (QA) Track (Dang, Kelly, and Lin 2007). Using the","acronyms":[[119,121]],"long-forms":[[99,117]]},{"text":"3.5 A Novel Lattice Statistical Language Paragon Representation Our final statistical linguistic model is a novel latent-variable statistical language model, called a Partial Lattice MRF (PL-MRF), with rich latent structure, displays in Figure 3. The","acronyms":[[184,190]],"long-forms":[[163,182]]},{"text":"3 Framework We model the information extraction task as a markov decisions process (MDP), where the model learns to utilize external sources to augment upon","acronyms":[[83,86]],"long-forms":[[58,81]]},{"text":"restrictive relative clause, and epithets ? trigger conventional implicatures (CI) whose truth is necessarily presupposed, even if the truth conditions","acronyms":[[80,82]],"long-forms":[[53,78]]},{"text":"utilised the written and spoken language corpora.  Occurrence probabilities (OPs) of expressions in the written and spoken vocabulary corpora can be used to dis-","acronyms":[[74,77]],"long-forms":[[48,72]]},{"text":" CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) managed by the Media Development Authority (MDA)","acronyms":[[81,84],[1,6],[135,138]],"long-forms":[[51,79],[106,133]]},{"text":"Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu  Matt Huenerfauth Department of Computer Scientifically Queens Academia and Graduate Center City University of Newest York (CUNY) 65-30 Kissena Blvd, Rinsing, NY 11367 matt@cs.quebec.cuny.edu  Abstract  American Sign Language (ASL) recap software can improves the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of contemporary ASL animation generation and scripting systems have limited handling of the countless ASL verb signs whom movement routes is inflected to indicate 3D locations in the signing space associated with discourse refer-ents.","acronyms":[[363,366],[87,91],[118,120],[263,267],[299,301],[530,533],[611,614]],"long-forms":[[339,361],[58,85],[108,116],[234,261]]},{"text":"Having presented the sequent parser, we now show its embedding in the learning algorithm GraSp (Grammar of Speech). ","acronyms":[[89,94]],"long-forms":[[96,113]]},{"text":" Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtained","acronyms":[[76,79]],"long-forms":[[59,74]]},{"text":"various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (","acronyms":[[126,128],[69,72]],"long-forms":[[108,124],[44,67]]},{"text":"owning (PO)  process (PR)  quantity (QU)  relation (RE) ","acronyms":[[41,43],[12,14],[26,28],[56,58]],"long-forms":[[31,39],[0,10],[17,24],[46,54]]},{"text":" 1 In t roduct ions   Word Sense Disambiguation (WSD) is an open prob-  lem in Natural Language Processing.","acronyms":[[48,51]],"long-forms":[[21,46]]},{"text":"is necessary to train sentence prediction models, a third approach that uses labeled comment data for training (CTr) but sentences for testing (STe) is included in the CTR\/STE row.","acronyms":[[144,147],[112,115],[168,175]],"long-forms":[[121,142],[85,110]]},{"text":"word-level features described in Section 5.  4.1 Ranking by regression (RR) The first ranking strategy is based on training a re-","acronyms":[[72,74]],"long-forms":[[49,70]]},{"text":"trees than the height-one rules of standard contextfree grammars. Tree substituting grammars (TSGs) have been shown to reinforce upon the standard En-","acronyms":[[94,98]],"long-forms":[[66,92]]},{"text":" 4 DOM Tree Alignment Model  The Document Object Model (DOM) is an application programming interface for valid HTML ","acronyms":[[56,59],[3,6],[111,115]],"long-forms":[[33,54]]},{"text":" Single-tokenization of compound verbs  and named entities (NE) provides major gains over the baseline PB-SMT ","acronyms":[[60,62],[109,115]],"long-forms":[[44,58]]},{"text":"3.1.1 Directed Acyclic Diagram The general SPRITE modelled can be thought of as a dense directed acyclic graph (DAG), where every document or subjects is connected to every compo-","acronyms":[[107,110]],"long-forms":[[83,105]]},{"text":"They are the One-error Loss (O-Loss) operandi, the Symmetric Loss (S-Loss) function, and the Hierarchical Loss (H-Loss) function: ?","acronyms":[[112,118],[29,35],[67,73]],"long-forms":[[93,110],[13,27],[51,65]]},{"text":"Here the building of the Chinese VP involves accede a prepositional phrase (PP) and a smaller verbal phrase (VP-A), with the preposition at the beginning as a PP marker.","acronyms":[[114,118],[37,39],[81,83],[164,166]],"long-forms":[[99,112],[59,79]]},{"text":"Conf.  Fifth Generation Computer Systems 1992 (FGCS'92),  pp.1133-1140, 1992.","acronyms":[[47,54]],"long-forms":[[7,45]]},{"text":"1991), can be characterized asknowledge-rich  in that they presuppose that known lexical  subjects possess Conceptual Dependence(CD)-  like descriptions.","acronyms":[[126,128]],"long-forms":[[104,124]]},{"text":"example, we have defined that all the subclasses of #COMMUNICATION-EVENTS (e.g.  #REPORT#,  #CONFIRM#, etc.) map their sentential complements (SENT-COMP)  to THEME, as shown below.","acronyms":[[143,152],[158,163]],"long-forms":[[119,141]]},{"text":"knowledge that we can get from these examples the required information to parse a new input sentencing .  Across our  approaches, examples are annotated with the Structured Chain Tree Correspondence (SSTC) annotation schema  where each SSTC describes a sentence, a representation tree as good as the correspondence b tween substrhzgs in ","acronyms":[[193,197],[229,233]],"long-forms":[[154,191]]},{"text":"that combining additional knowledge sources, including lexical features (LX1) and the non-verbal features, prosody (PROS), motion (MOT), and context (CTXT), yields a further improvement (of 8.8%","acronyms":[[116,120],[131,134],[73,75],[150,154]],"long-forms":[[107,114],[123,129],[55,70],[141,148]]},{"text":"8. Strong forms of pronouns not preceded by preposition (unless they carry IC) t Table 1: Annotation guidelines; IC = Intonation Center 4.2 Evaluation framework","acronyms":[[113,115],[75,77]],"long-forms":[[118,135]]},{"text":"a first-order statistical language model can reduce perplex-  ity by at least a factor of 10 with little computation, while  applying complete natural language (NL) models of syn-  tax and semantics to all partial hypotheses typically requires ","acronyms":[[161,163]],"long-forms":[[143,159]]},{"text":"Headline Generation. In the Proceedings of the  Document Understanding Conference (DUC). ","acronyms":[[83,86]],"long-forms":[[48,81]]},{"text":"respect to phonology, morphology, syntax and the lexicon. Linguistic resources (lexica, corpora) and natural language processing (NLP) tools for such dialects (parsers) are very rare. ","acronyms":[[130,133]],"long-forms":[[101,128]]},{"text":"These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree","acronyms":[[174,178]],"long-forms":[[142,172]]},{"text":"We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD) in our experiments.","acronyms":[[109,112],[78,81],[87,92]],"long-forms":[[93,107],[60,76]]},{"text":"is imperative to train sentence prognosis mannequins, a third approach that uses labeled comment data for training (CTr) but sentences for testing (STe) is included in the CTR\/STE row.","acronyms":[[144,147],[112,115],[168,175]],"long-forms":[[121,142],[85,110]]},{"text":"possibility can be totally exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted","acronyms":[[93,95],[116,119]],"long-forms":[[75,91]]},{"text":" In this work, we explore n-gram models over Minimal Translation Units (MTUs) to explicitly capture contextual dependencies across","acronyms":[[72,76]],"long-forms":[[45,70]]},{"text":"We tested two differing algorithms on text from the Wall Street  Journal (WSJ). Using BBN's part of speech tagger (POST), tagged  text was parsed using the full unification grammar of Delphi to fred ","acronyms":[[115,119],[74,77]],"long-forms":[[92,113],[52,72]]},{"text":"some freedom in the ordering of weighty phrasal categories like  NPs  and adverbial words - for example, in the linear order of  subject (SUB J), direct object (DOBJ), and indirect objects (lOB J)  with respect o one another.","acronyms":[[138,143],[63,66],[161,165],[189,195]],"long-forms":[[129,136],[146,159],[172,187]]},{"text":"Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  English ECS (E-ECS) \\[7\\] for English language. In the translation ","acronyms":[[132,137],[41,44],[79,84]],"long-forms":[[119,130],[65,77]]},{"text":"hardware) but also not immensely specialized (implying that it is not limited too severely in scope of employs). We believe  an architecture composed of a distributed set of processing elements (PE's), each containing local memory and high  speed ESE processors, with a narrow interconnecfion a d communications capability may suit our needs.","acronyms":[[188,192],[240,243]],"long-forms":[[167,186]]},{"text":"clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133?142.","acronyms":[[84,87],[22,25],[26,32]],"long-forms":[[47,75]]},{"text":"other templates, such as vector space model (VSM), Okapi model (Robertson et al, 1994) or language modeling (LM). The pipeline meth-","acronyms":[[103,105],[42,45]],"long-forms":[[87,101],[22,40]]},{"text":"There are two main threads in the research of paraphrasing, i.e., paraphrase acknowledge and paraphrase generation (PG). Paraphrase","acronyms":[[116,118]],"long-forms":[[93,114]]},{"text":"They refer to  syntactical features of a constituent such as number  (NUM), sexes (GEN) etc. and to grammatical functions ","acronyms":[[84,87]],"long-forms":[[76,82]]},{"text":"07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9  Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1  (S_S=simple sentence, C_S=complex sentence)  ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank","acronyms":[[151,154],[172,175],[253,258],[227,234],[198,204]],"long-forms":[[155,170],[176,192]]},{"text":"Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 89, Beijing, Augusto 2010 Multi-Word Phrase as Discourse Relation Markers (DRMs) Aravind K. Joshi","acronyms":[[166,170],[71,74]],"long-forms":[[138,164],[19,39],[112,133]]},{"text":"model, it then shows how meaning specificity affects the language deportment and semantic content of Chinese resultative verb compounds (RVCs). ","acronyms":[[137,141]],"long-forms":[[109,135]]},{"text":"JOPER (enclitic personal) DEMON (demonstrative) 1 SING (singular) INTG (interogative) 2 PLUR (plural) CREFX (common reflexive) 3","acronyms":[[66,70],[88,92],[0,5],[26,31],[50,54],[102,107]],"long-forms":[[72,84],[94,100],[33,46],[7,24],[56,64],[109,125]]},{"text":"Krone 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55) Turk 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95) Tables 1: Parsing precision of the undirected planar parser with unsuspecting (UPlanarN) and label-based (UPlanarL) postprocessing in comparing to the oriented planar (Planar) and the MaltParser arc-eager projective (MaltP)","acronyms":[[316,324],[343,351],[406,412],[455,460]],"long-forms":[[279,314],[398,404],[422,432]]},{"text":"The representative vectors for each phoneme category consist of the mean vectors of the Gaussian Mixture Model (GMM). ","acronyms":[[112,115]],"long-forms":[[88,110]]},{"text":"In that case partial semantic mapping will take place where no Logical Form is being constructing and only referring expressions are avowed in the Discourse Model ? but behold below.  3.2 Lexical Information The output of grammatical modules is then fed on the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG (see [13]) terms used f-structures as areas and grammatical tasks as entry points into the architecture. We show here below the architecture of the system.","acronyms":[[270,272],[328,331]],"long-forms":[[255,268]]},{"text":"that provide a buena compression rate of the text.  3.2 Byte Pair Encoding (BPE) Byte Pair Encoding (BPE) (Gage, 1994) is a sim-","acronyms":[[76,79],[101,104]],"long-forms":[[56,74],[81,99]]},{"text":"FA8750-09-C-0181. The first author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.","acronyms":[[81,84]],"long-forms":[[51,79]]},{"text":" This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). ","acronyms":[[93,95],[73,76]],"long-forms":[[81,91]]},{"text":"subset by eliminating the redundant features.  In this paper, Rough Set Theory (RST) based  featuring selection method is apply for sen-","acronyms":[[80,83]],"long-forms":[[62,78]]},{"text":"transmission envelope; the German and Portuguese services  are sent in the transmission enveloped esigned by the  International Press Telecommunications Council (IPTC). ","acronyms":[[162,166]],"long-forms":[[114,160]]},{"text":"The scores are lowercased BLEU computed on the held-out devtest set. NE = named entities. ","acronyms":[[71,73],[26,30]],"long-forms":[[76,90]]},{"text":"also accessible through a phrase interiors reordering. A negative consequence of source order (SO) score as done by (Zhang et al, 2007) and (Li","acronyms":[[94,96]],"long-forms":[[80,92]]},{"text":"Aux classify the NPs according to their type in biomedical terms, we have passed the Sequence Ontology (SO)2 (Eilbeck and Lewis, 2004).","acronyms":[[104,106],[16,19]],"long-forms":[[85,102]]},{"text":"entire sentence length.  SyntaxBased (SyntB): contextual features have been computed according to the ?","acronyms":[[38,43]],"long-forms":[[25,36]]},{"text":"When using only the former type of idiosyncrasies function, our classifier is equivalent to a maximum entropy (MaxEnt) model. ","acronyms":[[104,110]],"long-forms":[[87,102]]},{"text":" We apply both support vector machine (SVM)  and Maximum Entropy (ME) algorithms with the  help of the SVM-light4 and Mallet5 tools.","acronyms":[[66,68],[39,42],[103,106]],"long-forms":[[49,64],[15,37]]},{"text":"Numerous different algorithms  have been used for this task, including some  machine learns (ML) algorithms, such as  Na?ve Bayesian model, decision trees, and ","acronyms":[[91,93]],"long-forms":[[73,89]]},{"text":"They thus attract research from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that ut-","acronyms":[[89,92]],"long-forms":[[76,87]]},{"text":"didate substitutes, as described below.  Lexical Baseline (LB): In this approach we use the pre-existing lexical resources to provide a rank-","acronyms":[[59,61]],"long-forms":[[41,57]]},{"text":"extrinsic and language independent features.  The student response analysis (SRA) task (Dzikovska et al 2013) addresses the fol-","acronyms":[[77,80]],"long-forms":[[50,75]]},{"text":"trouble so that it includes finding the most probable corrections tags.  WDCLOREI arg max Pr( WDCLOREIIA ) WDCLOREI ","acronyms":[[94,104],[107,115]],"long-forms":[[73,89]]},{"text":"On the basis of these specifications, a mapping between VAML and Concrete AML (CAML) can be made. CAML","acronyms":[[79,83],[56,60],[98,102]],"long-forms":[[65,77]]},{"text":" 2.2 Recognizing subwords An automatic speech recognition (ASR) system (Jelinek, 1998) serves to recognize both queries","acronyms":[[59,62]],"long-forms":[[29,57]]},{"text":"Table 7: Feature template we use for our classification of event types. Feature examples are based on the sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ? he doesn?t","acronyms":[[192,194]],"long-forms":[[187,190]]},{"text":" 1 Introduction Most Open Information Extraction (Open-IE) systems (Banko et al, 2007) extract textual relational","acronyms":[[50,57]],"long-forms":[[21,48]]},{"text":"et al, 1993). The learning algorithm was inspired  by several Inductive Logic Programs (ILP) sys-  tems and primarily contains of a specific-to-general ","acronyms":[[91,94]],"long-forms":[[62,89]]},{"text":"not enter into speech recognize. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a vibrant programming (DP)?based search algorithm for statistical MT that monotonically translates the input condemnation from left to right.","acronyms":[[108,110],[151,153]],"long-forms":[[87,106]]},{"text":" 5 Conclusions Multiword expressions (MWEs) are a major obstacle that hinder precise natural language processing","acronyms":[[38,42]],"long-forms":[[15,36]]},{"text":"erature for further details.  Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sen-","acronyms":[[54,57]],"long-forms":[[30,52]]},{"text":"extraction. Use the alignments from HIER, we created phrase tables utilized model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks","acronyms":[[96,99],[38,42],[137,143]],"long-forms":[[75,80],[106,135]]},{"text":" This named-entity tagger agendas is based on a first edict Maximum Entropy Markov Model (MEMM) and is described in Yoshida and Tsujii (2007).","acronyms":[[90,94]],"long-forms":[[60,88]]},{"text":"Tables 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity, SL=sentence length). ","acronyms":[[95,97]],"long-forms":[[98,113]]},{"text":"in cooperation between HU Berlin, U Frankfurt and U Jena, conducted in the wider context of the Deutsch Diachron Digital (DDD) initiative. The","acronyms":[[122,125],[23,25]],"long-forms":[[96,120]]},{"text":"The training set consisted of 12,927 text: News reporting (News) about Education (Edu), Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med). ","acronyms":[[154,158],[81,84],[99,103],[121,124],[176,179]],"long-forms":[[131,152],[70,79],[87,97],[112,119],[166,174]]},{"text":"we combine different perspectives, the performance improves and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for run 2 (LPSSVR), and L+P+S with SVR using transductive learning","acronyms":[[99,104],[84,87],[133,139],[157,160]],"long-forms":[[107,129]]},{"text":"Per the time being,  the objects of the testing is the generative component (GC) of this  description enumerating semantic representations (SR's) of sentences. ","acronyms":[[139,143],[76,78]],"long-forms":[[113,137],[54,74]]},{"text":"middle value of 6.5, was tested.  The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency","acronyms":[[59,61]],"long-forms":[[38,44]]},{"text":"Winnows software package.  Maximum Entropy Model (MEM) is  especially suited for integrating evidences from ","acronyms":[[50,53]],"long-forms":[[27,48]]},{"text":"we have established direct contact with the southerly korean delegation through tribal elders .  Figure 2: Random sample of 5 things from study in Section 4: original Google translation (GT), results of aimed paraphrasing translation processes (TP), and a human reference translation.","acronyms":[[182,184],[241,243]],"long-forms":[[162,180],[220,239]]},{"text":" As an example, consider a user who is looking for information on digital video recorders (DVR), in singular, on how she can utilize a DVR with a","acronyms":[[91,94],[133,136]],"long-forms":[[66,89]]},{"text":" We perform our analyses on data from the 20082011 Text Analysis Conference (TAC)1 organized by the National Institute of Standards and Technol-","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":" 1 Introduction A Chinese natural language processing (NLP) platform always includes lexical analyzing (word","acronyms":[[55,58]],"long-forms":[[26,53]]},{"text":"process (PR)  quantity (QU)  relation (RE)  shape (SH) ","acronyms":[[39,41],[9,11],[24,26],[51,53]],"long-forms":[[29,37],[0,7],[14,22],[44,49]]},{"text":"reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).  Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization.","acronyms":[[134,137]],"long-forms":[[120,132]]},{"text":"provided for the slots over the course of the dialog.  These are our Chaining Consistency (SC) features. ","acronyms":[[89,91]],"long-forms":[[69,87]]},{"text":"A closer, more thorough, look at the  LOCATION data suggests that he alto payoff indicated  by the average REC and precision (PRE) scores was  attained because most of the data were listable.","acronyms":[[126,129],[107,110]],"long-forms":[[115,124]]},{"text":"apply shallow semantic (selectlonal) constraints, to filter out semantically anomalous parses, in a  second piloting. This procedure used PUNDIT's Selection Pattern Query and Answers (SPQR)  component ~Lang1988\\].","acronyms":[[187,191]],"long-forms":[[149,185]]},{"text":"rent participation week (Curr) and the second using data from the beginning participation week till the current week (TCurr). For the second setup,","acronyms":[[118,123]],"long-forms":[[100,111]]},{"text":" 156 The Basque Dependency Treebank (BDT) is a dependency treebank in its upfront design, due to","acronyms":[[37,40]],"long-forms":[[9,35]]},{"text":"1 3 Note that there is only one column for recalling, which is undamaged by the choice o f Matched\/Missing (M\/M) versus All Templates (AT) . ","acronyms":[[133,135],[106,109]],"long-forms":[[118,131],[89,104]]},{"text":"2004. On clusterings: Good, bad and spectral. Journal of the ACM (JACM), 51(3):497?515.","acronyms":[[66,70]],"long-forms":[[46,64]]},{"text":"by (Punyakanok et al, 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each","acronyms":[[89,92]],"long-forms":[[61,87]]},{"text":"Way (1991) emphasizes the importance of this taxonomy by positing a central role for a dynamic type hierarchy (DTH) in metaphor, one that can create new and com-","acronyms":[[111,114]],"long-forms":[[87,109]]},{"text":"Examples of flaw of analyzed  (i) JISSAI (in fact), CHOSHA-TACHI-WA (authors) {\\[KORE-WO (it) TSUKATTE  (using), JUURYOKU-SOUGO-SAYOU-GA (gravitationally interacting) SHIHAI-SURU (govern-  ing)\\] TENTAI-NO (astronomical) UNDOU-NI-TSUITE (about the motion), KOUSEIDO-DE (high- ","acronyms":[[137,139],[37,43],[55,70],[170,181],[199,208],[224,239]],"long-forms":[[141,156],[183,195],[210,222],[241,257]]},{"text":" 5 Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word basis on the backgrounds in which it occurs. { 4,5}","acronyms":[[29,32]],"long-forms":[[3,27]]},{"text":" A manually  prepared seed list that is used to frame the  lexical patterns for conjunct verbs (ConjVs)  contains frequently used Light Verbs (LVs).","acronyms":[[96,102],[143,146]],"long-forms":[[80,94],[130,141]]},{"text":" Two sorts of recursion can be distinguished: 1)  middle field (MF) recursion, where the embedded  base clause is framed by the left and right verb parts ","acronyms":[[64,66]],"long-forms":[[50,62]]},{"text":"coverage of the language's grammar rule.  This paper introduces a ulterior Markov model (HMM) which  has been developed for Japanese word segmentation.","acronyms":[[88,91]],"long-forms":[[67,86]]},{"text":"Every extractor receives a sentence as input and determines which noun phrases (NPs) in the punishments are fillers for the event role.","acronyms":[[79,82]],"long-forms":[[65,77]]},{"text":"He con~iciers them foolish.  2) ASTG (adicctive string), including  adjectival Vens and Vings (see VENDADJ I found it well-dcsigned.","acronyms":[[32,36],[100,106]],"long-forms":[[38,54]]},{"text":"the Extraction of Potential Opinion Phrases. Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object.","acronyms":[[89,91],[55,57]],"long-forms":[[92,103],[58,75],[79,87],[107,114],[118,127],[131,137]]},{"text":"the exploration and verbalization histories; and (4) it then sends semantic representations in the form of preverbal messages (PVMs) to the Developing & Articulation components.","acronyms":[[125,129]],"long-forms":[[105,123]]},{"text":"Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used similarity measure in Information Retrieval(IR). It has further been showed","acronyms":[[112,114],[43,49]],"long-forms":[[90,110],[0,41]]},{"text":"ly corrected. An n-gram (n= 2 and 3) language model was then built from the Sinica corpus released  by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) using  the SRILM toolkit (Stolcke, 2002).","acronyms":[[182,188],[201,206]],"long-forms":[[107,180]]},{"text":"(Vehicle). Each mention of an entity has a mention  type: NAM (proper name), NOM (nominal) or                                                            ","acronyms":[[77,80]],"long-forms":[[82,89]]},{"text":"coverage of the language's grammar rules.  This paper introduces a hidden Markov model (HMM) which  has been developed for Japanese word segmentation.","acronyms":[[88,91]],"long-forms":[[67,86]]},{"text":"ian.fletcher, peter.maguire@cs.bloke.ac.uk Abstract Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent","acronyms":[[65,68]],"long-forms":[[50,63]]},{"text":"there are two ways of food the context vector into the main recurrent language model (RLM); (1) early smelting (EF) and (2) late fusion (LF), from Sec.","acronyms":[[113,115],[138,140],[89,92]],"long-forms":[[99,111],[125,136],[63,87]]},{"text":"NLP-Based  Index ing  in  In fo rmat ion   Ret r ieva l   In information retrieval (IR), a typical task is to  fetch relevant documents from a large archive in ","acronyms":[[84,86],[0,3]],"long-forms":[[61,82]]},{"text":"c?2008 Association for Computational Language Dialect Classification for online podcasts fusing Acoustic and Language based Structure and Semantic Information   Rahul Chitturi, John. H.L. Hansen1 Centers for Robust Speech Systems(CRSS) Erik Jonsson Tuition of Engineering and Computer Sciences University of Texas at Dallas Richardson, Texas 75080, U.S.A {rahul.ch@student, john.hansen@}utdallas.edu Abstract  The variation in rhetoric due to dialect is a factor which radically impacts speech system per-formance.","acronyms":[[233,237],[350,355]],"long-forms":[[200,231]]},{"text":"we describe SCANMail, a system that usage automatic speech recognition (ASR), information salvaging (IR), information extraction (IE), and human computer interaction (HCI) technology to permit users to navigate and search their voicemail messages by content","acronyms":[[169,172],[12,20],[74,77],[103,105],[132,134]],"long-forms":[[141,167],[44,72],[80,101],[108,130]]},{"text":"Snow follows that of (Escudero et al, 2000c).  2.4 LazyBoost ing  (LB)  The main idea of boosting algorithms is to ","acronyms":[[67,69]],"long-forms":[[51,60]]},{"text":" Therefore it makes sense to similarly extract data from machine readable dictionaries (MRDs). ","acronyms":[[83,87]],"long-forms":[[52,81]]},{"text":"1Note: LM(language model); ME(maximum entropy).  Brand Name(BRA), Product Type(TYP), Product Denomination(PRO), and BRA and TYP are routinely embed-","acronyms":[[60,63],[7,9],[27,29],[79,82],[98,101],[108,111],[116,119]],"long-forms":[[49,54],[10,24],[30,45],[66,78],[85,92]]},{"text":"rameter set to decode WSMT 2006 Europal test set (TEST1) and used the second on WSMT news commentary test set 2007 (TEST2)6. Table 6 shows the","acronyms":[[116,121],[80,84],[22,26],[50,55]],"long-forms":[[101,114],[40,48]]},{"text":"portance of the edge trait and the resultant largemargin constraint, we also comparison against a standard binary Support Vector Machine (SVM) which uses node features alone to prognosis whether each","acronyms":[[138,141]],"long-forms":[[114,136]]},{"text":" 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data.","acronyms":[[55,58]],"long-forms":[[31,53]]},{"text":"We have proposed two independent evaluation measures: statistical analysis (SA) and classification accuracy (AC). ","acronyms":[[109,111],[76,78]],"long-forms":[[99,107],[54,74]]},{"text":"We additionally encompass a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model.","acronyms":[[125,131]],"long-forms":[[101,123]]},{"text":"semantic processing component. A detailed descrip-  tion of the lexical conccplual structure (LCS) which  serves as the interlingua is not given here, but see ","acronyms":[[94,97]],"long-forms":[[64,92]]},{"text":"340   Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 37?45, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[91,96],[25,29]],"long-forms":[[47,89]]},{"text":"The conclusions of dependency (ZPar-eager, Ours-standard, Ours-PS and Mate-tools) and constituent parsers (BerkeleyParser and ZPar-con) are measured by the unlabeled accuracy scoring (UAS), labeled accuracy score (LAS) and bracketing f-measure (BF), consecutive. ","acronyms":[[208,211],[239,241],[54,61],[178,181],[122,130],[27,37]],"long-forms":[[184,206],[217,229],[152,176]]},{"text":" 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data.","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"     elements; ? |? is used for alternating elements; TOP = topic marker. ","acronyms":[[54,57]],"long-forms":[[60,65]]},{"text":" 2011b. Overview of the entity relations (REL) supporting task of BioNLP Sharing Task 2011.","acronyms":[[42,45]],"long-forms":[[31,40]]},{"text":"user gender (GEN), the user identity (UID) (e.g. the user could be a person or an organization), and the source document ID (DID). We also mark the lan-","acronyms":[[125,128],[13,16],[38,41]],"long-forms":[[112,123],[5,11],[23,36]]},{"text":" The specific case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced ?","acronyms":[[69,75]],"long-forms":[[55,67]]},{"text":"PROD = Predicate $SUBJ -- TOPIC value of characteristics ~OL  DET = Determiner NO = Noun Phrase (B~R I)  ADJ = Adjective GD = Gender  NU '~= Number PS = Person ","acronyms":[[115,117],[0,4],[17,22],[52,54],[56,59],[73,75],[99,102],[128,130],[142,144]],"long-forms":[[120,126],[7,16],[26,33],[62,72],[78,82],[105,114],[135,141],[147,153]]},{"text":" In Trials of the Conference on Web Search and Web Data Mining (WSDM). ","acronyms":[[69,73]],"long-forms":[[37,67]]},{"text":" 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meaning of a sentence is repre-","acronyms":[[52,55]],"long-forms":[[23,50]]},{"text":"lates in the treebank; it is faulty because close to wholesale needs another layer of structure, namely adjectives phrase (ADJP) (Bies et al, 1995, p. 179). ","acronyms":[[125,129]],"long-forms":[[107,123]]},{"text":"1  Lawsuits of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47, Reykjavik, Sweden, April 27, 2014.","acronyms":[[72,77],[81,85]],"long-forms":[[38,70]]},{"text":"Instead of using words directly, it is possible to employ a (much smaller) controlled vocabulary: Medical Subject Headings (MeSH), consisting of 22,500 codes, are (mostly) manually assigned to","acronyms":[[124,128]],"long-forms":[[98,122]]},{"text":" Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for","acronyms":[[96,99],[105,107]],"long-forms":[[72,94]]},{"text":"3.3 Evaluation Metric We use both Root Mean Square (RMS) error and Correlation Coefficient (CRCoef) to evaluate our model, since the two metrics evaluate different as-","acronyms":[[92,98]],"long-forms":[[67,90]]},{"text":"Translation Equivalents and Semantic  Relations   Note that two translation equivalents (TE)  in a pair of languages stand in a lexical semantic ","acronyms":[[89,91]],"long-forms":[[64,87]]},{"text":"three things:  (1) a new formalism for logic grammars, which we  call modifier structure grammars (MSGs),  (2) an interpreter (or parser) for MSGs that takes all ","acronyms":[[99,103],[142,146]],"long-forms":[[70,97]]},{"text":"Stanford Dependencies (SDC), as described by de Marneffe et al (2006), were generated by converting Penn Treebank (PTB)-style (Marcus et al, 1993) output using the Stanford CoreNLP Tools2 into the","acronyms":[[115,118],[23,26],[177,180]],"long-forms":[[100,113],[0,21]]},{"text":"thinking was an early version of branching entropy, one of the experts in VE, and they developed an algorithm called Phoneme to Morpheme (PtM) circa it. ","acronyms":[[134,137],[70,72]],"long-forms":[[113,132]]},{"text":" 1 Introduction Biomedical Text Mining (TM) has become increasingly popular due to the pressing need to provide","acronyms":[[40,42]],"long-forms":[[27,38]]},{"text":"tive two-step model. Ours compare models based on the Akaike Information Criterion (AIC). ","acronyms":[[82,85]],"long-forms":[[52,80]]},{"text":"5 5 of such properties is acyclicity, as in Disguised Markov Models (HMMs). For","acronyms":[[66,70]],"long-forms":[[44,64]]},{"text":"for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor. ","acronyms":[[66,68],[28,30],[46,48]],"long-forms":[[71,85],[33,44],[51,64]]},{"text":"uses the mapping of concept terrier to the class of alternative  expressions for named individual (such as using the name,  2 VSFL (\"Very Simple Frame Language\") and SCORE CSproket  Nuclei\") were developed at BBN Systems and Tech by ","acronyms":[[122,126],[162,167],[168,176],[203,206]],"long-forms":[[129,155]]},{"text":"context dependence and mutual information. Yamamoto and Church (2001) experiment with both mutual information and residual inverse document frequency (RIDF)1 as criteria for deciding Japanese words, and their main contribution is in affording","acronyms":[[151,155]],"long-forms":[[114,149]]},{"text":"computational semantics. With the rise of massive and easily-accessible digital corpora, computation of co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas.","acronyms":[[201,205],[156,159]],"long-forms":[[169,199]]},{"text":"605  NP- - - -NP:NP NP=S:N I '  VP~-VP:VP  S~---S:S  NP=NI ' :PP  NP~-PP :NP  VP=VP:NP  S=S:N I '   NP  ~.-~NP :VP  PP -~-PP :PP  VP=VP:P I  ) S~S:  ","acronyms":[[56,58],[5,7],[14,16],[17,19],[20,22],[32,38],[39,41],[53,55],[62,64],[66,72],[74,76],[78,80],[100,102],[112,114],[116,118],[126,128],[130,132]],"long-forms":[[59,62],[81,86],[90,93],[133,137]]},{"text":" ? REL = relation; ARG = NP\/VP\/ADJ (6) ACADE?MIQUE = Qui manque d?originalite?,","acronyms":[[3,6],[19,22],[39,50]],"long-forms":[[9,17],[25,34],[53,78]]},{"text":"up in the bracketed terminal string as insertion o f \"  (* *) \" surround-  ing \" 1970 \".) Inverse noun phrase preposing (NPPREPOS), which  relates such surface pairs as \" GM's sales \" and \" the sales of GM \", ","acronyms":[[121,129],[171,173],[203,205]],"long-forms":[[98,119]]},{"text":"case where estimated user?s knowledge and preference are represented as discrete binary parameters instead of probability distributions (PDs). That is, the estimated","acronyms":[[137,140]],"long-forms":[[110,135]]},{"text":"sible classes we show the accuracy of the correct hashtag being amongst the top 1,5 or 50 hashtags as well as the Mean Reciprocal Rank (MRR). The","acronyms":[[136,139]],"long-forms":[[114,134]]},{"text":"3 Estimation  We estimate a model?s distributions with  probabilistic decision trees (DTs).4 We build  decision trees using the WinMine toolkit ","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"Discovery of ambiguous and unambiguous discourse connectives via annotation projection. In Proceedings of Workshop on Annotation and Exploitation of Parallel Corpora (AEPC), pages 83?82, Tartu, Estonia.","acronyms":[[167,171]],"long-forms":[[118,165]]},{"text":"vised taggers. One commonly-used unsupervised tagger is the Camouflaged Markov model (MMM), which models the joint distribution of a word se-","acronyms":[[81,84]],"long-forms":[[60,79]]},{"text":" Below we focus on a special case of the latter problem: noun compound (NC) coordination. Con-","acronyms":[[72,74]],"long-forms":[[57,70]]},{"text":"function (Section 4.1).  Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of","acronyms":[[55,57]],"long-forms":[[45,53]]},{"text":"the percentage of questions with a correct answer at rank 1, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP). The reported","acronyms":[[117,120],[83,86]],"long-forms":[[93,115],[61,81]]},{"text":"proach of (Liu et al, 2004), in which IDs (category seeds) and instances are represented by vectors in a usual IR-style Vector Space Model (VSM), and similarity is measured by the cosine function:","acronyms":[[140,143],[111,113]],"long-forms":[[120,138]]},{"text":"54  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al, 1996).","acronyms":[[116,119],[50,53]],"long-forms":[[95,114]]},{"text":"For each IDR triple  all the object grammar triples are generated whose CF-PS rules  conform with the linear precedence(LP) rules, the fourth rule set  of the metagrammar.","acronyms":[[120,122],[9,12],[72,77]],"long-forms":[[102,118]]},{"text":" 5Note that this is a recursive lexical rule, which  Adjunct Extraposition Lexical Rule (AELR)  \"r,oc \\[\\] ICATIHEAD nou,~ Vverb\\] ","acronyms":[[89,93]],"long-forms":[[53,87]]},{"text":"Transcutaneous Oxygen (TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7. ","acronyms":[[58,64],[23,28]],"long-forms":[[38,56],[0,21]]},{"text":" Table 5: EPPS task: translation quality and time for different input conditions (CN=confusion network, time in seconds per sentence).","acronyms":[[82,84],[10,14]],"long-forms":[[85,102]]},{"text":"9http:\/\/disi.unitn.it\/moschitti\/Tree-Kernel.htm 10for STS-2012 we also report the results for a concatenation of all five test sets (ALL) provement with the Mean = 0.7416 and Pearson","acronyms":[[133,136]],"long-forms":[[113,116]]},{"text":"1425  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 75?81, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":"Draw Team (DT_DT); Team ? Competition (TM_CP); Team ? City \/ Province \/ Country ","acronyms":[[39,44],[11,16]],"long-forms":[[33,37],[0,9]]},{"text":"1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Results for parsing section 0 ( \u0000 40 words) of the WSJ Penn Treebank: OP = overparsing, LP\/LR = labelled precision\/recall.","acronyms":[[137,139],[118,121],[155,160]],"long-forms":[[142,153],[163,188]]},{"text":"an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun, in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns","acronyms":[[109,111],[3,5],[42,44],[66,68],[141,144]],"long-forms":[[113,117],[20,40]]},{"text":"leaves=29). The first branching, which correspond to no AP (Absolute Position) and no C (Colouring), assigns n to as many as 1571 instances","acronyms":[[57,59]],"long-forms":[[61,78],[90,96]]},{"text":"Turian et al. ( 2010) applied word embeddings to chunking and Named Entity Recognised (NER). Collobert et al. (","acronyms":[[88,91]],"long-forms":[[62,86]]},{"text":"Feature F1,344 d Automated Readability Index (ARI) 0.187 0.047 Average Sentence Length (ASL) 3.870 0.213 Sentence Complexity (COM) 10.93 0.357","acronyms":[[88,91],[46,49],[126,129]],"long-forms":[[63,86],[17,44],[114,124]]},{"text":" ? Contention Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for","acronyms":[[24,30]],"long-forms":[[3,22]]},{"text":"      The system integrates both dependency parse  tree pattern and semantic role labeler (SRL) results  of each input sentence when extracting the triples.","acronyms":[[91,94]],"long-forms":[[68,89]]},{"text":" The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The nor-","acronyms":[[70,73]],"long-forms":[[51,68]]},{"text":"a r e  the copilot l lowing  -- Virtuoso,  Object, Purposes, Backgrounds,  Locat ion,   Signifies, Cause, and Enabler  -- and (2) s t r u c t u r a l  c a s e s ,  which a r e   R E E L  ( r f l a t i v e  c l a u s e )  and COMP (compounds). I w i l l  not  e x p l a i n  ","acronyms":[[208,212]],"long-forms":[[214,222]]},{"text":"1 Introduction Recent research shows that it is possible, using current natural language processing (NLP) and machine learning technology, to automatically induce lex-","acronyms":[[101,104]],"long-forms":[[72,99]]},{"text":"The bacteria track consists of two tasks, BB and BI.  2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al, 2011) is to ex-","acronyms":[[83,85],[102,104],[42,44],[49,51]],"long-forms":[[60,76]]},{"text":"the traditional k-nearest neighbors (kNN) algorithm.  Maximum a posteriori (MAP) principle is used to define which emotion set is related to the giv-","acronyms":[[75,78],[36,39]],"long-forms":[[53,73],[16,34]]},{"text":" 3.2.2 Optimization We use stochastic gradient descent (SGD) to maximize the simplifying objectives.","acronyms":[[56,59]],"long-forms":[[27,54]]},{"text":"The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence","acronyms":[[135,137],[4,7]],"long-forms":[[109,133]]},{"text":"Hypernym Hyponyms Co-Hyponyms Figure 2: The recommendation semantic word embedding (SWE) learning framework (The left part denotes the state-of-the-art skip-gram model; The right part represents the semantic impediment).","acronyms":[[78,81]],"long-forms":[[53,76]]},{"text":"describe as operating within the same three-stage framework as STRAND.  Parallel Texts Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web searches engines to locate pages by querying for pages in a given language that contain","acronyms":[[94,101],[64,70]],"long-forms":[[73,92]]},{"text":"ferent sources. The first feature fount comes  from our DSSMs (DSSM and DSSM_BOW) use the output layers as feature generators as de-","acronyms":[[57,62]],"long-forms":[[64,81]]},{"text":"sentences (Sent.), the number of tokens (Tokens) and the unlabeled attachment score (UAS) of MST. ","acronyms":[[85,88],[11,15],[41,47],[93,96]],"long-forms":[[57,83],[0,9],[33,39]]},{"text":"tic attachment. Eight categories of syntactic onstituent were used: sentence (S), noun  phrase (NP), verb phrase (VP), prepositional phrase (PP), wh-noun phrase (WHNP),  adjective or adverbial phrase (AP), any other constituent (O), and both words in the ","acronyms":[[114,116],[141,143],[96,98],[162,166],[201,203]],"long-forms":[[101,112],[119,139],[68,76],[82,94],[146,160],[170,199],[206,227]]},{"text":"itly branded objects of prepositions (POBJ), possessors in idafa constructions (IDAFA), conjuncts (CONJ) and conjunctions (CC), and the accusative specifier, tamyiz (TMZ).","acronyms":[[119,121],[35,39],[76,81],[95,99],[162,165]],"long-forms":[[105,117],[10,33],[56,74],[84,93],[154,160]]},{"text":"assignment for each annotator. We then performed an analysis of gap (ANOVA) on the outcomes of our experiment.","acronyms":[[74,79]],"long-forms":[[52,72]]},{"text":"its right neighbour. Hence, the tree in Figure 2 is assumed to have the structure ((AB)C). ","acronyms":[[84,88]],"long-forms":[[52,81]]},{"text":"7 8  Figure 2: The Sense Distribution  the help of a graphical user intefface(GUI) scans a  parsed sample article and indicates a series of se- ","acronyms":[[78,81]],"long-forms":[[53,76]]},{"text":"ferently by (1) and (2)8.  5 The Neutral Edge Directorate (NED) Measure","acronyms":[[57,60]],"long-forms":[[33,55]]},{"text":"It predicts four type of reordering patterns, namely MA (monotone adjacent), MG (monotone gap), RA (reverse neighbors), and RG (reversing gap).","acronyms":[[96,98],[53,55],[77,79],[123,125]],"long-forms":[[100,116],[57,74],[81,93],[127,138]]},{"text":"Abstract The quality of a sentence translated by a machine translation (MT) system is thorny to evaluate.","acronyms":[[72,74]],"long-forms":[[51,70]]},{"text":" Some implementation otes  The Carnegie Mellon Spoken Language Shell (CM-SLS)  was intentionally designed to have easily modifiable com- ","acronyms":[[70,76]],"long-forms":[[31,68]]},{"text":"Studies such as Cinque (2006) and Rizzi  (1999) propose detailed functional phrases such  as TopP (Topic Phrase) in edict to altogether describe  the syntactic structures of a language.","acronyms":[[93,97]],"long-forms":[[99,111]]},{"text":"Note that the autoPS heuristic for ranking senses is a more precise estimator than the WordNet most?frequent?sense (MFS). ","acronyms":[[116,119],[14,20]],"long-forms":[[95,114]]},{"text":"egie Group, Inc. (CGI) of Pittsburgh, PA is to promote  and further develop automatic Text Summarization  using a Maximal Minor Relevance (MMR) metric to  generates summaries of documents beret are directly rele- ","acronyms":[[142,145],[18,21],[38,40]],"long-forms":[[114,140]]},{"text":"as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF).","acronyms":[[84,87],[120,123]],"long-forms":[[59,82],[93,118]]},{"text":"structure come in two different types: ? Grammatical Functions (GFs) indicate the relationship between the predicate and depen-","acronyms":[[64,67]],"long-forms":[[41,62]]},{"text":"discrepancy distribution. In the hallmarks space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods","acronyms":[[71,74]],"long-forms":[[47,69]]},{"text":"data source: the Academia Sinica (AS) corpus, the Beijing Academies (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split","acronyms":[[118,123],[35,37],[71,74]],"long-forms":[[101,116],[18,33]]},{"text":" 2001. Linguistic Inquiry and Word Count (LIWC):  LIWC2001.","acronyms":[[42,46]],"long-forms":[[7,40]]},{"text":"Our initial experiment includes language model (LM), word posterior probability (WPP), confusion network (CN), and word lexicon (WL) features for a total of 11","acronyms":[[106,108],[48,50],[81,84],[129,131]],"long-forms":[[87,104],[32,46],[53,79],[115,127]]},{"text":"tation for extract entities from w. At our system, we let an extraction predicate be a simplified XML routes (XPath) such as \/html[1]\/body[1]\/table[2]\/tr\/td[1]","acronyms":[[111,116]],"long-forms":[[101,109]]},{"text":"its right neighbour. Accordingly, the tree in Figure 2 is assumed to have the structure ((AB)C). ","acronyms":[[84,88]],"long-forms":[[52,81]]},{"text":"(Paras), POS, syntactic dependency tree (DT), syntactic constituent tree (CARAT), named entities (NE), WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role tagging (SRL), causal relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words scoring (BOW), tree twinning (TreeMatch), linear (LM), log-linear (LLM), statistical learning","acronyms":[[220,222],[8,11],[40,42],[73,75],[94,96],[118,121],[145,149],[176,179],[200,202],[250,257],[289,292],[310,319],[330,332],[347,350]],"long-forms":[[205,218],[23,38],[55,71],[78,92],[99,116],[124,143],[152,174],[182,198],[225,248],[267,279],[295,308],[322,328],[335,345]]},{"text":"training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluate.","acronyms":[[93,96],[53,56]],"long-forms":[[69,91],[35,51]]},{"text":"verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.","acronyms":[[74,76],[42,44]],"long-forms":[[50,72],[24,40]]},{"text":"Lima or Jessica Alba??. Consequently, we decided to employ a Conditional Random Sphere (CRF) tagger (Lafferty et al, 2001) to the task, since CRF","acronyms":[[85,88],[139,142]],"long-forms":[[58,83]]},{"text":"Transactions of the Association for Computational Linguistics (TACL), 1:25?36. ","acronyms":[[63,67]],"long-forms":[[0,61]]},{"text":"the left hand side (LHS) of the regulations, and ? the right hand side (RHS) of the rule.","acronyms":[[65,68],[20,23]],"long-forms":[[48,63],[4,18]]},{"text":"Computational Linguistics Volume 40, Number 1 of the 22nd International Conference on Computational Linguistics (COLING?08), pages 71?84, Manchester.","acronyms":[[113,122]],"long-forms":[[86,111]]},{"text":" 3.1 Evaluation methods We use the Relative Usefulness (RU) methods (Radev et al, 2000) to compare our various summaries.","acronyms":[[53,55]],"long-forms":[[35,51]]},{"text":"edges the aiding of Defense Advanced Researching Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract No.","acronyms":[[132,136],[64,69]],"long-forms":[[101,130],[21,62]]},{"text":" 2.2 Human Intelligence Tasks una Human Intelligence Task (HIT) is a short paid task on MTurk.","acronyms":[[57,60]],"long-forms":[[32,55]]},{"text":"This problem  is of practical interest for the design of various types  of natural anguage interfaces (NLI's) that make use of  different knowledge sources.","acronyms":[[103,108]],"long-forms":[[75,101]]},{"text":"More recently, (Areces et al, 2008) analysed GRE as a problem in Description Logic (DL), a formalism which, like Conceptual Graphs, is specifically designed for","acronyms":[[84,86],[45,48]],"long-forms":[[65,82]]},{"text":"as defined in Section 2. During tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF).","acronyms":[[84,87],[120,123]],"long-forms":[[59,82],[93,118]]},{"text":"Measuring and estimating post-editing effort is therefore a growing concern addressed by Confidence Estimation (CE) (Specia, 2011). ","acronyms":[[112,114]],"long-forms":[[89,110]]},{"text":"sets used in the fast Tree Kernel.  2.3 A Fast Tree Kernel (FTK) To compute the kernels defined in the previous","acronyms":[[60,63]],"long-forms":[[42,58]]},{"text":" For example, the surface subject (S-SBJ) of a passive verb is further the logical object (L-OBJ). These","acronyms":[[88,93],[35,40]],"long-forms":[[72,86],[18,33]]},{"text":"learning this decision is learned automatically.  Reinforcement Learning (RL) has been successfully used for learning dialogue management","acronyms":[[74,76]],"long-forms":[[50,72]]},{"text":"methods to identify such targets. The first method depends on identifying noun groups (NG). We con-","acronyms":[[87,89]],"long-forms":[[74,85]]},{"text":"FEDERAL DATA ENCRYPTION STANDARD ENDORSED BY COMMERCE MINISTRIES  A data encryption algorithm, designed to protects digital information, was  approved in November ad a Federal .Information Processing Standard (FIPS)  by the Department of Commerce.","acronyms":[[209,213]],"long-forms":[[167,207]]},{"text":" Below we focus on a peculiar case of the latter problem: noun compound (NC) coordination. Con-","acronyms":[[72,74]],"long-forms":[[57,70]]},{"text":"Han, C-H., Ambrose, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean  Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on  Language Resource and Appraisals (LREC).(2002)  5.","acronyms":[[212,216]],"long-forms":[[177,195]]},{"text":"collocations in each sentence.  Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language","acronyms":[[64,67]],"long-forms":[[32,62]]},{"text":"6 6   Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 79?83, Seoul, South Korea, 5-6 July 2012.","acronyms":[[102,109]],"long-forms":[[52,100]]},{"text":"   (2)  LSA-based (Latent Semantic Analysis based)  trigger word analogy: LSA (Deerwester et ","acronyms":[[8,17],[77,80]],"long-forms":[[19,43]]},{"text":" 1 Introduction Base Phrase Chunking (BPC), also known as shallow syntactic parsing, is the process by which ad-","acronyms":[[38,41]],"long-forms":[[16,36]]},{"text":"pseudo-terms?. We also discuss the use of Hidden Markov Models (HMMs) to capture contextual information.","acronyms":[[64,68]],"long-forms":[[42,62]]},{"text":"especially the overall accuracy (Total-A) and the reminds with respect to in-vocabulary words (IV-R),  OOV words (OOV-R) or multi-POS words (MTR).","acronyms":[[109,114],[90,94],[125,128],[136,139]],"long-forms":[[98,107],[69,82]]},{"text":"changes and there is a complement particle between complement constituents(COP)  and verbs. So in the information dictionary, the characteristics of {V(CHA), VP+COP}  should be described.","acronyms":[[152,155],[75,78],[161,164],[158,160]],"long-forms":[[130,145],[51,73]]},{"text":"2 GT  and  Move-c~  The central operations of the Minimalist Program  are Generalized Transformation (GT) and Move-  ~. GT is a structure-building operation that builds ","acronyms":[[102,104],[2,4],[120,122]],"long-forms":[[74,100]]},{"text":"(O?Shaughnessy, 2000), lip aperture (ANGELES) is the normalized Euclidean distance between the lips, and lip protrusion (LP) is the normalized 2nd primary component of the midpoint between the lip.","acronyms":[[116,118],[36,39]],"long-forms":[[100,114],[23,35]]},{"text":"Abstract Crowd-sourcing approaches such as Amazon?s Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of","acronyms":[[69,74]],"long-forms":[[52,67]]},{"text":"treat applications, su ch as larg e-vocab u lary speech recog nition (L V CS R), statistical machine translate (S M T ) and information retrieve (IR), is the morpholog ical analy sis of w ords.","acronyms":[[154,156],[119,124],[75,83]],"long-forms":[[131,152],[86,117],[34,73]]},{"text":"to pour) means that the entity (wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle).","acronyms":[[117,120]],"long-forms":[[107,112]]},{"text":" ? Left Reveal (LRev) : Pop the supreme two nodes in the stack (left, right).","acronyms":[[16,20]],"long-forms":[[3,14]]},{"text":"e i Algorithm 1 Sparse projection (SP) Required: v \/\/ Vocabulary: vector of n words","acronyms":[[35,37]],"long-forms":[[16,33]]},{"text":"1995. A bi-directional Russian-toEnglish machine translation system (ETAP-3). In","acronyms":[[69,75]],"long-forms":[[33,67]]},{"text":"At addition, adding the soft joint-inference formula results in further gain, and our full system (FULL) obtains an F1 of 55.5. ","acronyms":[[99,103]],"long-forms":[[86,90]]},{"text":"far and formulate new ones inspired by latent semantic analysis (LSA), which was developed within the information retrieval (IR) community to treat synonymous and polysemous terms (Deerwester et","acronyms":[[125,127],[65,68]],"long-forms":[[102,123],[39,63]]},{"text":"and stochastic optimization. In Proceedings of the Conference on Learning Doctrine (COLT), pages 257?269.","acronyms":[[82,86]],"long-forms":[[51,80]]},{"text":"summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features. ","acronyms":[[103,106]],"long-forms":[[89,101]]},{"text":" 4.1 Representing Transformations as FSTs Finite State Transducers (FSTs) provide a natural formalism for represented output transformations.","acronyms":[[68,72],[37,41]],"long-forms":[[42,66]]},{"text":" 1 Introduction Typically, Information Retrieval (IR) and Statistical Natural Language Processing (NLP) applica-","acronyms":[[54,56],[103,106]],"long-forms":[[31,52],[74,101]]},{"text":" 3.2 Taboo Constraint Taboo duress (TABOO) requires that the substitute word is a taboo word or frequently used","acronyms":[[40,45]],"long-forms":[[22,38]]},{"text":"Na?ve Bayes (De Sitter and Daelemans, 2003).  Maximum Entropy (ME) conditional modeling like  ME Markov modelled (McCallum et al, 2000) and ","acronyms":[[63,65],[92,94]],"long-forms":[[46,61]]},{"text":" scores cw(ei) are combined: MEAN CM (cM (eI1)) is computed as the geometric mean of the confidence scores of the","acronyms":[[34,36]],"long-forms":[[38,40]]},{"text":"This paper discusses the automatic label of semantic relations in nominalized noun phrases (NPs) utilizes a support vector machines learning algorithm.","acronyms":[[95,98]],"long-forms":[[81,93]]},{"text":"Table 2  Abstracts of error rates with the language model only (LM), the prosody model only (PM), the  combines ecision tree (CM-DT), and the combined HMM (CM-HMM). ( a) showing word-based ","acronyms":[[124,129],[154,160],[62,64],[91,93]],"long-forms":[[101,122],[140,152],[41,55],[71,84]]},{"text":"In this cooperating, we apply Dirichlet Processed Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering.","acronyms":[[115,118],[57,62]],"long-forms":[[86,113],[23,55]]},{"text":"linked NIST metric (Doddington 2002), along with WER and PER, have been heavily used by several machine translation searchers. The Translation Edit Rate (TER) (Snover et al 2006) and the CDER measure (Leusch, Ueffing, and Ney 2006) are based on the edit","acronyms":[[153,156],[8,12],[50,53],[58,61],[186,190]],"long-forms":[[130,151]]},{"text":"                                                                  Barcelona, July 2004                                               Association for Computations Linguistics                        ACL Special Interest Group on Computational Phonology (SIGPHON)                                                     Proceedings of the Workshop of the","acronyms":[[252,259],[197,200]],"long-forms":[[201,250]]},{"text":"grammar frameworks (HLG). Combinatory  Categorial Grammars (CCG) (Steedman, 1987;  Steedman, 1996; Steedman, 1998; Steedman and ","acronyms":[[64,67],[24,27]],"long-forms":[[30,62]]},{"text":"extraction. Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks","acronyms":[[96,99],[38,42],[137,143]],"long-forms":[[75,80],[106,135]]},{"text":"However, to the best of our expertise only the modeling of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al, 2001), can computations word indices pairs?","acronyms":[[125,128]],"long-forms":[[99,123]]},{"text":" 1 Introduction Statistical machines translation (SMT) relies on tokenization to split sentences into meaningful units","acronyms":[[49,52]],"long-forms":[[16,47]]},{"text":"Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. ","acronyms":[[97,99],[107,109],[86,88]],"long-forms":[[89,95],[101,106],[78,85]]},{"text":"real object and \\[AI is the word that represents A. CS : = speaking una;yes refers A \\[A\\] ; yes  (111) Resource Situation(RS)  A resource situation is defined for each personally in a discourse; it ","acronyms":[[123,125],[54,56]],"long-forms":[[104,122]]},{"text":"The dataset used in the CIPS-ParsEval-2010 evaluation is converted from the Tsinghua Chinese Treebank (TCT). There are two subtasks: (1)","acronyms":[[103,106],[24,42]],"long-forms":[[93,101]]},{"text":"facts or splits the  goal into new subgo~ls uch as to show the facts in the premises of n. The derivation of a fact is  conveyed by so-called mathematics ommunicating acts (MCAs) and accompanied by storing the  fact as a chunk in the declarative memory.","acronyms":[[173,177]],"long-forms":[[142,171]]},{"text":"languages investigated differ widely, there is a quasistandard for presenting the material, in the form of interlinearized glossed text (IGT). IGT routinely","acronyms":[[132,135],[138,141]],"long-forms":[[102,130]]},{"text":"First, we provide background on Open IE and how it relates to Semantic Role Labeling (SRL). Section 3 de-","acronyms":[[86,89],[37,39]],"long-forms":[[62,84]]},{"text":"get, with results aggravation as we move to information retrieval (IR), multi-document summarization (SUM), and information extraction (IE). ","acronyms":[[138,140]],"long-forms":[[114,136]]},{"text":"patterns indicative of SLI. In this cooperate, we use Language Models (LMs) for this task since they are a powerful statistical measurements of language usage","acronyms":[[66,69],[23,26]],"long-forms":[[49,64]]},{"text":"1. Introduction Semantic role tagging (SRL) is the task of identifying arguments for a predicate and assigns semantically meaningful labels to them.","acronyms":[[40,43]],"long-forms":[[16,38]]},{"text":"Long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is a kinds of recurrent neural network (RNN) (Elman, 1990), and concretely addresses the","acronyms":[[119,122]],"long-forms":[[93,117]]},{"text":"The primary purpose of the toolkit is to allow students to concentrate on building natural language processing (NLP) scheme.","acronyms":[[112,115]],"long-forms":[[83,110]]},{"text":" 1 Introduction Information extraction (IE), defined as the task of extracting structured information (e.g., events, bi-","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"distribution which underlies natural language text   -- which is if not a pure Zipfian distribution at least  an LNRE (large number of rare events, cf. Baayen ","acronyms":[[113,117]],"long-forms":[[119,146]]},{"text":"cjlin\/libsvm\/ System P R F1 Schwartz & Horst (SH) .978 .940 .959 SaRAD .891 .919 .905","acronyms":[[47,49]],"long-forms":[[28,45]]},{"text":" We observe the following: First, pre-enrollment reviews have noun phrases(NP) that contain fewer leaf nodes than in the post-enrollment reviews.","acronyms":[[75,77]],"long-forms":[[62,73]]},{"text":"sented in the graphs. The first strategy can be applied when the data set contains a  functionally independent attribute (FIA) that is used as an organising device or \"an-  chor\" for the entire chart.","acronyms":[[123,126]],"long-forms":[[87,121]]},{"text":" Another major similarity measure is cosine similarity of Personalized PageRank (PPR) vectors.","acronyms":[[85,88]],"long-forms":[[62,83]]},{"text":"Wiebe, 2000).  4.1.4 Forceful Polar Expressions (STROPO) Instead of adding intensifiers in order to put more","acronyms":[[47,53]],"long-forms":[[21,45]]},{"text":" Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that con-","acronyms":[[77,80]],"long-forms":[[53,75]]},{"text":" A stacking based extraction Algorithm 1 was designed to retrieved a context free grammar (CFG) from the URDU.KON-TB treebank.","acronyms":[[86,89],[100,111]],"long-forms":[[64,84]]},{"text":"vincent.claveau@irisa.fr christian.raymond@irisa.fr R?SUM?Dans cet article, nous d?crivons notre participation au D?fi Fouille de Texte (DeFT) 2012. Ced?fi consistait en l?attribution automatique de mots-cl?s ?","acronyms":[[137,141],[52,57]],"long-forms":[[81,135]]},{"text":" enard Centre de Recherche Informatique de Montr?eal (CRIM) Montr?eal, QC, Canada","acronyms":[[54,58],[71,73]],"long-forms":[[7,52]]},{"text":"Where is that??).  Finally, the task trait (TASK) reflect conflicting instructions in the domain.","acronyms":[[47,51]],"long-forms":[[32,36]]},{"text":"the identified reading level. Text plans define  rules on Noun Phrase (NP) density and lexical  choice.","acronyms":[[71,73]],"long-forms":[[58,69]]},{"text":"The basic aim of Acquilex is the  development of techniques and methods in order to use  Machine Readable Dictionaries (MRD) * for building lexical  components for Natural Language Processing Systems.","acronyms":[[120,123]],"long-forms":[[89,118]]},{"text":"KODIAK (Keystone to Overall Design for Inte-  gration and Application of Knowledge) is an implemen-  tation of CRT (Cognitive Representation Theory), an  approach to knowledge representation that bears simi- ","acronyms":[[111,114],[0,6]],"long-forms":[[116,147],[8,82]]},{"text":"Added valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? corpus of","acronyms":[[62,65]],"long-forms":[[68,87]]},{"text":"There are four basic phrases in Korean: noun phrase (NP), verb words (VP), adverb phrase (ADVP), and independence phrase (IP). Thus, chunking by rules is","acronyms":[[122,124],[53,55],[71,73],[91,95]],"long-forms":[[102,120],[40,51],[58,69],[76,89]]},{"text":"not vacillate, vacillate is, line vacillate?  English Context(EC): shake\/vacillate  Putting on Search Engine and getting counts:  ","acronyms":[[62,64]],"long-forms":[[46,60]]},{"text":"few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192?199.","acronyms":[[64,70]],"long-forms":[[15,62]]},{"text":"perceptual space between each pair. We can do this with a multidimensional scaling (MDS) algorithms. Let us call","acronyms":[[84,87]],"long-forms":[[58,82]]},{"text":"TOP (PRP ? I?) ( VP (VBP ? NEED?) (","acronyms":[[17,19],[0,3],[5,8]],"long-forms":[[21,24]]},{"text":"MOR = morphological characters (set) ? LEM = lemma ?","acronyms":[[37,40],[0,3]],"long-forms":[[43,48],[6,19]]},{"text":"nextpos part of speech of future word in the penalties determiner if the word has a determiner prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition insidequotes if the word is indoors quotes","acronyms":[[157,159]],"long-forms":[[135,155]]},{"text":"tic models, which in this example are hidden Markov models (HMM), and describe in terms of wellknown Mel frequency cepstral coefficients (MFCCs) (Benesty et al, 2008).","acronyms":[[136,141],[57,60]],"long-forms":[[99,134],[35,54]]},{"text":"is associated with the data sparseness problem. Longer of the previously proposed methods to  excerpt compounds or to measure word association using mutual information (MI) either overlook  or sanction items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang ","acronyms":[[167,169]],"long-forms":[[147,165]]},{"text":"treebank. Among: Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24? ","acronyms":[[86,89]],"long-forms":[[51,84]]},{"text":" 2 CFN and Its SRL task Chinese FrameNet(CFN) (You et al, 2005) is a research project that has been developed by Shanxi","acronyms":[[41,44],[3,6],[15,18]],"long-forms":[[24,39]]},{"text":"(Roelofs, 2004), experiments on finishing (Schvaneveldt et al., 1976) or the tip of the tongue difficulty (TOT) (Brown and McNeill, 1996).","acronyms":[[102,105]],"long-forms":[[75,92]]},{"text":"1. INTRODUCTION  Hidden Markov Models (HMMs) have been used suc-  cessfully in a wide variety of recognition tasks ranging ","acronyms":[[39,43]],"long-forms":[[17,37]]},{"text":"proaches, namely Bisecting K-Means (Steinbach et al.,  2000), and Latent Semantic Analysis (TAS)-based document clustering (for short, LSA).","acronyms":[[92,95],[135,138]],"long-forms":[[66,90]]},{"text":"pick PRON up?, where PRON is the part of speech (POS) etiquette for pronouns.","acronyms":[[49,52],[21,25],[5,9]],"long-forms":[[33,47]]},{"text":"Na?ve Bayes (De Sitter and Daelemans, 2003).  Maximum Entropy (ME) conditional models like  ME Markov models (McCallum et al, 2000) and ","acronyms":[[63,65],[92,94]],"long-forms":[[46,61]]},{"text":"size ? is fixed to 0.0001. We refer to this model as Orthogonal Matrix Factorization (OrMF). ","acronyms":[[86,90]],"long-forms":[[53,84]]},{"text":"tiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al.,","acronyms":[[62,68]],"long-forms":[[49,60]]},{"text":"languages.  Mutual Information (MI)  For the purpose of this experiment we use a ","acronyms":[[32,34]],"long-forms":[[12,30]]},{"text":"tourist resources was made by the Direcc?a?o Geral de Turismo (DGT) and thereafter the Inventory of Tourist Finances (IRT) emerged. ","acronyms":[[119,122],[63,66]],"long-forms":[[87,117],[45,61]]},{"text":" 3.1 LSTMs for sequence generation A Recurrent Neural Network (RNN) is a generalization of feed forward neural networks to se-","acronyms":[[63,66],[5,10]],"long-forms":[[37,61]]},{"text":"VBL (Light Verb) is used in intricate predicates (Butt 1995), but its syntactic similarity with  VB (Verb) is a major source of chaos in automatic tagging.","acronyms":[[95,97],[0,3]],"long-forms":[[99,103],[5,15]]},{"text":"probabilistic Earley?s, and minimum edit distance algorithms). Dynamic agendas (DP) involves solving certain kinds of recursive equations","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":"the specified length limit. This idea is realized utilise the integer linear programming-based (ILP) optimization framework, with goal function set to","acronyms":[[94,97]],"long-forms":[[60,86]]},{"text":"                                                           8  Feature type: CB=Clause boundary based feature typing,  PT=Parse tree based feature type  9A ?","acronyms":[[116,118],[76,78]],"long-forms":[[119,129],[79,94]]},{"text":"shown in Figure 2. It is observed that the numbers of instances of Conjunct Verb (ConjV),  Passives (Pass), Auxiliary Construction (AC) ","acronyms":[[82,87],[101,105],[132,134]],"long-forms":[[67,80],[108,130],[91,99]]},{"text":"1 Introduction Linguistics studies have shown that action verbs often denote some change of state (CoS) as the result of an action, where the change of state of-","acronyms":[[99,102]],"long-forms":[[82,97]]},{"text":" 2 HMM transitions can be modeled using Weighted Finite State Automata (WFSAs), corresponding to regular expressions.","acronyms":[[72,77],[3,6]],"long-forms":[[40,70]]},{"text":"mental state labels that are highly similar to the context of the scene in a latent, conceptual vector space; and an information retrieval (IR) model that identifies labels commonly appearing in sentences","acronyms":[[140,142]],"long-forms":[[117,138]]},{"text":"lected randomly from some reference corpus.  Active Learning (AL) has recently shaped as a much more efficient alternating for the institution of","acronyms":[[62,64]],"long-forms":[[45,60]]},{"text":" 3.3 Cascaded ATN Grammars A Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperatives sequence of ATN transducers, each nutritional its output to the next stage.","acronyms":[[52,56],[14,17],[100,103]],"long-forms":[[29,41]]},{"text":"lowing three metrics are used in this experiment.  (a) EPN in total (EPN-T): The number of the enlargement problems which are produced in the","acronyms":[[69,74]],"long-forms":[[55,67]]},{"text":"describing each video. We then groups these verbs using Hierarchical Agglomerative Clustering (HAC) utilise the res metric from WordNet::Similarity by","acronyms":[[98,101]],"long-forms":[[59,96]]},{"text":"on a Wikipedia corpus alternatively of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI schemes that use dependency-parsed","acronyms":[[77,80],[37,42],[101,104]],"long-forms":[[55,75]]},{"text":" 2 HMM transitions can be modeled utilized Weighted Finite State Automata (WFSAs), corresponding to regular expressions.","acronyms":[[72,77],[3,6]],"long-forms":[[40,70]]},{"text":"Upsala College  INTRODUCTION  A computerized conference (CC) is a form of co~znunica-  tion in which participants type into and read frc~ a ","acronyms":[[57,59]],"long-forms":[[32,55]]},{"text":" 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifier We include Maximum Entropy classifier using sim-","acronyms":[[41,47]],"long-forms":[[24,39]]},{"text":"4} is not a most frequent token  and will reach at bi-gram queue administrator only  after passing  through all shapes generator (AFG).  ","acronyms":[[123,126]],"long-forms":[[102,121]]},{"text":"the sentence. The segmentation model is a chain LVM (latent variable model) that strives to maximize a linear intents defined by:","acronyms":[[48,51]],"long-forms":[[53,74]]},{"text":"229 2 Participation in STS-SEM2013  The Semantic Textual Similarity (STS) task consists of estimated the value of semantic similarity ","acronyms":[[69,72],[23,34]],"long-forms":[[40,67]]},{"text":"vided for training data.  the use of well-motivated Lexical Structures (LS's)  to capture the presuppositional nd anaphoric as- ","acronyms":[[72,76]],"long-forms":[[52,70]]},{"text":"Results on final test sets. LAS = labeled attachment score. UAS = unlabeled attachment score. ","acronyms":[[60,63],[28,31]],"long-forms":[[66,92],[34,58]]},{"text":"The entrance in the  subjectivity word list have been labeled with  part of sermons (POS) tags as well as either  strong or weak subjective tag depending on the ","acronyms":[[83,86]],"long-forms":[[67,81]]},{"text":"a baseline.  Language Model (LM): We model the semantic fit of a candidate substitute within the conferred context","acronyms":[[29,31]],"long-forms":[[13,27]]},{"text":"The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented us-","acronyms":[[147,150]],"long-forms":[[123,145]]},{"text":"as source domain training data (STrain), archive 271300 as source spheres testing data (STest) and files 590-596 as target domain testing data (TTest). We","acronyms":[[141,146],[32,38],[85,90]],"long-forms":[[113,139],[3,30],[57,83]]},{"text":"But nevertheless we can say that even in languages with  that kind of structural properties like Slavic languages have, named agencies (NE) form a subset of  natural language expressions that testifies ","acronyms":[[129,131]],"long-forms":[[113,127]]},{"text":"Machine learning in automation text categorization. ACM computing surveys (CSUR), 34(1):1?47.","acronyms":[[74,78],[51,54]],"long-forms":[[55,72]]},{"text":"CORROLA TION  NP1 = (COR) (APP) (ADJ) (NC) N  NP2 = (NC) N  NP3 = N ","acronyms":[[53,55],[21,24],[27,30],[33,36],[39,41],[14,17]],"long-forms":[[46,51],[0,12]]},{"text":"china learning model based on three different good known techniques, decision trees (C4.5), rule induction (RIPPER) and maximum entropy (MaxEnt), in order to find out which approach is the most suitable","acronyms":[[138,144],[109,115]],"long-forms":[[121,136]]},{"text":"Concepts across categories Hilke Reckman and Crit Cremers Leiden University Centre for Linguistics (LUCL) Leiden, Netherlands","acronyms":[[100,104]],"long-forms":[[58,98]]},{"text":" Acknowledgments This research was supported by the Deutsch Forschungsgemeinschaft (DFG) in the Center of Excellence in ?","acronyms":[[85,88]],"long-forms":[[52,83]]},{"text":" Phrasometer ? The phrasometer featuring (PM) is the summed log-likelihood of all n-grams the word","acronyms":[[40,42]],"long-forms":[[19,30]]},{"text":"The different resources used to build ArSenL.       The English WordNet (EWN) (Miller et al., ","acronyms":[[73,76]],"long-forms":[[56,71]]},{"text":" These algorithms are now getting impatient atten-  tion from the natural anguage processing (NLP)  researches community since the huge text corpus ","acronyms":[[89,92]],"long-forms":[[61,87]]},{"text":"Computational Linguistics, Volume 15, Number 1, March 1989 33  Michael C. McCord \\]Design of LMT: A Prolog-Based Machine Translation System  sions in a logical form language (LFL)  (McCord 1985a,  1987).","acronyms":[[175,178],[93,96]],"long-forms":[[152,173]]},{"text":"Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 89, Beijing, August 2010 Multi-Word Expressions as Discourse Relation Markers (DRMs) Aravind K. Joshi","acronyms":[[166,170],[71,74]],"long-forms":[[138,164],[19,39],[112,133]]},{"text":"comparison with SMO-n) 6 Conclusions Automatic Text Simplification (ATS) target to transforming complex texts into a simpler form, which is more accessible to a wider audience.","acronyms":[[68,71],[16,21]],"long-forms":[[37,66]]},{"text":"Conf.  Fifth Jill Computer Systems 1992 (FGCS'92),  pp.1133-1140, 1992.","acronyms":[[47,54]],"long-forms":[[7,45]]},{"text":"for si ? Bm do Use Breadth First Search (BFS) to check if ?","acronyms":[[41,44]],"long-forms":[[19,39]]},{"text":"ing default parameters. Error analysis was done by means of Mean Squared Error estimate (MSE). ","acronyms":[[89,92]],"long-forms":[[60,78]]},{"text":"mantic relations between referents. This task has a long tradition in natural language treating (NLP) since the early jours of artificial intelligence (Web-","acronyms":[[99,102]],"long-forms":[[70,97]]},{"text":"PROJECT TARGETING  This project involves the integration of speech and natural-  language processing for spoken language systems (SLS). The ","acronyms":[[126,129]],"long-forms":[[101,124]]},{"text":"1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). For exam-","acronyms":[[97,100]],"long-forms":[[68,95]]},{"text":"They found that using a combination of all the features in a Assists Vector Machine (SVM), they can obtain an accurate of 80% in the classification of 5 differ-","acronyms":[[85,88]],"long-forms":[[61,83]]},{"text":"here we only consider clusters which contain at least one ? Person (PER)? entity.","acronyms":[[68,71]],"long-forms":[[60,66]]},{"text":"extract phrasal translations or transliterations of  phrase based on machine learning, or more  specifically the conditional random fields (CRF)  model.","acronyms":[[140,143]],"long-forms":[[113,138]]},{"text":" 2.3 Parsing and DSyntS transforming We adopt Deep Syntactic Structured (DSyntSs) as a format for syntactic structures because they can","acronyms":[[71,78],[17,23]],"long-forms":[[44,69]]},{"text":"which predicts the aligned source positions for every target word, and (c) the Positional Unknown (PosUnk) ?","acronyms":[[99,105]],"long-forms":[[79,97]]},{"text":"We developed three ensemble learning approaches for mindful unrest entities and a Vector Space Model based ways for encoding. Our approaches reached top classify in both subtasks, with the bestest F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received prodigious attention for its vital role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This collaborated is authorized under a Creative Commons Attribution 4.0 International Licence.","acronyms":[[408,411],[612,615]],"long-forms":[[379,406],[586,610]]},{"text":"The  motivation for this work is presented in section 4. Unsupervised Morphology Learner (UML)  framework is presented in section 5.","acronyms":[[90,93]],"long-forms":[[57,88]]},{"text":"Elfardy H. and Diab M. 2012. Simplified guidelines for the creation of grande scale dialectal arabic annotations. In Proceedings of the 8th Worldwide Conference on Linguistic Resources and Evaluation (LREC), Istanbul, Turkey. Elfardy H. and Diab M. 2013.","acronyms":[[202,206]],"long-forms":[[167,185]]},{"text":"Abstract This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and","acronyms":[[72,76]],"long-forms":[[49,70]]},{"text":"cation), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked.","acronyms":[[95,98],[9,12],[21,24],[50,53],[67,70],[112,115]],"long-forms":[[84,93],[26,35],[55,62],[72,81],[104,110]]},{"text":"using a set of data-driven terms.  We investigated how likely term frequency (TF) based RF is to discover HEWs.","acronyms":[[78,80],[88,90],[106,110]],"long-forms":[[62,76]]},{"text":"716 Figure 5: The NUMBERS System Architecture (CA = communicative act) The module network topology of the system is","acronyms":[[47,49],[18,25]],"long-forms":[[52,69]]},{"text":"2Although independently-developed implementations of  essentially the same algorithm can be found in the source code  of The Attribute Logic Engine (ALE) version 3.2 (Carpenter  & Penn, 1999) and the SICStus Prolog term utilities library ","acronyms":[[149,152]],"long-forms":[[125,147]]},{"text":"To extract the verb?noun combinations that have been utilise by non-native speakers in practicing, we uses the Cambridge Learner Corpus (CLC), which is a 52.5 million-word corpus of learner En-","acronyms":[[131,134]],"long-forms":[[105,129]]},{"text":"linguistic resources  ? Semantic Analysis Module (SAM) interpreting  LPM output utilizes application knowledge ","acronyms":[[50,53],[69,72]],"long-forms":[[24,48]]},{"text":"this ways\\]., the linguistic facts that pertain solely  to the source language (SL) are supposed to be  clearly separated from the facts that pertain merely ","acronyms":[[80,82]],"long-forms":[[63,78]]},{"text":"cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT)  relation(REL) social_relation(SREL) ","acronyms":[[96,99],[10,13],[23,27],[41,44],[58,61],[69,73],[81,84],[111,114],[132,136]],"long-forms":[[86,94],[0,9],[15,22],[30,40],[46,57],[64,68],[75,80],[102,110],[116,131]]},{"text":"Into Proc. of the 4th Workshop  on Treebanks and Linguistic Theories (TLT), pages  149?160.","acronyms":[[68,71]],"long-forms":[[33,66]]},{"text":"This  paper focuses on this problem in the context of  Information Extraction (IE). 2 Many extraction ","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":" 4 . i .1  Bagging (BAG)  From a training set of n examples, severaI sam- ","acronyms":[[20,23]],"long-forms":[[11,18]]},{"text":"                                                                  Barcelona, July 2004                                               Association for Computations Linguistics                        ACL Specially Interest Grouping on Computational Phonology (SIGPHON)                                                     Proceedings of the Workshop of the","acronyms":[[252,259],[197,200]],"long-forms":[[201,250]]},{"text":"from the output of the parser we adopt a uniform meaning representation which is a structured Logical Form(LF). In other words we map our f-","acronyms":[[107,109]],"long-forms":[[94,106]]},{"text":"3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong","acronyms":[[96,98],[132,135]],"long-forms":[[79,94]]},{"text":"system architecture  The data is stored in one central Resource  Repository (RR). As training data may adjustments (for ","acronyms":[[77,79]],"long-forms":[[55,75]]},{"text":"IDF Approach. Procedural of International Conference on Language Resources and Evaluation (LREC) 2012, European Language Resources Association","acronyms":[[92,96],[0,3]],"long-forms":[[57,75]]},{"text":"280  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203?1209, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"Although there is a modest cost associated with annotating data, we show that a reduction of 40% relative in alignment error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003).","acronyms":[[126,129],[152,158]],"long-forms":[[109,124]]},{"text":" Some implementation otes  The Carnegie Mellon Spoken Language Shell (CM-SLS)  was intentionally designed to have easy modifiable com- ","acronyms":[[70,76]],"long-forms":[[31,68]]},{"text":"~968)). Le SN d~fini  es tmarqu~ par la involvements du d~ter-  minati f  d~fini  (DEF), lequel peu ~tre anaphorique (ANAPH),  ou d~monstrat i f  (DEM), selon qu'i l  se rs~f~re ~ l 'envi- ","acronyms":[[115,120],[11,13],[79,82],[144,147]],"long-forms":[[102,113],[49,76],[127,141]]},{"text":"The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality; DP = Discriminatory Power; Train = Trainability; Type = Hardwired Type Selection; Hum = Human Preference Modelling; FB = Full Brevity .","acronyms":[[219,221],[103,105],[130,135],[185,188],[169,173]],"long-forms":[[224,236],[108,128],[138,150],[191,196],[152,156]]},{"text":"He et al (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation.","acronyms":[[128,131]],"long-forms":[[100,126]]},{"text":"~  = unrecognized entry token.)  (ABC) = (A(BC)) = ((AB)C). Tim singletonbidi- ","acronyms":[[53,57]],"long-forms":[[42,46]]},{"text":"error criteria: ? WER (word error rate): The WER is calculate as the minimum","acronyms":[[18,21],[45,48]],"long-forms":[[23,38]]},{"text":"called D2S. D2S has been used as the foundation of a number of language-generating systems, include GOALGETTER, a system that generates soccer reports in Dutchman.1 D2S consists of two modules: (1) a parlance generation module (LGM) and (2) a speech generation module (SGM) which turns the produced text into a discourse signal.","acronyms":[[227,230],[12,15],[7,10],[164,167],[268,271]],"long-forms":[[199,225],[242,266]]},{"text":" i=1:n P (GR = gri|SCF = s) The tres terms, given the hyper-parameters and","acronyms":[[10,12]],"long-forms":[[15,22]]},{"text":"flagged of language attainment at different stages of learning. The English Profile (EP)2 research agenda aims to enhance the learning, teaching","acronyms":[[82,84]],"long-forms":[[65,80]]},{"text":" Trials of 24th International Conference on Computational Linguistics (COLING): Posters. ","acronyms":[[76,82]],"long-forms":[[49,74]]},{"text":"Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN = SentiWordNet, TLL = Turney?Littman lexicon.","acronyms":[[110,114],[157,160],[46,48],[60,63],[87,89],[190,193],[210,213]],"long-forms":[[117,155],[163,188],[24,44],[66,85],[92,108],[196,208],[216,238]]},{"text":"Domains  In this section we compare some characteristics of the  English Travel Domain (ETD) and the English Spon-  taneous Scheduling Task (ESST).","acronyms":[[88,91],[141,145]],"long-forms":[[65,86],[101,139]]},{"text":"can formulate natural language-like patterns as exploratory queries for relations against a text corpus.  We draw inspiration from the information seeking paradigm of Exploratory Search (ES) (Marchionini, 2006; White and Roth, 2009), where users start with a vaguely defined information need and - with a mix","acronyms":[[187,189]],"long-forms":[[167,185]]},{"text":"? This material is base on research supported in part by the U.S. National Science Base (NSF) under Grant No.","acronyms":[[96,99],[62,66]],"long-forms":[[67,94]]},{"text":"The unspecified role filler is not 'bound'  to a complement (i.f. any topics on the SUBCAT list)  but is existentially quantified (EX-Q). The ergative ","acronyms":[[129,133],[82,88]],"long-forms":[[103,127]]},{"text":"strategies for shrinking ambiguity.  4.2.1 Longest ending filtering (LEF) The first approach to ambiguity reduction is based","acronyms":[[68,71]],"long-forms":[[42,66]]},{"text":" One application area of increasing interest is  information extract (IE) (see, e.g., Cowie and  Lehnert (1996)).","acronyms":[[73,75]],"long-forms":[[49,71]]},{"text":" 1 At t roduct ion :  a problem  Grammar development environments (GDE's) for  analyzes and for generation have not yet come to- ","acronyms":[[67,72]],"long-forms":[[33,65]]},{"text":"tended to conflate the distances between all attainable sense pairs. Latent semantic analysis (LSA) (Landauer et al, 1998) has also been used to measure dis-","acronyms":[[91,94]],"long-forms":[[65,89]]},{"text":"5 System Description The PEZ system consists of three components, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest","acronyms":[[100,105],[25,28],[125,130]],"long-forms":[[76,98]]},{"text":"ambiguous word are included as features.  Medical Subject Headings (MeSH): The final feature is also specific to the biomedical do-","acronyms":[[68,72]],"long-forms":[[42,66]]},{"text":"common view of the semantics of time. Since the target application domain is an historical database, we  present the essential features of the Historical Relational Database Model (HRDM), an extension to the  relational model motivated by the desire to incorporate more \"real world\" semantics into a database at ","acronyms":[[181,185]],"long-forms":[[143,179]]},{"text":"Information Retrieval (CLIR). It is also notable for Machine Translating (MT), especially when the languages do not use the same scripts.","acronyms":[[76,78],[23,27]],"long-forms":[[55,74],[0,21]]},{"text":"Moreover, in (Hahn et al, 2008a) two more models are applied to SLU: a Maximum Entropy (EM) model and a model coming from the Statistical Machine Translation (SMT) commu-","acronyms":[[88,90],[64,67],[159,162]],"long-forms":[[79,86],[126,157]]},{"text":"These 67? 30 candidate claims were annotated employs Amazon?s Mechanical Turk (AMT). In each","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":"ple need access to information someplace, anytime. The  Adaptive Information Management (AIM) service in the  FASiL VPA strives to automatically prioritise and pre-","acronyms":[[88,91],[109,114],[115,118]],"long-forms":[[55,86]]},{"text":"pick PRON up?, where PRON is the part of speech (POS) tag for pronouns.","acronyms":[[49,52],[21,25],[5,9]],"long-forms":[[33,47]]},{"text":"1. Introduction  Named entity(NE) recognition is important for recent  sophisticated information service such as question answering ","acronyms":[[30,32]],"long-forms":[[17,28]]},{"text":"Economics neighborhood fbank  bank  Subject Code EC = Economies  account cheque money by ","acronyms":[[49,51]],"long-forms":[[54,63]]},{"text":"comparisons to the baseline and stem, respectively.  As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compare","acronyms":[[93,98]],"long-forms":[[61,79]]},{"text":"We displays that the best  predict of translation complexity is given by the  average number of syllables per word (ASW). The ","acronyms":[[115,118]],"long-forms":[[77,113]]},{"text":"C5 Compound Analyser for Robust Morphological Analysis]. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.","acronyms":[[85,90]],"long-forms":[[57,83]]},{"text":"social media. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), pages 291?300.","acronyms":[[96,100]],"long-forms":[[68,94]]},{"text":"90 Table 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization, POS=part-of-speech tagging, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition). ","acronyms":[[156,158],[179,182],[111,114]],"long-forms":[[159,177],[183,207],[97,109],[115,129],[141,154]]},{"text":"to five possible values (rules have been presented with the sentence pairs from which they have been acquired): entailment=yes (YES), i.e. correctness of the rule; entailment=more-phenomena (+PHEN), i.e.","acronyms":[[128,131],[192,196]],"long-forms":[[123,126],[180,189]]},{"text":"Labels Base NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR (proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation) Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR","acronyms":[[184,186]],"long-forms":[[188,199]]},{"text":"The next four columns show the number of true affirmative (PT)--verbs considered +S both  by machine and by hand; false positives (FP)--verbs judged +S by machine, -S  by  hand; true negative (TN)--verbs judged -S  both by machine and by hand; and untruthful  negatives (FN)--verbs judged -S  by machine, +S by hand.","acronyms":[[188,190],[57,59],[125,127],[261,263]],"long-forms":[[172,186],[41,54],[108,122],[243,258]]},{"text":"sociations to a number of affect categories encompass the six Ekman emotions (joy, sorrow, anger, frightened, disgust, and surprise).3 General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 cat-","acronyms":[[148,150]],"long-forms":[[130,146]]},{"text":"At each dataset, we report Pearson?s correlation r with human judgements on pairs that are found in both resources (BOTH). Otherwise, the re-","acronyms":[[116,120]],"long-forms":[[100,104]]},{"text":"the conjunction but. Turney and Littman (2003) utilized pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analyzes","acronyms":[[81,84]],"long-forms":[[51,79]]},{"text":"and Ripper, on the other hand, appear to take more advantage of some feature types than others. Onto the third task, lexical (LX) and discourse (DS) characters ostensibly have more predictive power for both C4.5 and SVM than the other sorts.","acronyms":[[125,127],[144,146],[213,216]],"long-forms":[[116,123],[133,142]]},{"text":"suggestion a newest inference method ? collective iterative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities","acronyms":[[70,73],[110,113]],"long-forms":[[33,68],[88,108]]},{"text":"languages studied differ widely, there is a quasistandard for presenting the material, in the form of interlinearized glossed text (IGT). IGT typically","acronyms":[[132,135],[138,141]],"long-forms":[[102,130]]},{"text":"Section 2 discusses related work. Section 3 introduces the Condition Random Fields(CRFs)  and the defined Long-Dependency CRFs ","acronyms":[[83,87],[122,126]],"long-forms":[[59,82]]},{"text":" As the noun or adjective occur in the first slit  of conjunct verbs (ConjVs) construction, the  search outset from the point of noun or adjec-","acronyms":[[70,76]],"long-forms":[[54,68]]},{"text":"example, the prepObject )f a LOCATION-PP needs be a  PLACE-NOUN. A description of \"on AI\" (as in \"ledger on  DID\") as a LOCATION-PP c~Id  not be constructed since AI ","acronyms":[[85,88],[159,161],[29,40],[52,62],[116,127]],"long-forms":[]},{"text":"in table 4. As we can see a small improved is profited for the interpretation error rate (IER) with the integrated strategy (strat2).","acronyms":[[93,96]],"long-forms":[[66,91]]},{"text":"lingual extensions of approaches predicated on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space authorize documents","acronyms":[[100,103],[50,53],[68,71],[73,77]],"long-forms":[[90,98]]},{"text":"In  Proceedings of the 16th International Conference  on World Wide Web (WWW), pages 697-706. ","acronyms":[[73,76]],"long-forms":[[57,71]]},{"text":"2.2 Singular Value Decomposition Given any matrix S, its singular values decomposition (SVD) is S = U?V T . The matrix Saskatchewan =","acronyms":[[87,90],[99,102]],"long-forms":[[57,85]]},{"text":"lion tokens to 0.3832 for 237 million tokens. At such data sized, Stupid Backoff (SB) with a ceaseless backoff parameter ?","acronyms":[[82,84]],"long-forms":[[66,80]]},{"text":"Measuring speech quality for text-to-speech systems: Development and assessment of a modified mean opinion score (MOS) scale. Computer Speech","acronyms":[[114,117]],"long-forms":[[94,112]]},{"text":"bic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the the Association","acronyms":[[98,104]],"long-forms":[[71,96]]},{"text":"ANTEST R E T U R N S  ** 1**  CHANGT, HAVE CSEXCE1 ONTO HESGEF IN  ARTEST CALLEC DURING 18\"REGVO I C E u  (AACC)  ANTEST E ton U E N S  ** l** ","acronyms":[[103,107],[43,50],[51,54],[55,61],[0,6]],"long-forms":[[66,96]]},{"text":"The rules, which may be different for each  dictionary, are written using a formalism in the spirit of  the \"definite clause grammars\" (DCG's) of Pereira and  Warren (1980) and \"modular logic grammars\" (MLG's) ","acronyms":[[136,141],[203,208]],"long-forms":[[109,133],[178,200]]},{"text":"The implementations of the oracles outlined in the first part of this work (sections 3 and 4) utilizing the common formalism of finite state acceptors (FSA) over divergent semirings and are implemented us-","acronyms":[[147,150]],"long-forms":[[123,145]]},{"text":"P (di|h(d1,..., di?1)) Using a neural network architecture called Simple Synchrony Networks (SSNs), the history representation h(d1,..., di?1) is incrementally computed from","acronyms":[[93,97]],"long-forms":[[73,91]]},{"text":" Throughout Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 865? ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":" X '1\"~,. \\[-1  Def in i t ion 2.2 A N:M sur face coerc ion  (SC)  ru le ix a quadruple (\/,c\/,c~,r) where l and r ","acronyms":[[62,64]],"long-forms":[[41,59]]},{"text":"to do the testing on authentic emotions. The Berlin Emotional Database (EMO-DB) contains the set of emotions from the MPEG-4 standard (fury,","acronyms":[[67,73],[113,119]],"long-forms":[[47,65]]},{"text":"Finally, it is clear from Figure 6 that certain relations are particularly difficult for both parsers. For example, indirect object (IObj) dependents are low scoring nodes: this is because they are often attached to the correct head but are","acronyms":[[133,137]],"long-forms":[[116,131]]},{"text":"Abstract We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essen-","acronyms":[[92,95]],"long-forms":[[62,90]]},{"text":" 2.1 Modeling Votes Ideals point (IP) models are a mainstay in quantitative political science, often applied to voting registering to","acronyms":[[33,35]],"long-forms":[[20,31]]},{"text":" In the shallowest level of attachment we find the conjunctions (CONJ+) +\u0000 w+ ? and?","acronyms":[[65,70]],"long-forms":[[51,63]]},{"text":"trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard En-","acronyms":[[94,98]],"long-forms":[[66,92]]},{"text":"Taggers have been developed for a several of languages, including Modernity Standard Arabic (MSA) (Khoja, 2001; Diab et al, 2004).","acronyms":[[90,93]],"long-forms":[[66,88]]},{"text":" 5 Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs. { 4,5}","acronyms":[[29,32]],"long-forms":[[3,27]]},{"text":"10 ESA on senses and Wikipedia Link Measure (WLM) compute similarity on a sense-level, however, sim-","acronyms":[[45,48],[3,6]],"long-forms":[[21,43]]},{"text":"studied three activation functions: 1. Rectified linear units (ReLUs) (Nair and Hinton, 2010):","acronyms":[[63,68]],"long-forms":[[39,61]]},{"text":"While much of the centred in developing a statistical machine translation (SMT) system revolves around the translation paragon (TM), most plans do not emphasize the role of the language model (LM).","acronyms":[[124,126],[73,76],[191,193]],"long-forms":[[105,122],[40,71],[175,189]]},{"text":"07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9  Tables 9  Constituent parsing evaluation results of Task 2-2 (Closes Track), ranks with Tot-F1  (S_S=simple sentence, C_S=complex sentence)  ID Sys-ID Modelling Constituents in S_S C_S constituent Total POS-A Classifications","acronyms":[[151,154],[172,175],[253,258],[227,234],[198,204]],"long-forms":[[155,170],[176,192]]},{"text":"vestigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since","acronyms":[[150,152],[37,39],[59,61],[102,104]],"long-forms":[[129,148],[24,35],[42,57],[82,100]]},{"text":" ? Shift(SH): Push FUTURE onto the stack. ","acronyms":[[9,11],[19,23]],"long-forms":[[3,8]]},{"text":"10?16). Results for per-predication (PR) and per-whole-graph (GRPH) labelled percentage accuracies are listed. (","acronyms":[[62,66],[37,39]],"long-forms":[[55,60],[24,35]]},{"text":"ACC mPUR + ACC The random baseline(BL) is calculated as follows: BL = 1\/number of classes","acronyms":[[35,37],[0,3],[4,8],[11,14],[65,67]],"long-forms":[[26,33]]},{"text":" Based on the statistics shown in Table 3, the likelihood ratio tests (LRT) model captures the statistical association between a pattern p and a word","acronyms":[[71,74]],"long-forms":[[47,69]]},{"text":" 1 Introduct ion  Some natural anguage processing (NLP) tasks can  be performed with only coarse-grained semantic in- ","acronyms":[[51,54]],"long-forms":[[23,49]]},{"text":"SaRAD .891 .919 .905 ALICE .961 .920 .940 Chang & Sch?utze (CS) .942 .900 .921 Nadeau & Turney (NT) .954 .871 .910","acronyms":[[60,62],[0,5],[96,98]],"long-forms":[[42,58],[79,94]]},{"text":"3.2 Conclusions of Chinese NER We evaluated our named entity recognizer on the SIGHAN Microsoft Research Asia(MSRA) corpus in both latched and open track.","acronyms":[[105,109],[22,25],[74,80]],"long-forms":[[81,104]]},{"text":"4.3 Classifiers All evaluation tests were performed using two classifiers, Maximum Entropy (MaxEnt) and Support Vector Machines (SVM).","acronyms":[[92,98],[129,132]],"long-forms":[[75,90],[104,127]]},{"text":"5.5.3 Corpus Statistics. For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dia-","acronyms":[[151,154]],"long-forms":[[136,149]]},{"text":"C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.  Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other verbs trigger state O).","acronyms":[[89,92],[0,6],[19,22],[23,25],[46,52],[56,64],[98,106],[108,111],[116,119]],"long-forms":[[67,81]]},{"text":"Metonymy Often a sentence relates entities in a way inconsistent with the intent ontology. For example, with the Component Library (CLib) ontology,movement properties (e.g., promptness, acceleration) are defined as properties of the movements events, rather","acronyms":[[132,136]],"long-forms":[[113,130]]},{"text":" The application will eventually be deployed using a Software as a Service (SaaS) model. It will","acronyms":[[76,80]],"long-forms":[[53,74]]},{"text":"  ME Classification  ME (Maximum Entropy) classification is used here  to directly estimate the posterior probability for ","acronyms":[[21,23],[2,4]],"long-forms":[[25,40]]},{"text":"pre-rendered animations.  The Natural Language Understanding (NLU) module needs to cope with both chat and military","acronyms":[[62,65]],"long-forms":[[30,60]]},{"text":"take scope over another.  Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate","acronyms":[[61,64]],"long-forms":[[32,59]]},{"text":" 2 Question Classification We define Question Classification(QC) here to be the task that, given a question, maps it to one of","acronyms":[[61,63]],"long-forms":[[37,59]]},{"text":"sampling the outputs at random locations.  INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) portrays the direction","acronyms":[[54,57]],"long-forms":[[59,81]]},{"text":"There are three options: French (FR), Spanish (SP), or, Merged languages (ML), where the results are obtained by merging the English output of FR","acronyms":[[74,76],[33,35],[47,49],[143,145]],"long-forms":[[56,72],[25,31],[38,45]]},{"text":" P rev ious  Accomplishments  We have previously constructed a UNIX Consultant (UC), an intelligent NL-capable \"helping\"  facility that allows naive users to learn about the UNIX operational system.","acronyms":[[80,82],[100,102],[171,175]],"long-forms":[[63,78]]},{"text":"machine learning tasks. We used character n-grams, word n-grams, Parts of Speech (POS) tag n-grams, and perplexity of character trigrams as features.","acronyms":[[82,85]],"long-forms":[[65,80]]},{"text":"PoS tagF5. Lemma (L)F6. Inflection (INFL)F7. Main verb of main clause (MV)F8.","acronyms":[[36,40],[0,3],[71,73]],"long-forms":[[24,34],[11,16],[45,54]]},{"text":"The second approach is based on statistical modeling. We adopted one typical  implementation called the \"vector space model\" (VSM) (Frakes and Baeza-Yates 1992;  Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch\/itze 1992), which has ","acronyms":[[126,129]],"long-forms":[[105,123]]},{"text":" 3.3 Bootstrapped Voting Experts The Bootstrapped Voting Experts (BVE) algorithm (Hewlett and Cohen, 2009) is an extension to VE.","acronyms":[[66,69],[126,128]],"long-forms":[[37,64]]},{"text":"ferent levels. For each texts pair on four cross levels, i.e., Paragraph to Sentence (P-S), Penalties to Phrase (S-Ph), Phrase to Word (Ph-W) and Word","acronyms":[[85,88],[111,115],[134,138]],"long-forms":[[62,83],[91,109],[118,132]]},{"text":"Abstract Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet.","acronyms":[[79,82]],"long-forms":[[56,77]]},{"text":" ? Goals types per Source type (TpS), i.e. the number of target types a specific source type","acronyms":[[33,36]],"long-forms":[[10,26]]},{"text":"They refer to  syntactical features of a constituent such as number  (NUM), gender (GEN) etc. and to grammatical functions ","acronyms":[[84,87]],"long-forms":[[76,82]]},{"text":"developing a ser ies of inc reas ing ly  soph is t icated natura l  language unders tand ing   systems which will serve as an in tegrated  in ter face  to severa l  faci l i t ies at the Pacif ic  F leet Command Center:  the In tegrated  Data Base (IDB), which conta ins  information  about  ships, the i r  read iness  s tates ,  the i r  capabi l i t ies,  etc.;","acronyms":[[249,252]],"long-forms":[[225,247]]},{"text":"ments were annotated with word-level labels by professional translators utilised the core categories in MQM (Multidimensional Quality Metrics) 13","acronyms":[[101,104]],"long-forms":[[106,138]]},{"text":"NST = Noun Stem V-FLEX = Verbal Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense ","acronyms":[[98,100],[131,133],[0,3],[16,22],[43,46],[57,63],[88,90],[119,121],[142,144],[162,164],[174,176]],"long-forms":[[93,97],[136,140],[6,15],[25,41],[49,56],[66,86],[113,118],[124,130],[147,161],[167,173],[179,184]]},{"text":"158  I. CONSTRUCT THE PROPOSED ANCHORS for Un  (a) Create set of referring expressions (RE's). ","acronyms":[[88,92]],"long-forms":[[65,86]]},{"text":"Pattern Pattern Patterns composed of higher frequency words (HFWs) 4","acronyms":[[59,63]],"long-forms":[[37,57]]},{"text":" 1 Introduction Referring Expression Generation (REG) is a keytask in NLG, and the topic of the REG 2008 Chal-","acronyms":[[49,52],[70,73],[96,99]],"long-forms":[[16,47]]},{"text":"ARG1, ARG2, MOD-BENEFICIARY, and MOD-TIME. To identify which slot has the most similarity  among its elements, we calculate the number of distinct elements (NDE) in each slot across the  propositions.","acronyms":[[157,160],[0,4],[6,10],[12,27],[33,41]],"long-forms":[[128,155]]},{"text":"The model consists of two subtasks of boundary identification(BI) and semantic role classification(SRC). ","acronyms":[[99,102],[62,64]],"long-forms":[[70,97],[38,61]]},{"text":"amples in 3.2).  In section 4, we describe the  specification of Korean TimeML (KTimeML). ","acronyms":[[80,87]],"long-forms":[[65,78]]},{"text":"to the recent evaluations of domain-independent Q\/A systems organized in the context of the Text REtrieval Conference (TREC)1. The TREC","acronyms":[[119,123],[48,51],[131,135]],"long-forms":[[92,117]]},{"text":"1 25   2. Madman l  Word  Grouper  (LWG)  The funct ion  of th i s  b locks  is to fo rm ","acronyms":[[34,37]],"long-forms":[[10,31]]},{"text":"function (Section 4.1).  Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the conceive of","acronyms":[[55,57]],"long-forms":[[45,53]]},{"text":"{csjxu, csluqin}@comp.polyu.edu.hk Abstract The Semantic Textual Similarity (STS) task aim to inspecting the degree of semantic","acronyms":[[77,80]],"long-forms":[[48,75]]},{"text":"a geared acyclic graph, and a set of conditionnal probablities, each node being representing as a Random Variable (RV). Parametrizing the BN","acronyms":[[116,118],[139,141]],"long-forms":[[99,114]]},{"text":"problem so that it includes finding the most probable corrections tags.  WDCLOREI arg max Pr( WDCLOREIIA ) WDCLOREI ","acronyms":[[94,104],[107,115]],"long-forms":[[73,89]]},{"text":"By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)based tagging model.","acronyms":[[92,95]],"long-forms":[[71,90]]},{"text":"NP NP\\NP (S\\NP )\/NP NP< >NP S\\NP <S Fruit fly likes bananas NP S\\NP (S\\S)\/NP NP< >S S\\S <S (b) The searched space in this work, with one node for each","acronyms":[[64,68],[0,2],[3,8],[61,63],[25,27],[28,32]],"long-forms":[[70,77]]},{"text":"Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN ? CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN","acronyms":[[54,57]],"long-forms":[[59,87]]},{"text":"ror rate (WWER), which gives a weight on errors from the viewpoint of ROE, instead of word mistaken rate (WER), which treats all words uniformly.","acronyms":[[102,105],[10,14],[70,72]],"long-forms":[[85,100]]},{"text":"(6) health issues (HI) trend other (7) personal issues (PI) trend dwindling (8) lectures attended (LA) trend other (9) revision (R) trend reducing","acronyms":[[100,102],[19,21],[56,58]],"long-forms":[[81,98],[4,16],[39,53],[120,128]]},{"text":"ofwi. Just like the statistical pproaches in many automatic POS tagging programs, our job is to select a  constituent boundary sequence B'with the highest score, P(BIS), from all possible sequences. ","acronyms":[[164,167],[60,63]],"long-forms":[[136,160]]},{"text":"Him washed it. With Kamp's  discourse representative theory (DRT) (Kamp 1981; Kamp and Reyle 1993) a discourse  representation structure (DRS) in which the reference to the pronoun he is braked ","acronyms":[[60,63],[137,140]],"long-forms":[[27,58],[100,135]]},{"text":"not observable from the sentence.  The dynamic nature of named entities (NEs) makes it difficult to enumerate all of their evolv-","acronyms":[[73,76]],"long-forms":[[57,71]]},{"text":"SN = Sa-inflection oun (nominal verb)  CM = lawsuit marker (-nom\/-acc argument)  PT = particle (other arguments)  VB = verb ","acronyms":[[78,80],[0,2],[39,41],[111,113]],"long-forms":[[83,91],[5,22],[44,55],[116,120]]},{"text":"document is multiple from the question. Further, in  Information Extraction (IE), in which the system  tries to extract elements of some events (e.g. ","acronyms":[[75,77]],"long-forms":[[51,73]]},{"text":"From every contention to the predicate, we extract all enfants  noun phrases (NP) and adjectival phrases (ADJP)  as candidate gaps as well.","acronyms":[[74,76],[102,106]],"long-forms":[[60,72],[82,100]]},{"text":" Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition  F-Measure (RFMdet): these metrics are designed ","acronyms":[[65,71],[32,37],[101,107]],"long-forms":[[41,63],[14,30],[77,99]]},{"text":"Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al.,","acronyms":[[104,106],[48,51],[107,110]],"long-forms":[[78,90]]},{"text":"storing our storage. We built a pattern matching  system based on Finite State Automata(FSA). After ","acronyms":[[88,91]],"long-forms":[[66,87]]},{"text":"Disco-En-Gold consists of 349 expressions splits into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0 to 100, indicate the level of compositionality (the","acronyms":[[108,113],[86,90],[65,71]],"long-forms":[[97,106],[74,84],[42,63]]},{"text":"Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the the number of lexical categories per word used (from first to last parsing attempt).","acronyms":[[186,188],[46,49],[128,135],[202,204],[218,220]],"long-forms":[[168,178],[23,44]]},{"text":"185  Table h Size of the corpora of HTML pages (in Mb) collected on the four patterns (1.a-d)  through AltaVista (AV) and Northern Light (NL). ","acronyms":[[114,116],[138,140],[36,40],[51,53]],"long-forms":[[103,112],[122,136]]},{"text":"2007). Two 5 DOF sensors - TT (Tongue Tip)  and TB (Tongue Body Back) - were attached on  the midsagittal of the tongue.","acronyms":[[48,50],[27,29],[13,16]],"long-forms":[[52,63],[31,41]]},{"text":"these basic models for email conversations.  4.1 Latent Dirichlet Allocation (LDA) Our first model is the probabilistic LDA model","acronyms":[[78,81],[120,123]],"long-forms":[[49,76]]},{"text":"We review the hierarchical Dirichlet document (HDD) paragon in section 2, and present our proposal hierarchical Dirichlet tree (HDT) document model in section 3.","acronyms":[[126,129],[47,50]],"long-forms":[[97,124],[14,45]]},{"text":"tion database (OID) and provides the corresponding interface;  - Knowledge Retriever (KR) ? retrieves KSs from ","acronyms":[[86,88],[15,18],[102,105]],"long-forms":[[65,84]]},{"text":"Queue to for user intervention.  4.2,  Document Processor (DP)   The DP identifies and extracts all SGML tags de- ","acronyms":[[59,61],[100,104]],"long-forms":[[39,57]]},{"text":"0.467 (+126%)?  Total Document Reciprocal Rank (TDRR) PubMed 0.495 0.137 0.038 0.331","acronyms":[[48,52]],"long-forms":[[16,46]]},{"text":" Based on the statistically indicated in Table 3, the likelihood ratio tests (LRT) model captures the statistical association between a pattern p and a word","acronyms":[[71,74]],"long-forms":[[47,69]]},{"text":"The knowledge about actions and plans is stored in  a plan library structured on the basis of two main hier-  archies: the Decomposition Hierarchy (DH) and the  Generalization Hierarchy (GH) \\[Kautz and Allen, 86\\].","acronyms":[[148,150],[187,189]],"long-forms":[[123,146],[161,185]]},{"text":"3 Polylingual Topic Paragon The polylingual topic model (PLTM) is an extension of inactive Dirichlet alocation (LDA) (Blei et al.,","acronyms":[[108,111],[55,59]],"long-forms":[[80,106]]},{"text":"calizations.  Direct responses (DS) are essentially characterized by introductory markers like yup\/no\/this is pos-","acronyms":[[32,34]],"long-forms":[[14,30]]},{"text":"ranking models on this data set, including Support Vector Machine (SVM) with a linear kernel, SVM with a radial basis function (RBF) kernel and Logistic Regression (LR).","acronyms":[[128,131],[67,70],[94,97],[165,167]],"long-forms":[[105,126],[43,65],[144,163]]},{"text":"Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammatical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing","acronyms":[[126,128],[108,110],[142,144],[158,160],[175,177]],"long-forms":[[131,140],[113,124],[147,156],[163,173],[180,187]]},{"text":" It considers all pronouns (PRP, PRP$), noun expression (NP) and heads of verb phrases (VP) as potential mentions.","acronyms":[[85,87],[28,31],[54,56],[33,37]],"long-forms":[[71,83],[40,51]]},{"text":"12  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Treatment (ANLP), pages 137?142, September 25, 2014, Doha, Qatar.","acronyms":[[82,86]],"long-forms":[[46,80]]},{"text":"tives falls in the middle range and what causes the big and small divergence of the document collection peers with different topics (DT) and the same themes (ST) or perspective (SP), respectively.","acronyms":[[135,137],[159,161],[179,181]],"long-forms":[[117,133],[147,157],[166,177]]},{"text":" 2007. The Syntax Augmented MT (SAMT) system at the Shared Tasks for the 2007 ACL Workshop on","acronyms":[[32,36],[77,80]],"long-forms":[[11,30]]},{"text":"The CoreSC scheme consists of three layers; the first layer corresponds to eleven concepts (Background (BAC), Suppositions (HYP), Motivates (MOT), Goal (GOA), Object (OBJ), Method (MET), Model","acronyms":[[122,125],[140,143],[4,10],[104,107],[152,155],[166,169],[180,183]],"long-forms":[[110,120],[128,138],[92,102],[146,150],[158,164],[172,178]]},{"text":"a different word, such as job, in a given backgrounds) by defining a one-class learn algorithm based on support vector machines (SVM). They train a one-","acronyms":[[128,131]],"long-forms":[[103,126]]},{"text":"We elected Gaussian distributions. If the parents of node X are Y, P (X|Y ) = N(m + W ?","acronyms":[[68,71]],"long-forms":[[56,63]]},{"text":" ? Lexical Overlap and Length (LO): This set of features represents the lexical overlap between","acronyms":[[31,33]],"long-forms":[[3,18]]},{"text":"learning is straightforward.  Very Reduced Regular Expression (VRRE):  Given a finite alphabet E,  the set of very ","acronyms":[[63,67]],"long-forms":[[30,61]]},{"text":"for the different settings. GM = Google Maps, CI = Campus Indoor, CO = College Outdoor. ","acronyms":[[66,68],[28,30],[46,48]],"long-forms":[[71,85],[33,44],[51,64]]},{"text":"smoothness. Before creating a POMDP structure, we used the dynamic Bayesian network (DBN) structure (Fig.","acronyms":[[85,88],[30,35]],"long-forms":[[59,83]]},{"text":"ferent setups with this parameter. We compare the following setups: (1) The majority baseline (BL) i.e., choosing the most frequent label (SR). (","acronyms":[[95,97],[139,141]],"long-forms":[[85,93]]},{"text":"segment of a conversation. We will consequently use the  terms initiating conversational participants (ICP) and other  conversational participant(s) (OCP) to distinguish the initi- ","acronyms":[[99,102],[146,149]],"long-forms":[[60,97],[108,141]]},{"text":" 1 In t roduct ion   For most natural language processing (NLP) systems,  thesauri comprise essentials linguistic knowledge.","acronyms":[[59,62]],"long-forms":[[30,57]]},{"text":"September\/NNP \\] \\[O .\/. \\]  we can extract following chunk patterns:  NP=NULL 90 PRP 99 VBZ  VP=PRP 99 VBZ 99 DT ","acronyms":[[74,78],[10,13],[71,73],[82,85],[89,92],[94,96],[97,100],[104,107],[111,113]],"long-forms":[]},{"text":"location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL)  motifs(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT) ","acronyms":[[75,78],[92,95],[9,12],[28,31],[44,47],[57,61],[103,107],[115,118],[130,133]],"long-forms":[[64,73],[80,90],[0,8],[14,27],[34,43],[49,56],[98,102],[109,114],[120,129]]},{"text":"relative-resource?, i.e.  EuroWordNet (EWN).1   In this paper we start by shortly recalling the ","acronyms":[[39,42]],"long-forms":[[26,37]]},{"text":"3.5 Rank   At our task of classifying ALI patients, we  picked the Maximum Entropy (MaxEnt) algorithm due to its good performance in text classi-","acronyms":[[95,101],[49,52]],"long-forms":[[78,93]]},{"text":" In Proceedings of the Conference on Web Search and Web Data Mining (WSDM). ","acronyms":[[69,73]],"long-forms":[[37,67]]},{"text":"Adobe website:2.03 Adobe Systems:1.82 Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discovery and Data Mining (KDD), is the process of automatically searching large volumes of data for patterns.","acronyms":[[51,53],[104,107],[149,152]],"long-forms":[[38,49],[70,101],[112,140]]},{"text":"description model, the Dublin Core Metadata Set, together with an interchange method provided by the Open Archives Initiative (OAI), make it possible to construct a union catalog over","acronyms":[[127,130]],"long-forms":[[101,125]]},{"text":"Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime.","acronyms":[[420,423]],"long-forms":[[385,418]]},{"text":"in a fully automatic fashion. Still, this is an enthralling possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methodology (namely, that sense-tagged corpora are very costly to acquire).","acronyms":[[150,153]],"long-forms":[[123,148]]},{"text":" The headline  generation system we present uses  Singular Value Decomposition (SVD) to  guide the generation of a headline ","acronyms":[[80,83]],"long-forms":[[50,78]]},{"text":"    The difference between the two models was the design of the input layer. The first model (henceforth, the diagnostics model DIAG) took diagnostics as input nodes, whereas the second model (henceforth, the semantic parameters model SEMANP) took semantic parameters as input nodes, as presented in detail below.    The Diagnostics Model (DIAG): Binary ac-ceptability values of the phrases or sentences formed by the syntactic diagnostics constituted the input nodes for the network (see above for the SI diagnostics). Each syntactic diagnostic provided a binary value (either 0 or 1) to one of the input nodes.","acronyms":[[340,344],[128,132],[235,241]],"long-forms":[[321,338],[110,120],[209,234]]},{"text":"exposed through the feature HOOK to facilitate further composition. These properties include pointers to the local top handle (LTOP), the constituent?s primary index (INDEX), and the external argument, if any (XARG).","acronyms":[[127,131],[167,172],[210,214],[28,32]],"long-forms":[[109,118],[160,165],[183,199]]},{"text":"System and Datasets We use the Moses phrasebased MT system (Koehn et al, 2007) and consider Urdu?English (UR?EN), Chinaman?Anglais (ZH?EN) translation, and Arabic?English","acronyms":[[106,111]],"long-forms":[[92,104]]},{"text":"grammatical featuring from a diachronic corpus of academic English, and visualise our extraction results with Structured Parallel Focal (SPC), a tool for the visualisation of structured multidi-","acronyms":[[142,145]],"long-forms":[[109,140]]},{"text":"els on a before unseen test set ? the test split of part 3 of the PATB (PATB3-TEST). Table 6 show","acronyms":[[76,86]],"long-forms":[[56,74]]},{"text":"Method (METH) the methods used Result (RES) the results achieved Conclusion (CON) the authors? conclusions","acronyms":[[77,80],[8,12],[39,42]],"long-forms":[[65,75],[0,6],[31,37]]},{"text":"al., 2005; Joachims et al, 2009) formulation, as shown in Optimization Problem 1 (OP1), to learn a weight vector w.","acronyms":[[82,85]],"long-forms":[[58,80]]},{"text":"From raw text extract the temporal entities (events and timexes), identify the pairs of temporal entities that have a temporal link (TLINK) and classify the temporal relation between them.","acronyms":[[133,138]],"long-forms":[[118,131]]},{"text":"RERANKED 92.7 42.9 92.0 32.6 ORACLE 97.6 81.2 96.7 72.5 Table 4: Word accuracies and error rate reductions (ERR) in percentages for CELEX G2P augmented by Combilex","acronyms":[[108,111]],"long-forms":[[85,106]]},{"text":"Table 3: Experimental Results (Microsoft?s Provided Train and Test Set) sorted the sentences pairs of the MSRP corpus according to the length difference ratio (LDR) defined in Section 3, and partitioned the sorted cor-","acronyms":[[160,163],[106,110]],"long-forms":[[135,158]]},{"text":"In l~,ooth's approach, the FSV is detined by re-  (:ursion on the truth conditional structure which  is itself derived from LF (i.e. Logical Form, the  Government and Binding level of semantic rep- ","acronyms":[[124,126],[27,30]],"long-forms":[[133,145]]},{"text":"protein interaction as an example. In Proceedings  of the Pacific Symposium on Biocomputing (PSB),  Hawaii, USA.","acronyms":[[93,96],[108,111]],"long-forms":[[58,91]]},{"text":" 5.1 Calculation of Emotion Tag weights  Sense_Tag_Weight (STW): The tag weight has  been calculated using SentiWordNet.","acronyms":[[59,62]],"long-forms":[[41,57]]},{"text":"pirical study for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).","acronyms":[[123,125],[155,159]],"long-forms":[[111,121],[131,154]]},{"text":"nese personal naming system. Therefore, we hold  Chinese personal name disambiguation (CPND) to  investigated those problems.","acronyms":[[87,91]],"long-forms":[[49,85]]},{"text":"Results on the dev set using two metrics: instance rank precision (CA), and soundbite name  recognition accuracy (RA). The oracle RA is 79.1%.","acronyms":[[123,125],[76,78],[139,141]],"long-forms":[[101,121],[51,74]]},{"text":"some freedom in the ordering of major phrasal categories like  NPs  and adverbial phrases - for example, in the linear order of  subject (SUB J), direct object (DOBJ), and indirect object (lOB J)  with respect o one another.","acronyms":[[138,143],[63,66],[161,165],[189,195]],"long-forms":[[129,136],[146,159],[172,187]]},{"text":"res. In Section 2 we present a structural reformulation of Support Vector Machines (SVMs) that can take similarities between different genres into ac-","acronyms":[[84,88]],"long-forms":[[59,82]]},{"text":"Sematnic data models are systems for  constructs precise descriptions of protions of  the real worldwide - semantic data description (SDD)-  using terms that come from the real world rather ","acronyms":[[132,135]],"long-forms":[[105,130]]},{"text":"Just as in Figure 2, the initial model is denoted with a audacious symbol in the left part of the intrigue. Also for reference the relevant Terminology 1 accuracy (LEX 1) is denoted with a ? at the far right.","acronyms":[[151,156]],"long-forms":[[131,140]]},{"text":"4.0 release. This was also the training data utilised in the experiment in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task","acronyms":[[93,96],[113,118]],"long-forms":[[76,91]]},{"text":"speech analysis phase, the situation  frame is interpreted, resulting in one or  more instantiated knowledge foundation (KB)  objects, which are state or event ","acronyms":[[118,120]],"long-forms":[[102,116]]},{"text":" An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induced representations that are maximally cor-","acronyms":[[91,94]],"long-forms":[[59,89]]},{"text":"(iv) Embedded appositional phrases.  (2) Very prolonged PE's (Phrasal Elements) appear occasionally. ( eg.","acronyms":[[51,55]],"long-forms":[[57,73]]},{"text":"retrieval with locality information uses smart. In  Text retrieval conferenc (TREC-1) (pp. 59-72).","acronyms":[[79,85],[88,90]],"long-forms":[[53,77]]},{"text":"  Abstract  Our past work concentrate on combining translation memory (TM) and statistical machine translation  (SMT) when the TM database and the SMT training set are the same.","acronyms":[[71,73],[113,116],[127,129],[147,150]],"long-forms":[[51,69],[79,110]]},{"text":" 04 (a) Same Topic (TK) 0.00 0.01 0.02 0.03 0.04 0.05 0.06","acronyms":[[20,22]],"long-forms":[[8,18]]},{"text":"the parameters. We trained our network with stochastic gradient descent (SGD), mini-batches and adagrad updates (Duchi et al, 2011), using","acronyms":[[73,76]],"long-forms":[[44,71]]},{"text":"With respect to the EUROTRA MT system this has  important implications for the translation between the syntactic  dependency level - the EUROTRA Relational Structure (ERS)  and the semant ic  level  - the in ter face  St ructure  (IS).","acronyms":[[167,170],[20,27],[28,30],[231,233]],"long-forms":[[137,165],[205,228]]},{"text":"   Simple Segmentation Algorithm(SSA):  1.","acronyms":[[33,36]],"long-forms":[[3,31]]},{"text":" 3 Baseline SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-","acronyms":[[68,71],[12,15]],"long-forms":[[35,66]]},{"text":"Kearns (2002) distinguishes between two usages of light verbs in LVCs: what she invitations a true light verb (TLV), as in give a groan, and what she calls a foggy measures verb (VAV), as in","acronyms":[[105,108],[65,69],[171,174]],"long-forms":[[88,103],[152,169]]},{"text":"In order to normalize Thai input character  sequences to a canonical Unicode form, we developed a finite state transducer (FST) which  detects and repairs a number of sequencing er-","acronyms":[[123,126]],"long-forms":[[98,121]]},{"text":"  Among the late 1970s a research team at USCInformation Sciences Institute (ISI) investigate natural  dialogues with particular interest in applying the ","acronyms":[[74,77],[39,42]],"long-forms":[[42,72]]},{"text":"suffix and prefix information, as well as information about the sorrounding words and their tags are used to develop a Maximum Entropy (MaxEnt) based Hindi NER system.","acronyms":[[136,142],[156,159]],"long-forms":[[119,134]]},{"text":"4.2 Preprocessing Part?Whole Lexico-Syntactic Patterns Since our discovery procedure is based on the semantic information provided by WordNet, we need to preprocess the noun phrases (NPs) extracted by the three clusters considered and identify the potential part and the whole concepts.","acronyms":[[183,186]],"long-forms":[[169,181]]},{"text":" 4KEY: E1S=singular first person ergative, INC=incompletive, PART=particle, PREP=preposition, PRON=pronoun, NEG=rejection, 37","acronyms":[[61,65],[76,80],[94,98],[108,111],[7,10],[43,46]],"long-forms":[[66,74],[81,92],[99,106],[112,120],[47,59],[11,41]]},{"text":"University of Massachusetts-Boston  Abstract  Word sense disambiguation (WSD) is one of  the main challenges in Computational ","acronyms":[[73,76]],"long-forms":[[46,71]]},{"text":"as ht, ct = LSTM(xt, ht?1, ct?1).  Residual Networks (ResNet) are among the pioneering works (Szegedy et al, 2015; Srivastava et","acronyms":[[54,60],[12,16]],"long-forms":[[35,52]]},{"text":"grammatically correct (readability). We engaged the services of Amazon Mechanical Turks (AMT) to judges the produced sentences based on a discrete","acronyms":[[89,92]],"long-forms":[[64,87]]},{"text":"defined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model.","acronyms":[[98,102]],"long-forms":[[73,96]]},{"text":" Proceedings of the lOth International Conference on  Computational Linguistics (COLING-84). Stanford ","acronyms":[[81,90]],"long-forms":[[54,79]]},{"text":"nications Advancement Foundation, Japanese, in part by the Center for Intelligent Informational Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023.","acronyms":[[164,169],[172,175]],"long-forms":[[121,162]]},{"text":" We trained a dialog independent (DI) class based LM and dialog dependent (DD) grammar based LM. In all LMs n is set to 3.","acronyms":[[75,77],[34,36],[93,95],[104,107]],"long-forms":[[64,73],[14,32]]},{"text":"MOR = morphological features (set) ? LEM = lemma ?","acronyms":[[37,40],[0,3]],"long-forms":[[43,48],[6,19]]},{"text":"?? ( sE) one or more times are regarded to be locative MWEs (LOC). In contrast, bigrams","acronyms":[[63,66],[57,61]],"long-forms":[[48,56]]},{"text":"The mode seems to be a simple pattern  matching tech in a left-to-right fashion  but it helps in case of conjunct verbs (ConjVs). ","acronyms":[[128,134]],"long-forms":[[112,126]]},{"text":"web. We require parallel data to build a statistical machine translations (SMT) system that translates from German into Sim-","acronyms":[[74,77]],"long-forms":[[41,72]]},{"text":"five different linear classifiers to extracting PPI from objectives: L2-SVM, 1-norm soft-margin SVM (L1-SVM), logistic regression (LR) (Fan et al, 2008), averaged perceptron (AP) (Collins,","acronyms":[[123,125],[45,48],[61,67],[88,91],[93,99],[167,169]],"long-forms":[[102,121],[146,165]]},{"text":"It is encourage that the results (after correcting the misaligned identifiers) for the patched system are approaching the Amongst Tagger Agreement (ITA) level advised for OntoNotes sense tags by the task or-","acronyms":[[148,151]],"long-forms":[[124,146]]},{"text":"qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, np), qdet?1.0 DET(the),","acronyms":[[52,54],[8,10],[30,32],[76,79]],"long-forms":[[56,60]]},{"text":"translation from one source language to multiple target languages, inspired by the recently proposed neural machine translation(NMT) framework proposed by Bahdanau et al (2014).","acronyms":[[128,131]],"long-forms":[[101,126]]},{"text":"dleware architecture (Scha?fer, 2006). It beginning with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in","acronyms":[[83,86]],"long-forms":[[54,71]]},{"text":" Association for Computational Linguistics.          ACL Particular Interest Group in Computational Phonology (SIGPHON), Philadelphia,        Morphological and Phonological Learns: Proceedings of the 6th Workshop of the","acronyms":[[108,115]],"long-forms":[[57,106]]},{"text":"of FIS model used to resolve each expression.  4.1 Similarity Features (SIM) Similarity feature represent the lexical overlap","acronyms":[[72,75],[3,6]],"long-forms":[[51,61]]},{"text":"sulting matrices be M1 and M2, respectively. In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by","acronyms":[[58,65]],"long-forms":[[67,96]]},{"text":" 1 Introduction Machine translation (MT) systems have different strengths and weaknesses which can be exploited","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"In this paper, we describe an initial  implementation of a general spoken language interface, the  Carnegie Mellon Spoken Language Shell (CM-SLS) which  provides voice interface services to a variable number of applica- ","acronyms":[[138,144]],"long-forms":[[99,136]]},{"text":" These classifiers are based on a discriminative model: Succour Vector Machine (SVM)6 (Vapnik, 1995).","acronyms":[[80,83]],"long-forms":[[56,78]]},{"text":"tical problems in class. It might also be useful in massive open online courses (MOOCs). In this for-","acronyms":[[81,86]],"long-forms":[[52,79]]},{"text":"This collaborate study four well-known specifications created by four different organizations: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (Beijing)","acronyms":[[112,114],[147,152]],"long-forms":[[95,110],[117,132]]},{"text":"Abstract  This paper describes a heuristic algorithm capable of automatically assigning a label to  each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a com-  putational-semantic lexicon for treatment of lexical ambiguity.","acronyms":[[153,156]],"long-forms":[[124,151]]},{"text":" 1 Intro Medical relation (MR) classification, an information extraction task in the clinical realms that was recently defined in the 2010 i2b2\/VA Challenge (Uzuner et al.,","acronyms":[[34,36]],"long-forms":[[16,32]]},{"text":" 1 Introduction Question answering (QA) has emerged as a practical research problem for pushing the borders","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":"2.85 0.001 8 29. ( VP (VBP ? REQUIRE?) (","acronyms":[[19,21]],"long-forms":[[23,26]]},{"text":"In  Proc. of Artful Tutoring Systems (ITS). ","acronyms":[[43,46]],"long-forms":[[13,41]]},{"text":"In Proc. of Seventh Text recovery Conference (TREC-7). ","acronyms":[[47,53]],"long-forms":[[12,45]]},{"text":"tional words and notional words. Throughout  the field  of Natural Language Processing(NLP), multiple  studies on text computing or word meaning ","acronyms":[[79,82]],"long-forms":[[51,77]]},{"text":"We showing how these strategies are captured in a grammar developed in the Grammatical Frames (GF).1 We evaluated our method by experimenting","acronyms":[[95,97]],"long-forms":[[72,93]]},{"text":" 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a frame for probabilis-","acronyms":[[49,52]],"long-forms":[[26,47]]},{"text":"137 pare our system with a non-sequential classifier,  a support vector machine (SVM), with the same  settings as those described above.","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"cation and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks.","acronyms":[[116,119]],"long-forms":[[87,114]]},{"text":"slightly higher F-measures than the UMass dictionary.  The error rates (MISDIRECT) for all three dictionaries were  identical.","acronyms":[[72,75]],"long-forms":[[59,70]]},{"text":"tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17], and succour avert local extrema by introducing opposite temperature ?.","acronyms":[[148,152],[13,16],[80,83]],"long-forms":[[120,146],[0,11]]},{"text":"There were four data sources used in the  training set: the Wall Street Journal, Associated  Press, Federal Register (FR), and Department of  215 ","acronyms":[[118,120]],"long-forms":[[100,116]]},{"text":"= Majority Class, Acc. = Accuracy, SE = Standard Error) ing on 5 \u0000 with 20-fold cross-validation achieves an","acronyms":[[35,37],[18,21]],"long-forms":[[40,54],[25,33]]},{"text":"sampling the outputs at random locations.  INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction","acronyms":[[54,57]],"long-forms":[[59,81]]},{"text":"Berwick and Weinberg (1982). Gazdar observed that if  transformational grammars (TG's) were stripped of  all their conversion, they became CFL- ","acronyms":[[78,82],[141,144]],"long-forms":[[51,76]]},{"text":"have a tea and read a good criminal book) and we cannot forget that entertainment is also one of this Embodied Conversational Agent (ECA)?s goal. ","acronyms":[[133,136]],"long-forms":[[102,131]]},{"text":"There have been p,'evious VCl'sions of I,'1\" (l,cpage 1986)  The I,T llSed ill our did!'k has been conducted on  MacilLtosh with CLOS (Common Lisp Aaaah.jeer System)  (l,afeurcade 1993) The realizalion is lmscd primarily on ","acronyms":[[130,134],[26,29]],"long-forms":[[136,162]]},{"text":"obtain a bilingual database. The databases is  called the ATR Dialogue Database(ADD). ","acronyms":[[79,82]],"long-forms":[[57,77]]},{"text":"Table 2: The evaluation for each combination of agents. LB = ListenerBot; DB = DialogBot. ","acronyms":[[56,58],[74,76]],"long-forms":[[61,72],[79,88]]},{"text":"semantically interpreted.  We apply conditional random fields (CRFs) to the task of SRL recommendations by the CoNLL shared","acronyms":[[63,67]],"long-forms":[[36,61]]},{"text":" The graph in figure 2 shows that as the number of relevant documents increases, average precision (AveP) after feedback raise considerably for each additional relevant","acronyms":[[100,104]],"long-forms":[[81,98]]},{"text":"that to go from the head of the chunk to the target in the dependency graph (Figure 3), you traverse a SUB (subject) link upwards. ","acronyms":[[103,106]],"long-forms":[[108,115]]},{"text":"Table 1 shows the feature template sets.  For training, we used soft confidence weighted (SCW) (Wang et al., 2012).","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":" 3 Bridgeman Art Library Bridgeman Art Library (BAL)2 is one of the world?s top image libraries for art, culture and history.","acronyms":[[48,51]],"long-forms":[[25,46]]},{"text":"is re-written as NN, and NNPS as NNP.  Parent (PP) The category of the parents node of the NP. ","acronyms":[[47,49],[17,19],[25,29],[33,36],[90,92]],"long-forms":[[39,45]]},{"text":"2013 temporal summarization. In Proceedings of the 22nd Text Salvaging Conference (TREC), November.","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":"Adobe website:2.03 Adobe Systems:1.82 Data mining (DM), also known as Knowledge-Discovery in Database (KDD) or Knowledge-Discovery and Data Mining (KDD), is the process of automatically browsing gargantuan volumes of data for patterns.","acronyms":[[51,53],[104,107],[149,152]],"long-forms":[[38,49],[70,101],[112,140]]},{"text":"819 location (LO) of the incident (e.g. airfield name), and the country (CO) where the incident occurred. ","acronyms":[[72,74],[14,16]],"long-forms":[[63,70],[4,12]]},{"text":"Where is that??).  Finally, the task features (TASK) reflect conflicting instructions in the domain.","acronyms":[[47,51]],"long-forms":[[32,36]]},{"text":"185  Table h Size of the corpora of HTML pages (in Mb) collects on the four patterns (1.a-d)  through AltaVista (AV) and Northerners Light (NL). ","acronyms":[[114,116],[138,140],[36,40],[51,53]],"long-forms":[[103,112],[122,136]]},{"text":"tiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Rhee and Eskenazi, 2013) and deep neural grids (DNN) (Henderson et al.,","acronyms":[[62,68]],"long-forms":[[49,60]]},{"text":" 3.2 Duluth Systems Assessed The FScore (F-10), F-Measure (F-SC), and Jaccard Coefficient result in a comparable and consistent","acronyms":[[61,65]],"long-forms":[[35,41]]},{"text":"automatic speech recognition (ASR), machine translation (MT), text-to-speech synthesis (TTS), as well as technological for language resource and fundamental instruments de-","acronyms":[[88,91],[30,33],[57,59]],"long-forms":[[62,86],[0,28],[36,55]]},{"text":"to find words with the same meanings. We use a simple approach termed the Direct Inversion (DR) approach in (Lam and Kalita, 2013) to create","acronyms":[[91,93]],"long-forms":[[74,89]]},{"text":"Recently, a larger set of word relatedness judgments was obtained by (Finkelstein et al, 2002) in the WordSimilarity-353 (WS-353) collection. Despite the","acronyms":[[122,128]],"long-forms":[[102,120]]},{"text":"tors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to ex-","acronyms":[[88,91],[35,38]],"long-forms":[[58,86]]},{"text":"The grands categories of components included in these skills include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc.","acronyms":[[190,193],[137,140]],"long-forms":[[166,188],[121,135]]},{"text":"and Darooneh, 2011).  Recurrent neuron networks(RNNs) (Elman, 1990) has been applied to many sequential prediction","acronyms":[[48,52]],"long-forms":[[22,46]]},{"text":"argument structure agreement (Das, 2009), the  analysis of Non-MonoClausal Verb (NMCV) or  Serial Verb, Control Construction (CC),  Modal Control Construction (MCC), Passives ","acronyms":[[126,128],[81,85],[160,163]],"long-forms":[[104,124],[59,79],[132,158]]},{"text":"In Tabled 1, we show the precision, recall, and F1 measures of our domain-aware method (DOM) and the baseline method (BL) in all three sets of experimenting.","acronyms":[[117,119],[87,90]],"long-forms":[[100,108],[66,85]]},{"text":"Non-MonoClausal Verbs (NMCV), Passives  (Pass) and Auxiliary Construction (AC) that  are identified as compound verbs (CompVs). ","acronyms":[[119,125],[23,27],[41,45],[75,77]],"long-forms":[[103,117],[0,21],[30,38],[51,73]]},{"text":"given the precorrected sentence. Each Noun Phrase (NP) in  the test sentence will be pre-corrected as correc-","acronyms":[[51,53]],"long-forms":[[38,49]]},{"text":"on the base. ( Code-a-phone, 1989)  (2d) In the STBY (standby) position, the telefono  will ring whether the handset .is on the foundations or ","acronyms":[[48,52]],"long-forms":[[54,61]]},{"text":"When ready and mature,  technology and language processing techniques will be  incorporated into Foreign Broadcast Informations Service (FBIS)  processing.","acronyms":[[136,140]],"long-forms":[[97,134]]},{"text":"pattern, up to mi example questions.  Issue Pattern (QPi):  When do Q_PRN Q_MVerb Q_BNP?","acronyms":[[56,59],[71,76],[77,84],[85,90]],"long-forms":[[38,54]]},{"text":"and tests 2 and 3 are open testing realized on different test data. DM (i.e., Default Model) assigns all  incoming cases with the most likely schoolroom and it is ","acronyms":[[67,69]],"long-forms":[[77,90]]},{"text":"a t tachment  (PP): the attachment ofa PP  in the sequence VP hip PP (VP = sneezing  phrase, 51P = noun expressions, PP = prepo-  sitional phrase).","acronyms":[[108,110],[15,17],[39,41],[59,61],[66,68],[70,72]],"long-forms":[[113,119],[75,87]]},{"text":"interested in formalisms which are being  used or have applications in the domain  of machine translation (MT). These can ","acronyms":[[107,109]],"long-forms":[[86,105]]},{"text":"2011.  Icelandic Parsed Historical Corpus (IcePaHC). ","acronyms":[[43,50]],"long-forms":[[7,41]]},{"text":"3.1 Vector SpaceModei for Text Catego-  r izat ion  The bulk of the VSM for Information Salvaging (ROE) is  representing naturallanguage xpressions as term ","acronyms":[[99,101],[68,71]],"long-forms":[[76,97]]},{"text":"where cLen is the length of a pattern; cMatch is  the number of matched tags; cPtn is the number of  protein name tag (PTN) skipped by the alignment  in the sentence;  cVb is the number of skipped ","acronyms":[[119,122],[168,171],[78,82],[6,10],[39,45]],"long-forms":[[101,113],[18,24],[64,71]]},{"text":"PropBank defines core roles ARG0 through ARG5, which perceive different interpretations for several predicates. Extras modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a).","acronyms":[[154,162],[28,32],[41,45],[139,145],[178,186]],"long-forms":[[164,172],[188,199]]},{"text":"6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs.","acronyms":[[53,56],[92,94],[2,4],[32,34],[107,109],[135,137]],"long-forms":[[59,90],[97,105],[7,30],[37,51],[112,129],[140,147]]},{"text":"semble learning. Ren et al (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised senti-","acronyms":[[73,75]],"long-forms":[[54,71]]},{"text":"email: adam@itri.bton.ac.uk  People have been writing programs for auto-  matic Word Sense Disambiguation (WSD) for  forty years now, yet the validity of the task has ","acronyms":[[107,110]],"long-forms":[[80,105]]},{"text":"translations: ovc~iflow in the Data Processing  category (DPR,) and out-o\\[-.\/lushnc.ss in the Air-  m'M't Structure category (STR). As shown in the ","acronyms":[[127,130],[58,61]],"long-forms":[[107,125],[31,56]]},{"text":"(Joachims, 1999) software). In it, we implemented: the String Kernel (SK), the Syntactic Tree Kernel (STK), the Shallow Semantic Tree Kernel","acronyms":[[70,72]],"long-forms":[[55,68]]},{"text":" During the medical domain, Castan?o et al (2002) used UMLS (Unified Medical Language System)7 as their knowledge backgrounds.","acronyms":[[51,55]],"long-forms":[[57,90]]},{"text":"EUROTYP + + + + + + Leipzig Glossing Rules + + + + + + + + Penn Treebank (POS) + + + + + + + + STTS + + + + + + +","acronyms":[[74,77],[0,7],[95,99]],"long-forms":[[59,72]]},{"text":"(UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance). ","acronyms":[[90,92],[1,3],[46,48]],"long-forms":[[95,111],[6,15],[51,60]]},{"text":"Japanese  (Jap). Germanic (Get), and Southern Romance (SRom). Len the ","acronyms":[[55,59],[11,14],[27,30]],"long-forms":[[37,53],[0,8],[17,25]]},{"text":"scoring n-grams in the sentence.  Mean logprob (ML) This score is the logprob of the entire sentence divided by the length of the","acronyms":[[48,50]],"long-forms":[[34,46]]},{"text":" 66 adopt a minimum Bayes risk (MBR) approach, with the goal of finding the graph with the lowest","acronyms":[[32,35]],"long-forms":[[12,30]]},{"text":"to systems that rely on brittle features is that many texts are not well-formed. One  such class of texts are those that are the output of optical character recognition (OCR);  typically these texts contain many extraneous or incorrect characters.","acronyms":[[170,173]],"long-forms":[[139,168]]},{"text":"3 Experiments We executed closes track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong","acronyms":[[96,98],[132,135]],"long-forms":[[79,94]]},{"text":"\/fi:\/. 2.1 Exterior Words From an information recovering (IR) perspective, foreign words in Arabic can be classified into two gen-","acronyms":[[56,58]],"long-forms":[[33,54]]},{"text":"Table 4. WSD precision recall and F-measure for  the algorithm based on aligned wordnets (AWN),  for AWN with clustering (AWN+C) and for ","acronyms":[[90,93],[9,12],[122,127],[101,104]],"long-forms":[[72,88]]},{"text":"1 Introduction In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task.","acronyms":[[77,79],[119,125]],"long-forms":[[64,75]]},{"text":"characters ? are often referred to by pronoun or definite noun phrases (NPs) instead of explicit repetition. ","acronyms":[[73,76]],"long-forms":[[59,71]]},{"text":"For each IDR triple  all the object grammar triples are generated whose CF-PS rule  conform with the linear precedence(LP) regulation, the fourth rule set  of the metagrammar.","acronyms":[[120,122],[9,12],[72,77]],"long-forms":[[102,118]]},{"text":" In this work, we explore n-gram models over Minuscule Translation Units (MTUs) to explicitly capture contextual dependencies across","acronyms":[[72,76]],"long-forms":[[45,70]]},{"text":"The focus of the robust CSR techniques  on SLS applications is being facilitated by development and implementation of a well-structured  interface between a CSR and a natural language processor (NLP), allowing collaboration with other  groups developing NLPs for SLS applications.","acronyms":[[195,198],[24,27],[43,46],[157,160],[254,258],[263,266]],"long-forms":[[167,193]]},{"text":"recommends by (Jia and Zhao, 2013). We will mainly consider MIU accuracy (MIU-Acc) which is the ratio of the number of abundantly corrected gen-","acronyms":[[72,79]],"long-forms":[[58,70]]},{"text":"The score of an recap based on extracted PICO element, SPICO, is broken into individual ingredient according to the following formula: SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4) The first components in the equation, Sproblem, reflects a match between the primary","acronyms":[[139,144],[44,48],[59,64]],"long-forms":[[147,196]]},{"text":"clude substitution, splitting and merging statistics. Given  an input (ASCII) word, and the above statistics, candidate  (corrupted) mots are created based on simulating and pro- ","acronyms":[[71,76]],"long-forms":[[61,69]]},{"text":"of Reuters newswire articles annotated with four entity types: person (PER), placements (LOC), organisations (ORG), and miscellaneous (MISC). The data","acronyms":[[132,136],[71,74],[87,90],[107,110]],"long-forms":[[117,130],[63,69],[77,85],[93,105]]},{"text":"Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN):","acronyms":[[102,105],[135,138]],"long-forms":[[75,100],[115,133]]},{"text":"the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. ","acronyms":[[145,149]],"long-forms":[[106,143]]},{"text":"To our knowledge there exist two off the shelf English Arabic Machine Translation (MT) systems: Tarjim and Almisbar.3 We use both MT systems to translate","acronyms":[[83,85],[130,132]],"long-forms":[[62,81]]},{"text":"  Abstract  Named entity acknowledgement (NER) is nowadays an important task, which is responsi-","acronyms":[[38,41]],"long-forms":[[12,36]]},{"text":" 1 Introduction Tree substitutes grammar (TSG) is a promising formalism for modeling language data.","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":" 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly","acronyms":[[47,49]],"long-forms":[[27,45]]},{"text":"parts of speech, and for different confidence levels. We comparison our method to the Semantic Guidelines from PMI (SO-PMI) methods described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method","acronyms":[[114,120]],"long-forms":[[83,112]]},{"text":" Table 5: EPPS task: translation quality and time for differing entry conditions (CN=confusion network, time in seconds per sentence).","acronyms":[[82,84],[10,14]],"long-forms":[[85,102]]},{"text":"not glaring from the sentence.  The dynamic nature of named entities (NEs) makes it tough to enumerate all of their evolv-","acronyms":[[73,76]],"long-forms":[[57,71]]},{"text":"case. In this paper, we investigation using Amazon?s Mechanical Turk (MTurk) to make MT test sets cheaply.","acronyms":[[68,73]],"long-forms":[[51,66]]},{"text":"Certain CT+ (factual) CT? ( counterfactual) cat (certain but unknown output) Probable PR+ (probable) PR? ( not probable) [NA]","acronyms":[[86,89],[8,11],[22,24],[44,47],[101,104],[122,125]],"long-forms":[[91,99],[111,119],[49,75],[28,42]]},{"text":"PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labeled precision (LP), recall (LR), F 1","acronyms":[[98,100],[0,5],[111,113]],"long-forms":[[79,96]]},{"text":"2.1 Purver, Ginzburg and Healey (PGH) Purver, Ginzburg and Healey (2003) scrutinized CRs in the Uk National Corpus (BNC) (Burnard, 2000).","acronyms":[[122,125],[33,36],[86,89]],"long-forms":[[97,120],[4,31]]},{"text":"3.3 Participant and Tasks Eighty participants were recruited from Amazon?s Mechanical Turk2 (MTurk) for this between2http:\/\/www.mturk.com","acronyms":[[93,98]],"long-forms":[[75,91]]},{"text":" On the other hand, the final, \"concrete\", level  of semantic representation (SemRep) is more  like a fully-fledged logical form and it is no ","acronyms":[[78,84]],"long-forms":[[53,76]]},{"text":"of the log-likelihood.     We used the tempered EM (TEM) as described  by Hofmann (1999).","acronyms":[[52,55]],"long-forms":[[39,50]]},{"text":"tion (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for seman-","acronyms":[[82,84],[6,8]],"long-forms":[[66,80]]},{"text":"C H A N grammes E .  H A V E  CSEXCH FOB BEPGFF IN 1 4   ANTEST CALLEC FOR 23\"NASREL: \" (AACC) S D =  24, DEF= 3. SUPERIOR= 1:s ","acronyms":[[65,68],[83,87],[89,92],[100,103],[108,111],[24,30],[31,34],[35,41]],"long-forms":[[51,64]]},{"text":"overall 412 5298 1519 750 Table 1: Corpus statistics: number of sentences (S), words (W), frame elements (FE) and alignments.","acronyms":[[106,108]],"long-forms":[[90,104],[64,73],[79,84]]},{"text":"Several different learning algorithms have been explored for texts rankings (Dumais et al 1998) and support vector machines (SVMs) (Vapnik, 1995) were found to be the most computationally ef-","acronyms":[[130,134]],"long-forms":[[105,128]]},{"text":"  Procedural of the EMNLP 2014 Atelier on Arabic Natural Langauge Processing (ANLP), pages 143?147, October 25, 2014, Doha, Qatar.","acronyms":[[80,84],[21,26]],"long-forms":[[44,78]]},{"text":"The traits av is derived from unsupervised segmentation as in (Cho and Kit, 2008a), and the accessor several (AV) (Feng et al, 2004) is adopted as the unsupervised segmentation crite-","acronyms":[[112,114]],"long-forms":[[94,110]]},{"text":"Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29 Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03 Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64 Unigram Frequencies (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72","acronyms":[[176,180],[17,21],[99,103],[257,261]],"long-forms":[[157,174],[0,15],[78,97],[246,255]]},{"text":"f ~ r l rs ~ f  TRANSFORIATICNS *llrllrt*  WIPING C A L L E D  AT 1 I  ANTEST CALLED FOR 1 '*REDVOW \" (AACC) ,SD= 2. RES= 6.","acronyms":[[82,85],[100,104],[107,109],[114,117]],"long-forms":[[68,81]]},{"text":"Throughout ? Proceedings of  the Eighth Text REtrieval Conference (TREC-9)?, ","acronyms":[[59,65]],"long-forms":[[25,57]]},{"text":"semble learning. Ren et al (2011) examined the utilized of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised senti-","acronyms":[[73,75]],"long-forms":[[54,71]]},{"text":"segmentation While the model structure is reminiscent of a factorial hidden Markov model (HMM), there are important differences that prevent the direct application of","acronyms":[[90,93]],"long-forms":[[69,88]]},{"text":"End the tutoring problem? Cause another round of dialogue\/essay revision ITSpoke (Crafty Tutoring SPOKEn talks system) 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  156\/206","acronyms":[[73,80]],"long-forms":[[82,109]]},{"text":"The modifier*\"its\" of TOK188 was transforming to a modifier of the shape  (POSSBY SCAFFOLD184), which was semantically processes to deliver TOK188 a  LOCPART (LOCationlPAIiT) SFRAME whwe SEMOBJ (SEMantic ODJect) is  SCAFFOLD1 84; idelltif'ication of the location referents of TOK 188 yieldad the two ","acronyms":[[143,150],[22,28],[71,77],[78,89],[133,139],[168,174],[180,186],[188,196],[197,203],[209,218],[269,272]],"long-forms":[[152,166]]},{"text":"knowledge from a corpus\\[2\\]\\[6\\]\\[91.  Machine translation (MT) systems are no  exemptions.","acronyms":[[61,63]],"long-forms":[[40,59]]},{"text":"Ill the last two  experimeuts a memory with the analysis of tile  most frequent word-forms (MFW) in Basque  was used, so that only word-forms not found ","acronyms":[[92,95]],"long-forms":[[66,84]]},{"text":"sHDP 0.162 0.046 0.442 0.102 Table 2: Average topic coherence for various baselines (HDP, Gaussian LDA (G-LDA)) and sHDP. ","acronyms":[[104,109],[116,120],[85,88],[0,4]],"long-forms":[[90,102]]},{"text":"These are selected from the LDC English Gigaword corpus. AFP = Agence France-Presse; AFW = Associated Press Worldstream; NYT = New York Times; XIN = Xinhua News Agency; and CNA = Central News Agency of Taiwan denote the sections of the LDC English Gigaword","acronyms":[[121,124],[143,146],[28,31],[57,60],[85,88],[173,176],[236,239]],"long-forms":[[127,141],[149,160],[63,83],[91,119],[179,198]]},{"text":"Table 1). Firstly, each term candidate is mapped to  a specific canonical representative (CR) by  semantically isomorphic transformations.","acronyms":[[90,92]],"long-forms":[[64,88]]},{"text":"The\/AT table\\]NN is\/BEZ prepped\/J\/.\/.  (PPS = subject pronoun; MD = modal; V'B =  verb (no inflection); UNDER = article; NN = noun;  BEZ ffi present 3rd sg forme of \"to be\"; Jl = ","acronyms":[[102,104],[116,118],[38,41],[61,63],[73,76]],"long-forms":[[107,114],[121,125],[66,71],[80,84],[44,59]]},{"text":"as automatic speech recognition (ASR), natural language understanding (NLU), dialogue management (DM), natural language generation (NLG), and speech synthesis (TTS).","acronyms":[[132,135],[33,36],[71,74],[98,100],[160,163]],"long-forms":[[103,130],[3,31],[39,69],[77,96]]},{"text":"Throughout our development of 60 Japanese predicates (verb and verbal noun) frequently appearing in Kyoto Academia Text Corpus (KTC) (Kurohashi and Nagao, 1997) , 37.6% of the frames included","acronyms":[[122,125]],"long-forms":[[92,120]]},{"text":"ous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine transla-","acronyms":[[83,86],[89,92]],"long-forms":[[54,81]]},{"text":" Classifier models. We uses a first-order linear chain conditional random minefields (CRF) model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a","acronyms":[[82,85],[138,144]],"long-forms":[[55,80],[121,136]]},{"text":"6. Demonstrative pronoun labels are collapsed to DEM PRON (person and number information is easily recovered)","acronyms":[[53,57],[49,52]],"long-forms":[[59,65]]},{"text":"Table 2: Parsing accuracy for 2-planar parser in comparing to MaltParser with (PP) and without (P) pseudo-projective transmutation. LAS = labeled attachment score; UAS = unlabeled attachment score; NPP = precision on non-projective arches; NPR = recall on non-projective arcs.","acronyms":[[167,170],[80,82],[241,244],[201,204],[135,138]],"long-forms":[[173,199],[141,165],[257,271]]},{"text":"s+trsl Alhnd qmrA<STnAEyA <lY Almryx ? India will send a satellite to Mars [in 2013]?. In every tree node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB = verb, PRT = particle, PROP = proper noun, NOM = nominal)and relation (MOD = modifier, SBJ = subject, OBJ = object). The terms under the line are the Buckwalter POS tag, thelemma and the gloss, respectively.","acronyms":[[181,184],[193,196],[209,213],[229,232],[257,260],[273,276],[288,291],[176,179],[147,152],[347,350]],"long-forms":[[187,191],[199,207],[216,222],[235,246],[263,271],[279,286],[294,300]]},{"text":"At example, both the terms chiaroscuro and collage are classified under picture, imaging, icon in WordNet, but in the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective and shadow technical whereas collage is classified under image-making processes and","acronyms":[[147,151]],"long-forms":[[117,145]]},{"text":"Extraction algorithms: ReV = REVERB; Comp = Compressing; Data sets: NS = NewsSpike URLs; All = news 2008-2014.","acronyms":[[83,87],[23,26],[37,41]],"long-forms":[[73,82],[29,35],[44,55]]},{"text":"In particular, we use transitions probability and emission might in Hidden Markov Model (HMM) (Leek, 1997) to capture this dependency.","acronyms":[[94,97]],"long-forms":[[73,92]]},{"text":"The s t ruc ture  11ke a combinat ionic of  a sneeze (V) and  an adverb ia l  par t tc lai  (ADVPART) tn th ts  sequences  wtth or w i thout  a pronoun (PRON) tn between tn  Engltsh t swr t t ten  as fo l lows .","acronyms":[[147,151],[88,95]],"long-forms":[[138,145],[44,48],[61,85]]},{"text":"cies and percentages of which are given in Table 1 (where the letters in example (3)  correspond to the letters in the table). Example (3a) uses a to infinitive form (TNF). ","acronyms":[[167,170]],"long-forms":[[147,165]]},{"text":"constituent border prediction algorithm, the  followiug measures were used:  1) The cost time(CT) of the kernal  duties(CPU: Celeron TM 366, RAM: 64M).","acronyms":[[96,98],[125,128],[138,140],[146,149]],"long-forms":[[86,94]]},{"text":"Estimating Proficiency  Item Response Theory (IRT)  Item Response Theory (IRT) is the basis of modern  language tests such as TOEIC, and enables Com-","acronyms":[[74,77],[46,49],[126,131]],"long-forms":[[52,72],[24,44]]},{"text":"1997). Theory retinement is mainly used (and has its  origin) in Knowledge Based Systems (KBS) (Craw  and Sleeman, 1990).","acronyms":[[90,93]],"long-forms":[[65,88]]},{"text":" ? Shift(SH): Push NEXT onto the stack. ","acronyms":[[9,11],[19,23]],"long-forms":[[3,8]]},{"text":"For this task we train and test three different statistical models: an n-gram language model, a maximum entropy model (MaxEnt) and a (linear) support vector machine (SVM).","acronyms":[[119,125],[166,169],[71,77]],"long-forms":[[96,111],[142,164]]},{"text":"establishment of globally dispersed virtual communities, one of which is the very active and increasing movement of Open Source Software (OSS) development.","acronyms":[[131,134]],"long-forms":[[109,129]]},{"text":"Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee*  *Department of Computer Science and Genius  Pohang University of Science & Technique (POSTECH)  San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea ","acronyms":[[142,149]],"long-forms":[[99,140]]},{"text":"The seed set is compiled from popular mental and emotional state dictionaries, including the Profiles of Humor States (POMS) (McNair et al.,","acronyms":[[117,121]],"long-forms":[[93,115]]},{"text":"constructed using only surface Alterf patterns; for the GLM and text versions, we can use either surface patterns, logical form (LF) patterns, or both. ","acronyms":[[129,131],[56,59]],"long-forms":[[115,127]]},{"text":"As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the standard implementation","acronyms":[[107,110]],"long-forms":[[71,78]]},{"text":"words words  Fig. 1 Structure of Bunruigoihy6 (BGH)  This paper focuses on classifying only nouns in terms of ","acronyms":[[47,50]],"long-forms":[[33,45]]},{"text":" (Choudhury et al, 2007) modeled each standard English word as a hides Markov model (HMM) and calculated the probability of observation the noisy-","acronyms":[[86,89]],"long-forms":[[65,84]]},{"text":"niscent of balanced-tree structures using left and right  rotations. A left rotation changes a (A(BC)) structure to  a ((AB)C) structure, and vice versa for a right rotation.","acronyms":[[96,100],[120,126]],"long-forms":[[69,92]]},{"text":"All experiments carried out in this examining are for the English (EN) - French (FR) language pair. ","acronyms":[[77,79],[63,65]],"long-forms":[[69,75],[54,61]]},{"text":"The application of the programmed is revealed utilise the Aberdeen Report Judgment Scales (ARJS; Sporer, 2004) with a set of 72 untrue and true accounts of a driving examination. Data on divergent genus of inter-coder reliabilities are presents and implications for future researches with computer-assisted qualitative ciphers procedures as well as training of coders are outlined. Credits This researching has been supported by a grant from the German Science Foundation (Deutsche Forschungsgemeinschaft (DFG): Sp262\/3-2) to the present author. The author would like to thank Edda Niederstadt and Nina F. Petermann for the coding of the data, and to Jaume Masip, Valerie Hauch, and Sarah Treiber for comments on an ago version of this manuscript.","acronyms":[[503,506],[90,94]],"long-forms":[[470,501],[57,88]]},{"text":"analysis. To include more of the corpus, parameters are relaxed: the high group (HH) includes anyone whose score is above .5 SD","acronyms":[[81,83],[125,127]],"long-forms":[[69,73]]},{"text":" 1 Introduction Greater Open Information Extraction (Open-IE) systems (Banko et al, 2007) extract textual relational","acronyms":[[50,57]],"long-forms":[[21,48]]},{"text":"5.1 Overall Result Tables 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG","acronyms":[[74,76],[78,82],[101,103],[105,109],[144,146],[148,152]],"long-forms":[[67,72],[88,99],[118,142]]},{"text":" 1 Introduction In this paper, we propose TroFi (Trope Finder), a nearly unsupervised clustering method for sep-","acronyms":[[42,47]],"long-forms":[[49,61]]},{"text":"    The difference between the two modelled was the design of the input layer. The first model (henceforth, the diagnostics model DIAG) took diagnostics as inputs nodes, whereas the secondly model (henceforth, the semantic parameters modelled SEMANP) taken semantic parameter as input nodes, as presented in detail below.    The Diagnostics Modelled (DIAG): Binary ac-ceptability values of the phrases or sentences formed by the syntactic diagnostics constituted the inputs nodes for the network (see above for the SI diagnostics). Each syntactic diagnostic gave a binary value (either 0 or 1) to one of the input nodes.","acronyms":[[340,344],[128,132],[235,241]],"long-forms":[[321,338],[110,120],[209,234]]},{"text":"vocabulary (viz. \/hu:pana-taNa\/ and \/Fi:tiki-RaNa\/), I also searched the Ma?ori Broadcast Corpus (MBC) for phrases ending as if they had gerundial suffixes","acronyms":[[98,101]],"long-forms":[[73,96]]},{"text":"1 Motivation  Question Answering has emerged as a key area in  natural language processing (NLP) to apply question parsing, information extraction, summariza-","acronyms":[[92,95]],"long-forms":[[63,90]]},{"text":"?  Figure 2: Hierarchical Dirichlet Process (HDP) for WSI. ","acronyms":[[45,48],[54,57]],"long-forms":[[13,43]]},{"text":"mvzaanen@uvt.nl Gerhard van Huyssteen Centre for Text Technology (CTexT) North-West University","acronyms":[[66,71]],"long-forms":[[38,64]]},{"text":"tial state for HMM, then experiment with different inference algorithms such as ExpectationMaximization (EM), Variational Bayers (VB) or Gibbs samples (GRAMS).5 Gao and Johnson (2008) compare","acronyms":[[130,132],[15,18],[105,107],[153,155]],"long-forms":[[110,128],[80,103],[137,151]]},{"text":"son mari.  Les confessions (CO) is much most loyal to the content, yet, the translator has significantly departed","acronyms":[[28,30]],"long-forms":[[15,26]]},{"text":"structural and behavioral parts have to be fully specified at this level.  2.3 Eclipse Modeling Framework (EMF) We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-","acronyms":[[107,110],[130,133]],"long-forms":[[79,105]]},{"text":"pears or no other symptomatic is included.  Surfacing Text (ST): To measure the effectiveness of the semantic analysis (attribute labels and ","acronyms":[[56,58]],"long-forms":[[42,54]]},{"text":" ? Minimum Bayes Jeopardy (MBR). We rescore the","acronyms":[[23,26]],"long-forms":[[3,21]]},{"text":"sentences (Dispatch.), the number of tokens (Tokens) and the unlabeled attachment score (UAS) of MST. ","acronyms":[[85,88],[11,15],[41,47],[93,96]],"long-forms":[[57,83],[0,9],[33,39]]},{"text":"compared is a familiar difficulty from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural vocabulary processing (NLP) (Lebart and Rajman, 2000).","acronyms":[[119,122],[73,75],[91,93],[157,160]],"long-forms":[[96,117],[50,71],[78,89],[128,155]]},{"text":"74.80 ?  Table 2: Parsing accuracy; AS = attachment score; ER = error lower w.r.t. projective baseline (%)","acronyms":[[36,38],[59,61]],"long-forms":[[41,57],[64,79]]},{"text":"the problem as a multi-label classification task,  we trained a binary classification model for each  code using support vector machine (SVM) with  ten-fold cross-validation.","acronyms":[[137,140]],"long-forms":[[113,135]]},{"text":"{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com Abstract In natural language question answering (QA) systems, questions often contain terms and","acronyms":[[101,103]],"long-forms":[[81,99]]},{"text":"Constant 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; OR = Odds ratio or Exp(?); CI = confidence Interval. ","acronyms":[[116,118],[89,91],[53,56]],"long-forms":[[121,140],[94,104]]},{"text":" Other valuable classes are, for example,  pronominal adverbs (PAV) and infinitives of auxil-  iary verbs (VAINF), where the differences between ","acronyms":[[66,69],[110,115]],"long-forms":[[46,64]]},{"text":"topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we use a Latent Dirichlet Allocation (LDA) to estimate their topics. Figure 1 illustrates an example, where","acronyms":[[136,139]],"long-forms":[[107,134]]},{"text":"labeled as negative; otherwise, the review is labeled as positive.  3.2 Lexicon-Based Method in Chinese Language: LEX(CN) This method first translates English sentiment lexica into Chinese lexica, and then","acronyms":[[118,120],[114,117]],"long-forms":[[96,112]]},{"text":"ing decision can be made directly based on attention weights from the two query components or further rescored by the maximum spanning tree (MST) search algorithm.","acronyms":[[141,144]],"long-forms":[[118,139]]},{"text":" As far as discriminative models are concerned,  the Maximum Entropy (MaxEnt) model has been  implemented (Bohus and Rudnicky, 2006).","acronyms":[[70,76]],"long-forms":[[53,68]]},{"text":"2010). Domain event extraction has been advanced in particular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have","acronyms":[[90,92],[70,76]],"long-forms":[[77,88]]},{"text":"Then, a supervised machine learning algorithm (e.g., Support Vector Machines  (SVM), na?ve Bayesian classifier (NB)) is applied  to the training examples to build a classifier that is ","acronyms":[[79,82],[112,114]],"long-forms":[[85,99],[53,76]]},{"text":"In Proc. of the Eighth  Text Retrieval Conferences (TREC-8), pages 151162.","acronyms":[[51,57]],"long-forms":[[16,49]]},{"text":"and intangible factors known to the engineer but anonymous to the computer. For instance, the engineer  may need to postpone the placement of appliance in a certain CSA (Carrier Serving Sphere) due to a fixed  cap on near-term expenditures, or she may decide to activate DLC (Digital Loop Transporters) equipment in ","acronyms":[[162,165]],"long-forms":[[167,187]]},{"text":"Abstract This paper introduces a tensor-based approach to semantic role labeling (SRL). The motiva-","acronyms":[[82,85]],"long-forms":[[58,80]]},{"text":"cannot co-exist on nouns. Next comes the sorts of particle proclitics (PART+): +  l+ ?","acronyms":[[71,76]],"long-forms":[[50,69]]},{"text":"relates only loosely to the semantics of natural language. The work we present in  this paper differs from all previous work in natural anguage processing (NLP) in at  least two respects.","acronyms":[[156,159]],"long-forms":[[128,154]]},{"text":"evidences, making the coupled approach efficiency enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the frst time.","acronyms":[[122,124],[130,133]],"long-forms":[[103,120]]},{"text":"uses the mapping of concept dog to the class of alternative  expressions for named individual (such as using the name,  2 VSFL (\"Very Simple Frame Language\") and SCORE CSproket  Core\") were developed at BBN Systems and Technologies by ","acronyms":[[122,126],[162,167],[168,176],[203,206]],"long-forms":[[129,155]]},{"text":"I first  contour the language used to characterize the semantics of  lexical items, SEL (for Mere Episodic Logic), then the  syntax and interpretation f logical forms.","acronyms":[[85,88]],"long-forms":[[94,115]]},{"text":" We  also  produced  an  upper  bound  using  Naive  Bayes  multinomial  (NBm)  and Support Vector Machine (SVM)6 classifiers  with the NTU Sentiment  Dictionary (Ku  et al, ","acronyms":[[108,111],[74,77],[136,139]],"long-forms":[[84,106],[46,71]]},{"text":"Holes, 2004). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.","acronyms":[[82,85],[114,117]],"long-forms":[[53,80]]},{"text":"2 Complexity of GPSG Components  A generalized phrase structure grammar contains five language-  particular components - -  immediate dominance (ID) rules, meta-  rules, linear priorities (LP) statements, characteristics co-occurrence ","acronyms":[[145,147],[16,20],[189,191]],"long-forms":[[124,143],[170,187]]},{"text":"The subword-based tagging was implemented employs the maximum entropy (MaxEnt) and the conditional random fields (CRF)","acronyms":[[69,75],[112,115]],"long-forms":[[52,67],[85,110]]},{"text":"have a tea and read a good criminal cookbook) and we cannot forget that entertainment is also one of this Incarnated Conversational Agent (ECA)?s goal. ","acronyms":[[133,136]],"long-forms":[[102,131]]},{"text":"work for sentence level trait extraction. During the Window Processing component, each token is further represented as Word Features (WF) and Position Attribute (PF) (see section 3.4.1 and 3.4.2). Then, the","acronyms":[[132,134],[159,161]],"long-forms":[[117,130],[140,157]]},{"text":"other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other","acronyms":[[115,119],[95,98],[133,136]],"long-forms":[[121,127],[100,112],[150,158]]},{"text":"5 Conclusion In this pilot experiment, we explore the prospective of used Amazon Mechanical Turk (MTurk) to collect bilingual word alignment data to assist automatic word align-","acronyms":[[99,104]],"long-forms":[[82,97]]},{"text":"(Suchanek et al 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&A), etc. ","acronyms":[[132,135],[108,110],[63,65]],"long-forms":[[113,130],[88,107]]},{"text":"the birds comes.  Table 2: Stories generated by a system that using plots and genetic browse (PlotGA), a system that uses only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly","acronyms":[[92,98]],"long-forms":[[66,90]]},{"text":"2004. A maxi-mum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu.","acronyms":[[143,148]],"long-forms":[[88,141]]},{"text":" 5 for SK with the employs of POS information (SK-POS). ","acronyms":[[43,49]],"long-forms":[[7,29]]},{"text":"Table 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.  Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragment than other methods.","acronyms":[[129,133]],"long-forms":[[112,127]]},{"text":"ofwi. Mere like the statistic pproaches in many automatic POS tagging programs, our labour is to select a  constituent boundary sequence B'with the highest score, P(BIS), from all possible sequences. ","acronyms":[[164,167],[60,63]],"long-forms":[[136,160]]},{"text":"2 Methods In this IRB-approved study, we obtained the Shared Annotated Resource (ShARe) corpus originally generated from the Beth Israel Dea-","acronyms":[[81,86]],"long-forms":[[54,79]]},{"text":"of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;","acronyms":[[217,220]],"long-forms":[[188,215]]},{"text":"pendency and constituency tracks are shown in tables 1. The label attachment score (LAS) was used by the organizer for estimate the dependency versions,","acronyms":[[83,86]],"long-forms":[[59,81]]},{"text":"categories, as showed in Figure 1. Messages may  involve a request (REQ), provide information  (INF), or declined into the category of interpersonal ","acronyms":[[67,70],[95,98]],"long-forms":[[58,65],[81,92]]},{"text":"#and (a probabilistic AND) is used. Otherwise, the  probabilistic passage operator #UWn (unordered window)  is usage.","acronyms":[[84,87],[22,25]],"long-forms":[[89,105]]},{"text":"EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. ","acronyms":[[88,90],[0,3],[34,39]],"long-forms":[[69,86]]},{"text":"amples in 3.2).  In section 4, we portray the  specification of Korean TimeML (KTimeML). ","acronyms":[[80,87]],"long-forms":[[65,78]]},{"text":"Prevalence and count of conditions by temporal  category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Contingency Department, ","acronyms":[[75,77]],"long-forms":[[80,97]]},{"text":"word and sentence level QE. In this cooperation we describe the Fondazione Bruno Jerome (FBK), Universitat Polit`ecnica de Val`encia (UPV) and Uni-","acronyms":[[83,86],[24,26],[128,131]],"long-forms":[[57,81],[89,126]]},{"text":"For exam-  ple, an analysis of the texts using Mann and Thomp-  son's (1987) Rhetorical Structure Theory (RST) would  outcomes primarily in the relationship sequence  and jo in t  ","acronyms":[[106,109]],"long-forms":[[77,104]]},{"text":" The joint model is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.","acronyms":[[81,86]],"long-forms":[[58,79]]},{"text":"capture all the types of entities. Typical structures  of Chinese person name (CN), location name (LN)  and organization name (ON) are as follows: ","acronyms":[[79,81],[99,101]],"long-forms":[[58,77],[84,97]]},{"text":"gorithm (PAS-PTK), which is exceedingly more efficient and more accurate than the SSTK and (ii) a new kernel telephoned Part of Speech sequence kernel (POSSK), which prove very accurate to represent shallow syn-","acronyms":[[143,148],[9,16],[77,81]],"long-forms":[[111,141]]},{"text":"Cleveland Family study dceweb1.case.edu\/ serc\/collab\/project_family.shtml), CHS (the Cardiovascular Heart Study www. ","acronyms":[[76,79]],"long-forms":[[85,111]]},{"text":" 1  0  20  40  60  80  100  120  140 information density (IDS)Fisher information (FIR)query-by-committee (SVE)random Sig+Reactions","acronyms":[[58,60],[81,84],[105,108]],"long-forms":[[37,56],[61,67]]},{"text":"using the distributional similarity metric described by Lin (1998). We utilise WordNet (WN) as our sense inventory.","acronyms":[[84,86]],"long-forms":[[75,82]]},{"text":"Spanish data set as Trec4S.  We used a Chinese-English lexicon from the  Linguistic Data Consortium (LDC). We pre- ","acronyms":[[101,104]],"long-forms":[[73,99]]},{"text":"and get close to the top level in several other tracks.  Recently, Maximum Entropy model(ME) and CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai","acronyms":[[89,91],[97,101]],"long-forms":[[67,82]]},{"text":"Suffixes (S): able, est, ful, ic, ing, ive, ness etc.  Word Sentiment Polarity (SP): POS, NEG, NEU Pivoting on the head facet, we gaze forward and","acronyms":[[80,82]],"long-forms":[[60,78]]},{"text":"information. In Lawsuits of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 317?325, Columbus, USA.","acronyms":[[100,103],[131,134]],"long-forms":[[57,98]]},{"text":"                  Threshold is a function of Length(LR) and text  size. The basic brainchild is wider amount of length(LR)  or text size matches larger amount of Threshold.","acronyms":[[114,116]],"long-forms":[[90,96]]},{"text":"cabbage 9 chou 1 chou blossom 25 fleur 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bath 178 bain 1 bain butterfly 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond cottage 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 enfant 1 enfant bed 806 lit 2 table beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 ville 1 ville girl 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bleu hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bible 1 Bible foot 23548 pied 8 siffler chair 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the language pair English ? French. The meaning of the columns is as follows: ESW = English source word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. ","acronyms":[[925,927],[1069,1071],[898,901],[971,973],[1025,1027]],"long-forms":[[930,946],[1074,1094],[904,923],[976,996],[1039,1055]]},{"text":"274  Procedural of the 2014 Conference on Empirical Methods in Natural Language Treatments (EMNLP), pages 1192?1202, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"We discuss related working in section 5, and finalize in section 6.  2 Index of Fruitful Syntax (IPSyn) The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-","acronyms":[[96,101]],"long-forms":[[68,94]]},{"text":"of the label candidacy. For the supervised method, we use a support vector regression (SVR) model (Joachims, 2006) over all of the feature.","acronyms":[[88,91]],"long-forms":[[61,86]]},{"text":" 1 Introduction Automatic evaluation of machine translation (MT) quality is indispensable to developing high-quality ma-","acronyms":[[61,63]],"long-forms":[[40,59]]},{"text":" 1 Introduct ion  Some natural anguage processing (NLP) tasks can  be accomplished with only coarse-grained semantic in- ","acronyms":[[51,54]],"long-forms":[[23,49]]},{"text":"Table 1 provides an overview of all entity classes and relations. The workflow consists of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section","acronyms":[[162,165]],"long-forms":[[136,160]]},{"text":"Text5 holiday(0.432) humor(0.23) 0.099 blues(0.15) Table 2: Percent Agreement(PA) to manually extracted index terms","acronyms":[[78,80]],"long-forms":[[60,76]]},{"text":" 1 Introducing Word Sense Disambiguation (WSD) is an important component in many information organization","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"We begin by outlines how for our typical model, the Viterbi EM intent can be formulated as a mixed integer quadratic programming (MIQP) problem with nonlinear constraints (Figure 2).","acronyms":[[135,139]],"long-forms":[[98,133]]},{"text":"and Darooneh, 2011).  Recurrent neural networks(RNNs) (Elman, 1990) has been applied to many sequential prediction","acronyms":[[48,52]],"long-forms":[[22,46]]},{"text":"document-level CLOA tasks, respectively. The evaluations on simplified Chinese (SC) opinion analysis by using small SC training data and large","acronyms":[[80,82],[15,19],[116,118]],"long-forms":[[60,78]]},{"text":"transcripts. The print news consisted of 22 New Yorke Times (NYT) articles from January 1998.","acronyms":[[60,63]],"long-forms":[[44,58]]},{"text":"init ial ly be outlines. I wil l  c laim that  such an initial descr ipt ion (ID) is  cr it ical  to both modelling synthesis and ","acronyms":[[79,81]],"long-forms":[[56,69]]},{"text":"kitchen cabinet and will hardly be able to win the elections]. The parse tree contains phrase labels NP (Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Sentence), and CS (Coordinated Sentence).","acronyms":[[119,121],[146,148]],"long-forms":[[123,143],[150,161]]},{"text":"precision, recall and f-measure. Precision action the number of correct Named Entities(NEs) in the 107","acronyms":[[89,92]],"long-forms":[[74,87]]},{"text":"(1987) presents in his Case grid.  Case Analysis (CA) extracts the acts and Case  constellations around them from the structure ","acronyms":[[50,52]],"long-forms":[[35,48]]},{"text":"PARSEVAL scores for our parse results. We concentrations on the Labeled Precision (LP) and Labeled Recalling (LR) scores only in this paper, as these are","acronyms":[[74,76],[0,8],[98,100]],"long-forms":[[55,72],[82,96]]},{"text":" 4.2 Data  We used the Wall Street Journal (WSJ) of the yr  88-89.","acronyms":[[44,47]],"long-forms":[[23,42]]},{"text":"process (PR)  quantity (QU)  relating (RE)  shape (SH) ","acronyms":[[39,41],[9,11],[24,26],[51,53]],"long-forms":[[29,37],[0,7],[14,22],[44,49]]},{"text":"Three transliteration models have been used that  can generate the Hindi transliteration from an  English named entity (NE). An English NE is ","acronyms":[[120,122],[136,138]],"long-forms":[[106,118]]},{"text":"In the leftmost column SSE-TRA and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS illustrates the number of sessions; the columns PSucc (viewed success), PAtt (perceived attention acknowledgment), PUnd (perceived understanding), PNat (perceived naturalness), and POv (perceived totals performance) give average","acronyms":[[153,158],[180,184],[23,30],[35,42],[79,82],[104,106],[220,224],[252,256]],"long-forms":[[160,177],[186,205],[226,249],[258,279],[291,320],[286,289]]},{"text":" In a second experiment, we used the original TGRD corpus but added the language variety (LV) (i.e., MSA and DA) features.","acronyms":[[90,92],[46,50],[101,104],[109,111]],"long-forms":[[72,88]]},{"text":"non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by permit fragments with partial rule expansions.","acronyms":[[86,89]],"long-forms":[[64,84]]},{"text":"(TEMPO1) and YOUTH-TIME (TEMPO2) where the choice is a secor~d  order Pa~t and for the pair YOUTH-TIME (TEMPO2) and BUILDING-  TIME (TEMPO3) where the choice is a third order F~Jture. As a ","acronyms":[[133,139]],"long-forms":[[127,131]]},{"text":" Since this is a binary ranked task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Launches an","acronyms":[[71,74],[107,110],[140,143]],"long-forms":[[76,98],[112,131],[145,157]]},{"text":"corpus (DUC2002) and the second is automatically extracted from related web news stories (WNS) automatically extracts.","acronyms":[[90,93],[8,15]],"long-forms":[[72,88]]},{"text":"consider in (14) five salient phases of the passage: at the beginning of the process, the parts of river are localized to the exterior EXT(LOC), then to the boundary FRO(LOC), they arrive in","acronyms":[[139,142],[135,138],[166,169],[170,173]],"long-forms":[[109,118]]},{"text":"ing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL). ","acronyms":[[93,96]],"long-forms":[[50,91]]},{"text":"A List of POS-tags ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives),","acronyms":[[71,73],[85,87],[10,13],[19,22],[37,40],[52,54],[104,106],[128,131]],"long-forms":[[75,82],[89,101],[24,34],[42,49],[56,68],[108,125],[133,147]]},{"text":"Secondly, there is the panels of speech particles which are not parties of the core sentence construction,  yet pragmatically cannot stand on their own. These 'sentence external' (SE) elements can be subclassified into two class.","acronyms":[[176,178]],"long-forms":[[156,173]]},{"text":"internal structure. This type of expression is an example  of what will be called a \"comp\\]ex basic expression\" (CBE). ","acronyms":[[113,116]],"long-forms":[[85,110]]},{"text":"low boating directions. In Proceedings of the Association for Computational Linguistics (ACL), 2010.","acronyms":[[94,97]],"long-forms":[[51,92]]},{"text":"construct the desired model in a way that allows efficient inference, even for monumental datasets, using determinantal point processes (DPPs). We startup","acronyms":[[132,136]],"long-forms":[[101,130]]},{"text":"Multi-document person name resolution, Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.","acronyms":[[124,127]],"long-forms":[[81,122]]},{"text":"of the set of terminals symbols) or empty twine. A Phrase Structure Tree (PST) is a shue in which all and only the leaf nodes are labeled with words or","acronyms":[[75,78]],"long-forms":[[52,73]]},{"text":"84  Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 144?151, Ann Arbor, June 2005.","acronyms":[[82,87]],"long-forms":[[41,80]]},{"text":"y) 7. Mutual dependency (MD) checkin P (xy)2P (x?)P (? y)","acronyms":[[25,27]],"long-forms":[[6,23]]},{"text":"On Complexity of Word Order. Traitement Automatique des Langues (TAL), 41(1):273?300. ","acronyms":[[65,68]],"long-forms":[[29,62]]},{"text":"Our initial pilot contains language model (LM), word posterior probability (WPP), confusion network (CN), and word lexicon (WL) features for a total of 11","acronyms":[[106,108],[48,50],[81,84],[129,131]],"long-forms":[[87,104],[32,46],[53,79],[115,127]]},{"text":"All the words were categorized  into three types: Lexicon words (LWs), Factoid  words (FTs), Named Entity (NEs). Accordingly, ","acronyms":[[107,110],[65,68],[87,90]],"long-forms":[[93,105],[50,63],[71,85]]},{"text":" 1 Introduction Predicate argument structure (PAS) analysis is a shallow semantic parsing task that identifies ba-","acronyms":[[46,49]],"long-forms":[[16,44]]},{"text":"below.  4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary Englishman","acronyms":[[38,41]],"long-forms":[[14,36]]},{"text":"Table 7 shows the effects of merging the sub-  ject accessibility bias with both recency biases and  the restricted memory bias (RM). The results in ","acronyms":[[129,131]],"long-forms":[[105,122]]},{"text":" On the other hand, the last, \"concrete\", level  of semantic representation (SemRep) is more  fond a fully-fledged logical form and it is no ","acronyms":[[78,84]],"long-forms":[[53,76]]},{"text":"coloration histograms drifted from images.  In the RGB (Red Green Blue) colour model, each pixel is represented as an integer in range of","acronyms":[[47,50]],"long-forms":[[52,66]]},{"text":"ing laptop devices. Such an application typically uses a speech recognizer (ASR) for transforming the user?s speech input to text and a searching component","acronyms":[[76,79]],"long-forms":[[55,74]]},{"text":"LR = Lagrangian relaxation; DP = exhaustive dynamic programming; ILP = integer linear programming; LP = linear programming (LP does not recover an exact solution).","acronyms":[[65,68],[0,2],[28,30],[99,101],[124,126]],"long-forms":[[71,97],[5,26],[44,63],[104,122]]},{"text":"In International Conference on Autonomous Agents and Multiagent Systems (AAMAS). ","acronyms":[[73,78]],"long-forms":[[31,71]]},{"text":"The general architecture of D2S is represented in Figure 1. It consists of two modules, the Language Gen-  eration Module (LGM), and the Speech Generation Module (SGM). The LGM takes data as input .and ","acronyms":[[163,166],[28,31],[123,126],[173,176]],"long-forms":[[137,161],[92,121]]},{"text":" 1 Introduction Massive Open Online Courses (MOOCs), run by organizations such as Coursera, have been among","acronyms":[[45,50]],"long-forms":[[16,43]]},{"text":" 1 Introduction Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":"Sofa. The viewer makes use the open-source library Java Speech Toolkit (JSTK)5. ","acronyms":[[72,76]],"long-forms":[[51,70]]},{"text":"Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005).","acronyms":[[126,128]],"long-forms":[[102,124]]},{"text":"Into E.M. Voorhees and  D.K. Harman, editors, The 3d Text RE-  trieval Conference (TREC-3). ","acronyms":[[81,87],[3,6],[22,25]],"long-forms":[[48,79]]},{"text":"Here we performed a set of experiments where we investigate the potential of multi-source transfer for NER, in German (DE), Brits (EN), Spanish (ES) and Dutch (NL), using cross-lingual","acronyms":[[131,133],[101,104],[117,119],[145,147],[160,162]],"long-forms":[[122,129],[109,115],[136,143],[153,158]]},{"text":"fidence information about each system?s hypothesis. This feature class includes the confusion network (CN) word confidence, CN slot entropy, and the number of alter-","acronyms":[[103,105],[124,126]],"long-forms":[[84,101]]},{"text":"The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). ","acronyms":[[100,103],[58,61]],"long-forms":[[73,98]]},{"text":"   First Paragraph (FPAR): We examined several  hundred pages, and saw that a human could ","acronyms":[[20,24]],"long-forms":[[3,18]]},{"text":"is re-written as NN, and NNPS as NNP.  Parent (PP) The category of the parent node of the NP. ","acronyms":[[47,49],[17,19],[25,29],[33,36],[90,92]],"long-forms":[[39,45]]},{"text":"On the one hand, we built machine learned classifiers based on Support Vector Machines (SVMs) and Conditional Random Spheres (CRFs).","acronyms":[[89,93],[126,130]],"long-forms":[[64,87],[99,124]]},{"text":"tity in the contrast set until no distractors are left.  Dale & Reiter speaker frequency (DR-sf) uses a different preferred attribute list for each speaker,","acronyms":[[90,95]],"long-forms":[[57,88]]},{"text":"contributes to the irregularity of hangul orthography is the differences in spelling between South Korean (S.K.) and North Korea (N.K.). The","acronyms":[[107,111],[130,134]],"long-forms":[[94,105],[117,128]]},{"text":"1. INTRODUCTION  Disguising Markov Models (HMMs) have been used suc-  cessfully in a wide variety of recognition tasks ranging ","acronyms":[[39,43]],"long-forms":[[17,37]]},{"text":" Proceedings of the lOth International Lectures on  Computational Linguistics (COLING-84). Stanford ","acronyms":[[81,90]],"long-forms":[[54,79]]},{"text":"referred to as Maximum Reciprocal Information Estimation (MMIE) and the second component Maximum Likelihood Estimation (MLE), therefore in this paper we employs a brief scoring for (1) just for conve-","acronyms":[[116,119]],"long-forms":[[89,114]]},{"text":"2 Shared-task evaluation in HLT Over the past twenty years, virtually every field of research in human language technology (HLT) has introduced STECs.","acronyms":[[124,127],[28,31],[144,149]],"long-forms":[[97,122]]},{"text":"NER model was revealed in Table 4. We utilize the Peking University (PKU) named entity corpus to train the models.","acronyms":[[62,65],[0,3]],"long-forms":[[43,60]]},{"text":"Associated Press Worldstream English Service (APW) ? The New York Times Newswire Service (NYT) ? The Xinhua News Entities English Service (XIE) For each source, Gigaword articles are classified into myriad types, consisting newswire advisories, etc.  We restricted our investigations to actual news tales.","acronyms":[[137,140],[46,49],[90,93]],"long-forms":[[101,135],[0,28],[57,71]]},{"text":"Spanish data set as Trec4S.  We use a Chinese-English lexicon from the  Linguistic Data Consortium (LDC). We pre- ","acronyms":[[101,104]],"long-forms":[[73,99]]},{"text":"Cordova Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,  Effective Kernel-based Learning for Shue, to appear in the IEEE Symposium on  Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  ","acronyms":[[210,214],[147,151]],"long-forms":[[166,208]]},{"text":"representation. Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  English ECS (E-ECS) \\[7\\] for English language.","acronyms":[[95,100],[57,60],[148,153]],"long-forms":[[81,93],[135,146]]},{"text":"4.3 Experiment with the QA data In the first set of experiments we focus on the Question Answering (QA) realm (CLEF corpus). ","acronyms":[[101,103],[25,27],[113,117]],"long-forms":[[81,99]]},{"text":" 3.2.2 Optimization We use stochastic gradient descent (SGD) to maximize the simplified objectives.","acronyms":[[56,59]],"long-forms":[[27,54]]},{"text":"0.467 (+126%)?  Entire Document Reciprocal Rank (TDRR) PubMed 0.495 0.137 0.038 0.331","acronyms":[[48,52]],"long-forms":[[16,46]]},{"text":"to such an extent that?)  Multiword interjections (MWI) are a small category with expressions such as mile sabords (?","acronyms":[[51,54]],"long-forms":[[26,49]]},{"text":"VB (Verb) is a major source of confusion in automatic tagging.  It is collapsed with VB (Verb). ","acronyms":[[85,87],[0,2]],"long-forms":[[89,93],[4,8]]},{"text":"Probable PR+ (probable) PR? ( not probable) [NA] Possible PS+ (possible) PS? ( not certain) [NA]","acronyms":[[58,61]],"long-forms":[[63,71]]},{"text":"Abstract Data sparsity is one of the main factors that make word feeling disambiguation (WSD) difficult.","acronyms":[[87,90]],"long-forms":[[60,85]]},{"text":"5.5.3 Corpus Statistics. For training of the question detection module, we using the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (sass);15 for training of the answer detected component, we employs the eight English CALLHOME dia-","acronyms":[[151,154]],"long-forms":[[136,149]]},{"text":"I.e., we consider three increasingly constrained conditions: (1) substitution according only to the form constraints (FORM), (2) substitution according to both form and taboo","acronyms":[[118,122]],"long-forms":[[100,104]]},{"text":"tions\" as per the gold standard.  D = True Negatives (TN) = Pairs that were identified as \"Incorrect Transliterations\" by the par-","acronyms":[[54,56]],"long-forms":[[38,52]]},{"text":"BUS (business) Belgium Workers Federation 9 4440 11.0 NOB (greatness) Albert SECONDLY 6 4179 15.1 COM (comics) Suske and Wiske 3 4000 10.5 MUS (music) Sandra Kim, Urbanus 3 1296 14.6","acronyms":[[90,93],[0,3],[53,56],[131,134]],"long-forms":[[95,101],[5,13],[58,66],[136,141]]},{"text":" Figure 1. Reference answer representation revisions  Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c).","acronyms":[[178,182],[283,287],[303,307]],"long-forms":[[184,197]]},{"text":" 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random Sig+Reply","acronyms":[[58,60],[81,84],[105,108]],"long-forms":[[37,56],[61,67]]},{"text":"P i jPMI P i P j=   Equation 2: Pointwise Mutual Information (PMI)  between two terms i and j. ","acronyms":[[62,65],[5,8]],"long-forms":[[32,60]]},{"text":" 2 Pertaining Work Dialog law (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle","acronyms":[[28,30]],"long-forms":[[16,26]]},{"text":"cognition(COG) sentiment(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT)  relation(REL) social_relation(SREL) ","acronyms":[[96,99],[10,13],[23,27],[41,44],[58,61],[69,73],[81,84],[111,114],[132,136]],"long-forms":[[86,94],[0,9],[15,22],[30,40],[46,57],[64,68],[75,80],[102,110],[116,131]]},{"text":"tion of predicate signs. Ramchand dividing events into a maximum of three hierarchical phrases: an begins phrase (InitP), a process phrase (ProcP), and a result phrase (ResP).","acronyms":[[117,122],[143,148]],"long-forms":[[98,115],[127,141]]},{"text":"attempted to cut down on certain items in the process.  Silhouette 2: Translation examples (SRC = fount, BASE = baseline system, BACKOFF = backoff","acronyms":[[88,91]],"long-forms":[[94,100]]},{"text":"contains the result for Eisner?s algorithm using no transformation (N-Proj), projectivized training data (Proj), and pseudo-projective parsing (P-Proj). The","acronyms":[[144,150],[68,74],[106,110]],"long-forms":[[124,142]]},{"text":"20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M Listing Y 9.0 3.3 0.9 13.3 16030 Table 2: Consequence for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), ton = overall WER (%).","acronyms":[[137,139],[99,104],[227,230]],"long-forms":[[142,153],[182,191],[201,211],[221,226]]},{"text":"Abstract  This article focuses on the development of  Natural Language Processing (NLP) tools for  Computer Assisted Language Learning ","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"j.nerbonne@rug.nl Abstract Pair Hidden Markov Models (PairHMMs) are trained to align the pronunciation tran-","acronyms":[[54,62]],"long-forms":[[27,52]]},{"text":"(adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (figures), CONJ (conjunctions), PRT (particles), PUNC (notation marks) and","acronyms":[[102,105],[118,122],[14,17],[29,33],[46,49],[65,68],[139,142],[156,160]],"long-forms":[[107,115],[124,136],[19,26],[35,43],[51,62],[70,99],[144,153],[162,173]]},{"text":"typically expressed inTopic Statements.  The Subject Field Coder (SFCoder) employs an  establishet~ semantic oding scheme from the machine- ","acronyms":[[66,73]],"long-forms":[[45,64]]},{"text":"appropriate.  Terminology Data banks (TD) are the least ambitious  systems because access frequently is not made during a ","acronyms":[[38,40]],"long-forms":[[14,30]]},{"text":" In addition, the tense,                                                             1 S=Subject; IO=Indirect Object; DO=Direct Object;  V=Verb; ERG=Ergative; DAT=Dative ","acronyms":[[98,100],[118,120],[145,148],[159,162]],"long-forms":[[101,116],[121,134],[89,96],[139,143],[149,157],[163,169]]},{"text":"fer to semantically similar words. We have applied the Markov Cluster algorithm (MCL) (van Dongen, 2000) to group semantically similar terms together.","acronyms":[[81,84]],"long-forms":[[55,79]]},{"text":" Recently, there have been increasing interests for dialogue act (DA) recognition in talked and written conversations, which include meetings,","acronyms":[[66,68]],"long-forms":[[52,64]]},{"text":"1 Introduction Semantic Parsing, the process of converting text into a formal meaning representation (MR), is one of the key challenges in natural language process-","acronyms":[[102,104]],"long-forms":[[78,100]]},{"text":"  Abstract  A Named Entity Recognizer (NER) routinely  has worse performance on machine translated ","acronyms":[[39,42]],"long-forms":[[14,37]]},{"text":"Model (LEM) We propose an headless latent variable model, called the Latent Events Model (LEM), to extract events from tweets.","acronyms":[[93,96],[7,10]],"long-forms":[[73,91]]},{"text":"In order to  deliver the concept of focusing more precise I will used the  notion of Reference Time (RT), adopted from Reichen-  bach 1947 but reinterpreted pragmatically: RT is to be ","acronyms":[[96,98],[167,169]],"long-forms":[[80,94]]},{"text":"even for very high-dimensional data.  4.2 Open Directory Project (ODP) Open Directory Project (ODP)7 is a multilingual","acronyms":[[66,69],[95,98]],"long-forms":[[42,64],[71,93]]},{"text":"3.3 Aspect term extraction Our approach for aspect term extraction is based on Conditional Random Fields (CRF). The choice","acronyms":[[106,109]],"long-forms":[[79,104]]},{"text":"That is, it learns some new knowledge.  Static Interactive Learning (SIL): Whenever the  system encounters a sentence out o f  i t s  processing ","acronyms":[[69,72]],"long-forms":[[40,67]]},{"text":"Association measure Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood ratio (LL) and chi-square (?","acronyms":[[127,130],[155,157]],"long-forms":[[97,125],[133,147]]},{"text":"Table 2: Comparison of adaptation supertagging (AST) and a lesser restrictive setting (Reverse) with Viterbi and oracle F-scores on CCGbank Section 00. The table displaying the labeled F-score (LF), precision (LP) and recall (LR) and the the number of lexical categories per word used (from frst to last parsing attempt).","acronyms":[[186,188],[46,49],[128,135],[202,204],[218,220]],"long-forms":[[168,178],[23,44]]},{"text":"attempts to use synchronous parsing to produce the  tree structure of both source language (SL) and  target language (TL) simultaneously, most SSMT  approaches make use of monolingual parser to ","acronyms":[[118,120],[92,94],[143,147]],"long-forms":[[101,116],[75,90]]},{"text":"the sentence. The segmentation model is a chain LVM (latent variable model) that aims to maximize a linear objective defined by:","acronyms":[[48,51]],"long-forms":[[53,74]]},{"text":"\\[ Extra~:~nouns I IE~rac~'~ n?unsl i Calculating f requ~ vectors (FreqVa) I ICalculating frequency vectors (FreqVe)l  ._1 Calculating similarity I ","acronyms":[[109,115],[67,73]],"long-forms":[[90,107],[50,65]]},{"text":"EUROTYP + + + + + + Leipzig Glossing Ordinances + + + + + + + + Penn Treebank (POS) + + + + + + + + STTS + + + + + + +","acronyms":[[74,77],[0,7],[95,99]],"long-forms":[[59,72]]},{"text":"? ( S (NP (DT the) (NN men)) (VP (VBZ plays) (NP (DT the) (NN piano)))","acronyms":[[34,37],[7,9],[11,13],[20,22],[30,32],[46,48],[50,52],[59,61]],"long-forms":[[38,43]]},{"text":" 3. Transitional Phrases (TRP) We hypothesize that a more cohesive essay, being easier for a","acronyms":[[26,29]],"long-forms":[[4,24]]},{"text":"the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis","acronyms":[[81,84]],"long-forms":[[51,79]]},{"text":"(NL) into formal mean representations (MR), and are implemented for both parsing of textual input and in Spoken Language Understanding (SLU). In data-","acronyms":[[135,138],[1,3],[42,44]],"long-forms":[[104,133],[17,40]]},{"text":"tation from a user?s discourse. That is, it consists of automatic speech recognition (ASR) and language understanding (LU).","acronyms":[[83,86],[116,118]],"long-forms":[[53,81],[92,114]]},{"text":"crafted features, lexicons, and grammars.  Meanwhile, recurrent neural networks (RNNs) what are the major cities in utah ?","acronyms":[[81,85]],"long-forms":[[54,79]]},{"text":"leaves=29). The first branching, which corresponds to no AP (Absolute Position) and no C (Colour), assigns n to as many as 1571 instances","acronyms":[[57,59]],"long-forms":[[61,78],[90,96]]},{"text":"object (J), which is bad for the Jews (the  event is marked by minus), and after that (>) a  Jewish bearer of Sacrosanct Power (J*SP) miracu-  lously (MIR) safeguards the (above) Jewish objects, ","acronyms":[[124,128],[147,150]],"long-forms":[[93,122],[130,145]]},{"text":"534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a","acronyms":[[75,78]],"long-forms":[[49,73]]},{"text":"transmission envelope; the German and Portugese services  are dispatches in the transmission enveloped esigned by the  International Press Telecommunications Council (IPTC). ","acronyms":[[162,166]],"long-forms":[[114,160]]},{"text":"researching has encore'aged the FI{UM1 ) api)rosh. The  SUMMONS (SUMMarizing Online News artMes)  system (McKeown and Radev, 1999) takes tem- ","acronyms":[[52,59]],"long-forms":[[61,84]]},{"text":" In Section 3, we report on two Amazon Mechanical Turk (MTurk) experiments, which demonstrate that crowdsourcing is a feasible way","acronyms":[[56,61]],"long-forms":[[39,54]]},{"text":"5.1 Experimental Settings To evaluate our algorithm?s performance, we designed a Mechanical Turk (MTurk) experiment in which humanity annotators evaluated the quality of the","acronyms":[[98,103]],"long-forms":[[81,96]]},{"text":"Acknowledgment This cooperated is supported by the 6th Framework Research Program of the European Union (UE), LUNA Project, IST contract no 33549,www.ist-luna.eu","acronyms":[[99,101],[104,108],[118,121]],"long-forms":[[83,97]]},{"text":"This gave rise to a relatively new research area within the emerging field of Textto-Text Generation (TTG) called Multiple-Choice Test Item Generation (MCTIG).1","acronyms":[[102,105],[152,157]],"long-forms":[[78,100],[114,150]]},{"text":"Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark.","acronyms":[[95,98],[121,124],[75,78],[138,141],[152,156]],"long-forms":[[101,110],[127,136],[69,73],[81,86],[144,150],[159,170]]},{"text":"ability integral transform to generate empirical cumulative density functions (ECDF): now instead of the probability density function (PDF) space, we are working in the ECDF space where the value of each","acronyms":[[135,138],[79,83],[169,173]],"long-forms":[[105,133],[39,77]]},{"text":"EN 94,725 2.58 Tables 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=Englishman, BG=Bulgarian Tagset The Multext-East corpus is manually an-","acronyms":[[67,69],[79,81],[0,2],[43,45],[55,57]],"long-forms":[[70,77],[82,91],[46,53],[58,65]]},{"text":"tion task which involves acquiring patterns in the distribution of opinion-bearing words and targets using machine learning (ML) techniques. In partic-","acronyms":[[125,127]],"long-forms":[[107,123]]},{"text":"Reject? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the","acronyms":[[66,70],[45,48]],"long-forms":[[54,64],[24,32]]},{"text":"In Proceedings of the 23rd International Conference on Computational Linguistics (COLING?10), pages 617? ","acronyms":[[82,91]],"long-forms":[[55,80]]},{"text":"I  zwemmen  (17) is ttms obtain by setting VPo = VPh Zo = Yo,  attd Zl = Vl.","acronyms":[[51,54],[45,48],[70,72]],"long-forms":[[55,57]]},{"text":"TIGER treebank. Across Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42. ","acronyms":[[89,92],[0,5]],"long-forms":[[54,87]]},{"text":"paraphernalia freely available for multiple languages. That is the case for morphosyntactic analyzers (MSA), but not yet for full or even shallow parsers.","acronyms":[[91,94]],"long-forms":[[64,89]]},{"text":"ments were annotated with word-level labels by professional translators using the core categories in MQM (Multidimensional Quality Metrics) 13","acronyms":[[101,104]],"long-forms":[[106,138]]},{"text":"edge in Y .  The Wall Street Journal Penn Treebank (PTB) (Marcus et al, 1993) comprises parsed constituency","acronyms":[[52,55]],"long-forms":[[37,50]]},{"text":"AT(- +)  Stems to which the suffixes +ation and +ative may  attach are marked as (AT +), while those taking the  corresponding forms +ion and +ive are (AT -).","acronyms":[[82,86],[152,154],[0,2]],"long-forms":[[60,66]]},{"text":"tution is the same as the cost of insertion or deletion. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of","acronyms":[[85,88]],"long-forms":[[59,83]]},{"text":"  Definition: Asymmetric nearest common  ancestor (ANCA)  The asymmetric nearest common ancestors from ","acronyms":[[51,55]],"long-forms":[[14,49]]},{"text":" These features of Latino influencing the choice  of Dependency Grammars (DG)2 as the most  suitable grammar framework for building Latin ","acronyms":[[72,74]],"long-forms":[[51,70]]},{"text":"nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w\/o B.O.) srl 0 487 437 63 13 0.9740 0.1260 55.0% Table 1: Results of the three systems on the SSI-testsuite ( TN = authentic negatives, FN = faked negative, TP = true positives, FPS = spurious supportive, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no","acronyms":[[165,167],[186,188],[208,210],[229,231],[255,257],[260,262],[268,270],[273,275],[277,281]],"long-forms":[[170,184],[191,206],[213,227],[234,249],[284,293]]},{"text":"Abstract  This paper proposes a novel reordering model  for statistical machine translation (SMT) by  means of modeling the translation orders of ","acronyms":[[93,96]],"long-forms":[[60,91]]},{"text":" We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR).","acronyms":[[75,81],[101,105]],"long-forms":[[59,73],[87,99]]},{"text":"emes are represented, and not function words.  Conceptual representations (ConcSs) usage by  PRESENTOR are inspired by the characteristics ","acronyms":[[75,81],[92,101]],"long-forms":[[47,73]]},{"text":"3.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has been widely used and shown to outperform other SSL meth-","acronyms":[[114,117],[171,174]],"long-forms":[[88,112]]},{"text":"2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).  Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training.","acronyms":[[117,120]],"long-forms":[[93,115]]},{"text":" 2.3 Perceptron Sequential Tagger This system uses a Global Linear Model (GLM), a sequential tagger using the perceptron algorithm","acronyms":[[74,77]],"long-forms":[[53,72]]},{"text":"He con~iciers them farcical.  2) ASTG (adicctive rope), including  adjectival Vens and Vings (see VENDADJ I found it well-dcsigned.","acronyms":[[32,36],[100,106]],"long-forms":[[38,54]]},{"text":"al. ( 1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the acquisition of the lexical concept ","acronyms":[[47,49]],"long-forms":[[23,45]]},{"text":"to the recent estimation of domain-independent Q\/A scheme organized in the context of the Text REtrieval Conference (TREC)1. The TREC","acronyms":[[119,123],[48,51],[131,135]],"long-forms":[[92,117]]},{"text":"template includes, for each sentence, syntactic Infor-  mation that Is represented in a tree whose nodes are  syntacti~ categories such as S (sentence), CL (clause),  SUBJECT or VERB.","acronyms":[[153,155]],"long-forms":[[157,163],[142,150]]},{"text":"In this paper, we depicts a mechanism which gen-  erates rebuttals to such rejoinders in the context of  arguments generated from Bayesian etworks (BNs)  (Perla, 1988).","acronyms":[[149,152]],"long-forms":[[131,147]]},{"text":" The two-level analysis of the cited forms ap-  pears below ST = sm'face tape, PT -- pattern  tape, 115.\\[' -- root tape, VT : vocal{sin tat)e , and ","acronyms":[[60,62]],"long-forms":[[65,77]]},{"text":"task yet to be tackled by TM but identified as an important potential application for it (Lewin et al 2008): Cancer Risk Assessment (CRA). Over the","acronyms":[[133,136],[26,28]],"long-forms":[[109,131]]},{"text":" 4.1 BRAT The brat rapid annotation tool (BRAT) is an opensource web-based annotation tool that helps a","acronyms":[[42,46]],"long-forms":[[14,40]]},{"text":" 1 Introduction Named entity recognition (NER) is the most studied information extraction (IE) task.","acronyms":[[42,45],[91,93]],"long-forms":[[16,40],[67,89]]},{"text":"assignment for each annotator. We then performed an analysis of variance (ANOVA) on the outcomes of our experiment.","acronyms":[[74,79]],"long-forms":[[52,72]]},{"text":"Abstract  There has been a long-standing methodology for  evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either ","acronyms":[[97,99]],"long-forms":[[77,95]]},{"text":"2010. The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results.","acronyms":[[55,62]],"long-forms":[[17,53]]},{"text":"for more general audience of users.  Helping our own (HOO) is an initiative that could in future spark a new interest in the re-","acronyms":[[54,57]],"long-forms":[[37,52]]},{"text":"For each  model sentence MSij,, the model builder selects  the Required Lexicon (RLijk), a set of the most  essential lexical entities required to appear in a ","acronyms":[[81,86],[25,29]],"long-forms":[[63,79]]},{"text":" 3 Corpus description The England National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4","acronyms":[[51,54],[88,91],[108,115]],"long-forms":[[26,49]]},{"text":"automatic speech recognition (ASR), machine translation (MT), text-to-speech synthesis (TTS), as well as technology for language resource and fundamental tool de-","acronyms":[[88,91],[30,33],[57,59]],"long-forms":[[62,86],[0,28],[36,55]]},{"text":"and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS). ","acronyms":[[94,98]],"long-forms":[[55,92]]},{"text":"CN Bank(CNB): 200,000 samples  ? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 sample ","acronyms":[[42,46],[8,11],[74,78]],"long-forms":[[33,40],[0,7],[65,73]]},{"text":"C = connector  TR = terse reply  FS = false start  E = echo ","acronyms":[[33,35],[15,17]],"long-forms":[[38,49],[4,13],[20,31],[55,59]]},{"text":"BLEU-4 (Papineni et al, 2002) used in the two  experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.","acronyms":[[118,122],[0,6]],"long-forms":[[97,116]]},{"text":"(3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts:","acronyms":[[91,93],[53,56],[102,105]],"long-forms":[[70,89]]},{"text":" Tables 1: Classifier features in predicate disambiguation (PredDis), argument identification (ArgId), and argument labeling (ArgLab).","acronyms":[[94,99],[59,66],[125,131]],"long-forms":[[69,92],[33,57],[106,123]]},{"text":"at meeting the require of CSR research, and at serving in  a complimentary ole to the corpora being collected in  the interactive Air Travel Information System (ATIS) do-  main.","acronyms":[[159,163],[24,27]],"long-forms":[[128,157]]},{"text":"tries to use synchronous parsing to produce the  tree structure of both source language (SL) and  target language (TL) simultaneously, most SSMT  approaches make uses of monolingual parser to ","acronyms":[[118,120],[92,94],[143,147]],"long-forms":[[101,116],[75,90]]},{"text":"tors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has displayed to have the skill to ex-","acronyms":[[88,91],[35,38]],"long-forms":[[58,86]]},{"text":"Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.","acronyms":[[151,156]],"long-forms":[[131,140]]},{"text":"TC 59% 59% 79% 85% BTC 86% 86% 86% 92% Table 1: Task Completion (TC) and Binary Task Completion (BTC) prediction results, using auto-","acronyms":[[65,67],[0,2],[19,22],[97,100]],"long-forms":[[48,63],[73,95]]},{"text":"to inspect and easily modify discourse-planning specifications for rapid  iterative refinement. The Explanation Design Package (EDP) formalism  is a convenient, schema-like (McKeown 1985; Paris 1988) programming ","acronyms":[[128,131]],"long-forms":[[100,126]]},{"text":"al., 2005) to quickly find possible answers, given the relational conjunction (RC) of the question. ","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":" NOM (nominative), ACC (accusative), DAT (dative), ABL (ablative), CMI (comitative), GEN (genitive) and TOP (theme marker).","acronyms":[[51,54],[67,70],[85,88],[1,4],[19,22],[37,40],[104,107]],"long-forms":[[56,64],[72,82],[90,98],[6,16],[24,34],[42,48],[109,121]]},{"text":"Youtube2text: Recognizing and describing arbitrary activities usage semantic hierarchies and zero-shot acknowledgment. In IEEE International Conference on Computer Vision (ICCV), December.","acronyms":[[169,173],[119,123]],"long-forms":[[124,167]]},{"text":"coord dep coord (c) Previous conjunct headed (PH) Je vois Jean , Paul et Marie","acronyms":[[46,48]],"long-forms":[[20,44]]},{"text":"All words are labeled as basic or not basic according to Ogden?s Basic English 850 (BE850) list (Ogden, 1930).3 In order to measure the lexical complexity","acronyms":[[84,89]],"long-forms":[[65,82]]},{"text":" 6 Over Segmentation  For wrongly spelled or OOV (out of vocabulary)  Urdu words, the system may forcibly break the ","acronyms":[[45,48]],"long-forms":[[50,67]]},{"text":"J~:u phom:,l:ical (graphemic) empressio~ in to  5 l \\ [ ,+vei~  tecLogt+ahh~illa~ieg (levm+l o+ US+st abbrev,  TR) + ,+:monsieur+ar:e  synkam (SR) + (neP'pfie\/r+ic5 (RH) '+ phnnemics ,~nd phnnetit:ts  (gram raphmaics} , ,  Eact+ ees the  ieve l~ i s  in terpreted  a~ a set, ","acronyms":[[136,138],[110,112],[159,161]],"long-forms":[[128,134]]},{"text":"3 Trait Modelling based on LDA 3.1 Controled LDA This section introduces Controled LDA (C-LDA), a weakly supervised variant of LDA.","acronyms":[[91,96],[30,33],[48,51],[130,133]],"long-forms":[[76,89]]},{"text":"former networks for image recognition. Bulletin of the International Statistical Institute (ISI). ","acronyms":[[92,95]],"long-forms":[[55,90]]},{"text":"sented in the graphic. The first strategy can be applied when the data set contains a  functionally independent attribute (FIA) that is used as an organizing device or \"an-  chor\" for the entire graphic.","acronyms":[[123,126]],"long-forms":[[87,121]]},{"text":" Issues, Tasks and Program Structures to Roadmap Research in Issue & Answering (Q&A) http:\/\/www-nlpir.nist.gov\/projrcts\/","acronyms":[[83,86]],"long-forms":[[61,81]]},{"text":"list of Frames alphabetically surrounding Compliance runs as follows: Compatibility, Competition, Complaining, Completeness, Compliance, Concessive.... Next, we attempt to catalogue the Lexical Units (LUs) associated with the frame. ","acronyms":[[201,204]],"long-forms":[[186,199]]},{"text":"SUBS = substantif  compl~ment  VT  = verbe conjugu6  PROC = pronom compl~ment  SUSU = substantif  sujet ","acronyms":[[53,57],[0,4],[31,33],[79,83]],"long-forms":[[60,72],[7,17],[37,42],[86,103]]},{"text":"Proceedings of the Fourteenth  International Joint Conference in Artificial  Intelligence (IJCAI?95), pp. 1395 ?","acronyms":[[91,99]],"long-forms":[[77,89]]},{"text":" The implementing of our approach is a system called LetSum (Legal text Summarizer), which has been worded in Java and Perl.","acronyms":[[55,61]],"long-forms":[[63,84]]},{"text":"Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers.","acronyms":[[387,389],[494,496]],"long-forms":[[365,385]]},{"text":"calculating the posterior probabilities.  Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has","acronyms":[[73,78],[99,103]],"long-forms":[[42,52]]},{"text":"steels@arti.vub.ac.be Abstract Fluid Construction Grammar (FCG) is a new linguistic formalism designed to ex-","acronyms":[[59,62]],"long-forms":[[31,57]]},{"text":"The majority of dependency-based characteristic are constructed utilised the properties of edges and vertices along the shortest path (SP) of an entity pair. ","acronyms":[[126,128]],"long-forms":[[111,124]]},{"text":"characteristic, we regard the sentiment  classification as a sequence labeling problem and  use conditional random field (CRFs) model to  capture the relation between two adjacent ","acronyms":[[122,126]],"long-forms":[[96,120]]},{"text":"Donald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars","acronyms":[[67,69]],"long-forms":[[48,65]]},{"text":"strated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM cor-","acronyms":[[114,117],[166,169]],"long-forms":[[93,112]]},{"text":"three statistical models: Conditional Random  Fields(CRF), Highest Entropy(ME), and  Supports Vector Machine(SVM), which have  good performance and used widely in the ","acronyms":[[108,111],[53,56],[75,77]],"long-forms":[[85,106],[26,52],[59,74]]},{"text":"3.1 Characteristic Structure To implement the twin model, we adoptions the log linear or maximum entropy (MaxEnt) model (Berger et al, 1996) for its flexibility of combining diverse origins of","acronyms":[[95,101]],"long-forms":[[78,93]]},{"text":"conditional models are computed directly from data.  In this study, we use a Maximum Entropy (MaxEnt) classifier to combine the decision characteristic fea-","acronyms":[[94,100]],"long-forms":[[77,92]]},{"text":"2http:\/\/www.nist.gov\/speech\/tests\/mt\/ Table 1: Training, development and test data from Basic Travel Expression Corpus(BTEC) Japanese English","acronyms":[[119,123]],"long-forms":[[88,117]]},{"text":"Elfardy H. and Diab M. 2012. Simplified guidelines for the creation of large scale dialectal arabic annotations. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey. Elfardy H. and Diab M. 2013.","acronyms":[[202,206]],"long-forms":[[167,185]]},{"text":"ran, and the historical ancestor of the other varieties. Modern Standard Arabic (MSA) is the trendy  version of CA and is, broadly talk, the univer-","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"1125 VNC token expressions (CFS07 has 1180).  We then partition them into a development (DEV) set and a test (ESSAYS) set.","acronyms":[[85,88],[5,8],[28,33],[106,110]],"long-forms":[[72,83],[100,104]]},{"text":"any parser; the third requirement is not easily met  in all languages, but even in those languages where  nonrestrictives are not easily identifiable, (II)  works reasonable well.","acronyms":[[152,154]],"long-forms":[[137,149]]},{"text":"prior polarity of verb, verb score (V_score).  Verb-PP (prepositional expressions) rules:  1.","acronyms":[[52,54]],"long-forms":[[56,76]]},{"text":"driver was measured in two method. The driver performed a Tactile Detection Task (TDT) (van Winsum et al, 1999).","acronyms":[[80,83]],"long-forms":[[56,78]]},{"text":"gree (TTCD) 1, tongue body constriction location (TBCL) and degrees (TBCD), weakest tooth height (LTH), and glottal vibration (GLO). For example,","acronyms":[[124,127],[6,10],[50,54],[68,72],[95,98]],"long-forms":[[105,122],[15,48],[75,93]]},{"text":" One proposition is is to use as a natural anguage  grammar the Core Language Engine (CLE)  (Alshawi 1992).","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"dhi\/NEP chemin\/NEL. The structure of the labelled  element using the Shakti Standard Format (SSF)5  will be as follows: ","acronyms":[[89,92],[0,7],[8,16]],"long-forms":[[65,87]]},{"text":"2003) ? Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set, target","acronyms":[[37,41]],"long-forms":[[8,35]]},{"text":"In  Proceedings of the 16th International Conference  on World Large Web (WWW), pages 697-706. ","acronyms":[[73,76]],"long-forms":[[57,71]]},{"text":"Acknowledgment This work is supported by the 6th Framework Research Program of the European Union (EU), LUNA Project, IST contract no 33549,www.ist-luna.eu","acronyms":[[99,101],[104,108],[118,121]],"long-forms":[[83,97]]},{"text":"UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score. ","acronyms":[[63,66],[0,3],[34,37]],"long-forms":[[69,93],[6,32],[40,61]]},{"text":"directed approach conforming to (Ephraim and Malah, 1985) bases on two different noise estimation schemes, i.e. the minimum statistic approach (MS) as described in (Martin, 2001) and the minimum","acronyms":[[144,146]],"long-forms":[[115,133]]},{"text":"Score(f), where E is an example of Pat To improvement ranking, we apart try to find the longest similar subsequence (LSS) between the user input, Sent and retrieved example, Exm","acronyms":[[112,115]],"long-forms":[[83,110]]},{"text":"other synthesis techniques.  The TD-PSOLA (Time Domains Pitch Synching Overlap-add) developed by CNET is a very simple  but crafty method which assures high voice quality, the only disadvantage is that it is based on a time: ","acronyms":[[33,41],[99,103]],"long-forms":[[43,84]]},{"text":" 2 Question Classification We define Matter Classifications(QC) here to be the task that, given a question, maps it to one of","acronyms":[[61,63]],"long-forms":[[37,59]]},{"text":"and the parameters ? can be estimated using maximum likelihood estimation(MLE) on a training corpus(Och and Ney, 2003).","acronyms":[[74,77]],"long-forms":[[44,73]]},{"text":"This paper describes the UNITOR system involvement in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), outlining in (Agirre et al 2013):","acronyms":[[128,131],[25,31],[75,78]],"long-forms":[[99,126]]},{"text":"The method seems to be a simple pattern  matching technique in a left-to-right fashion  but it helps in case of conjunct verbs (ConjVs). ","acronyms":[[128,134]],"long-forms":[[112,126]]},{"text":"reranking stepping works on top of the generative model.  \u0001 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear paragon over CCG derivations.","acronyms":[[56,60],[138,141]],"long-forms":[[62,90]]},{"text":"determine the suitable xpressional shape. Hovy's text structurer (Hovy 1988b),  for example, uses rhetorical relations as defined in Rhetoric Structure Theory (RST)  (Mann and Thompson 1987) to order a set of propositions to be expressed.","acronyms":[[164,167]],"long-forms":[[135,162]]},{"text":"tion using a method similar to FOIL (Quin-  lan, 1990) and bottom-up generalization using  Least General Generalizations (LGG's). Ad- ","acronyms":[[122,127],[31,35]],"long-forms":[[91,120]]},{"text":"same time, Intelligent Computer-Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) apart tend to emphasis more on gram-","acronyms":[[102,105],[60,65]],"long-forms":[[71,100],[11,58]]},{"text":"Soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity (STS), obtaining 3rd place in SemEval-2012.","acronyms":[[133,136],[162,174]],"long-forms":[[104,131]]},{"text":"a seminal works on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase architecture trees of the Penn Treebank (PTB) (Marcus et al, 1993) are converting into CCG derivations and a","acronyms":[[118,121],[18,25],[163,166]],"long-forms":[[103,116]]},{"text":"opinions are about the 20 most popular Chicago hotels; untruthful opinions were generated using the Amazon Mechanistic Turk (AMT)3, whereas ?","acronyms":[[123,126]],"long-forms":[[99,121]]},{"text":"irrespective of word order.  Longest Common Substring (LCS): This measures the longest sequence of words shared between","acronyms":[[55,58]],"long-forms":[[29,53]]},{"text":"The GALE guide WAH corpus and the Chinese to English corpus from the shared task of the NIST open machine translation (OpenMT) 2006 evaluation 6 were employ as the experimental corpus","acronyms":[[119,125],[4,8],[16,18],[88,92]],"long-forms":[[93,117]]},{"text":"2008).  The semantic role labeler (SRL) consists of a pipeline of independent, local classifiers that iden-","acronyms":[[35,38]],"long-forms":[[12,33]]},{"text":"ond, task B referred to as task of normalization involves the mapping of each disorder mention to a  UMLS concept unique identifier (CUI).The mapping was limited to UMLS CUI of SNOMED clin-","acronyms":[[133,136]],"long-forms":[[106,131]]},{"text":"To model the relations between objects and verbs, we follow the data preparation in (Le et al 2013), using the British National Corpus (BNC) which has been preprocessed and parsed using TreeTagger and","acronyms":[[136,139]],"long-forms":[[111,134]]},{"text":"traits: O-SEM ('ordinary semantics') and  I,'-SKEL (F-skeleton) of the type of a semantic ob-  ject, tile set-valued IS-CSTR (IS obstacle) and  the binary MAX-F (for potential maximal focus).","acronyms":[[119,126],[10,15],[44,52],[160,165]],"long-forms":[[128,142],[18,36],[54,64]]},{"text":"Leiter and Obj3 and the counts f(gf, fe) of occurrences of the grammatical functions entire with the roles DEGREE (DEG), THEME (THM), DEPICTIVE (DEP) and LOCATION (LOC).","acronyms":[[128,131],[9,13],[115,118],[145,148],[164,167]],"long-forms":[[121,126],[107,113],[134,143],[154,162]]},{"text":"In Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In Discovery Lawsuits (DESI-4). ","acronyms":[[122,128]],"long-forms":[[99,120]]},{"text":" 1 Introduction Question answering (QA) systems have received a great deal of attention because they provide both","acronyms":[[36,38]],"long-forms":[[16,34]]},{"text":"1  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47, Gothenburg, Sweden, April 27, 2014.","acronyms":[[72,77],[81,85]],"long-forms":[[38,70]]},{"text":"Th~se de l'Universitt~ Joseph  Fourier, Grenoble I, Mars 1990  \\[8\\] TEl (Text Encoding Initiative), Guidance for the  Encoding and lnterchange of Machines Readable Texts.","acronyms":[[69,72]],"long-forms":[[74,98]]},{"text":"various subsets of the documents in the English Gigaword corpus, chiefly drawn from New York Times (NYT) and Agence France Presse (AFP).1 2.1 Are Discounts Constant?","acronyms":[[131,134],[100,103]],"long-forms":[[109,129],[84,98]]},{"text":"Probabilistic topic models (PTM), such as probabilistic inactive semantic indexing(PLSI) (Hofmann, 1999) and inactive Dirichlet alocation(LDA) (Blei et al.,","acronyms":[[134,137],[28,31],[81,85]],"long-forms":[[107,132],[0,26],[42,80]]},{"text":"interpreters. In International Conference on Learning Representations (ICLR). ","acronyms":[[71,75]],"long-forms":[[17,69]]},{"text":"ous word in a given background. As a fundamental task in natural language treatment (NLP), WSD can benefit applications such as machine transla-","acronyms":[[83,86],[89,92]],"long-forms":[[54,81]]},{"text":" We have tre versions of reference summaries based on summarization ratio(SR): 10%, 15% and 20% respectively.","acronyms":[[76,78]],"long-forms":[[56,75]]},{"text":"Parse tree of tagged sentence in Box 1  3 Geographic Information Recoup  3.1 Propositional logic of context (PLC)  As previously discussed, candidate named organizations ","acronyms":[[112,115]],"long-forms":[[80,110]]},{"text":"of Petrov et al. ( 2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP","acronyms":[[54,57],[72,75]],"long-forms":[[59,69],[77,84]]},{"text":"tracting sentencing plan construction rules from the only publicly accessible corpus of discourse trees, the RST Discourse Treebank (RST-DT) (Carlson et al, 2002).","acronyms":[[130,136]],"long-forms":[[106,128]]},{"text":"five different linear classifiers to extract PPI from AIMed: L2-SVM, 1-norm soft-margin SVM (L1-SVM), logistic regression (LR) (Fan et al, 2008), averaged perceptron (AP) (Collins,","acronyms":[[123,125],[45,48],[61,67],[88,91],[93,99],[167,169]],"long-forms":[[102,121],[146,165]]},{"text":"conceptualizing the relation of coincidence or proximity with the whole  Landmark (LM) when it is conceived as a  point.","acronyms":[[83,85]],"long-forms":[[73,81]]},{"text":"far and formulate new ones inspired by latent semantic analyzes (LSA), which was developed within the informational retrieval (IR) community to treat synonymous and polysemous terms (Deerwester et","acronyms":[[125,127],[65,68]],"long-forms":[[102,123],[39,63]]},{"text":" 1 Introduction Machine translation (MT) systems have differing strengths and weaknesses which can be exploited","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"s), correlation with error counts (re and ? e), medium precision (AP) and pairwise accuracy.","acronyms":[[67,69]],"long-forms":[[48,65]]},{"text":" For all NER experiments, we utilise a sequential firstorder conditional random field (CRF) with a unit discrepancies Normal prior, trained with L-BFGS until","acronyms":[[83,86],[9,12],[136,142]],"long-forms":[[57,81]]},{"text":"We further validate our approach on a large publicly available manipulation action dataset (INSANE) from (Aksoy et al, 2014), attain promising ex-","acronyms":[[92,98]],"long-forms":[[63,82]]},{"text":"? SRI has developed the DECIPHER speaker-independent speech recognition system, a  hidden Markov model (HMM)-based system that achieves tate-of-the-art recognition  performance through accurate modeling of phonetic and phonological detail.","acronyms":[[104,107],[2,5],[24,32]],"long-forms":[[83,102]]},{"text":"ambiguous verb structure in a garden-path in two ways; one is as a subordinate clause (MV), the other is a Reduced Relative (RR). He defined","acronyms":[[125,127],[87,89]],"long-forms":[[107,123]]},{"text":"2 GT  and  Move-c~  The central activity of the Minimalist Programmed  are Generalized Transformation (GT) and Move-  ~. GT is a structure-building operation that builds ","acronyms":[[102,104],[2,4],[120,122]],"long-forms":[[74,100]]},{"text":"that is neither terminal nor lexical. An interior knot is  said to meet he foot condition (FC) iffeach foot hallmarks  that it contains transpires also on at least one daughter ","acronyms":[[91,93]],"long-forms":[[75,89]]},{"text":" The seconds set of experiment was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test","acronyms":[[64,67]],"long-forms":[[46,62]]},{"text":"Jiang, Hua. Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt. Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge.","acronyms":[[190,198]],"long-forms":[[136,188]]},{"text":"of MT and HT in terms of the following two ratios, LC = lexical cohesion devices \/ content words, RC = repetition \/ content words. ","acronyms":[[98,100],[3,5],[10,12],[51,53]],"long-forms":[[103,123],[56,72]]},{"text":" ? Bigram Predictability (BP): Defined as the predictability of a word given a former word, it","acronyms":[[26,28]],"long-forms":[[3,24]]},{"text":"Tabled 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Sensor. ","acronyms":[[125,127],[35,38],[57,60],[86,90]],"long-forms":[[128,143],[61,84],[91,123]]},{"text":" 2003. Social Communication Quiz (SCQ). ","acronyms":[[43,46]],"long-forms":[[7,41]]},{"text":"of MT and HT in terms of the following two ratios, LC = lexical consistency devices \/ content words, RC = repetitions \/ content words. ","acronyms":[[98,100],[3,5],[10,12],[51,53]],"long-forms":[[103,123],[56,72]]},{"text":"LS (List item marker) LS  MD (Modal) MD  NN (Noun, singular or mass) N  NNS (Noun, plural) N ","acronyms":[[41,43],[0,2],[22,24],[26,28],[37,39],[72,75]],"long-forms":[[45,49],[30,35],[77,81]]},{"text":"a wordbreak (WB). In other mots, we models Chinese word segmentation as wordbreak (WB) identification which takes all CB?s as candidates and","acronyms":[[83,85],[13,15],[118,122]],"long-forms":[[72,81],[2,11]]},{"text":"2 BP Neural Network  At the moment, there are about more than 30 kinds of  artificial neural network (ANN) in the domain of  research and application.","acronyms":[[102,105],[2,4]],"long-forms":[[75,100]]},{"text":"volved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.","acronyms":[[85,88]],"long-forms":[[61,83]]},{"text":"adapt the model to character or word level, or limit  the conversion purposes to only noun or widen it to  other Part of Speech (POS) tags, a serials of  experiments has been performed.","acronyms":[[128,131]],"long-forms":[[112,126]]},{"text":"al., ( 2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al.,","acronyms":[[78,81]],"long-forms":[[52,76]]},{"text":"We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"lists are derived automatically from the training data.  Frequent Word List (FWL) This list consists of words that occur in more than 5 different documents.","acronyms":[[77,80]],"long-forms":[[57,75]]},{"text":"We valuation our Chinese word segmenter in the open track, on all 4 corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Investigative (MSR), and","acronyms":[[101,103],[136,141],[164,167]],"long-forms":[[84,99],[106,121],[144,162]]},{"text":"tice. They uncovered that a domain-specific corpus performs better than a Wall Street Diary (WSJ) corpus for the trigram LM.","acronyms":[[91,94]],"long-forms":[[70,89]]},{"text":"i. The Eng!\\[sh t_qMal_a~franslationSsSSSSSSSS_2~trm  Baak~reusd  Computer Aided T~anslation (CAT) research at Universiti  Sa~m MalsysL~ (USM) initiate in 1976 as an individual investigate ","acronyms":[[94,97],[138,141]],"long-forms":[[66,82],[111,136]]},{"text":"as feature vectors. The model of our choice is the maximum entropy model (MaxEnt), also known as logistic regression (?).","acronyms":[[74,80]],"long-forms":[[51,66]]},{"text":"PLC = partition left context  (has been done)  PRC = partition right context  (yet to be effected) ","acronyms":[[47,50],[0,3]],"long-forms":[[53,76],[6,28]]},{"text":"Dependency-Based Open Information Extraction Pablo Gamallo and Marcos Garcia Centre de Investigac?a?o sobre Tecnologias da Informac?a?o (CITIUS) Universidade de Santiago of Compostela","acronyms":[[137,143]],"long-forms":[[77,119]]},{"text":"Turian et al. ( 2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (","acronyms":[[88,91]],"long-forms":[[62,86]]},{"text":"The score of an abstract based on extracted PICO elements, SPICO, is broken into individual components according to the following formula: SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4) The first component in the equation, Sproblem, reflects a match between the primary","acronyms":[[139,144],[44,48],[59,64]],"long-forms":[[147,196]]},{"text":"| lexica: The relative frequency of classes, based on the Linguistic Inquiries and Word Count (LIWC) dictionary (Pennebaker et al, 2007).","acronyms":[[96,100]],"long-forms":[[61,94]]},{"text":"from Association. ACM Transactions on Information Regimes (TOIS) 21:315-346. ","acronyms":[[59,63],[18,21]],"long-forms":[[22,57]]},{"text":"targeted text. This type of question has been studied extensively in the Text Retrieval Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the","acronyms":[[119,121]],"long-forms":[[99,117]]},{"text":"build a bridge between UNL and one of the  inside representations of ETAP, namely  Normalized Syntactic Structures (NormSS), and in  this way binding UNL with all other levels of text ","acronyms":[[117,123],[23,26],[71,75],[148,151]],"long-forms":[[85,115]]},{"text":"audiences have little trouble mapping a collection  of noun phrases onto the same entity, this task of  noun phrase (NP) coreference r solution can present  a formidable challenge to an NLP system.","acronyms":[[117,119],[186,189]],"long-forms":[[104,115]]},{"text":"First, we rewrite equation 1 in a more detailed fashion as: A?R = argmax una","acronyms":[[60,63]],"long-forms":[[66,72]]},{"text":"be compared, for any section of the corpus. The tool also calculation the majority tag (MajTag). Av-","acronyms":[[87,93]],"long-forms":[[73,85]]},{"text":"Second, we demonstrate correlation to a database of real-world international conflict events, the Militarized Interstate Dispute (MID) dataset (Jones et al, 1996).","acronyms":[[130,133]],"long-forms":[[98,128]]},{"text":"ings. The technique utilized is adapted from unsupervised word sense disambiguation (WSD). ","acronyms":[[85,88]],"long-forms":[[58,83]]},{"text":"Chore (Pradhan et al 2011), one text from each of the five represented genres: Broadcast Dialogue (BC), Broadcasts News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups","acronyms":[[124,126],[139,141]],"long-forms":[[108,122],[129,137]]},{"text":"Table 5 shows the results when experimenting with various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We","acronyms":[[217,220],[113,115],[181,184],[248,250]],"long-forms":[[193,215],[101,111],[127,179],[232,246]]},{"text":"Table 3: Validation results for metaphor interpreting for English and Russia.  (ALL), or just two (TWO) validators. In most of","acronyms":[[102,105]],"long-forms":[[97,100]]},{"text":"and Rozovskaya and Rothe (2011). The article system is trained utilize the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), imple-","acronyms":[[93,95]],"long-forms":[[72,91]]},{"text":"Litkowski, K. C.: Syntactic Clues and Lexical Resources in Question-Answering. In E. M. Voorhees & D. K. Harman (eds.), The Ninth Text Retrieval Conference (TREC-9). ( 2001) 157-166 10.","acronyms":[[157,163]],"long-forms":[[120,155]]},{"text":" Reference:  MedLine sample # 6  Autonym:  decoy receptor 3 (DcR3)  Information a soluble decoy recipient  ","acronyms":[[61,65]],"long-forms":[[43,59]]},{"text":"In this paper, we propose  methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs)  based WOEs detection models identify the sentence segments containing WOEs.","acronyms":[[111,115],[57,61],[124,128],[188,192]],"long-forms":[[84,109]]},{"text":"increasingly more important.  In ordinary phrase structure grammars (PSG's),  the only mechanism for capturing the kinds of merg- ","acronyms":[[69,74]],"long-forms":[[42,67]]},{"text":"4.1 The NIST evaluations scheme The National Institute of Science and Technology (NIST) proposals an evaluation scheme that looks at the following properties when","acronyms":[[81,85],[8,12]],"long-forms":[[35,79]]},{"text":"Stride 1  Tagging using Global Distribution (NEIG) Trained Model Statistically System (MEMM) Step 2 MEMM Based Statistical System(S-MEMM)Final Tagged DataSet Added    as a feature","acronyms":[[126,132],[43,47],[83,87],[146,153]],"long-forms":[[89,100]]},{"text":"The development of effectiveness estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Discourses Recognition (ASR) (Young et al.,","acronyms":[[147,150]],"long-forms":[[117,145]]},{"text":"fcn@dsic.upv.es Abstract State-of-the-art Machine Translation (MT) systems are again far from being perfect.","acronyms":[[63,65]],"long-forms":[[42,61]]},{"text":"subject and object with the terrain truth. ETS\/ETO = Emotions towards themes\/object, MAS=mean absolute error, and RMSE= root mean square error","acronyms":[[85,88],[42,49],[114,118]],"long-forms":[[89,102],[52,83],[120,142]]},{"text":"the train and development sets in MC160 and MC500 and divide them arbitrarily into a 250-story training set (TRAIN) and a 200-story development set (DEV).","acronyms":[[105,110],[34,39],[44,49],[145,148]],"long-forms":[[91,99],[128,139]]},{"text":"2010). Alternatively, iteratively optimized embeddings such as Skip Gram (SG) model (Mikolov et al.,","acronyms":[[74,76]],"long-forms":[[63,72]]},{"text":"Daniele Vannella, 2013). The two system types are WSI (Word Sense Induction) and WSD (Word Sense Disambiguation).","acronyms":[[50,53],[81,84]],"long-forms":[[55,75],[86,111]]},{"text":"posterior distribution.  We utilize maximum entropy (MaxEnt) model  (Berger et al, 1996) to design the basic classifier ","acronyms":[[53,59]],"long-forms":[[36,51]]},{"text":"from standard formats to OpenNLP specific ones. We represented standard formats with EMF (an Ecore model for each one) and we created specific transformations using Java Emitter Templates (JET) 16","acronyms":[[189,192],[29,32],[85,88]],"long-forms":[[165,187]]},{"text":"The scores returned by the similarity measures are using as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learned to sepa-","acronyms":[[90,92]],"long-forms":[[73,88]]},{"text":"Table 4. Comparative results in targeted. The number of positive instances (POS) and negative instances (NEG) and macro-averaged accuracy (ma-P), recall (ma-R) and F1-score (ma-F) are shown.","acronyms":[[73,76],[32,37],[102,105],[137,141],[152,156],[172,176]],"long-forms":[[53,71],[82,100],[111,135],[144,150],[162,170]]},{"text":"Hidden topic markov models. Contrived Intelligence and Statistics (AISTATS). ","acronyms":[[68,75]],"long-forms":[[28,66]]},{"text":" Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation pe-","acronyms":[[85,88],[9,12]],"long-forms":[[63,83]]},{"text":"A mobile real-time speech-to-speech translation (S2ST) device is one of the phenomenal difficulty in natural language processing (NLP). It involves","acronyms":[[125,128],[49,53]],"long-forms":[[96,123],[19,47]]},{"text":"described as operating within the same three-stage framework as STRAND.  Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web search engines to locate pages by querying for pages in a given language that contain","acronyms":[[94,101],[64,70]],"long-forms":[[73,92]]},{"text":"are usually followed by a number n ? 0. DU = discourse unit, ce = conversational event, K = DRS, u = utterance, sem","acronyms":[[40,42],[92,95]],"long-forms":[[45,54],[101,110]]},{"text":"conjuncts depend on it. Nilsson et al (2007)  advocate the Mel?cuk style (MS) for parsing  Czech, taking the first conjunct as the head, ","acronyms":[[74,76]],"long-forms":[[59,72]]},{"text":"the curve (AUC) from the trained detector on the heldout test set. Results are reported in terms of the mean and standard deviation (SD) of the AUC across all splits. ","acronyms":[[133,135],[11,14],[144,147]],"long-forms":[[113,131]]},{"text":"301 3. False Attributes (FA) - these are situations where a value for an attribute has been spec-","acronyms":[[25,27]],"long-forms":[[7,23]]},{"text":"Above all, our goal is to integrate cross-media inference and create the linkage among the information extracted from those heterogenous data. Our novel Multi-media Information Networks (MiNets) representation initializes our idea about a basic ontology of the ranking system.","acronyms":[[187,193]],"long-forms":[[159,185]]},{"text":"sLDA is a model that is an extend of Latent Dirichlet Allotment (LDA) (Blei et al, 2003) that models each document as having an output variable in addition to","acronyms":[[69,72],[0,4]],"long-forms":[[40,67]]},{"text":"SemEval 2012 competition for evaluating Natural  Language Processing (NLP) systems presents a  new task called Semantic Textual Similarity (STS)  (Agirre et al, 2012).","acronyms":[[140,143],[70,73]],"long-forms":[[111,138],[40,68]]},{"text":"Table 7: Peculiarities template we use for our ranked of phenomena types. Feature examples are based on the sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ? he doesn?t","acronyms":[[192,194]],"long-forms":[[187,190]]},{"text":"Additionally, we have acquired gazetteer lists for Hindi and used these gazetteers in the Maximum Entropy (MaxEnt) based Hindi NER system.","acronyms":[[107,113],[127,130]],"long-forms":[[90,105]]},{"text":"The modelled is presents below. 4 The Model This study used feed-forward artificial neu-ral networks with a backpropagation algorithm as computational mannequins for the analysis of un-accusative\/unergative distinction in Turkish. 4.1 Artificial Neural Networks and Learn-ing Paradigms  An artificial neural network (ANN) is a compu-tational model that can be used as a non-linear stats data model tool. ANNs are gener-ally use for deriving a function from observa-tions, in applications where the data are com-plex and it is challenging to devise a relationship ","acronyms":[[314,317]],"long-forms":[[287,312]]},{"text":"The computation of associative responses to multiword stimuli. In  Proceedings of the  Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, UK","acronyms":[[133,140],[181,183]],"long-forms":[[99,131]]},{"text":" 1 Introduction Todays natural user interfaces (NUI) for applications running on smart devices, e.g, phones (SIRI,","acronyms":[[48,51],[109,113]],"long-forms":[[23,46]]},{"text":"In  * This work has been sponsored by the Fonds zur  FSrderung der wissenschaftlichen Forschung (FWF),  Grant No.","acronyms":[[97,100]],"long-forms":[[53,95]]},{"text":"nese kanji and words. The now available  JWAD Version 1 (JWAD-V1) consists of  104,800 free word association replied col-","acronyms":[[63,70]],"long-forms":[[47,61]]},{"text":"The parser used a semantic grammar with approx-  imately 1000 rules which charting the input condemnation  onto an interlingua representation (ILT) which rep-  resents the meaning of the sentence in a language- ","acronyms":[[135,138]],"long-forms":[[107,133]]},{"text":"Long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the","acronyms":[[119,122]],"long-forms":[[93,117]]},{"text":"structures. 4 Such an algorithm has to define for every LFG a relation  F?(~,s)  (s is generable from (I)) between directed acyclic graphs (DAGs)  and terminal strings.","acronyms":[[140,144],[56,59],[72,74]],"long-forms":[[115,138]]},{"text":"  1. RecallCorrectTransliteration  (RTrans)  The recall is going to be computed using the ","acronyms":[[36,42]],"long-forms":[[5,33]]},{"text":"Abstract We present the design, preparation, results and analyzed of the Cancer Genetics (CG) event extraction task, a main tasks of the","acronyms":[[90,92]],"long-forms":[[73,88]]},{"text":"Bangalore, India Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper","acronyms":[[52,55]],"long-forms":[[26,50]]},{"text":"An alternative to QE is to perform the expansion in the document. Documenting Expansion (DE) was first proposed in the sermons retrieval commu-","acronyms":[[86,88],[18,20]],"long-forms":[[66,84]]},{"text":"tiguous correspondence. The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) alluded by (Zhang et al, 2008a) can be viewed","acronyms":[[93,96],[56,59]],"long-forms":[[65,91],[28,54]]},{"text":"Configured P0.1 P0.25 P0.33 P0.5 Best-F1 ContextSim (CS) 42.9 69.6 60.7 58.7 49.6 SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9 (a) from basic modelling","acronyms":[[92,94],[50,52]],"long-forms":[[79,90],[38,48]]},{"text":"presented at the 16th International Conference on Computational Linguistics, Copenhagen. Gupta, D., Saul, M., & Gilbertson, J. (2004). Evaluation of a deidentification (DE-ID) software engine to share pathology reports and clinical documents for research. Am J Clin Pathol, 121(2), 176-186.","acronyms":[[169,174]],"long-forms":[[151,167]]},{"text":"The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practising, a sequenced","acronyms":[[135,137],[4,7]],"long-forms":[[109,133]]},{"text":" 3 BUDS dialogue manager The Bayesian Update of Dialogue State (BUDS) dialogue manager is a POMDP-based dialogue","acronyms":[[64,68],[3,7],[92,97]],"long-forms":[[29,62]]},{"text":"Final step in treebuilding.  The English Destressin8 Rule (EDR) is used to  determ\/ne which vowels should be reduced.","acronyms":[[59,62]],"long-forms":[[33,57]]},{"text":"easily inspectable. The generalizing competence of the evolutionary reinforcement learning (RL) algorithm, XCS, can dramatically lower the size of the opti-","acronyms":[[89,91],[104,107]],"long-forms":[[65,87]]},{"text":"In the  past a few year, we have put forward methods for Kazakh morphological analysis, which includes  stem extraction, part of speech(POS) tagging, spelling check, etc. Recently, we are working on syntax ","acronyms":[[136,139]],"long-forms":[[121,134]]},{"text":" 2 Hidden Markov Models Hidden Markov models (HMMs) are commonly used to represent a wide range of linguistic phe-","acronyms":[[46,50]],"long-forms":[[24,44]]},{"text":"We tested the Arabic feeling system on deux existing Arabic datasets (Mourad and Darwish (2013) (MD) and Refaee and Rieser (2014a) (RR)) and two newly sentiment-annotated Arabic datasets (BBN","acronyms":[[98,100]],"long-forms":[[117,123]]},{"text":"Lowe HJ, Barnett GO. ( 1994) Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.","acronyms":[[83,87],[5,7]],"long-forms":[[57,81]]},{"text":"tistical machine translation. In Proceedings of the Machine Translations Summit (MT-Summit). ","acronyms":[[80,89]],"long-forms":[[52,78]]},{"text":"computed from the rewrite rules by the examination of the interdependencies of the rules with the help of  KIT = Ktinsdiche lntelligenz und Textverstehen  (artificial intelligence and text understanding), FAST = ","acronyms":[[107,110],[205,209]],"long-forms":[[113,135]]},{"text":"Figure 1: Sample transcript from a TD child 3 Narrative Topic Analysis Using LDA Latent Dirichlet Allocation (LDA) (Blei et al 2003) has been used in NLP to model topics within","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"in the yesteryear verb expression (PV), the jefe noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007).","acronyms":[[131,133],[30,32],[79,81]],"long-forms":[[108,122],[17,28],[56,77]]},{"text":"There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase","acronyms":[[116,118]],"long-forms":[[93,114]]},{"text":"GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters ? Indotag (Indonesian): Conditional Random Field (CRF) POS tagger. ","acronyms":[[136,139],[141,144]],"long-forms":[[110,134]]},{"text":" Figure 2 shows the learning curve for pseudoprojective parsing (P-Proj), compared to using only projectivized training data (Proj), measured as error","acronyms":[[65,71],[126,130]],"long-forms":[[45,63]]},{"text":" 2 In Pisa two dictionaries of Italian, ,are being used: the  Nuovo Dizionario Garzanti (GRZ) and the Dlzionario-  Macchina dell'ltaliano (DMI), a MRD mainly based on the ","acronyms":[[89,92],[139,142],[147,150]],"long-forms":[[79,87],[102,137]]},{"text":"In this paper, we describe a mechanism which gen-  erates rebuttals to such rejoinders in the context of  arguments generated from Bayesian etworks (BNs)  (Pearl, 1988).","acronyms":[[149,152]],"long-forms":[[131,147]]},{"text":"This article describes the collaborative work on applying  the newly proposed ISO standard for dialogue act  annotation to the Switchboard Dialogue Act (SWBD-DA)  Corpus, as part of our on-going effort to promote ","acronyms":[[153,160],[78,81]],"long-forms":[[127,151]]},{"text":"R5   95 7 Antecedent Contained Abolishing(ACD)  Further evidence for the proposed analysis comes ","acronyms":[[40,43]],"long-forms":[[10,39]]},{"text":"tell verb basis (VB), tell VB tell told verb past tense (VBD), tell VBD,VBN tell verb past participle (VBN), tell tells verb present 3rd persona sing (VBZ), say VBZ tell","acronyms":[[102,105],[16,18],[26,28],[56,59],[67,70],[71,74],[149,152],[160,163]],"long-forms":[[80,100],[5,14],[39,54],[119,142]]},{"text":"Table 1 shows the feature template sets.  For training, we utilized soft confidence weighted (SCW) (Wang et al., 2012).","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":"2005a; Jeon et al, 2005b) compared four different  retrieval processes, i.e. vector space model, Okapi,  linguistic model (LM), and translation-based model,  for automatically fixing the lexical chasm between ","acronyms":[[119,121]],"long-forms":[[103,117]]},{"text":"? SRI has developed the DECIPHER speaker-independent speech recognition system, a  hidden Markov modelling (HM)-based system that achieves tate-of-the-art recognition  performance through accurate modeling of phonetic and phonological detail.","acronyms":[[104,107],[2,5],[24,32]],"long-forms":[[83,102]]},{"text":"Metonymy Often a sentence relates entities in a way inconsistent with the target ontology. For example, with the Component Library (CLib) ontology,movement properties (e.g., speed, acceleration) are defined as properties of the movement events, rather","acronyms":[[132,136]],"long-forms":[[113,130]]},{"text":"context    Chinese Context(CC): ???????? ","acronyms":[[27,29]],"long-forms":[[11,25]]},{"text":"also included as features.  Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al (2007) to","acronyms":[[56,60]],"long-forms":[[28,54]]},{"text":"Bielefeld University  2 Center of Excellence ? Cognitive Interactions Technology?(CITEC), Bielefeld University     ","acronyms":[[81,86]],"long-forms":[[47,79]]},{"text":"same time, Intelligent Computer-Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also tend to focus more on gram-","acronyms":[[102,105],[60,65]],"long-forms":[[71,100],[11,58]]},{"text":"640,000 non-empty abstracts were found.  Query Set (QSet): We download from PubMed the abstracts that mention a given gene name and its syn-","acronyms":[[52,56]],"long-forms":[[41,50]]},{"text":" 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifier We include Highest Entropy classifier using sim-","acronyms":[[41,47]],"long-forms":[[24,39]]},{"text":" In order to acquire labeled instances for training, we decompose the gold standard (GS) events into multiple events with single arguments.","acronyms":[[85,87]],"long-forms":[[70,83]]},{"text":"Semantic Interpretation of Prepositions for NLP Applications Sven Hartrumpf Hermann Helbig Rainer Osswald Intelligent Information and Communication Systems (IICS) University of Hagen (FernUniversita?t in Hagen)","acronyms":[[157,161],[44,47]],"long-forms":[[106,155]]},{"text":" A natural solutions would be to take advantages  of machine readable dictionaries (MILD's), such  as Longman's Dictionary of Contemporary En- ","acronyms":[[82,88]],"long-forms":[[51,80]]},{"text":"   FIGURE 1: Community of Inquiry (CoI) model   (Adapted from: Garrison et al 2000) ","acronyms":[[35,38]],"long-forms":[[13,33]]},{"text":"assessed metrics. Legend: d = dependency f-score, _pr =  predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accurate ","acronyms":[[95,97],[53,55],[110,114],[138,142]],"long-forms":[[100,107],[32,42],[59,73],[87,93],[117,136],[145,159]]},{"text":" The idiosyncratic of a knot is one of part of speech (POS), lexical value (LEX), or dependency label (DEP), as for instance LEX(QUEUE0)","acronyms":[[72,75],[51,54],[99,102],[121,124],[125,131]],"long-forms":[[57,64],[35,49],[81,91]]},{"text":" 1 Intro Quality Appraisals (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translating (Blatz et al.,","acronyms":[[36,38],[65,67],[122,124]],"long-forms":[[16,34],[44,63]]},{"text":"1. Introducing  A verb phrase ellipsis (VPE) exists when a  sentence has an auxiliary verb but no verb phrase ","acronyms":[[41,44]],"long-forms":[[19,39]]},{"text":"We prolonging the SPARSELDA (Yao et al, 2009) inference scheme for latent Dirichlet alocation (LDA) to tree-based topic models.","acronyms":[[91,94],[14,23]],"long-forms":[[63,89]]},{"text":"(perhaps maller than traditional linguistic units) out of  component words: 1. noun group (NG), which consists  of a noun and its immediately preceding words (e.g., ","acronyms":[[91,93]],"long-forms":[[79,89]]},{"text":"annotating unlabeled data, for adapting  existing CRF-based named entity recognition (NER) systems to new texts or  domains.","acronyms":[[86,89],[50,53]],"long-forms":[[60,84]]},{"text":" The pattern we used consists of a mix between the  part of speech (POS) tags and the mention tags for  the words in the training instance.","acronyms":[[68,71]],"long-forms":[[52,66]]},{"text":"ways available. Ours therefore produce the training data using a na??ve phrase aligner (NPA) instead of resorting to a real one.","acronyms":[[87,90]],"long-forms":[[64,85]]},{"text":"2http:\/\/www.statmt.org\/wmt12\/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (MT). ","acronyms":[[121,123]],"long-forms":[[100,119]]},{"text":"Adverb Variation (AdvV) ? Modifier Variation (ModV) ?","acronyms":[[46,50]],"long-forms":[[26,44]]},{"text":"descriptions 3800 4247 Table 2: Properties of the annotated two subcorpora, genetics (GEN) and computational linguistics (CL)","acronyms":[[86,89],[122,124]],"long-forms":[[76,84],[95,120]]},{"text":"tion, Generation, Question Answering (QA), etc.  STS is related to both Textual Entailment (TE) and Paraphrasing, but differs in a number of ways","acronyms":[[92,94],[38,40],[49,52]],"long-forms":[[72,90],[18,36]]},{"text":"This usage is domain specific and the referent is fixed as a second person subject.  pora: 1) three transcripts of family conversation (FaCon) drawn from Australian English Corpus (Monash University 1996~1998) collecting family interviews about their past holidays (Nariyama 2004); 2) three 30-minute-TV Australian drama transcripts (TV) (Nariyama 2004); 3) Switchboard corpus consisting of telephone conversation on a variety of specified every day topics (Cote 1996).  Referent   FaCon TV dramas Switchboard I we you he\/she it they ","acronyms":[[136,141],[482,487],[334,336],[301,303]],"long-forms":[[115,134]]},{"text":" To overcome this problem, Gliozzo et al (2005) introduced the domain model (DM) and show how to define a domain VSM in which texts and terms","acronyms":[[77,79],[113,116]],"long-forms":[[63,75]]},{"text":"Abstract One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assess users","acronyms":[[68,70]],"long-forms":[[55,66]]},{"text":"limits to do top-down filtering.  1986 Kameyama (1986) proposed a fourth transition type, Centre Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood.","acronyms":[[117,120]],"long-forms":[[102,115]]},{"text":"2004. Orn clusterings: Good, bad and spectral. Journal of the ACM (JACM), 51(3):497?515.","acronyms":[[66,70]],"long-forms":[[46,64]]},{"text":"the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent (VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is propagated to the head child.","acronyms":[[155,158],[57,59],[63,65],[85,87]],"long-forms":[[136,153],[70,83],[41,55]]},{"text":"comparability between documents that are  chosen for exploitation in the current research,  are Mutual Infromation (MI) and Normalized  Mutual Infroamtion (NMI).","acronyms":[[116,118],[156,159]],"long-forms":[[96,114],[124,154]]},{"text":"Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity, SL=sentence length). ","acronyms":[[95,97]],"long-forms":[[98,113]]},{"text":"Route INJECTION ORAL SMOKING SNORTING Aspect CHEMISTRY (Pharmaceuticals, TEK) CULTURE (Culture, Setting, Social, Spiritual) EFFECTS (Effects)","acronyms":[[75,82],[70,73],[121,128]],"long-forms":[[84,91],[130,137]]},{"text":"data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split","acronyms":[[118,123],[35,37],[71,74]],"long-forms":[[101,116],[18,33]]},{"text":"redundancy at low and medium allophonic complexities, estimated by the Jaccard indices between their false positives (FP) and false negatives (FN). ","acronyms":[[118,120],[143,145]],"long-forms":[[101,116],[126,141]]},{"text":"rithm to handle this setting. To do so, we use dynamic programming (DP) together with greedy searched.","acronyms":[[68,70]],"long-forms":[[47,66]]},{"text":"fornls of mots. For instance, the rule  \\[ S= ion I=  (NNS VBZ) R= (NN) M=8\\]  says if by eradicated the suffix \"ion\" from a word ","acronyms":[[60,63],[69,71]],"long-forms":[[56,59]]},{"text":"processing beyond keyword indexing, typically supported by Natural Language Processing (NLP)  and Information Extract (CI) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li ","acronyms":[[122,124],[88,91]],"long-forms":[[98,120],[59,86]]},{"text":"LMs by perplexity (PPL). We use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB). ","acronyms":[[88,91],[0,3],[19,22],[57,60]],"long-forms":[[73,86],[7,17],[36,55]]},{"text":"We however saw the relative degradation of quality in coordinating conjunctions (CC), determiners (DT) and personal pronouns (PRP). ","acronyms":[[130,133],[85,87],[103,105]],"long-forms":[[111,128],[58,83],[90,101]]},{"text":"against the human annotation.  2.3 Distributed Tree Kernel (DTK) Distributed Tree Kernel (DTK) (Zanzotto and","acronyms":[[60,63],[90,93]],"long-forms":[[35,58],[65,88]]},{"text":"as the World Vast Web. Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech- ","acronyms":[[66,68],[124,127]],"long-forms":[[44,64],[96,123]]},{"text":"Abstract  This paper describes a reestimation method for stochastic language models uch as the  N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations. It is ","acronyms":[[137,140]],"long-forms":[[117,135]]},{"text":"proposed two novel characters, Intra-sentence positional term weighting (IPTW) and the Patched language model (PLM), and showed their effectiveness by conducts automatic","acronyms":[[109,112],[71,75]],"long-forms":[[85,107],[29,69]]},{"text":"participate in the interpretive f the CLS (e.g., elements bearing the thematic roles allocate by  the predicate, etc.). DPSs (DP structures) semantically characterize noun phrases. They consist ","acronyms":[[122,126],[40,43]],"long-forms":[[128,141]]},{"text":"Table 3: Validation results for metaphor interpretation for English and Russian.  (ALL), or just two (TWO) validators. In most of","acronyms":[[102,105]],"long-forms":[[97,100]]},{"text":"In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL). ","acronyms":[[91,94]],"long-forms":[[49,89]]},{"text":"ACL 2006 paper (see experiments). Crossed document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technologies that consolidates named entities","acronyms":[[62,65],[0,3]],"long-forms":[[34,60]]},{"text":" These classifiers are based on a discriminative model: Support Vector Machine (SVM)6 (Vapnik, 1995).","acronyms":[[80,83]],"long-forms":[[56,78]]},{"text":"PROP  VP  I ACCESSORY = propositions  These-fragments would match a locative object use of a p r epos~ .~ lu r~  arlu L H ~ .","acronyms":[[12,16]],"long-forms":[[19,30]]},{"text":"C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.  Verbs Solely ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needs for this chore (other verbs trigger state O).","acronyms":[[89,92],[0,6],[19,22],[23,25],[46,52],[56,64],[98,106],[108,111],[116,119]],"long-forms":[[67,81]]},{"text":"edge of syntax and semantics.   In connection to conjunct verbs (ConjVs),  (Mohanty, 2010) defines two types of conjunct ","acronyms":[[65,71]],"long-forms":[[49,63]]},{"text":"Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological disorder.  Generic Union Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that","acronyms":[[146,150],[21,25],[231,237]],"long-forms":[[115,144]]},{"text":"multilingual MT system developed at the Laboratoire d'Analyse et de Technologie du Langage (LATL), University of Geneva. ","acronyms":[[92,96],[13,15]],"long-forms":[[40,90]]},{"text":"consult, and their performance asymptotes by the time they get to the second query.  This effect is confirmed by an analysis of variance (ANOVA)8, which displays a exceptionally significant effect of order of submissions (F = 9.8427; p< .0001).","acronyms":[[136,141]],"long-forms":[[114,134]]},{"text":"End the tutoring problem? Cause another round of dialogue\/essay revision ITSpoke (Intelligent Tutoring SPOKEn dialogue system) 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  156\/206","acronyms":[[73,80]],"long-forms":[[82,109]]},{"text":"[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and H. Zheng, Bandera: Extracting finite-state models from java source code, in: Proceedings of the International Conference on Software Engineering (ICSE), 2000. ","acronyms":[[231,235]],"long-forms":[[181,229]]},{"text":"Table 5: Final test accuracies for Chinese. UAS = unlabeled attachment score; UEM = unlabeled exact match; LA = labeled attachment score.","acronyms":[[78,81]],"long-forms":[[84,105]]},{"text":"bic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistic (COLING) and the 44th Everyyear Meeting of the the Association","acronyms":[[98,104]],"long-forms":[[71,96]]},{"text":"  TGTM P=p,kp ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l  ","acronyms":[[24,26],[2,6],[19,23],[46,50],[51,53]],"long-forms":[[27,29],[55,58]]},{"text":"There have been p,'evious VCl'sions of I,'1\" (l,cpage 1986)  The I,T llSed ill our wo!'k has been implemented on  MacilLtosh with CLOS (Common Lisp Oh.jeer System)  (l,afeurcade 1993) The realizalion is lmscd mainly on ","acronyms":[[130,134],[26,29]],"long-forms":[[136,162]]},{"text":"opinions are about the 20 most popular Chicago hotels; deceptive opinions were generated using the Amazon Mechanical Turk (AMT)3, whereas ?","acronyms":[[123,126]],"long-forms":[[99,121]]},{"text":"Errors are italicized and marked in red.  LDA with phrases (LDA-P): As aspect-sentiment phrase are often noun phrase, a basic approach","acronyms":[[60,65]],"long-forms":[[42,58]]},{"text":"to) Peet a friend?  Figure 10: An argument post particle phrase (PP) (supreme) and an adjunct PP (lower).","acronyms":[[65,67],[92,94]],"long-forms":[[48,63]]},{"text":" ? Subordinate clause reordering (SubCR) which involve moving postnominal relative","acronyms":[[34,39]],"long-forms":[[3,32]]},{"text":"It is namely interesting to see that when hypotheses selection is implemented, oracle error rate (OER) drops of 2% points from an already accurate OER","acronyms":[[100,103],[149,152]],"long-forms":[[81,98]]},{"text":"(Bjo?rne et al 2011).  The Turku Event Extraction System (TEES)1 is an open source program for extract event and re-","acronyms":[[58,62]],"long-forms":[[27,56]]},{"text":"This examination can be held by plotting values of recall, precision and F-measure during each step  of merging process. Figure 5 shows the fluctuation of positive recall(PR), positive preclsion(AP),  averaged recall(AR), averaged precision and F-measure (FM).","acronyms":[[171,173]],"long-forms":[[155,169]]},{"text":"maps: 2-d space-filling approach. ACM Transactions on Graph (TOG), 11(1):92?99. ","acronyms":[[64,67],[34,37]],"long-forms":[[38,62]]},{"text":"HLT\/EMNLP, 2005  http:\/\/www.nist.gov\/speech\/tests\/ace\/ace07\/doc, The  ACE 2007 (ACE07) Evaluated Plan, Estimation of  the Detection and Recognition of ACE Entities, Val-","acronyms":[[80,85],[0,9],[152,155]],"long-forms":[[70,78]]},{"text":"for assessments the ASR,  2. Concept F-measure (ConF) ? the F-measure of ","acronyms":[[47,51],[19,22]],"long-forms":[[28,37]]},{"text":" 1 Introduction Stanford Dependencies (SD) providing a functional characterization of the grammatical relations in","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":"Our systems use both corpus-based and knowledge-based approaches: Maximum Entropy(ME) (Lau et al, 1993; Berger et al, 1996; Ratnaparkhi, 1998) is","acronyms":[[82,84]],"long-forms":[[66,81]]},{"text":"is associated with the data sparseness problem. Most of the previously proposed methods to  extract compounds or to measure word association using mutual information (MI) either ignore  or penalize items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang ","acronyms":[[167,169]],"long-forms":[[147,165]]},{"text":"2,5 y1,5 Figure 1: The Partial Lattice MRF (PL-MRF) Model for a 5-word sentence and a 4-layer lattice.","acronyms":[[44,50]],"long-forms":[[23,42]]},{"text":"When these approaches are applied to normal Twitter users accuracy results significantly decrease.  Sentiment Analysis (SA) has been widely studied in the last decade in multiple domains. Most work","acronyms":[[120,122]],"long-forms":[[100,118]]},{"text":"few labels. In Advances in Societal Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192?199.","acronyms":[[64,70]],"long-forms":[[15,62]]},{"text":"Model of Argumentation. Proceedings of American Association .for  Artificial Intelligence (AAAI) Conference: 313-315. ","acronyms":[[91,95]],"long-forms":[[39,89]]},{"text":"tion system; then the output is sorted into a  small number of domain-specific classes telephoned  Spheres Acts (DAs) that can indicate directly to  the dispatcher the general intended meaning of ","acronyms":[[112,115]],"long-forms":[[99,110]]},{"text":"between interdependent ? E steps (as might arise for an  assumption such as ((ANB)?C)). It is straightforward ","acronyms":[[78,84]],"long-forms":[[57,72]]},{"text":"are evaluated utilise nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are appraisals using nuggets from citation texts (CT). ","acronyms":[[143,145],[55,57],[74,76],[83,85]],"long-forms":[[127,141],[39,53],[63,72]]},{"text":"al., 2008; NIST, 2008). We evaluate names (NAM) mentions for cross-lingual person (PER) and organi-","acronyms":[[42,45],[11,15],[82,85]],"long-forms":[[36,40],[74,80]]},{"text":" 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly","acronyms":[[41,44],[7,10]],"long-forms":[[11,39]]},{"text":"We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolved 807","acronyms":[[77,79]],"long-forms":[[61,75]]},{"text":"our method in this domain. The analysis of variance (ANOVA) and Tukey?s honestly significant differences (HSD) tests on the classification accuracies","acronyms":[[53,58],[106,109]],"long-forms":[[31,51],[72,104]]},{"text":"left AV and right AV. For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in","acronyms":[[89,92],[5,7],[18,20]],"long-forms":[[66,87]]},{"text":"guished in Boxer: 1. Discourse Representative Structures (DRSs) 2.","acronyms":[[58,62]],"long-forms":[[21,56]]},{"text":"In STS, we encoded only similarity feature between the two sentences. Thus, we used two classes of kernels: (1) the syntactic\/semantic class (SS) with the final kernel defined as K(p 1","acronyms":[[142,144],[3,6]],"long-forms":[[126,140]]},{"text":"1 3 Note that there is only one column for recall, which is unaffected by the choice o f Matched\/Missing (M\/M) versus All Templates (AT) . ","acronyms":[[133,135],[106,109]],"long-forms":[[118,131],[89,104]]},{"text":" 1 Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al.,","acronyms":[[36,38],[70,77]],"long-forms":[[16,34]]},{"text":"natural language utterances. It accesses a database typi-  cal for the information retrieval task (CORDIS). ","acronyms":[[99,105]],"long-forms":[[59,97]]},{"text":"tribution from most features but C1 seems to benefit more from GENIA named entity tagged, Human Phenotype Ontology (HPO), Fundamentals Model of Anatomy (FMA) and","acronyms":[[117,120],[33,35],[63,68],[152,155]],"long-forms":[[91,115],[123,150]]},{"text":"229  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26, Sofia, Bulgaria, August 9, 2013.","acronyms":[[70,77]],"long-forms":[[36,68]]},{"text":"  We extracted bag-of-word features and trained a  Support Vector Appliance (SVM) classifier (Burges, 1998) using the above dataset.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW). Chalkboard 2 displays the","acronyms":[[119,121]],"long-forms":[[102,110]]},{"text":"(section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity. ","acronyms":[[47,49]],"long-forms":[[50,71]]},{"text":"Mean values (with standard deviations) of each of the first eight features on each sub-corpus are displayed in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value does not differ significantly between the original and simplified versions of the texts in any of the four","acronyms":[[161,167]],"long-forms":[[134,153]]},{"text":" B. MTE features We use the following MTE metrics (MTFEATS), which compare the similarity between the question and a nominee replying:","acronyms":[[51,58],[4,7],[38,41]],"long-forms":[[42,49]]},{"text":"the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: volume, velocity, and variety1. Natural Language Processing (NLP) methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source.","acronyms":[[187,190]],"long-forms":[[158,185]]},{"text":"follows. NLC(:A): the analysis of concepts that play  a role in natural anguage; (NL)CA: the lattice the-  Karaphuis and Sarbo 205 Natural Language Concept Analysis ","acronyms":[[82,84],[9,12],[85,87]],"long-forms":[[64,79]]},{"text":"3.2 Graph-based Label Propagation Graph-based label propagation, a fundamental subclass of semi-supervised learning (SSL), has been heavily used and shown to outperform other SSL meth-","acronyms":[[114,117],[171,174]],"long-forms":[[88,112]]},{"text":"Hyderabad, India Abstract Named Entity Recognition(NER) is the task of identifying and classifying tokens in a","acronyms":[[51,54]],"long-forms":[[26,49]]},{"text":" 1 Introduction  When a natural language processing (NLP) system  is created in a modular manner, it can be relatively ","acronyms":[[53,56]],"long-forms":[[24,51]]},{"text":"ifications) to be the first on the COMPS list, and further assigns a positive value for an additional feature INV (inverted) on verbs. This feature may","acronyms":[[110,113],[35,40]],"long-forms":[[115,123]]},{"text":"MusicalArtist:album or Book:genre. So, in addition to recognising entities with Stanford NERC, we also execution our own named entity recogniser (NER), which only recognises entity frontiers, but does not classify them.","acronyms":[[153,156],[96,100]],"long-forms":[[128,151]]},{"text":"Two categories of neural networks have been used for linguistics modeling: 1) recurring neural networks (RNN), and 2) feedfoward network (FFN):","acronyms":[[102,105],[135,138]],"long-forms":[[75,100],[115,133]]},{"text":"programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).  The alignment ","acronyms":[[122,125],[55,58]],"long-forms":[[111,120],[43,53]]},{"text":"NIST-06. The bilingual training corpus comes from Language Data Consortium (LDC)6, which consists of 3.4M sentence pair with 64M\/70M Chi-","acronyms":[[78,81],[0,7]],"long-forms":[[50,76]]},{"text":"ailehor ~  The following entrance is associated with the class of  verbs taking an NP as indirect objects(IOBJ) which  may be perhaps found within a prepositional phrase or ","acronyms":[[103,107],[80,82]],"long-forms":[[86,101]]},{"text":"CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07 CoTrain vs. SVM(CN) 5.7E-08 2.91E-09 2.27E-11 CoTrain vs. SVM(EN) 3.74E-15 5.77E-17 1.18E-20","acronyms":[[110,112],[12,15],[16,18],[59,62],[63,65],[152,155],[156,158]],"long-forms":[[94,101]]},{"text":"Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff system, REF = reference). OOVs and their trans-","acronyms":[[96,99],[32,35],[46,50],[70,77],[114,118]],"long-forms":[[102,111],[38,44],[53,61],[80,87]]},{"text":"paper is the following:  Def. A generative system (GS) is a sequences TI,... ,Tn of TS,  whe~'~-TR(Tl,... ,Tn) is a relation between string and D-trees and ","acronyms":[[51,53],[83,85],[95,97]],"long-forms":[[32,49]]},{"text":" 1 Introduction Foundations Phrase Chunking (BPC), also known as shallow syntactic parsing, is the process by which ad-","acronyms":[[38,41]],"long-forms":[[16,36]]},{"text":"Studies such as Cinque (2006) and Rizzi  (1999) propose detailed functional phrases such  as TopP (Topic Phrase) in order to fully describe  the syntactic structures of a language.","acronyms":[[93,97]],"long-forms":[[99,111]]},{"text":"? ? ? ?  Figure 2: Laten Event Model (LEM). ","acronyms":[[38,41]],"long-forms":[[19,36]]},{"text":"In Proceedings of the 2012 ACM Interntional Conference on Shrewd User Interfaces (IUI), pages 189?198. ","acronyms":[[87,90],[27,30]],"long-forms":[[58,85]]},{"text":"CoTrain vs. BaseCN2 0.000144 4.77E-05 0.000247 CoTrain vs. BaseCN3 0.0009 0.000287 0.00139 CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07","acronyms":[[107,109],[150,153],[154,156]],"long-forms":[[91,98]]},{"text":"+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system","acronyms":[[91,93],[104,106],[125,127]],"long-forms":[[94,102],[107,123],[128,143]]},{"text":"During recent decades, this concept emerges in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. ","acronyms":[[204,207]],"long-forms":[[209,238]]},{"text":"teams employed vector-based linear classifiers of different types: Hacioglu et al (2004) and Park et al (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al (2004) used Voted Percep-","acronyms":[[141,144]],"long-forms":[[116,139]]},{"text":"proach to automatically recognises predicate  heads of Chinese sentences bases on a preprocessing step for maximal noun phrases 1(MNPs). ","acronyms":[[129,133]],"long-forms":[[106,126]]},{"text":"Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional","acronyms":[[151,154],[157,159]],"long-forms":[[138,149]]},{"text":" As far as discriminative models are concerned,  the Maximum Entropy (MaxEnt) model has been  applied (Bohus and Rudnicky, 2006).","acronyms":[[70,76]],"long-forms":[[53,68]]},{"text":"frequency pairs, Erk et al(2010) drew (R, t)  pairs from each of five frequency bands in the  entire British National Corpus (BNC):  50-100  occurrences; 101-200; 201-500; 500-1000; and ","acronyms":[[126,129]],"long-forms":[[101,124]]},{"text":"Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, Mike Rosner, & Daniel Tapias, 310?314, Valletta, Malta. European Language Resources Association (ELRA).Ferra?ndez, Oscar, Michael Ellsworth, Rafael Mun?oz, & Collin F. Baker. 2010b.","acronyms":[[183,187]],"long-forms":[[142,181]]},{"text":"corporates contracts features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional feature.","acronyms":[[95,97],[45,47],[113,115]],"long-forms":[[87,93]]},{"text":"Under the former, they include  personal pronouns, sentential \" i t ,\"  and null-comple-  ment anaphora, and under the latter, verb phrase (VP)  ellipsis, sluicing, gapping, and stripping.","acronyms":[[140,142]],"long-forms":[[127,138]]},{"text":"to facilitate understanding. We utilizing latent Dirichlet alocation (LDA) (Blei, Ng, and Jordan, 2003) as our exploratory tool.","acronyms":[[65,68]],"long-forms":[[37,63]]},{"text":"morphological types and variables.  The Encyclopedia Specialist (ES) is able to access the Encyclo-  pedia for extracting semantic information and world knowledge.","acronyms":[[65,67]],"long-forms":[[40,63]]},{"text":"as director of Pentagon telecom~~nicntions and commc~nd :~nd supervisory systcms.  Requests for coments on a propose Federal information processing -t.lndard (PII'S)  for the National Communications System have been asked by .Innuor).","acronyms":[[156,161]],"long-forms":[[105,144]]},{"text":" 2 Abstract Syntax Trees We describes abstract syntax trees (ASTs) using an example from CFR Section 610.11:","acronyms":[[60,64],[88,91]],"long-forms":[[37,58]]},{"text":"A necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference (NLI): the task of determining whether a natural-language","acronyms":[[138,141]],"long-forms":[[110,136]]},{"text":"by using multiple learners and a label integrator.  We have developed a forward (FR) and a backward relationship (BR) learner to learn relation-","acronyms":[[81,83],[114,116]],"long-forms":[[72,79],[91,112]]},{"text":"? plane?. We will call this the GN (general noun) lexicon.","acronyms":[[35,37]],"long-forms":[[39,51]]},{"text":"1 Quantification resolution We are apprehensive with ambiguously quantified noun phrases (NPs) and their interpretation, as showed by the following examples:","acronyms":[[87,90]],"long-forms":[[73,85]]},{"text":"tance Metric from Relative Comparisons. Advances in Neural Information Processing Systems (NIPS).. J. Weeds, D. Weir and D. McCarthy.","acronyms":[[91,95]],"long-forms":[[52,89]]},{"text":"Pour each disjunetiort in indef.\"  Let compatible-disjuncts = CHECK-DISJ (disjunction, cond). ","acronyms":[[61,71]],"long-forms":[[73,84]]},{"text":" ? Left Reveal (LRev) : Pop the top two nodes in the stack (left, right).","acronyms":[[16,20]],"long-forms":[[3,14]]},{"text":"We also have examined our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) (Xue et al.,","acronyms":[[102,105]],"long-forms":[[84,100]]},{"text":" (5) Rank Value:  i. Top Ranking (T-Rank): The rank of snippet  that first contains the candidate.","acronyms":[[31,37]],"long-forms":[[21,29]]},{"text":"As the entire sentences is informative to deciding the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that improves both syntactic and lexical in-","acronyms":[[120,124]],"long-forms":[[90,118]]},{"text":"(i) string kernels applied to sentences, or PTK applied to structural representations with and without embedded relational information (REL). This","acronyms":[[136,139],[44,47]],"long-forms":[[112,122]]},{"text":"(Vehicle). Each mention of an entity has a mention  genera: NAM (proper name), NOM (nominal) or                                                            ","acronyms":[[77,80]],"long-forms":[[82,89]]},{"text":" In Table 3 the number of questions that get a higher and lower reciprocal classifying (RR) after applying the individual lexico-semantic funds are","acronyms":[[81,83]],"long-forms":[[64,79]]},{"text":"Figure 1: An entity and relation example (Roth and Yih, 2004). Person (FOR) and location (LOC) entities are connected by Viva in and Located in","acronyms":[[71,74],[90,93]],"long-forms":[[63,69],[80,88]]},{"text":"This paper exposes the UNITOR system that participated in the *SEM 2013 exchanges task on Semantic Textual Similarity (STS). The task is","acronyms":[[117,120],[24,30],[64,67]],"long-forms":[[88,115]]},{"text":"For the bilingual corpus, we  used the BTEC and PIVOT data of IWSLT 2008,  HIT corpus 5  and other Chinese LDC (CLDC)                                                            ","acronyms":[[112,116],[39,43],[48,53],[62,67],[75,78]],"long-forms":[[99,110]]},{"text":"CP = Relating Pronoun  +  IP  PP = Preposition  + NP AdjP = Adjective + NP","acronyms":[[30,32]],"long-forms":[[35,46]]},{"text":"to the Markov Logic system. At each step, we compute both the maximum a posteriori (MAP) assignment of coreference relationships as well","acronyms":[[84,87]],"long-forms":[[62,82]]},{"text":"In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 1?5. ","acronyms":[[101,111]],"long-forms":[[22,99]]},{"text":"(una~.TG, ~ROC)I ^  (SUBS, SUSU, VT),  (ARTG, PR.OC)I ^  (SUBS, SUSU, FAIRMONT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment ","acronyms":[[76,80],[99,103],[4,6],[9,12],[19,23],[25,29],[31,33],[38,42],[44,49],[56,60],[62,66],[68,70]],"long-forms":[[83,97],[106,116]]},{"text":"least than linear is the sampled size, m. We formalize  this as a variant of \"Set Cover\" problem which we call  \"Factored Set Cover~(WSC), and prove the existence of  an approximation algorithm with a performance guar- ","acronyms":[[131,134]],"long-forms":[[111,129]]},{"text":"Translation  The first method compared is a transitive  translation utilize MT (machine translation). The ","acronyms":[[74,76]],"long-forms":[[78,97]]},{"text":"tation from a user?s speech. That is, it consists of automatic speech recognition (ASR) and language understanding (LU).","acronyms":[[83,86],[116,118]],"long-forms":[[53,81],[92,114]]},{"text":"media, where evidences for both actions and ties are available. We begun by necessary description of preliminaries and notations, we then present the mutual latent random graphics (MLRGs) model, upon which both sources of testimony could be exploited simultaneously to capture their mutual impacts.","acronyms":[[179,184]],"long-forms":[[150,177]]},{"text":"PCFG = Probabilistic  Context-Free Grammar, LM = Bigram Model with Witten-Bell smoothing,  PM = Priorities Model. ","acronyms":[[91,93],[0,4],[44,46]],"long-forms":[[96,110],[7,42],[49,61]]},{"text":"(TEMPO1) and YOUTH-TIME (TEMPO2) where the chooses is a secor~d  edict Pa~t and for the pair YOUTH-TIME (TEMPO2) and BUILDING-  TIME (TEMPO3) where the selection is a third order F~Jture. As a ","acronyms":[[133,139]],"long-forms":[[127,131]]},{"text":" 4.3 Spanish?English (ES?EN), French?English (FR?EN) In Tableau 3, we consults that on the ES?EN and","acronyms":[[46,51]],"long-forms":[[30,44]]},{"text":" In addition, the tense,                                                             1 S=Subject; IO=Indirect Object; DO=Direct Objects;  V=Verb; ERG=Ergative; DAT=Dative ","acronyms":[[98,100],[118,120],[145,148],[159,162]],"long-forms":[[101,116],[121,134],[89,96],[139,143],[149,157],[163,169]]},{"text":"make assertions that personal pronouns like \\she\" cannot co-refer with \\company\".  In MUC-7, we developed a word sense disambiguation (WSD) module, which removes some of the implausible senses from the list of potential senses.","acronyms":[[135,138],[86,91]],"long-forms":[[108,133]]},{"text":"There are four goodness algorithms inspect by Zhao and Kit (2008a). The algorithms, comprises Description Length Gained (DLG) (Kit and Wilks 1999), Accessor Variety (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii","acronyms":[[120,123]],"long-forms":[[95,118]]},{"text":"  At example, the s t ruc ture  tn whtch  ad jec t ives  (ADJ) restate  a rb i t ra ry  ttmes and a noun  (N) copilot l lows  them tn Engl lsh ts expressed as ","acronyms":[[59,62]],"long-forms":[[43,56],[100,104]]},{"text":"representation. Thus, it is necessity to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  Englishmen ECS (E-ECS) \\[7\\] for English language.","acronyms":[[95,100],[57,60],[148,153]],"long-forms":[[81,93],[135,146]]},{"text":"Table 2: Key particulars of semantic orientation (SO) lexicons. ASL = affix seeds thesaurus, GI = Generals Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN = SentiWordNet, TLL = Turney?Littman lexicon.","acronyms":[[110,114],[157,160],[46,48],[60,63],[87,89],[190,193],[210,213]],"long-forms":[[117,155],[163,188],[24,44],[66,85],[92,108],[196,208],[216,238]]},{"text":"labeled DGs.  (C3) Projectivity constraint(PJC): No cra crosses another arc5","acronyms":[[43,46],[8,11]],"long-forms":[[19,41]]},{"text":"nym, Instance Hypernym, Part Holonym, Member Holonym, Substance Meronym, Entailment Table 1: Similarity specifics using WordNet (WN). ","acronyms":[[128,130]],"long-forms":[[119,126]]},{"text":" For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until","acronyms":[[83,86],[9,12],[136,142]],"long-forms":[[57,81]]},{"text":"ability and to give analytical insights into the  features. Classification Accuracy (CAcc), the  percentage of the correctly labeled instances over ","acronyms":[[85,89]],"long-forms":[[60,83]]},{"text":"and only these. The feasible binary tree architecture of ABCD are cover by  ABCD = A(BCD) U (AB)(CD) U (ABC)D.  Since we are to handle binary concatenations only, we consider two concatenations ","acronyms":[[76,80],[55,59]],"long-forms":[[83,88]]},{"text":"tion. So we developed a method to optimize the CRFs towards the alignment error rate (AER) or the F-score with sure and possible links as introduced","acronyms":[[86,89]],"long-forms":[[64,84]]},{"text":"However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).","acronyms":[[124,127]],"long-forms":[[95,122]]},{"text":"fier. We likewise implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the goals and co-occurring","acronyms":[[95,98]],"long-forms":[[65,93]]},{"text":"NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective  Table h Patterns for partOf(basement,building) ","acronyms":[[70,73],[0,2],[11,16],[32,35],[50,54],[88,91]],"long-forms":[[76,86],[5,9],[19,30],[38,48],[57,68],[94,103]]},{"text":"ual resources on pairwise comparison task (Diff. = Troubles lexicon, CF = Crowdflower) Features","acronyms":[[71,73]],"long-forms":[[76,87]]},{"text":"PoS tagF5. Lemma (L)F6. Inflection (INFL)F7. Main verb of main provisions (MV)F8.","acronyms":[[36,40],[0,3],[71,73]],"long-forms":[[24,34],[11,16],[45,54]]},{"text":"belief from the belief tracker in 6.3% of the dialogs.  The mean Word Error Rate (WER) per worker on the test set is 27.5%.","acronyms":[[82,85]],"long-forms":[[65,80]]},{"text":"Prevalence and count of conditions by temporal  category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department, ","acronyms":[[75,77]],"long-forms":[[80,97]]},{"text":" The output is a prediction of whether the tweet is interior region (IR) or outside region (OR). We","acronyms":[[67,69],[90,92]],"long-forms":[[52,65],[74,88]]},{"text":"tor machines: learning with many relevant features. In European Conference on Machine Learning (ECML). ","acronyms":[[96,100]],"long-forms":[[55,94]]},{"text":"The availability of massive greatness data sets of manuals annotated predicate?argument structures has recently preferred the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices","acronyms":[[205,208]],"long-forms":[[181,203]]},{"text":"Discourses Relations (DR) 48.04 Entity Grid (EG) 67.74 Lexical Cohesion (LC) 61.63 Document Length 69.40","acronyms":[[72,74],[21,23],[44,46]],"long-forms":[[54,70],[0,19],[31,42]]},{"text":"information. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 317?325, Columbus, USA.","acronyms":[[100,103],[131,134]],"long-forms":[[57,98]]},{"text":"........ ? ................... ? ............... + ................... +  CLS = Clause NP = Noun Phrase (BAR 2)  PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL ","acronyms":[[87,89],[74,77],[105,108],[113,117],[130,135],[165,167]],"long-forms":[[92,103],[80,86],[120,129],[139,146]]},{"text":"248   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295, October 25-29, 2014, Doha, Qatar.","acronyms":[[94,99]],"long-forms":[[44,92]]},{"text":"Frustration Frustrated (F), Neutral (N), Correctness Correct (C), Incorrect (I) Partial Correct (PC) Percent Correct 50-100% (High), 0-50% (Low)","acronyms":[[99,101]],"long-forms":[[80,97],[12,22],[28,35],[41,60],[66,75]]},{"text":"certainty factor equals to 0.6 (3\/5). The general formula for the certainty factor (CF) is shown as follow: CFi = Total number of the answer elements at leaf node i","acronyms":[[84,86],[108,110]],"long-forms":[[66,82]]},{"text":"placed in a transcript. Here, we focus on the syndrome of primary progressive aphasia (PPA). PPA","acronyms":[[87,90],[93,96]],"long-forms":[[58,85]]},{"text":".  Reattachment Heuristic (RH) targets nonargument head errors that occur if a TL argument","acronyms":[[27,29],[79,81]],"long-forms":[[3,25]]},{"text":" 260 SentiWordNet(SWN) (Baccianella et al., ","acronyms":[[18,21]],"long-forms":[[5,16]]},{"text":"ENTITY_A appos Figure 2: Dependency tree (DT) for the entity blinded sentence ?","acronyms":[[42,44]],"long-forms":[[25,40]]},{"text":"Question is defined as a Question term (QTerm).  The Answer Term (ATerm) is the Answer given by the KM corpus.","acronyms":[[66,71],[40,45],[100,102]],"long-forms":[[53,64],[25,38]]},{"text":"We used four appliance learning algorithms implemented in Mallet (McCallum, 2002): decision tree, Naive Bayes, maximum entropy (MaxEnt), and conditional random field (CRF).5 Table 4 exhibitions the","acronyms":[[126,132],[165,168]],"long-forms":[[109,124],[139,163]]},{"text":" 2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can","acronyms":[[71,78],[17,23]],"long-forms":[[44,69]]},{"text":"transcription is carried out by using vibrant  programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is utilize as reference (REF).","acronyms":[[102,105],[169,172]],"long-forms":[[90,100],[158,167]]},{"text":"trol agreement principle.  Consider first the foot feature principle (FFP). ","acronyms":[[70,73]],"long-forms":[[46,68]]},{"text":" 3.2 Latent Structural SVM We employ the latent structural SVM (LS-SVM) model for learning the discriminative model of query","acronyms":[[64,70],[23,26]],"long-forms":[[41,62]]},{"text":"actual dependency annotations cheaply. We uses the Graph Fragment Parlance (GFL), which was created with the goal of making annotations eas-","acronyms":[[75,78]],"long-forms":[[50,73]]},{"text":"The \u0002 grams in an utterance SSG can be extracted by converting it to a finite state transducer (FST), \f\u000e  .","acronyms":[[96,99],[28,31]],"long-forms":[[71,94]]},{"text":"construct such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT)  (Carlson et al, 2001) is an available resource to ","acronyms":[[77,83]],"long-forms":[[53,75]]},{"text":"1. Grider, T., Mosley, H., Snow, L, and Wilson, W., \"Users Manual for the  Dynamic Analytical Replanning Tool (DRAFT)\", prepared for BBN by  Systems Research and Applications Corporation, 9 November 1990.","acronyms":[[111,116]],"long-forms":[[75,109]]},{"text":"and so on. Recently, sequential learning methods, such as hidden Markov modelled (HMMs) and conditional random fields (CRFs), have been uses suc-","acronyms":[[80,84]],"long-forms":[[58,78]]},{"text":" 3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA, simply using the anaphoricity information it pro-","acronyms":[[42,45],[73,75]],"long-forms":[[27,40]]},{"text":"following formula.  PPe = Pe(Se)? ","acronyms":[[21,24]],"long-forms":[[27,32]]},{"text":"From every argument to the predicate, we extract all child  noun phrases (NP) and adjectival phrases (ADJP)  as candidate gaps as well.","acronyms":[[74,76],[102,106]],"long-forms":[[60,72],[82,100]]},{"text":"easily inspectable. The generalizing ability of the evolutionary reinforcement learning (RL) algorithm, XCS, can dramatically reduce the size of the opti-","acronyms":[[89,91],[104,107]],"long-forms":[[65,87]]},{"text":"ing different linguistic aspects and an effective way to combine such information is made available by Support Vector (SV) regression, described in (Smola and Scho?lkopf, 2004).","acronyms":[[119,121]],"long-forms":[[103,117]]},{"text":"Deep-syntactic structural (DSyntSs);  ? Surface syntactic structures (SSyntSs);  61 ","acronyms":[[70,77]],"long-forms":[[40,68]]},{"text":"If we look at the permutations, we have in 2. to-  picalization, with OBJect NP in focus structure (FS) I in I. the grammatical relations of 25 are preserved ","acronyms":[[100,102],[77,79]],"long-forms":[[83,98]]},{"text":"(Pierce and Cardie, 2001). A related idea is to utilise Expectation Maximization (EM) to impute labels.","acronyms":[[78,80]],"long-forms":[[52,76]]},{"text":"4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Pager) dataset. ","acronyms":[[77,82],[38,41]],"long-forms":[[84,96]]},{"text":"sible transliteration candidates. We measured performance using the Mean Reciprocal Rank (MRR) measures.","acronyms":[[90,93]],"long-forms":[[68,88]]},{"text":"see chapter 4.3.  WIV(1): Weighted Identity Value (with the weight 1):  see chapter 2.2.","acronyms":[[18,21]],"long-forms":[[26,49]]},{"text":"Valerio Basil and Johan Bos and Kilian Evang and Noortje Venhuizen {v.basile,johan.bos,k.evang,n.j.venhuizen}@carpeting.nl Center for Linguistic and Cognition Groningen (CLCG) University of Groningen, The Netherlands","acronyms":[[163,167]],"long-forms":[[118,161]]},{"text":"Proceedings of the Fourteenth  International Joint Conference in Artificial  Intellect (IJCAI?95), pp. 1395 ?","acronyms":[[91,99]],"long-forms":[[77,89]]},{"text":" 3.2 Formal foundation: Lexical Remedies Semantics Lexical Resource Semantics (LRS) (Richter and Sailer, 2003) is an underspecified semantic formal-","acronyms":[[74,77]],"long-forms":[[46,72]]},{"text":"mantic traits called the latent topic feature, which is extracted by exploit the Latent Dirichlet Allocation (LDA) algorithm. Unlike syntactic fea-","acronyms":[[114,117]],"long-forms":[[85,112]]},{"text":"require. We solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a PAS.","acronyms":[[78,82]],"long-forms":[[48,76]]},{"text":"2 Classifiers 2.1 NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996).","acronyms":[[33,35]],"long-forms":[[21,31]]},{"text":"This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394","acronyms":[[76,79],[28,30]],"long-forms":[[48,74]]},{"text":"to cover terms frequently utilizes by students, such as acronyms: E.L.C. (the English Language Center), R.O.C. (Republic of China), and so on. Else","acronyms":[[100,106],[62,67]],"long-forms":[[108,125],[74,97]]},{"text":"measure specifically their performance, a select of them is also yielded: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the","acronyms":[[127,132],[87,90],[104,107],[152,157]],"long-forms":[[110,125],[75,85],[93,102],[135,150]]},{"text":"nuqaqa llakiy qhachqa p?achakunata churakurqani.  Abbreviations: AMLQ = Academia Mayor de la Lengua Quechua en Cusco, norm = normalized, span = Spanish orthography, boliv = (old) Bolivian orthography Table 1: Different Orthographies with Corresponding Standardized Version","acronyms":[[65,69]],"long-forms":[[72,107]]},{"text":"as a statistical model of natural anguage, t and weak-  eus Jelinek et al's contention that \"in an ambiguous  but appropriately chosen probabilistic CFG (PCFG),  correct parses are Ifigh probability parses\" (p. 2).","acronyms":[[154,158]],"long-forms":[[135,152]]},{"text":"We show each sentence to three unique labour on Amazon Mechanical Turk (MTurk) and ask each to judge how good the paraphrase retains the mean-","acronyms":[[73,78]],"long-forms":[[56,71]]},{"text":"subset by eliminating the redundant features.  In this paper, Rough Set Theory (RST) based  feature selection method is applied for sen-","acronyms":[[80,83]],"long-forms":[[62,78]]},{"text":"The language is defined by about 50 simpler grammar rules. ? P5E2N3S4, F W A Standards Language (SLANG). See Section 4.1. ?","acronyms":[[95,100]],"long-forms":[[76,93]]},{"text":"We have been emphasis on the Hk  Kong Hansard, which are the parliamentary pro-  ceedings of the Legislative Council (LegCo). Anal- ","acronyms":[[125,130]],"long-forms":[[104,123]]},{"text":"tools freely available for many languages. That is the case for morphosyntactic analyzers (MSA), but not yet for full or even shallow parsers.","acronyms":[[91,94]],"long-forms":[[64,89]]},{"text":"approaches (Gasic and Jeune, 2011; Lee and  Eskenazi, 2012; Williams, 2010; Young et al  2010) and Bayesian network (BN)-predicated  methods (Raux and Ma, 2011; Thomson and ","acronyms":[[117,119]],"long-forms":[[99,115]]},{"text":"not be effective due to the brevity of contributions.  Barzilay and Lee?s algorithm (B&L) did not  generalize well to either dialogue corpus.","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":"renowned as NP The corpus of English Wikipedia pages, known as EnWiki NP ( * NP) Disguising Markov Model (HMM) is used to solve ...","acronyms":[[99,102],[74,76],[67,69],[9,11]],"long-forms":[[78,97]]},{"text":"j.nerbonne@rug.nl Abstract Pair Hiding Markov Models (PairHMMs) are trained to align the pronunciation tran-","acronyms":[[54,62]],"long-forms":[[27,52]]},{"text":"Abstract We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from deux treebanks by using","acronyms":[[81,84]],"long-forms":[[56,79]]},{"text":"translations. Although initially intended for  learners of English as Foreign Language (EFL)  in Taiwan, it is a gold mine of texts in English ","acronyms":[[88,91]],"long-forms":[[59,86]]},{"text":"syntactically correct) to 1.0 (completely wrong).  ISER (information item semantic error rate): The test verdicts are segmented into information topics; for each of these items, the translation candidates","acronyms":[[51,55]],"long-forms":[[57,93]]},{"text":"transcripts of user utterances, and included lexical, syntactic, numeric, and traits from the output of Linguistic Probing and Word Count (LIWC) (Pennebaker et al.,","acronyms":[[141,145]],"long-forms":[[106,139]]},{"text":"! JIM: { Person67 \/ Person83 \/ Name18 \/ (TYPE=&Person, SEX=Male, NAME=NamelS) }  Finally, the association between grammatical functions and ","acronyms":[[65,69]],"long-forms":[[70,76]]},{"text":"take et al (2005)) for the representation of meaning.  LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copestake, 2002), a development environment for constraint-based grammars.","acronyms":[[112,115],[55,61]],"long-forms":[[82,110]]},{"text":"special lawsuit describing whether\/t \/  is glottalized. A context other than preceding phoneme and following  phoneme is embedded; the first split (nodes 0 and 10) in this tree is on syllable restricts (SYLL-BDRY),  explaining that when \/ t \/  is glottalized it is generally in syllable-final position.","acronyms":[[203,212]],"long-forms":[[184,201]]},{"text":" Entities On the level of entity extraction, Named Entities (NE) were defined as proper names and quantities of interest. ","acronyms":[[61,63]],"long-forms":[[45,59]]},{"text":"The most  important forms of discourse of interest o  Natural Language Processing (NLP) are text  and dialogue.","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"our system; where we accomplished 0.627 top1 accuracy for Japanese transliterated to Japanese Kanji(DD), 0.713 for English-toChinese(E2C) and 0.510 for English-to-","acronyms":[[96,98],[129,132]],"long-forms":[[81,94],[111,128]]},{"text":"every lost arc translates to a set of lost parts, we can avoid repeating calculates by shops the partial loss of every arc in a data structure (DS): e ?? ","acronyms":[[148,150]],"long-forms":[[132,146]]},{"text":"where C is the concept that subsumes both C1 and C2 and has the highest information content (i.e., it is the least common subsumer (LCS)). ","acronyms":[[132,135]],"long-forms":[[109,130]]},{"text":"EUD1.2 has the added benefit of being natively annotated with gold-standard Universal Dependencies (UD) parses (Nivre et al, 2015).","acronyms":[[100,102],[0,3]],"long-forms":[[76,98]]},{"text":"sub-tasks: ? Multimedia Information Network (MiNet) Construction: Construct MiNet from cross-media and cross-genre information (i.e. tweets, images, sentences of web doc-","acronyms":[[45,50],[76,81]],"long-forms":[[13,43]]},{"text":"some programmes P such that, if H executes P. then in the re-  sulting state, there exists a \\['F discernible term P' such  that H knows that Denotation(Pl = Dem;tation(DI),  and 5\" intends that H running P. ","acronyms":[[164,166]],"long-forms":[[153,163]]},{"text":"We performed the same experiments on three dif-  ferent corpora:  Corpus SN (Spanish Novel) train: 15Kw, test:  2Kw, tag set size: 70.","acronyms":[[73,75]],"long-forms":[[77,90]]},{"text":"generative models which are respectively estimated  on their corresponding named entity lists using  maximum likelihood estimation (MLE), together  with smoothing methods4.","acronyms":[[132,135]],"long-forms":[[101,130]]},{"text":"ily' (lists truncated). Scoring = log-likelihood score; f = occurrence frequency of keyterm; NN = noun; VV = verb; AR =  article; AP = article+preposition; JJ = qualifier; CC = con-","acronyms":[[91,93],[24,29],[102,104],[113,115],[128,130],[154,156],[170,172]],"long-forms":[[96,100],[69,78],[47,52],[107,111],[119,126],[133,152],[159,168]]},{"text":"Visweswariah et al (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words.","acronyms":[[92,95],[109,112]],"long-forms":[[64,90]]},{"text":"with parts of the annotated logical form.  \u0001 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra (disharmonic) combinators to increase the expressive power of the model.","acronyms":[[45,49],[89,93]],"long-forms":[[51,79]]},{"text":"? P5E3N3S3, F W A Computer Processable Frenchman (CPE) (Pulman 1996; Sukkarieh and Pulman 1999) is a controlled parlance that can be ?","acronyms":[[48,51],[2,10]],"long-forms":[[18,46]]},{"text":" 3.1 Identifying verbal blocks (Vbs) Verbal blocks are composed of a head (Vb-H) and possibly accompanying dependents (Vb-D).","acronyms":[[75,79],[32,35],[119,123]],"long-forms":[[37,73],[17,30]]},{"text":"The primary purpose of the toolkit is to allow students to concentrate on building natural language processing (NLP) systems.","acronyms":[[112,115]],"long-forms":[[83,110]]},{"text":"guided learning. The approach taken has been to en-  code an artificial neural network (ANN) in a genome  which stores its architecture and learning rules.","acronyms":[[88,91]],"long-forms":[[61,86]]},{"text":"Certain scholar rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theories (RST) (e.g., (Bouayad-Aga et al, 2000; Moore and Paris,","acronyms":[[139,142]],"long-forms":[[110,137]]},{"text":" It considers all pronouns (PRP, PRP$), noun phrases (NP) and heads of verb phrases (VP) as potential mentions.","acronyms":[[85,87],[28,31],[54,56],[33,37]],"long-forms":[[71,83],[40,51]]},{"text":"For example, PropBank annotates 8,037 ARGM-MNR relations (10.7%) out of 74,980 adjunct-like arguments (ARGMs). There are verbs","acronyms":[[103,108],[38,46]],"long-forms":[[92,101]]},{"text":"timing. This flexibility is in contrast to speeches output in spoken dialogue systems (SDSs) which typically engender, synthesize and deliver speech","acronyms":[[85,89]],"long-forms":[[60,83]]},{"text":" 3.2 Matching a inspected to an object Given the above review language model (RLM), we now state how to teaming a given review to an","acronyms":[[75,78]],"long-forms":[[52,73]]},{"text":"To tackle this challenge, we incorporate multiple graphs probabilistic factorization with two otherwise designed combination strategies into collaborate topic regression (CTR). Experimental results on real dataset demonstrate","acronyms":[[177,180]],"long-forms":[[145,175]]},{"text":"974   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875?879, October 25-29, 2014, Doha, Qatar.","acronyms":[[94,99]],"long-forms":[[44,92]]},{"text":"and tfidf of unigrams, bigrams, and trigrams.  DAL (Dictionary of Impacting in Language) is a psycholinguistic resources to measure the emo-","acronyms":[[47,50]],"long-forms":[[52,84]]},{"text":"are contained in a XML archive and each query  consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Matter  question(DESC),Topic Narrative(NARR) and ","acronyms":[[90,93],[19,22],[107,112],[130,134],[152,156]],"long-forms":[[83,88],[101,106],[142,151]]},{"text":" 9 Here, we present the generation-oriented PG Workbench (PGW), which assisting grammar developers, amongst other things, in testing whether the executed syntactic and lexical knowledge allows all and only well-formed permutations. In Section 2, we outlined PG?s topology-based linearizer implemented in the PGW gen-erator, whose software conceived is sketched in Section 3.","acronyms":[[58,61],[307,310],[257,261]],"long-forms":[[44,56]]},{"text":"In this section, we comparisons the running time5 of our sampled algorithm (SWIFTER) and our algorithm with the refined bucket (RB) against the unfactored Gibbs sampler (NAI?VE) and examine the effect of sorting.","acronyms":[[122,124],[73,77],[164,170]],"long-forms":[[106,120]]},{"text":"are the formal-language theoretic foundation for n-gram models (Garcia et al, 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated","acronyms":[[139,142]],"long-forms":[[110,137]]},{"text":"sentence are not participate in evaluation. Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three ","acronyms":[[87,90],[62,65],[116,119]],"long-forms":[[68,85],[44,61],[98,115]]},{"text":"Intersection (I), Union (U), (Koehn et al, 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Meaning (PMn) aligned sets where n = 5.","acronyms":[[133,136],[70,73]],"long-forms":[[121,131],[0,12],[18,23],[49,68]]},{"text":"Specifically, the groups include children with ASD with language impairment (ALI); ASD with no language impairment (ALN); SLI alone; and typically developing (TD), which is","acronyms":[[116,119],[47,50],[77,80],[122,125],[159,161]],"long-forms":[[83,114],[56,75],[137,157]]},{"text":"We show that the best  prediction of translation complexity is given by the  average number of syllables per word (ASW). The ","acronyms":[[115,118]],"long-forms":[[77,113]]},{"text":" Symptom name acknowledgment rate (RRdet),  recognition error rate (RERdet) and recognition  F-Measure (RFMdet): these metrics are designed ","acronyms":[[65,71],[32,37],[101,107]],"long-forms":[[41,63],[14,30],[77,99]]},{"text":"based on a probabilistic model. We investigate two methods using Latent Dirichlet Allocation (LDA) (Blei, 2003) in ?","acronyms":[[94,97]],"long-forms":[[65,92]]},{"text":"  ? suffixes (SUF), such as verb endings, nominal  cases, the nominal feminine termination -at, etc.;","acronyms":[[14,17]],"long-forms":[[4,12]]},{"text":"-4, -12, and -109 are all disjoint loudspeaker sets.)  (Cipher: SD=speaker dependent (2400 training sentences for RM2), MS=multi-speaker, TR=speaker independent, -4=all 4  RM2 speakers combine, -12=all 12 RM1 SD speakers combined, -109=109 RM1 SI training speakers, SDG=SD Gaussians, ","acronyms":[[59,61],[133,135],[109,111],[115,117],[167,169],[201,203],[205,207],[236,238],[240,242],[262,265],[266,268]],"long-forms":[[62,79],[136,155],[118,131]]},{"text":"following formulas.  PPe = Pe(Se)? ","acronyms":[[21,24]],"long-forms":[[27,32]]},{"text":"news headlines (headlines); mapping of lexical resources from Ontonotes to Wordnet (OnWN) and from FrameNet to WordNet (FNWN); and evaluation of machine translation (SMT).","acronyms":[[120,124]],"long-forms":[[99,118]]},{"text":" 1 Introduction Using natural language processing (NLP) techniques to mine software corpora such as code com-","acronyms":[[51,54]],"long-forms":[[22,49]]},{"text":"Into l~,ooth's approach, the FSV is detined by re-  (:ursion on the truth conditional structure which  is itself derived from LF (i.e. Logical Forms, the  Governmental and Binding level of semantic rep- ","acronyms":[[124,126],[27,30]],"long-forms":[[133,145]]},{"text":" Conf. on Language Resources and Assessed (LREC). ","acronyms":[[45,49]],"long-forms":[[10,28]]},{"text":"represent in Table 3 predicated on the place (bilabial  (BL), lab-dental (LD), dental (DE), alveopalatal  (AP), velar (VL), uvular (UV) and glottal (GT))  and manner of formulation (stops (ST), fricatives ","acronyms":[[146,148],[54,56],[71,73],[84,86],[104,106],[116,118],[129,131],[187,189]],"long-forms":[[137,144],[43,51],[59,69],[76,82],[89,101],[109,114],[121,127],[180,185]]},{"text":"CCG, as well as others.  A combinatory categorial grammar (CCG) is a categorial grammar who rule system consists of","acronyms":[[59,62],[0,3]],"long-forms":[[27,57]]},{"text":"The word nchi is dis-  ambiguated with a rule relying on the Ncl of the  following genitive connector (GEN-CON). ","acronyms":[[103,110],[61,64]],"long-forms":[[83,101]]},{"text":" 1 Introduction Electroencephalography (EEG) and magnetoencephalography (MEG) are similar methods for","acronyms":[[40,43],[73,76]],"long-forms":[[16,38],[49,71]]},{"text":"1. Introduction  Onto a Spoken Language System (SLS) we must use all avail-  able knowledge wellspring (KSs) to decide on the spoken sen- ","acronyms":[[46,49],[99,102]],"long-forms":[[22,44],[80,97]]},{"text":"tributed to trait in the translation. For instance, Emma (EM) seems very difficult to align, which can be attributed to the utilise of an old transla-","acronyms":[[67,69]],"long-forms":[[61,65]]},{"text":"tiguous correspondence. The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) mentioned by (Zhang et al, 2008a) can be regarded","acronyms":[[93,96],[56,59]],"long-forms":[[65,91],[28,54]]},{"text":" 4.3 Spanish?English (ES?EN), French?English (FR?EN) In Table 3, we see that on the ES?EN and","acronyms":[[46,51]],"long-forms":[[30,44]]},{"text":"Pattern Pattern Patterns composed of high frequency words (HFWs) 4","acronyms":[[59,63]],"long-forms":[[37,57]]},{"text":"gle word maze); B-M (beginning of multi-word 72 maze); I-M (in multi-word maze); and E-M (end of multi-word maze).","acronyms":[[55,58],[16,19],[85,88]],"long-forms":[[60,68],[21,39],[90,103]]},{"text":"eling the sequential characters of the output. The constraint satisfaction inference (CSInf) approach (Bosch and Canisius, 2006) improves the","acronyms":[[82,87]],"long-forms":[[47,80]]},{"text":" Preclinical data have supported the use of  fludarabine and cyclophosphamide (FC) in  combination for the treatment of indolent ","acronyms":[[79,81]],"long-forms":[[45,77]]},{"text":"dimensional space, in which both texts and terms are represented by means of Domain Vectors (DVs), i.e. vectors representing the domain relevances entre the language object and","acronyms":[[93,96]],"long-forms":[[77,91]]},{"text":"participant Japanese systems were developed in a four-  month period of time and output results comparable to the Message Understanding Conference-6 (MUC-6) \\[1\\]  English language systems with F-Measures between 70 - ","acronyms":[[150,155]],"long-forms":[[114,148]]},{"text":"ical functions. Apart from normally accepted grammatical functions, such as SB (subject) or OA (accusative object), Negra grammatical func-","acronyms":[[76,78],[92,94]],"long-forms":[[80,87],[96,113]]},{"text":"CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB = base form verb; VBD = past tense verb; VBZ = third-person singular present verb.","acronyms":[[179,181],[0,2],[31,33],[53,55],[69,71],[81,83],[101,104],[120,124],[147,150],[166,168],[188,190],[209,212],[232,235]],"long-forms":[[184,186],[5,29],[36,44],[58,67],[74,79],[95,99],[107,118],[127,145],[153,164],[171,177],[203,207],[215,230],[238,273]]},{"text":"Figure 3: Dialogue system architecture    The Dialogue Manager (DM) uses sceneobject attributes, such as type, angle or interval ","acronyms":[[64,66]],"long-forms":[[46,62]]},{"text":"  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"POS tag distribution. We also use features based on part of speech (POS). We tag using","acronyms":[[68,71],[0,3]],"long-forms":[[52,66]]},{"text":"internal structure. This types of expression is an cases  of what will be called a \"comp\\]ex basic expression\" (CBE). ","acronyms":[[113,116]],"long-forms":[[85,110]]},{"text":" 8SB = theme, OA = accusative object, OA2 = seconds accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal ob-","acronyms":[[72,74],[1,4],[16,18],[40,43],[85,87],[107,109],[134,136],[150,152]],"long-forms":[[77,83],[7,14],[21,38],[46,70],[90,105],[112,132],[139,148],[155,165]]},{"text":"  Procedural of the of the EACL 2014 Workshop on Dialog in Motion (DM), pages 53?57, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[70,72],[28,32]],"long-forms":[[50,68]]},{"text":"Var. 47.9 60.7 67.9 70.8 75.0 77.3 Pronunciation (PHL) with Pron. Var.","acronyms":[[50,53],[66,69],[60,64]],"long-forms":[[35,48]]},{"text":"To further investigate the effectiveness of our  method, the third set of experiments evaluate the  negative transfer detection (NTD) compared to  co-training (CO) without negative transfer ","acronyms":[[129,132],[160,162]],"long-forms":[[100,127],[147,158]]},{"text":"attempted to cut down on certain items in the process.  Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff","acronyms":[[88,91]],"long-forms":[[94,100]]},{"text":"Imply valued (with standard deviations) of each of the first eight hallmarks on each sub-corpus are displayed in Table 4. The number of extraordinaire punctuation marks (UnPunc) is the only feature whose value does not differ significantly between the initial and simplified versions of the texts in any of the four","acronyms":[[161,167]],"long-forms":[[134,153]]},{"text":"bbai@nec-labs.com Abstract We develop a recursive neural network (RNN) to extract answers to arbitrary natural language","acronyms":[[66,69]],"long-forms":[[40,64]]},{"text":"model in their study is the relative height by range modeling, where (in our notation): (13) concerning height by range (RH-R): ? ","acronyms":[[116,120]],"long-forms":[[90,114]]},{"text":"fast method to train SVM. SMO breaks the large  quadratic programming (QP) optimization problem needed to be resolved in SVM into a series ","acronyms":[[71,73],[21,24],[26,29],[121,124]],"long-forms":[[48,69]]},{"text":"to systems that rely on brittle features is that many texts are not well-formed. One  such classroom of texts are those that are the output of optical character recognition (OCR);  typically these texts contain many extraneous or erroneous hallmarks.","acronyms":[[170,173]],"long-forms":[[139,168]]},{"text":"we will describe in detail in Section 3. They then  introduced a ClueWordSummarizer (CWS), a  graph-based unsupervised summarization ap-","acronyms":[[85,88]],"long-forms":[[65,83]]},{"text":"tribution from most features but C1 seems to benefit more from GENIA named entity tagging, Human Phenotype Ontology (HPO), Foundation Model of Anatomy (FMA) and","acronyms":[[117,120],[33,35],[63,68],[152,155]],"long-forms":[[91,115],[123,150]]},{"text":" Acknowledgments This research was supported by the Deutsche Forschungsgemeinschaft (DFG) in the Center of Excellence in ?","acronyms":[[85,88]],"long-forms":[[52,83]]},{"text":"day(x, fri) ? during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights","acronyms":[[54,56]],"long-forms":[[40,52]]},{"text":"summarization. During these intervening decades,  progress in Natural Language Processing (NLP), coupled  with great increases of computer memory and speed, ","acronyms":[[91,94]],"long-forms":[[62,89]]},{"text":"the exploration and verbalization history; and (4) it then sends semantic representations in the form of preverbal messages (PVMs) to the Formulation & Articulation components.","acronyms":[[125,129]],"long-forms":[[105,123]]},{"text":"  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 53?57, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[70,72],[28,32]],"long-forms":[[50,68]]},{"text":" 1356 Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing","acronyms":[[22,26],[45,47]],"long-forms":[[6,20]]},{"text":" Snow et al (2008) explored the use of the Amazon Mechanical Turk (MTurk) web service for gathering annotations for a variety of natural lan-","acronyms":[[67,72]],"long-forms":[[50,65]]},{"text":"validate the performance of our method:  1. Precision@N (P@N). P@N measures how ","acronyms":[[57,60],[63,66]],"long-forms":[[44,55]]},{"text":"Discussions were recorded and system and user behavior were logged automatically. The concept accuracy (CA) of each turn was manually labeled. Though the","acronyms":[[102,104]],"long-forms":[[84,100]]},{"text":"2.4.1 English Gigaword We created large-scale n-gram language model using English Gigaword Second Edition6 (EGW). ","acronyms":[[109,112]],"long-forms":[[75,91]]},{"text":" We split annotated data into two parts: the BLOB (Binary Large OBject) and the XML annotations that refer to specific regions of the BLOB.","acronyms":[[45,49],[80,83],[134,138]],"long-forms":[[51,70]]},{"text":"An entity has  three genus of mention: NAM (proper name), NOM  (nominal) or PRO (pronoun). A relation was ","acronyms":[[76,79],[39,42],[58,61]],"long-forms":[[81,88],[51,55],[64,71]]},{"text":"strated via an algorithm for joint unsupervised learning of the topology and parameter of a hidden Markov model (HM); states and short state-sequences through this HMM cor-","acronyms":[[114,117],[166,169]],"long-forms":[[93,112]]},{"text":"On the other hand, multi-lingual  ontology is very important for natural language  processing, such as machine translation (MT), web  mining (Oyama et al 2004) and cross language ","acronyms":[[124,126]],"long-forms":[[103,122]]},{"text":"Instead of using words squarely, it is achievable to employ a (much smaller) controlled vocabulary: Medical Subject Headings (MeSH), consisting of 22,500 codes, are (mostly) manually assigned to","acronyms":[[124,128]],"long-forms":[[98,122]]},{"text":"greatest indicating better.  We used Amazon?s Mechanical Turk (MTurk)5 to collect the human judgements.","acronyms":[[61,66]],"long-forms":[[44,59]]},{"text":"gradient method. In The Conference on Advances in Neural Information Processing Plan (NIPS). ","acronyms":[[89,93]],"long-forms":[[50,87]]},{"text":"Framing representat ion ( for  cartography 4):  \\[agent\/an\\] (i\/PRP)  5PO$ abbreviations: PRP=personal pro-  noun, AUX=auxiliary verb, VB=main verb (non-inflected), ","acronyms":[[84,87],[109,112],[129,131]],"long-forms":[[88,101],[113,122],[137,141]]},{"text":"a. the 1000-headlines text (purpose spheres) 1,181 40.2 32.1 35.7 b. the TEC (roots domain) 32,954 29.9 26.1 27.9 c. the 1000-headlines text and the TEC (target and source) c.1.","acronyms":[[148,151],[71,74]],"long-forms":[[153,170]]},{"text":"The open class  word (host) tends to be uninflected, and only  the light verb (LV) carries tense, agreement  and aspect markers.","acronyms":[[79,81]],"long-forms":[[67,77]]},{"text":"  Here the parameters are set using an algorithm  whose uniformed resource name (URN),  xyz.edu\/algo-1, is declared as an symptomatic of the ","acronyms":[[79,82]],"long-forms":[[56,77]]},{"text":"to ? constraints? in interactive topic models (ITM) (Hu et al, 2014).","acronyms":[[47,50]],"long-forms":[[21,45]]},{"text":" 4.3 MORPHOTACT1C MODEL  An associative Morphotactic Modelled (MTModel) is a pair  <{MRi},<*>, where {MRi} is a set of morphotactic rule ","acronyms":[[60,67],[82,85],[99,102]],"long-forms":[[40,58]]},{"text":"WordNet Domains (Magnini and Cavagli a`, 2000).  Conceptual Density (CD) is a measure of the correlation among the sense of a given word and its","acronyms":[[69,71]],"long-forms":[[49,67]]},{"text":"explicitly deals the language ambivalence issue. Key to our approach is the use of Word Sense Induction (WSI), that is, methods aimed at automatically discovering the different meanings of a given term (i.e., query).","acronyms":[[107,110]],"long-forms":[[85,105]]},{"text":"3 Attribute Modeling based on LDA 3.1 Controled LDA This section introduces Controled LDA (C-LDA), a weakly supervised variant of LDA.","acronyms":[[91,96],[30,33],[48,51],[130,133]],"long-forms":[[76,89]]},{"text":"with the overall metric of mistaken per response fill  (ERR), overgeneration (OVG) does not correlate  with it, and replace (SUB) correlates with it  only to a limited extent.","acronyms":[[127,130],[53,56],[75,78]],"long-forms":[[113,125],[59,73]]},{"text":"patterns allows a 4-way classification of slopes of lines: fast rising, rising, level, falling.  These are the 4 Fundamental Pattern Features (FPF). A combination of 2 or  3 (of the 4) ","acronyms":[[143,146]],"long-forms":[[113,141]]},{"text":" MAI is one of the four projects elected for  the Johns Hopkins University (JHU) Summer  Workshop 2000.1 Ours research focus is on the ","acronyms":[[76,79],[1,4]],"long-forms":[[50,74]]},{"text":"majority instances of all the clusters.   Mutual information (MI) is more theoretically  well-founded than purity.","acronyms":[[62,64]],"long-forms":[[42,60]]},{"text":"the method described in Section 3.2). We also present the number of linear equations (L.Eq.) used","acronyms":[[86,91]],"long-forms":[[68,84]]},{"text":"An alternative to QE is to perform the expansion in the document. Document Expansion (DE) was first proposed in the speech retrieval commu-","acronyms":[[86,88],[18,20]],"long-forms":[[66,84]]},{"text":"{tvu, aaiti, mzhang}@i2r.a-star.edu.sg  Abstract  Term Extraction (TE) is an important component of many NLP applications.","acronyms":[[67,69],[105,108]],"long-forms":[[50,65]]},{"text":"2 Background  2.1 Gene Expression Programming  Gene Expression Programming (GEP), first introduced by (Ferreira 2001), is an evolutionary algo-","acronyms":[[76,79]],"long-forms":[[47,74]]},{"text":"Typically, the weights of the log-linear combination in Equation 3 are optimised by means of Minimum Error Rate Training (MERT) (Och, 2003).","acronyms":[[122,126]],"long-forms":[[93,120]]},{"text":"2 Lang Short-Term Memory Networks 2.1 Overview Recurrent neural networks (RNNs) are able to process inlet sequences of arbitrary length via the re-","acronyms":[[74,78]],"long-forms":[[47,72]]},{"text":"rently, a pure statistical MT system based on Pharaoh is developed by BPPT and National News  Entities (ANTARA) using 500K sentences pair,  foreseen to have better accuracy and robustness ","acronyms":[[102,108],[27,29],[70,74]],"long-forms":[[79,100]]},{"text":"Senior Researcher and Lecturer Knowledge Management Group Applied Computer Science Institute (AIFB) University of Karlsruhe, Germany","acronyms":[[94,98]],"long-forms":[[58,92]]},{"text":"Clear And Simple English (CASE) Caterpillar Fundamental English (CFE) Caterpillar Technical English (CTE) Diebold Controlled English (DCE)","acronyms":[[101,104],[26,30],[65,68],[134,137]],"long-forms":[[70,99],[0,24],[32,63],[106,132]]},{"text":"Probability PR+ (probable) PR? ( not probable) [NA] Possible PS+ (possible) PS? ( not certain) [NA]","acronyms":[[58,61]],"long-forms":[[63,71]]},{"text":"  Proceedings of the 2014 Conference on Empirical Modes in Naturel Language Processing (EMNLP), pages 928?937, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"most u n e x p a n d f d  knot o e  TI: for  3 b t h i s  rs e s u l t s  i n :   3 e .  (S ( vs  m a i l )  (NP ( N P  ( N  B o x e s )  (N*)) P P * )  ( P P * ) ) .  ","acronyms":[[108,110],[142,145]],"long-forms":[[113,116]]},{"text":"This  paper focuses on this problem in the context of  Information Extraction (IE). 2 Numerous extraction ","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":"AFIPS Washington Off ice   KAO REPORTS ONTO DOMINION MODELING  The General Accounting 0fEic.e (GAO') has released a repor t -  on \"Ways to  Improves  &mug-nt of Federally kcnded ~o@ute r i z ed  ~ o d e Z s I ~  (#-  enc luse $1.00) .","acronyms":[[92,96],[0,5],[27,30]],"long-forms":[[64,90]]},{"text":"onomy using a Japanese-English bilingual dictionary as  a \"bridge\", in order to support semantic processing in a  knowledge-based machine translation (MT) system. ","acronyms":[[151,153]],"long-forms":[[130,149]]},{"text":" 53 Creative Information Retrieval (CIR) can be used as a platform for the design of many Web services that offering linguistic creativity on de-mand. By allow the flexible retrieval of n-gram data for non-literal queries, CIR enabled a wide variety of imaginative tasks to be reimagined as mere IR tasks (Veale 2013).","acronyms":[[36,39],[223,226]],"long-forms":[[4,34]]},{"text":"Results are exhibited through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this chore.","acronyms":[[74,77]],"long-forms":[[58,72]]},{"text":"Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth Worldwide Conference on Language Resources and Valuation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA). ","acronyms":[[203,207],[127,134]],"long-forms":[[162,201],[92,125]]},{"text":"contrast, L\/RMC = left\/right-most char,  L\/RMW = gauche\/right-most word, VS = vowel  sequenced, HYPH = hyphenation, CASE =  case, PM = parenthesized material.","acronyms":[[94,98],[10,15],[41,46],[71,73],[114,118],[128,130]],"long-forms":[[101,112],[18,38],[49,69],[76,92],[122,126],[133,155]]},{"text":"discourses presents to the human subjects.  6.1 Semantically Slanted Discourse (SSD) Methodology: The Motivation for the  First Part ","acronyms":[[81,84]],"long-forms":[[49,79]]},{"text":"HG-ALN 0.266 0.359 Table 1: The Pk and WindowDiff scores of uniform segmentation (UNI), TextTiling (TT), baseline alignment (B-ALN), and alignment with hier-","acronyms":[[100,102],[82,85],[0,6],[125,130]],"long-forms":[[88,98],[60,67],[105,123]]},{"text":"4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task","acronyms":[[93,96],[113,118]],"long-forms":[[76,91]]},{"text":" In unvocalised text, the standard written form of Modern Standard Arabic (MSA), it may happen that the stem and the root of a word form are one and the","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"In Proceedings of the 5th International Conference on Language Resources and Valuation (LREC), pages 417?422.","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":"MUC-6, 1995; Agirre, 2007). By rematch, gold  standard Named Entity (NE) annotations are effortless  to produce; indeed, there are many NE annotated ","acronyms":[[70,72]],"long-forms":[[56,68]]},{"text":"110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP)  contact(CeNT) motion(MOT) ","acronyms":[[48,51],[65,69]],"long-forms":[[38,46],[53,63]]},{"text":"gramming for probabilistic programming. In International Workshop on Statistical Relational AI (StarAI). ","acronyms":[[96,102]],"long-forms":[[69,94]]},{"text":" 1 Introduction Semantic Role Labeling (SRL), independently of the approach adopted, comprehends two steps be-","acronyms":[[40,43]],"long-forms":[[16,38]]},{"text":"Verb classification performance (precision, recall, and F for MS are macro-averaged). Global accuracy supplemented by 95% binomial confidence intervals (CI). ","acronyms":[[153,155],[62,64]],"long-forms":[[131,151]]},{"text":"90 Tables 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization, POS=part-of-speech labelled, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition). ","acronyms":[[156,158],[179,182],[111,114]],"long-forms":[[159,177],[183,207],[97,109],[115,129],[141,154]]},{"text":" 4 Active Learning Active Learning (AL) is a machine learning paradigm that let the learner decide which data it","acronyms":[[36,38]],"long-forms":[[19,34]]},{"text":"PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL  DET = Determiner NO = Noun Phrase (B~R I)  ADJ = Adjective GD = Gender  NU '~= Number PS = Person ","acronyms":[[115,117],[0,4],[17,22],[52,54],[56,59],[73,75],[99,102],[128,130],[142,144]],"long-forms":[[120,126],[7,16],[26,33],[62,72],[78,82],[105,114],[135,141],[147,153]]},{"text":" ? Unlabeled Elementary Dependencies (UED) identical to CULMINATED, except ignoring all labeling","acronyms":[[38,41],[56,59]],"long-forms":[[3,36]]},{"text":"heres to the dimensions presented in Table 2, and negation sphere are modeled utilized a outset order linear-chain conditional random field (CRF)2, with a label set of size two indicating whether a","acronyms":[[137,140]],"long-forms":[[111,135]]},{"text":"sion with LSA and filtering conforming to the ET.  Further on, we applies sentiment analysis (SA)  using the approach described in Section 5.3 and ","acronyms":[[91,93],[10,13],[45,47]],"long-forms":[[71,89]]},{"text":"It is particularly interesting to see that when hypotheses selection is applied, oracle error rate (OER) drops of 2% points from an already accurate OER","acronyms":[[100,103],[149,152]],"long-forms":[[81,98]]},{"text":"The tag B-X (Begin) represents the first word of a named entity of type X, for example, PER (Person) or LOC (Location). The tag I-X (In-","acronyms":[[88,91],[104,107]],"long-forms":[[93,99],[109,117]]},{"text":"ical functions. Apart from commonly accepted grammatical functions, such as SB (subject) or OA (accusative object), Negra grammatical func-","acronyms":[[76,78],[92,94]],"long-forms":[[80,87],[96,113]]},{"text":"We tested the Arabic sentiment system on two existing Arabic datasets (Mourad and Darwish (2013) (MD) and Refaee and Rieser (2014a) (RR)) and two newly sentiment-annotated Arabic datasets (BBN","acronyms":[[98,100]],"long-forms":[[117,123]]},{"text":" Because of data sparseness, we cannot reliably use a  maximum likelihood estimator (MLE) for bigram prob-  abilities.","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":"learning community: ? Rectified Linear Units (ReLU) (Nair and Hinton, 2010): max(0, x);","acronyms":[[45,49]],"long-forms":[[22,43]]},{"text":"X at Y (verb phr:~se, noun phrase)  X a.m. (corot?rand word)  Later step (I) is complete, steps (II)-(IV) are repeated  recursively.","acronyms":[[97,99]],"long-forms":[[77,88]]},{"text":"In this paper we present our entry to the WMT?13 shared task: Quality Estimation (QE) for machine translation (MT). ","acronyms":[[111,113],[42,48],[82,84]],"long-forms":[[90,109],[62,80]]},{"text":"tual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that maintaining tracks of the contexts (e.g.,","acronyms":[[92,96]],"long-forms":[[60,90]]},{"text":"Language Weaver, Inc. This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. ","acronyms":[[198,201]],"long-forms":[[165,196]]},{"text":"For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word to Sense (W-Se), participants are required to re-","acronyms":[[119,123]],"long-forms":[[103,117]]},{"text":"data are classified manually (Human) into three  stability classes. Decision Tree (DT) automation  algorithms C4.5 (Quinlan, 1993; Weiss & Kulikowski, ","acronyms":[[83,85]],"long-forms":[[68,81]]},{"text":"is Tws, for \"Translator's Workstation.\" We besides use the  C-based X11 toolkit called MOTIF (Motif, 1991) and its Com-  monLisp interface called CLM (Babatz et.","acronyms":[[85,90]],"long-forms":[[92,97]]},{"text":"knowledge powered model to several baselines.  Random Guess Model (RG). Random guess is","acronyms":[[67,69]],"long-forms":[[47,59]]},{"text":"though this is never the case.) The second such  feature \"Theme as Chomeuf  (TAC) is the only  non-trinary-valued feature in our learner; it spec- ","acronyms":[[77,80]],"long-forms":[[58,74]]},{"text":"and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking.","acronyms":[[87,89],[100,102],[121,123]],"long-forms":[[90,98],[103,119],[124,139]]},{"text":"partially finish subproof or function of the system. The implementation f this  was the IPSIM (Interruptible Prolog SIMulator) theorem prover, which can maintain  a set of partially finish evidence and jump to the appropriate one as dialog pro- ","acronyms":[[91,96]],"long-forms":[[98,128]]},{"text":"2011.  Icelandic Parsed History Corpus (IcePaHC). ","acronyms":[[43,50]],"long-forms":[[7,41]]},{"text":"Orlando, Florida 32810  AFIPS CONSTITUENT SOCIETIES  Instrument Society of America (ISA)  Purpose ","acronyms":[[84,87],[24,29]],"long-forms":[[53,82]]},{"text":"Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping intention for tongue blade constriction degrees (TBCD), lip aperture (LA), and glottis (GLO).","acronyms":[[134,138],[155,157],[173,176]],"long-forms":[[100,132],[141,153],[164,171]]},{"text":"TF (Term Frequency)  is the word frequency within a document;  IDF (Inverse Document Frequency) is the  logarithm of the ratio of the total number of ","acronyms":[[63,66],[0,2]],"long-forms":[[68,94],[4,18]]},{"text":"To address this trouble, Xiong et al (2006) strengthened the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual","acronyms":[[84,90],[56,59]],"long-forms":[[67,82]]},{"text":"oleary@cs.umd.edu Abstract The Text Analysis Conference (TAC) ranks summarization systems by their average score","acronyms":[[57,60]],"long-forms":[[31,55]]},{"text":"programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is utilised as referencing (REF).  The alignment ","acronyms":[[122,125],[55,58]],"long-forms":[[111,120],[43,53]]},{"text":" Undoubtedly, Web 2.0 and the constant increase of User Generated Content (UGC) lead to a  higher demand for translation.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"As the entire sentence is informative to determine the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that enhances both syntactic and lexical in-","acronyms":[[120,124]],"long-forms":[[90,118]]},{"text":"annotations of sentiment values for individual syntactic phrases in a binarized tree, and an approach predicated on recursive neural tensor networks (RNTN) which yields considerable improvements over the ear-","acronyms":[[145,149]],"long-forms":[[111,143]]},{"text":"Technician (NAST), Lao PDR  ? Madan Puraskar Pustakalaya (MPP),  Nepal  ","acronyms":[[58,61]],"long-forms":[[30,56]]},{"text":" Undoubtedly, Web 2.0 and the constant augmentation of User Generated Content (UGC) lead to a  high demand for translation.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":" 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical difficulty of apply-","acronyms":[[13,16]],"long-forms":[[18,40]]},{"text":"direct analogy to Sidner's \\[26\\] potential local foci,  and assumes only one temporal referent in the  temporal focus (TF). ","acronyms":[[120,122]],"long-forms":[[104,118]]},{"text":" ) are bases on the pairwise mutual information (PMI) between two phrases.","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":"  2 NII-Speech Finances Consortium  The National Institute of Informatics (NII) was  founded in Tokio, Japan in April 2000 as an in-","acronyms":[[76,79],[4,7]],"long-forms":[[41,74]]},{"text":"Recent working examining ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien","acronyms":[[139,144],[64,67],[170,177]],"long-forms":[[126,137]]},{"text":"Translation. In Proceedings of the 13th Internationally  Conference on Computational Linguistics (COLING-90),  Vol.","acronyms":[[96,105]],"long-forms":[[55,94]]},{"text":"formation retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1. Researchers in","acronyms":[[136,140]],"long-forms":[[109,134]]},{"text":"At addition, results from the machine learning based model are refine by a rule-based postprocessing, which is implemented using a finite state transducer (FST). The","acronyms":[[157,160]],"long-forms":[[132,155]]},{"text":"an algorithm that combines the reference choice rules for  reason and the reference choice rules for methods, to pro-  duce preverbal messages (PMs) from PCAs. As such, the ","acronyms":[[144,147]],"long-forms":[[124,142]]},{"text":"Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, SUPERIOR = topic bookmark, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark.","acronyms":[[95,98],[121,124],[75,78],[138,141],[152,156]],"long-forms":[[101,110],[127,136],[69,73],[81,86],[144,150],[159,170]]},{"text":"vector built from training set. However, this  could lead to many out of vocabulary (OOV)  cases, in addition to long vector.","acronyms":[[85,88]],"long-forms":[[66,83]]},{"text":"Given an occurrence of a word \u0002 in a naturel parlance text, the task of word sense disambiguation (WSD) is to determine the correct sense of \u0002 in that context.","acronyms":[[99,102]],"long-forms":[[72,97]]},{"text":"Tables 3: Results on DevTest and Test Sets compared with the Average Performance in CoNLL?07. LAS = Labelled Attachment Score, UAS = Unlabelled Attachment Scoring, LAcc = Etiquette Accuracy, AV = Average score.","acronyms":[[126,129],[184,186],[93,96],[161,165],[83,88]],"long-forms":[[132,159],[189,196],[99,124],[168,182]]},{"text":"ellieioncy has been substantially improved for n)orpho-  logical analysed by represented large dictionaries with  Finite State Automata (FSA) and by representhig two-  level rnles and le?ical hlforination with finite-state ","acronyms":[[136,139]],"long-forms":[[113,134]]},{"text":"automatique, GDR I3 ATALA, Paris, November 1999.  Tang E.K., Natural languages Analysis in machine translation (MT) groundwork on the STCG, PhD thesis, Sains Malaysia Academies, Penang, March 1994","acronyms":[[112,114],[13,16],[20,25],[129,133],[135,138],[55,58]],"long-forms":[[91,110]]},{"text":"value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for","acronyms":[[70,74],[125,129],[138,145],[83,90]],"long-forms":[]},{"text":"  ? PT (parse tree)  ","acronyms":[[4,6]],"long-forms":[[8,18]]},{"text":"Concealment topic markov models. In Artificial Intelligence and Statistics (AISTATS), San Juan, Puerto Rico. ","acronyms":[[71,78]],"long-forms":[[31,69]]},{"text":" 1 Introduction Issue answering(QA) system aims at finding exact answers to a natural language question.","acronyms":[[35,37]],"long-forms":[[16,33]]},{"text":"2004. Evaluation of a Deidentification (De-Id) Software Engine to Share Pathology Report and Clinical Documents for","acronyms":[[40,45]],"long-forms":[[22,38]]},{"text":"2 Long Short-Term Memory Networks 2.1 Overview Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the re-","acronyms":[[74,78]],"long-forms":[[47,72]]},{"text":"173 Figure 3: Frequency of word classes in the three corpora (BN = Broadcast News, Est = Press, Euro = Europarl).","acronyms":[[62,64],[96,100]],"long-forms":[[67,81],[103,111]]},{"text":"are either too wordy or too ungrammatical. Table 1 shows the compression rates (CompR) for the two 8","acronyms":[[80,85]],"long-forms":[[61,78]]},{"text":" More recently, i2b2 organizers also reported a  Maximum Entropy (ME) based approach for the  2009 challenge (Halgrim, Xia, Solti, Cadag, & ","acronyms":[[66,68],[16,20]],"long-forms":[[49,64]]},{"text":" As stated earlier, our unlabeled data consists of email (EMAIL) and online forum (FORUM) data.","acronyms":[[58,63],[83,88]],"long-forms":[[51,56],[76,81]]},{"text":"Reverse Gap 0.072 0.033 Table 1: Percentage of reordering patterns ` reverse gap (RG): The two source phrases are not adjacent, and are in the reverse order as","acronyms":[[82,84]],"long-forms":[[69,80]]},{"text":"ously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA). However, our","acronyms":[[122,125],[86,89]],"long-forms":[[94,120],[60,84]]},{"text":"observed in Dutch. Dutch shows a pattern in which  an arbitrary number of noun phrases (NP's) may be  followed by a finite verb and an arbitrary number ","acronyms":[[88,92]],"long-forms":[[74,86]]},{"text":"six basic emotion tags to the Bengali blog sentences. Conditional Random Field (CRF)  based word level emotion classifier classifies ","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":"We also include a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model.","acronyms":[[125,131]],"long-forms":[[101,123]]},{"text":"disasters or political crises in the media. There are two main approaches to audio hotspotting; one involves speech-to-text (STT), also known as large vocabulary continuous speech recognition (LVCSR),  and the other employs phonetic speech recognition.","acronyms":[[193,198],[125,128]],"long-forms":[[145,191],[109,123]]},{"text":"carded in LSI-based approaches. We dub our model ONETA (OrthoNormal Explicit Topic Analysis) and empirically show that on a cross-lingual retrieval","acronyms":[[49,54],[10,13]],"long-forms":[[56,91]]},{"text":"For NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25?32, Rochester, NY.","acronyms":[[70,74],[3,8],[101,103]],"long-forms":[[45,68]]},{"text":"In Proc. of the Association for Computational Linguistics (ACL), pages 523?530.","acronyms":[[59,62]],"long-forms":[[16,57]]},{"text":" As stated sooner, our unlabeled data consists of email (EMAIL) and online forum (FORUM) data.","acronyms":[[58,63],[83,88]],"long-forms":[[51,56],[76,81]]},{"text":"~  I n  recent  years  the  prob lem o f  man 'mach ine  communicat ion  by  means   o f  natura l  language (NL) i s  becoming  a pract i ca l  one .  And the  ","acronyms":[[110,112]],"long-forms":[[90,108]]},{"text":" Preclinical data have supported the utilized of  fludarabine and cyclophosphamide (FC) in  combination for the treatment of indolent ","acronyms":[[79,81]],"long-forms":[[45,77]]},{"text":"On the standard view in transformational theory (Chomsky, 1981) both subject raising and object raising, or Exceptional Case Marking (ECM), cases are explained by the same principles.","acronyms":[[134,137]],"long-forms":[[108,132]]},{"text":" 1 Introduction Relation extract (RE) is the task of determining the existence and type of relation between two tex-","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6 Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets Broadcast News (BNEWS) Newswire (NWIRE) True Mentions System Mentions True Mentions System Mentions","acronyms":[[203,208],[220,225],[121,124]],"long-forms":[[187,201],[210,218]]},{"text":"A necessary (if not sufficiently) condition for true natural language understanding is a mastery of open-domain natural linguistics inference (NLI): the task of determining whether a natural-language","acronyms":[[138,141]],"long-forms":[[110,136]]},{"text":" The system determines the position of the main part of  the situation relative to the point of speech (PS). The ","acronyms":[[104,106]],"long-forms":[[87,102]]},{"text":" To combat this inefficiency, after all state transition we estimate the effectiveness sample size (ESS) of the particle weights as ??","acronyms":[[98,101]],"long-forms":[[75,96]]},{"text":"Abstract This paper reports on the participation of the TALP Research Center of the UPC (Universitat Polit?cnica de Catalunya) to the ACL WMT 2008 evaluation","acronyms":[[84,87],[56,60],[134,137],[138,141]],"long-forms":[[89,112]]},{"text":"Table 1: Number of routes, direction, and tokens for the different settings. GM = Google Maps, CI = Universities Indoor, CO = Campus Outdoor.","acronyms":[[78,80],[96,98],[116,118]],"long-forms":[[83,94],[101,114],[121,135]]},{"text":" 4.3 Counting and Calculation The SRI Language Modelling Toolkit (SRILM) (Stolcke and others, 2002) is used to count the frequencies in our work.","acronyms":[[66,71]],"long-forms":[[34,56]]},{"text":" 3 The V IT  Format   The VIT (short for Verbmobil Interface Term) was  designed as a common output format for the two ","acronyms":[[26,29],[7,11]],"long-forms":[[41,65]]},{"text":"lates in the treebank; it is erroneous because close to wholesale needs another layer of structure, namely adjective phrase (ADJP) (Bies et al, 1995, p. 179). ","acronyms":[[125,129]],"long-forms":[[107,123]]},{"text":"25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work.","acronyms":[[65,70],[34,38]],"long-forms":[[51,63],[19,32]]},{"text":"In Proc. of IEEE\/ACL workshop on Spoken Language Technology (SLT). ","acronyms":[[61,64],[12,20]],"long-forms":[[33,59]]},{"text":"As proposed from the tables, the accuracy values of the component classifiers (Ccn and Cen) in CoTrain are almost steadily higher than those of the corresponding TSVM(CN) and TSVM(EN), based on any machines translation service.","acronyms":[[166,168],[161,165],[174,178],[179,181],[80,83],[88,91]],"long-forms":[]},{"text":"The most analog paraphernalia to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,","acronyms":[[67,72],[29,36],[42,45],[117,121],[122,125]],"long-forms":[[74,108]]},{"text":"Tokens (T) .72 \/ .77 .83 \/ .84 .72 \/ .76 .85 \/ .84 .82 \/ .84 .88 \/ .86 Named Entities (NE) .75 \/ .80 .84 \/ .79 .75 \/ .77 .85 \/ .78 .89 \/ .78 .89 \/ .73 NE targeted (NE-T) .54 \/ .55 .49 \/ .47 .66 \/ .64 .60 \/ .57 .64 \/ .64 .57 \/ .58 Host (H) .72 \/ .57 .64 \/ .48 .67 \/ .51 .58 \/ .41 .67 \/ .63 .59 \/ .55","acronyms":[[164,168]],"long-forms":[[151,162]]},{"text":"Mauser et al 2009). One promising approach is the Discriminative Word Lexicon (DWL). In this","acronyms":[[79,82]],"long-forms":[[50,77]]},{"text":"6 Scope Resolution One way of dealing with scope ambiguities is by using underspecified representations (URs). A","acronyms":[[105,108]],"long-forms":[[73,103]]},{"text":" An alternative paradigm is to view error correction as a statistical machine translation (SMT) problem from ?","acronyms":[[91,94]],"long-forms":[[58,89]]},{"text":"document-level CLOA tasks, respectively. The evaluations on simplified Chinese (SC) opinion analyzes by using small SC training data and mammoth","acronyms":[[80,82],[15,19],[116,118]],"long-forms":[[60,78]]},{"text":"or certain part-of-speech tags (e.g., interjection).8 4.2.3 Performance of the formality classifier Ours trained a Maximum Entropy (MaxEnt) classifier in the Mallet pack (McCallum, 2002).","acronyms":[[130,136]],"long-forms":[[113,128]]},{"text":"are assigned the correct head and dependency type ? and unlabeled attachment score (UAS) ? the per-","acronyms":[[84,87]],"long-forms":[[56,82]]},{"text":"dependency analyzer, respectively.  (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complements object, ERG =  ergative, S: singular, auxmod = ancillary, ncsubj = non-clausal theme, B-NP = beginning of NP, I-NP = inside an NP, ","acronyms":[[62,66],[85,90],[144,147],[224,228],[248,252]],"long-forms":[[69,83],[93,103],[56,60],[41,50],[151,159],[164,172],[231,246],[255,267]]},{"text":"method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1","acronyms":[[125,129]],"long-forms":[[98,123]]},{"text":"We use the same evaluation metrics as in (McDonald et al, 2005). Dependency accuracy (DA) is the proportion of non-root words that are assigned the","acronyms":[[86,88]],"long-forms":[[65,84]]},{"text":"Note also that there is some overlap of infer-  marion between the Lexical Systems analysis and  the Brandeis analysis, such as SUISCAT(TRAN)  and DO.","acronyms":[[136,140],[147,149],[128,135]],"long-forms":[]},{"text":"have to be induced from parallel corpora.  An invert transduction grammar (ITG) strikes a good equilibrium between STGs and SDTGs,","acronyms":[[78,81],[114,118],[123,128]],"long-forms":[[46,76]]},{"text":"The Meter Corpus chosen as the test data  is a collection of court reports from the British Press Association (PA) and some leading  British newspapers (Gaizauskas 2001; Clough ","acronyms":[[111,113]],"long-forms":[[92,109]]},{"text":"Semantic Interpretation of Prepositions for NLP Applications Sven Hartrumpf Hermann Helbig Rainer Osswald Intelligent Information and Communicative Schemes (IICS) University of Hagen (FernUniversita?t in Hagen)","acronyms":[[157,161],[44,47]],"long-forms":[[106,155]]},{"text":"ways available. We therefore generate the training data using a na??ve phrase aligner (NPA) instead of resorting to a real one.","acronyms":[[87,90]],"long-forms":[[64,85]]},{"text":"4. Crowdsourcing We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different types of opposites.","acronyms":[[53,56]],"long-forms":[[29,51]]},{"text":" 3.3 Cascaded ATN Grammars A Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperating sequence of ATN transducers, each feeding its output to the next stage.","acronyms":[[52,56],[14,17],[100,103]],"long-forms":[[29,41]]},{"text":"The two main Modern Standard Arabic dependency treebanks currently accessible are the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009) and the Prague Arabic Dependence","acronyms":[[111,116]],"long-forms":[[85,109]]},{"text":"ID full papers domain (TCS) 10 BB web pages domain (BB) 2 BI abstracts domain (BS) 10 Table 1: Characteristics of BioNLP-ST 2011 main tasks.","acronyms":[[79,81],[0,2],[23,26],[31,33],[52,54],[114,123]],"long-forms":[[58,70]]},{"text":" These algorithms are now getting keen atten-  tion from the natural anguage processing (NLP)  research community since the huge text corpus ","acronyms":[[89,92]],"long-forms":[[61,87]]},{"text":" 2.5 Noise Contrastive Estimation Noise contrastive estimation (NCE) is another sampling-based technique (Hyv?arinen, 2010;","acronyms":[[64,67]],"long-forms":[[34,62]]},{"text":" 2.3 Perceptron Successive Tagger This system uses a Global Linear Model (GLM), a sequential tagger utilizes the perceptron algorithm","acronyms":[[74,77]],"long-forms":[[53,72]]},{"text":"argument structure agreement (Das, 2009), the  analysis of Non-MonoClausal Verb (NMCV) or  Serial Verb, Control Constructed (CC),  Modal Control Erected (MCC), Passives ","acronyms":[[126,128],[81,85],[160,163]],"long-forms":[[104,124],[59,79],[132,158]]},{"text":"Data Annotated. The data to be annotated in WSsim-1 were taken essentially from Semcor (Miller et al1993) and the Senseval-3 English lexical sampled (SE-3) (Mihalcea, Chklovski, and Kilgarriff 2004).","acronyms":[[147,151]],"long-forms":[[112,122]]},{"text":"SC is mainly used in  mainland China while TC is mainly used in Taiwan  and Hong Kong (HK). In this experiment, we further ","acronyms":[[87,89],[0,2],[43,45]],"long-forms":[[76,85]]},{"text":"The task of location normalization is to identify  the correct sense of a possibly ambiguous  location Named Entity (NE). Ambiguity is very ","acronyms":[[117,119]],"long-forms":[[103,115]]},{"text":" 3.2 Convolutional Neural Network model The Convolutional Neural Network (CNN) model using raw audio as input is shown in Figure 1.","acronyms":[[74,77]],"long-forms":[[44,72]]},{"text":"on three official testsets.  NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from 2 domains: Newswire and Web.","acronyms":[[65,71]],"long-forms":[[39,63]]},{"text":"Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN ? CREST, JST (Japan Science and Technical Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN","acronyms":[[54,57]],"long-forms":[[59,87]]},{"text":"In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. ","acronyms":[[204,207]],"long-forms":[[209,238]]},{"text":"Lack of Orientation (LO). If there is at least one obstacle of the former, more serious kind, we will speak of Dead End (DE). For example, in the case of the DE Exam-","acronyms":[[121,123],[20,23],[158,160]],"long-forms":[[111,119],[0,19]]},{"text":"     elements; ? |? is used for alternative elements; TOP = topic marker. ","acronyms":[[54,57]],"long-forms":[[60,65]]},{"text":"1 This work has been developed in the project KFr-FAST (KIT = Kilnstliche  Intelligenz und Textverstehen (Artificial Intelligence and Text  Understanding); FAST = Functor Argument S ructure for Translation), which  constitutes the Berlin component of the complementary research project of ","acronyms":[[156,160]],"long-forms":[[163,189]]},{"text":" 3.4 MAP Inference Highest a posteriori (MAP) inference seeks the solution to","acronyms":[[41,44],[5,8]],"long-forms":[[19,39]]},{"text":"We then review some standard online pupil (e.g. perceptron) before introduces the Bayes Point Machine (BPM) (Herbrich et al, 2001; Harrington et al, 2003).","acronyms":[[106,109]],"long-forms":[[85,104]]},{"text":"and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used suc-","acronyms":[[80,84]],"long-forms":[[58,78]]},{"text":"the troubles as a multi-label classification task,  we trained a binary classification modelling for each  code using support vector machine (SVM) with  ten-fold cross-validation.","acronyms":[[137,140]],"long-forms":[[113,135]]},{"text":"represents the UTB statistics. For Telugu, the Telugu Treebank (TTB) released for ICON 2010 Shared Task (Husain et al. ( 2010)) was utilize for evaluations.","acronyms":[[64,67],[15,18],[82,86]],"long-forms":[[54,62]]},{"text":"emoeion(ENO) perception(PER)  possession(VENTED) stat ive(STA)  ~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD) ","acronyms":[[85,88],[8,11],[24,27],[41,45],[56,59],[70,73],[85,88],[95,98],[107,110],[117,120]],"long-forms":[[75,84],[0,7],[13,23],[30,40],[47,56],[63,69],[91,94],[100,106],[112,116]]},{"text":"tity in the contrast set until no distractors are left.  Dale & Reiter speaker frequency (DR-sf) utilise a varied preferred attribute list for each speaker,","acronyms":[[90,95]],"long-forms":[[57,88]]},{"text":"In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670?2676. ","acronyms":[[86,94]],"long-forms":[[27,84]]},{"text":"tence in Japanese; N = noun, TOP = theme marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark. ","acronyms":[[106,110],[29,32],[49,52],[75,78],[92,95]],"long-forms":[[113,124],[23,27],[35,40],[55,64],[81,90],[98,104]]},{"text":"XC (compound),   NN (noun),   NNP (befitting ","acronyms":[[17,19],[0,2],[30,33]],"long-forms":[[21,25],[4,12]]},{"text":"Our systems employs both corpus-based and knowledge-based approaches: Highest Entropy(ME) (Lau et al, 1993; Berger et al, 1996; Ratnaparkhi, 1998) is","acronyms":[[82,84]],"long-forms":[[66,81]]},{"text":"3.4 Example  of  in tegrat ion   Figure 3 shows the starting point of an integra-  tion process with the trigger word (TW) lelter, its  definition, its temporary graph (TG), the concept ","acronyms":[[119,121],[169,171]],"long-forms":[[105,117],[152,167]]},{"text":" BBLT Input Screen      We originally developed BBLT for ourselves as machine translation (MT) developers and evaluators, to rapidly see the meanings of Arabic strings","acronyms":[[91,93],[1,5],[48,52]],"long-forms":[[70,89]]},{"text":"reranking step works on top of the generative model.  \u0001 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear model over CCG derivations.","acronyms":[[56,60],[138,141]],"long-forms":[[62,90]]},{"text":"1 Introduction For the past three decades, there has been a great deal of work on the automatic identification (ID) of languages from the speech signal alone.","acronyms":[[112,114]],"long-forms":[[96,110]]},{"text":"32  Lawsuits of the 13th Annual Meeting of the Special Interest Cluster on Discourse and Dialogue (SIGDIAL), pages 169?178, Seoul, South Korea, 5-6 July 2012.","acronyms":[[100,107]],"long-forms":[[50,98]]},{"text":"4 Learning Algorithms We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and de-","acronyms":[[97,100],[153,155],[134,137]],"long-forms":[[72,95],[140,150],[103,111]]},{"text":"systems that learn new representations for opendomain NLP using latent-variable language models like Hidden Markov Models (HMMs). In POS-","acronyms":[[123,127],[54,57],[133,136]],"long-forms":[[101,121]]},{"text":"ceedin.qs, IEEE-1ECEJ-ASJ htternational Con-  ference on Audible, Speech, and Signal Process-  ing (ICASSP), 2bkyo, April 1986. ","acronyms":[[102,108],[11,25]],"long-forms":[[26,100]]},{"text":"EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian Tagset The Multext-East corpus is manually an-","acronyms":[[67,69],[79,81],[0,2],[43,45],[55,57]],"long-forms":[[70,77],[82,91],[46,53],[58,65]]},{"text":"nications Advancement Foundation, Japan, in part by the Center for Intelligent Information Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023.","acronyms":[[164,169],[172,175]],"long-forms":[[121,162]]},{"text":"construction relies on existing natural language processing tools, e.gs., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or wealthy lexical resources such","acronyms":[[130,132]],"long-forms":[[106,128]]},{"text":"ing instance is created as during training, and then presented to the decision tree, which returns a confidence value (CF)2 indicating the likelihood that NPi is coreferential to NPj .","acronyms":[[119,121],[155,158],[179,182]],"long-forms":[[101,111]]},{"text":"  While we use the same techniques to derive global features (assessor variety (AV) characteristics with 2~6 grams) from both training and test-","acronyms":[[83,85]],"long-forms":[[65,81]]},{"text":" Table 10: Average value of the mutual infor-  mation (MI) of compound noun seeds  .Number of elements \\[ 2 I 3 ","acronyms":[[55,57]],"long-forms":[[47,53]]},{"text":"sisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the","acronyms":[[101,103],[81,87]],"long-forms":[[88,99]]},{"text":"3.2 Generation Performance Ours now comparative the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al.,","acronyms":[[99,102],[46,50]],"long-forms":[[67,97]]},{"text":"Texts The performance of punctuation predictions on both Chinese (CN) and English (EN) texts in the correctly recognized output of the BTEC and TC datasets are","acronyms":[[65,67],[82,84],[134,138],[143,145]],"long-forms":[[56,63],[73,80]]},{"text":"Var. 47.9 60.7 67.9 70.8 75.0 77.3 Pronunciation (PHL) with Pron. Var.","acronyms":[[50,53],[66,69],[60,64]],"long-forms":[[35,48]]},{"text":"The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-) divergence (Cover and Thomas 1991): Fixednesssyn (v, n)","acronyms":[[136,139]],"long-forms":[[118,134]]},{"text":"3.4 The  NATO Research  Study  Group  on  Speech  Process ing   The North Atlantic Treaty Organization (NATO) Re-  search Study Group on Speech Processing (RSG10) \\[87\\], ","acronyms":[[104,108],[9,13],[156,161]],"long-forms":[[68,102],[110,154]]},{"text":"these areas. Speci\fcally, our goals when entering MUC-7 were to: \u000f Increase the accuracy in the Template Element (TE) task and the Template Relation (TR) task su\u000eciently for operational use, i.e., F-Measures of 85% and 80% respectively,","acronyms":[[114,116],[150,152],[50,55]],"long-forms":[[96,112],[131,148]]},{"text":"explicitly addresses the language ambiguity issue. Key to our approach is the use of Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the different meanings of a given term (i.e., query).","acronyms":[[107,110]],"long-forms":[[85,105]]},{"text":"by ranking the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR). In particular,","acronyms":[[129,132]],"long-forms":[[101,127]]},{"text":"lation of term-lists, pertaining studies are found in the  area of target word selection (for content mots) in  conventional full-text machine translation (MT). ","acronyms":[[154,156]],"long-forms":[[133,152]]},{"text":"integrate Chinese word segmentation and NE  identification into a unified framework using a  class-based language model (LM).\u0016 &ODVV\u0010EDVHG\u0003\/0 IRU\u00031(\u0003,GHQWLILFDWLRQ The n-gram LM is a stochastic model which ","acronyms":[[121,123],[40,42],[175,177]],"long-forms":[[105,119]]},{"text":" We focus on the following languages: German (DE), French (FR), Italian (IT), and Dutch (NL).","acronyms":[[59,61],[73,75],[46,48],[89,91]],"long-forms":[[51,57],[64,71],[38,44],[82,87]]},{"text":"(Koehn, 2004a). Furthermore, they extendedWSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a).","acronyms":[[78,81]],"long-forms":[[49,76]]},{"text":"3.1 Vector SpaceModei for Text Catego-  r izat ion  The bulk of the VSM for Information Retrieval (IR) is  representing naturallanguage xpressions as term ","acronyms":[[99,101],[68,71]],"long-forms":[[76,97]]},{"text":" NPHIL = Stray NP: Volume I: Syntax,  SUBJ = Theme: H__~e reads., ","acronyms":[[38,42]],"long-forms":[[45,52]]},{"text":"J08b 97.74 93.37 N07 97.83 93.32 SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score. ","acronyms":[[33,35],[60,62],[88,91]],"long-forms":[[38,52],[65,70]]},{"text":"10-best(+AG) 4.35 90.0 Table 1: Task completion rate according to using the AG (Agenda Graph) and n-best hypotheses for n=1 and n=10.","acronyms":[[76,78],[9,11]],"long-forms":[[80,92]]},{"text":" For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These","acronyms":[[88,93],[35,40]],"long-forms":[[72,86],[18,33]]},{"text":" 2.2 Thread-level analysis Forthcoming, we perform named entity recognition (NER) over each threaded to identify entities such as package","acronyms":[[70,73]],"long-forms":[[44,68]]},{"text":"than three thousand five hundred days of imprisonment .  Figure 4: Case translation utilize the back-off and the continuous space language model (CSLM). ","acronyms":[[147,151]],"long-forms":[[114,145]]},{"text":"We set aside the blind TEST set for rating the final performance of our named entity recognition (NER) and relation extraction (RE) 2http:\/\/ciphers.google.com\/apis\/ajaxsearch","acronyms":[[132,134],[102,105]],"long-forms":[[111,130],[76,100]]},{"text":"5.1 Experimental Settings To evaluate our algorithm?s performance, we designed a Mechanical Turk (MTurk) experiment in which human annotators assess the quality of the","acronyms":[[98,103]],"long-forms":[[81,96]]},{"text":"Univers i ty  of Vienna  The fiirst part of this paper is dedicated to an overv iew  of the parser of the system VIE-LANG (Viennese Language  Understand System).","acronyms":[[112,120]],"long-forms":[[122,139]]},{"text":"In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 417?422.","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":"AT&T Labs-Research Abstract Statistical Machine Translation (SMT) systems are heavily dependent on the qual-","acronyms":[[61,64],[0,4]],"long-forms":[[28,59]]},{"text":"labels.5 We simulate a user?s constraints by ranking words in the training split by their information gain (IG).6 After ranking the top 200 words for each class","acronyms":[[108,110]],"long-forms":[[90,106]]},{"text":"ergistic working of several components: speech recognition (ASR), spoken language understanding (SLU), dialog management (DM), language generation (LG) and text-to-speech synthesis","acronyms":[[122,124],[60,63],[97,100],[148,150]],"long-forms":[[103,120],[40,58],[66,95],[127,146]]},{"text":"have been opened.  Named entity recognition (NER) is one of the  many fields of NLP that rely on machine learn?","acronyms":[[45,48],[80,83]],"long-forms":[[19,43]]},{"text":"lowing rules are used in the detailed example.  if tense of el and of e2 is simple past (SP)  with perfective AP tben there is justification for ","acronyms":[[89,91],[110,112]],"long-forms":[[76,87]]},{"text":"The data are package as the loads of incremental units (IU) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and groundedin link (GRIN), the former allowing the linking of IUs as a grew sequence, the latter allowing that","acronyms":[[172,175],[59,61],[101,104],[199,203],[241,244]],"long-forms":[[154,170],[40,57],[181,197]]},{"text":"ed AIC   a) Dialogue ActsOnly (DAONLY)    N (number of hidden states) ","acronyms":[[31,37],[3,6]],"long-forms":[[12,29]]},{"text":"LL; it is calculated from contingency tableau information as follows: LO = log (O11 + 0.5)(O22 + 0.5)","acronyms":[[68,70]],"long-forms":[[73,76]]},{"text":"  Figure 1: Example of a BDT sentence in the CoNLL-X format  (V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj =  clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing.,","acronyms":[[77,81],[100,104],[147,150]],"long-forms":[[84,98],[107,118],[153,163]]},{"text":"  ? suffixes (SUF), such as verb endings, nominal  cases, the nominal feminine ending -at, etc.;","acronyms":[[14,17]],"long-forms":[[4,12]]},{"text":"Step 1  Tagging using Global Distribution (NEIG) Trained Model Statistical System (MEMM) Step 2 MEMM Based Statistical System(S-MEMM)Final Tagged DataSet Added    as a feature","acronyms":[[126,132],[43,47],[83,87],[146,153]],"long-forms":[[89,100]]},{"text":"Among Proc. of IEEE\/ACL workshop on Spoken Language Technology (SLT). ","acronyms":[[61,64],[12,20]],"long-forms":[[33,59]]},{"text":"parser on merged development PTB\/PRBK data (section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity.","acronyms":[[75,77],[29,37],[90,92]],"long-forms":[[78,88],[93,114]]},{"text":"In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels work on mereotopological","acronyms":[[62,64]],"long-forms":[[41,60]]},{"text":"{FirstName.SecondName}@dfki.de Condensed The IDEX system is a prototype of an interactive dynamic Information Extract (IE) system. A user of the system","acronyms":[[121,123],[44,48]],"long-forms":[[97,119]]},{"text":"Improvement part-of-speech tagging for context-free parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai, Thai.","acronyms":[[144,150]],"long-forms":[[81,142]]},{"text":"Perhaps the most well-known is that of Merialdo (1994), who using MLE to train a trigram cloaked Markov model (HMM). More recent","acronyms":[[109,112],[65,68]],"long-forms":[[88,107]]},{"text":"be expressed via correspondences. We will define a  variant of SSTC called synchronous SSTC (S-SSTC).  ","acronyms":[[93,99],[63,67]],"long-forms":[[75,91]]},{"text":" From the results shown in Table 3, we could find the proposed semantic word embedding (SWE) model can consistently achieve 0.8% (or more) ab-","acronyms":[[88,91]],"long-forms":[[63,86]]},{"text":"Chief+Routes 80.0 72.8 31.8 22.4 Path 80.0 72.7 31.6 22.0 Table 5: Parsing accuracy (AS = attachment score, EM = exact matches; U = unlabeled, L = labeled) Unlabeled Labeled","acronyms":[[82,84],[105,107]],"long-forms":[[87,103],[110,121],[127,136],[142,149]]},{"text":"context    Chinese Backgrounds(CC): ???????? ","acronyms":[[27,29]],"long-forms":[[11,25]]},{"text":" 3 Results and Discussions Chain frequency (CF) and chain length (CL) reflect the dyad?s tweeting behaviors.","acronyms":[[44,46],[66,68]],"long-forms":[[27,42],[52,64]]},{"text":"in the clustering.  Cumulative Micro Precision (CMP). As highlighted","acronyms":[[48,51]],"long-forms":[[20,46]]},{"text":" 2.6 Adverb  Group (AdvG)   Adverb grouped (AdvG) is used in the realization  of several circumstantial roles given in Sec- ","acronyms":[[42,46],[20,24]],"long-forms":[[28,40]]},{"text":"SaRAD .891 .919 .905 ALTAR .961 .920 .940 Shang & Sch?utze (CS) .942 .900 .921 Nadeau & Turney (NT) .954 .871 .910","acronyms":[[60,62],[0,5],[96,98]],"long-forms":[[42,58],[79,94]]},{"text":" Take for example the word clive which could be a  adequate (NP) 1 or a common noun (NN) (ignoring ca-  pitalization of proper nouns for the moment).","acronyms":[[83,85],[59,61]],"long-forms":[[77,81]]},{"text":" In this definition, WHEN has two formal parameters x and y; each of them refers to a  situation that occurs at the same point in time (PTIM). Any occurrence of the relation ","acronyms":[[136,140]],"long-forms":[[121,134]]},{"text":"monly uses tasks: small vocabulary recognition (TI-digits), read and spontaneous text dictate (WSJ), and goal-oriented spoken dialog (ATIS). The broadcast news task is quite general, covering a","acronyms":[[136,140],[48,57],[97,100]],"long-forms":[]},{"text":"Each corpus uses a varied set of entity labels.  MUC branded locations (LOC), organisations (ORG) and personal names (PER), in addition to numeri-","acronyms":[[73,76],[94,97],[52,55],[119,122]],"long-forms":[[62,71],[79,92],[103,111]]},{"text":"pears or no other attribute is included.  Surface Text (ST): To measure the effectiveness of the semantic analysis (attribute labels and ","acronyms":[[56,58]],"long-forms":[[42,54]]},{"text":"(3) range, e.g., (age(value(0 100))).  The notion of IMPORTANCE VALUES (IVs) are  introduced here and are used to numerically describe ","acronyms":[[72,75]],"long-forms":[[53,70]]},{"text":"Suppose that the feature f2 is an agreements feature and that a locale  tree t which is a projections of this ID rule has been constructed, then  the Agreement Principle (AP) forces X = Y = Z and therefore the  AP has to consider three examples 6: ","acronyms":[[168,170],[107,109],[208,210]],"long-forms":[[147,166]]},{"text":"In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistic (ACL). ","acronyms":[[92,95]],"long-forms":[[49,90]]},{"text":"1 Introduction Newswire text has long been a primary target for natural linguistics processing (NLP) technologies such as information extraction, summarization, and ques-","acronyms":[[93,96]],"long-forms":[[64,91]]},{"text":"outer: the perceived concrete or abstract source, aim, or  location of the action,event, or state  Correspondent (CAR):  inside: the entity perceived as being in correspondence with ","acronyms":[[115,118]],"long-forms":[[100,113]]},{"text":"to find words with the same meanings. We use a simple approach called the Direct Reversal (DR) approach in (Lam and Kalita, 2013) to create","acronyms":[[91,93]],"long-forms":[[74,89]]},{"text":"1 In t roduct ion   Finding base noun phrases is a sensible first step  for many natural anguage processing (NLP) tasks:  Accurate identification of base noun phrases is ar- ","acronyms":[[109,112]],"long-forms":[[81,107]]},{"text":"of spoken dialogue systems is database retrieval.  Some IVR (interactive voice response) systems using the speech recognition technology are being put","acronyms":[[56,59]],"long-forms":[[61,87]]},{"text":" 1 Introduction Weighted Context Free Grammars (WCFG) define an important categories of languages.","acronyms":[[48,52]],"long-forms":[[16,46]]},{"text":"4 Experiments 4.1 Event Extraction We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).","acronyms":[[92,95],[105,110]],"long-forms":[[68,90]]},{"text":"@\"' itself is counted. Another way is to count how  many parts of words (PWs) are eontMnmd in the  SA.","acronyms":[[73,76],[99,101]],"long-forms":[[57,71]]},{"text":" 1 Introduction Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to","acronyms":[[42,45],[82,84]],"long-forms":[[16,40],[65,80]]},{"text":"con. In Proceedings of the 31st Annual Conference of the Cognitive Scientific Society (CogSci). ","acronyms":[[84,90]],"long-forms":[[57,82]]},{"text":"196  Proceedings of the 2014 Conference on Empirical Method in Natural Language Treatment (EMNLP), pages 1779?1785, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"1 Introduction In the past few ages, a number of studies have focused on verbal semantic role labeling (SRL). ","acronyms":[[105,108]],"long-forms":[[81,103]]},{"text":"PP) MDI Missed Samples (MS) Bigram Missed Sample (MS) Figure 4: Valuing of PP and MS for automata for ad-hoc automata","acronyms":[[51,53],[0,2],[4,7],[24,26],[82,84],[75,77]],"long-forms":[[35,49],[8,22]]},{"text":"nority preference algorithm that mannequins bridging recognition as a subtask of learning finegrained information status (IS). We substan-","acronyms":[[118,120]],"long-forms":[[98,116]]},{"text":"discourse analysis phase, the situation  frame is interpreted, resulting in one or  more instantiated knowledge base (KB)  objects, which are state or event ","acronyms":[[118,120]],"long-forms":[[102,116]]},{"text":"Information Retrieval (CLIR). It is also important for Machine Translation (MT), especially when the languages do not use the same scripts.","acronyms":[[76,78],[23,27]],"long-forms":[[55,74],[0,21]]},{"text":"2011). The default loss function is anticipate error (EE) (Och, 2003; Cherry and Foster, 2012).","acronyms":[[52,54]],"long-forms":[[36,50]]},{"text":"Relative standard deviation of three intervals, left edge to anchor (LE-A), centre to  anchorage (CC-A), right edge to anchor (RE-A) calculated across productions of word sets by one ","acronyms":[[124,128],[69,73],[95,99]],"long-forms":[[102,122],[48,67],[76,93]]},{"text":"symbols as well as full names. Groups such as the  Human Genome Organisation (HUGO), Mouse Genome  Institute (MGI), UniProt, and the National Center for ","acronyms":[[78,82],[110,113]],"long-forms":[[51,76],[85,108]]},{"text":" 2. Effort of Association (EA): a mc~sure of the effort  need to associate some entity with lira description ","acronyms":[[27,29]],"long-forms":[[4,25]]},{"text":"a major Department of Agriculture system, due to inadequate agency planning.  Complete sets of the Federal mfomtion processing staodards (FIPS) are now avail-  able from the National Bureau of Standards at $46.00 each.","acronyms":[[138,142]],"long-forms":[[99,136]]},{"text":"It includes the four original partners  of the AFTER project and the following newest partners: University of Amsterdam (UvA) in the Netherlands, Free University of Bolzano-Bozen (FUB) ","acronyms":[[118,121],[47,52],[177,180]],"long-forms":[[93,116],[143,175]]},{"text":"280  Proceedings of the 2014 Conference on Empirical Means in Natural Language Treat (EMNLP), pages 1203?1209, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"  1. RecallCorrectTransliteration  (RTrans)  The recall was computed using the sample as ","acronyms":[[36,42]],"long-forms":[[5,33]]},{"text":" c?2015 Association for Computational Linguistics Building a Scientific Concept Hierarchy Database (SCHBASE) Eytan Adar","acronyms":[[100,107]],"long-forms":[[61,98]]},{"text":"the  locat ions  shown in F ig .4 .  The s lo t  SH~P represent  whether  the  par t   cor respond ing  to  th i s  frame is  a reg ion(~EG)  or a branch(BR A) The  SU~P s lo t  records  i t s  subpar ts  and the i r  locat ions  of or re ia t ions  to ","acronyms":[[154,158],[49,53],[136,139],[165,169]],"long-forms":[[147,152]]},{"text":"Technology (NAST), Lao PDR  ? Madan Puraskar Pustakalaya (MPP),  Nepal  ","acronyms":[[58,61]],"long-forms":[[30,56]]},{"text":"The task of location normalization is to identify  the rectified sense of a probable ambiguous  location Named Entity (NE). Ambiguity is very ","acronyms":[[117,119]],"long-forms":[[103,115]]},{"text":"Clear And Simple English (CASE) Caterpillar Fundamental English (CFE) Caterpillar Technical Englishmen (CTE) Diebold Controlled English (DCE)","acronyms":[[101,104],[26,30],[65,68],[134,137]],"long-forms":[[70,99],[0,24],[32,63],[106,132]]},{"text":"sual scenes. Across their seminal work Dale and Reiter (1995) present the Incremental Algorithm (IA) for GRE.","acronyms":[[93,95],[101,104]],"long-forms":[[70,91]]},{"text":" 1 Introduction Statistical machine translation (SMT) starts from sequence-based templates.","acronyms":[[49,52]],"long-forms":[[16,47]]},{"text":"17 classes set in Sun et al(2008).  We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but","acronyms":[[69,73]],"long-forms":[[48,67]]},{"text":"variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods","acronyms":[[71,74]],"long-forms":[[47,69]]},{"text":"description model, the Dublin Core Metadata Set, together with an interchange methods provided by the Openings Archives Initiative (OAI), make it possible to construct a union catalog over","acronyms":[[127,130]],"long-forms":[[101,125]]},{"text":"edge of syntax and semantics.   In connection to conjunct verbs (ConjVs),  (Mohanty, 2010) defines two typing of conjunct ","acronyms":[[65,71]],"long-forms":[[49,63]]},{"text":"extract phrasal translations or transliterations of  expressions based on machine learning, or more  specifically the conditional random realms (CRF)  model.","acronyms":[[140,143]],"long-forms":[[113,138]]},{"text":"model.  Case Frame Editor (CFE): Maintains the lexical  Case Frame Factbase, a data file of how to infer ","acronyms":[[27,30]],"long-forms":[[8,25]]},{"text":" 4 DOM Tree Alignment Model  The Document Object Paragon (DOM) is an application programming interface for valid HTML ","acronyms":[[56,59],[3,6],[111,115]],"long-forms":[[33,54]]},{"text":" (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).","acronyms":[[47,49],[2,4],[91,93]],"long-forms":[[52,61],[7,16],[96,112]]},{"text":"1 Introduction Semantic Parsing, the process of converting text into a formal meaning representation (OLLI), is one of the essentials challenges in natural language process-","acronyms":[[102,104]],"long-forms":[[78,100]]},{"text":"Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on text-based traits for automatically predicting eight different discourse acts derived from a taxonomy called Verbal Responses Modes (VRM). The experiments are conducted","acronyms":[[217,220]],"long-forms":[[194,215]]},{"text":"121 domain adaptation algorithm mentioned in (Daume, 2007) based on Maximum Entropy model (MaxEnt) (Ratnaparkhi, 1996).","acronyms":[[91,97]],"long-forms":[[68,83]]},{"text":"In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440?447, Hong Kong, October.","acronyms":[[86,89]],"long-forms":[[43,84]]},{"text":"If we look at the permutations, we have in 2. to-  picalization, with OBJect NP in spotlight structural (FS) I in I. the grammatical relations of 25 are preserved ","acronyms":[[100,102],[77,79]],"long-forms":[[83,98]]},{"text":"H A V f  CSEXCH F O R  MOVEF ACROSS 11  C H A N gram E ,  CALL EL3MOP P O R  E R una S E  0 I 2   ANTEST CALLED FOR 14'IGLOT \" (AACC) S D =  15. RES= '3.","acronyms":[[103,106],[119,123],[125,128],[9,15],[16,21],[23,28],[136,139]],"long-forms":[[89,102],[107,115]]},{"text":"adaptation led to a considerable improvement of +4.1 BLEU and large improvements in terms of METEOR and Translation Edit Rate (TER). We","acronyms":[[127,130],[53,57],[93,99]],"long-forms":[[104,125]]},{"text":"is still room for enhancements.  2 The Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough,","acronyms":[[66,71]],"long-forms":[[38,64]]},{"text":"In our particular application, accesses to Getty?s Art and Architecture Thesaurus (AAT), to other museum and collecting databases or online auction cata-","acronyms":[[81,84]],"long-forms":[[49,79]]},{"text":"ment. In Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci), Sapporo. ","acronyms":[[82,88]],"long-forms":[[55,80]]},{"text":"Given an input pair (q,a), where q is a question and a is a candidate answer, first we retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a","acronyms":[[117,120]],"long-forms":[[100,115]]},{"text":"and Innovation action). This research is part of the Interactive sYstems for Responding Search (IYAS) project, conducted by the Arabic Parlance Tech-","acronyms":[[92,96]],"long-forms":[[53,90]]},{"text":"the reference than the rest. Considering this, we use a Longest Common Subsequence(LCS) based criterion to calculate s(x, y).","acronyms":[[83,86]],"long-forms":[[56,81]]},{"text":"model for sequence classification. In Proceedings of International Conference on Data Mining (ICDM). ","acronyms":[[94,98]],"long-forms":[[53,92]]},{"text":"Aux tackle this  problem, we propose to employ the  Support Vector Machines(SVM) in  determining the grammatical functions.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"with one object sentences (IMP_VNP), V_NP_PP  agentless passive sentences (PAS_VNPP), V_NP bypassives (BYPAS_VN), and N_PP clauses (NPP) and  these are all decisions that happen in the realiser, ","acronyms":[[132,135],[27,34],[37,44],[75,83],[103,111]],"long-forms":[[118,122],[86,101],[46,73]]},{"text":"3http:\/\/www.cjk.org 4https:\/\/translit.i2r.a-star.edu.sg\/news2009\/assess\/ 5The six metrics are Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Grading","acronyms":[[122,125]],"long-forms":[[103,111]]},{"text":"The most common and obvious way to  deal with disjunctive constraints i to expand the grammat-  ical description to disjunctive normal form (DNF) during a  pre-processing step, thereby eliminating disjunction from the ","acronyms":[[141,144]],"long-forms":[[116,139]]},{"text":"2 D IA        1 In order to testing this hypothesis,            1   a double-stranded oligonucleotide investigative that corresponds to bp +10 to +60 of the CCR3 gene was prepared, 3 0 NN            referred to as E1-FL (exon 1- full length, Figure 2A). 3 D A    1 1   This is the exact sequencing 3 D N          that was eliminated in the CCR3(-exon1).pGL3 plasmid 3 D N          that revealed cutback activity 3 D N      1   comparing to the full lifespan 1.6 kb construct [27].","acronyms":[[203,208],[146,150],[174,176],[324,328]],"long-forms":[[210,229]]},{"text":"(DE), Greek (EL), English (EN), Spaniards (ES), French (FR), Italiano (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) ?","acronyms":[[123,125],[1,3],[13,15],[27,29],[41,43],[54,56],[68,70],[81,83],[93,95],[109,111],[137,139],[154,156]],"long-forms":[[114,121],[18,25],[32,39],[46,52],[59,66],[73,79],[86,91],[98,107],[128,135],[145,152]]},{"text":"referred to as Maximum Mutual Information Estimation (MMIE) and the second component Maximum Likelihood Estimation (MLE), therefore in this paper we use a brief notation for (1) just for conve-","acronyms":[[116,119]],"long-forms":[[89,114]]},{"text":"probabilities. Analysis showed that the correct tag most fre-  quently missing from the lattice was the DT (determiner)  tag.","acronyms":[[104,106]],"long-forms":[[108,118]]},{"text":"blogging which accumulated a grandes number of positions during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased","acronyms":[[123,125],[140,142],[82,84],[167,170],[181,183],[174,176]],"long-forms":[[105,121],[129,138],[68,80],[150,164]]},{"text":"Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems.","acronyms":[[107,110],[23,27]],"long-forms":[[78,105],[0,21]]},{"text":" The practical application of flame-based knowledge-based systems, such as in expert systems, requires the maintenance of  potentially very large amounts of declarative knowledge stored in their knowledge bases (KBs). As a KB grows in size and ","acronyms":[[212,215],[223,225]],"long-forms":[[195,210]]},{"text":"The problem of assessments machine translation output without reference translations is called quality evaluations (QE) and has recently been the centre of attention (Bojar et al.,","acronyms":[[114,116]],"long-forms":[[94,112]]},{"text":"oleary@cs.umd.deu Abstract The Text Analysis Conference (TAC) ranks summarization systems by their average score","acronyms":[[57,60]],"long-forms":[[31,55]]},{"text":"Haile) and (?????, Selassie). TM has been shown to be effective in diverse Informational Retrieval (IR) and Natural Language Processing (NLP) requests. For instance, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieving documents and then using transliterations to expand queries (Udupa et al, 2009a).","acronyms":[[98,100],[135,138],[185,188],[189,191],[30,32]],"long-forms":[[75,96],[106,133]]},{"text":"end returnmodels [] Algorithm 1: Positive Diversity Tuning (PDT) lectively produce very diverse translations.3","acronyms":[[60,63]],"long-forms":[[33,58]]},{"text":"(wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle). ","acronyms":[[127,130],[86,89],[82,85],[123,126]],"long-forms":[[97,106],[76,81]]},{"text":"words in similar context have similar meanings ?  distributed semantic models (DSM)s build vector representations based on corpus-extracted backdrop.","acronyms":[[79,82]],"long-forms":[[50,77]]},{"text":" In addition, NE alignment can be very useful for  Statistical Machines Translation (SMT) and CrossLanguage Information Retrieval (CLIR).","acronyms":[[84,87],[14,16],[130,134]],"long-forms":[[51,82],[93,128]]},{"text":"transitions, depending on whether the backward-looking center of Kt?1 is maintained or not in Ui and on whether CB(Ui) is also the most crucially ranked entity (CP) of Ui: Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most vastly classed CF (CP) of Ui (i.e., CP(Kt) = CB(Ui))","acronyms":[[190,193],[112,114],[94,96],[65,67],[158,160],[196,198],[252,254],[256,258],[282,284],[273,275]],"long-forms":[[176,188],[199,201]]},{"text":"173 Figure 3: Frequency of word classes in the three corpora (BN = Telecast News, Est = Press, Euro = Europarl).","acronyms":[[62,64],[96,100]],"long-forms":[[67,81],[103,111]]},{"text":" Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996),","acronyms":[[57,59]],"long-forms":[[37,55]]},{"text":"The full TBCNN-pair model outperforms all existing sentence encoding-based approaching, encompass a 1024d gated recurrent unit (GRU)-based RNN with ?","acronyms":[[127,130],[9,14],[138,141]],"long-forms":[[105,125]]},{"text":"When using only the former type of feature function, our classifier is equivalent to a maximum entropy (MaxEnt) model. ","acronyms":[[104,110]],"long-forms":[[87,102]]},{"text":"Condensed This paper reports on the participation of the TALP Research Center of the UPC (Universitat Polit?cnica de Catalunya) to the ACL WMT 2008 assessing","acronyms":[[84,87],[56,60],[134,137],[138,141]],"long-forms":[[89,112]]},{"text":"init ial ly be described. I wil l  c laim that  such an initial descr ipt ion (ID) is  cr it ical  to both model synthesis and ","acronyms":[[79,81]],"long-forms":[[56,69]]},{"text":"37  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 97, Reykjavik, Sweden, April 27, 2014.","acronyms":[[73,78]],"long-forms":[[39,71]]},{"text":" 3 Keystroke Data Collection Amazon?s Mechanical Turkic (MTurk) is a web service that enable crowdsourcing of tasks that are dif-","acronyms":[[55,60]],"long-forms":[[38,53]]},{"text":" 1 Introduction  Natural Language Generation (NLG) systems must of  course be evaluated, like all NLP systems.","acronyms":[[46,49],[98,101]],"long-forms":[[17,44]]},{"text":"Conforming to the fifth regulations, the Arabic letter Z may equalize an empty string on the English side, if there is an English consonant (EC) in the right context of the English side.","acronyms":[[130,132]],"long-forms":[[111,128]]},{"text":"each new realms and scenario, as discussed in the impending section.  The lexical analysis module (LexAn) is responsible for splitting the document into sentences, and the sentences into tokens.","acronyms":[[94,99]],"long-forms":[[69,85]]},{"text":"-4, -12, and -109 are all disjoint speaker sets.)  (Codes: SD=speaker dependent (2400 training sentences for RM2), MS=multi-speaker, SI=speaker independent, -4=all 4  RM2 speakers combined, -12=all 12 RM1 SD speakers combined, -109=109 RM1 SI training speakers, SDG=SD Gaussians, ","acronyms":[[59,61],[133,135],[109,111],[115,117],[167,169],[201,203],[205,207],[236,238],[240,242],[262,265],[266,268]],"long-forms":[[62,79],[136,155],[118,131]]},{"text":"figure 5).  Foundations clauses (BC)  are subclauses of type sub-  junctive and subordinate.","acronyms":[[26,28]],"long-forms":[[12,24]]},{"text":"Table 10: A=acoustic, P=psycholinguistic, POS=part-of-speech, C=complexity, F=fluency, VR=vocabulary richness, CFG=CFG production rule features.","acronyms":[[87,89],[42,45],[111,114],[115,118]],"long-forms":[[90,109],[12,20],[24,40],[46,60],[64,74],[78,85]]},{"text":"There is no person boiling noodles A woman is boiling noodles in water Example 9051 (ENTAILMENT) A pair of kiddies are sticking out blue and archer colored tongues","acronyms":[[85,95]],"long-forms":[[71,83]]},{"text":"cos(d, c).  Candidate Rank (CR) The features described so far disambiguate every surface form s ?","acronyms":[[28,30]],"long-forms":[[12,26]]},{"text":"(Figure 2). Argviz is a web-based application, built using Google Web Toolkit (GWT),4 which enabled users to visualizing and manipulate SITS?s outputs en-","acronyms":[[79,82],[133,138]],"long-forms":[[59,77]]},{"text":"PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a).","acronyms":[[154,162],[28,32],[41,45],[139,145],[178,186]],"long-forms":[[164,172],[188,199]]},{"text":"non relevant texts has the lower expectation. Figure 1 describes the probability density function (PDF ) for domain frequency scores of the SPORT domain","acronyms":[[99,102],[140,145]],"long-forms":[[69,97]]},{"text":"kept on a ventilator for medical reasons.     Change of state (COS) is most often understood  as an aspectual difference that is reflected in verb ","acronyms":[[63,66]],"long-forms":[[46,61]]},{"text":"1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). ","acronyms":[[105,108]],"long-forms":[[81,103]]},{"text":"denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ","acronyms":[[92,94],[113,115],[79,81]],"long-forms":[[95,111],[116,131],[82,90]]},{"text":"known as conditional random fields (CRFs) (Lafferty et al, 2001), when all variables are observed, and as hidden conditional random fields (HCRFs) (Quattoni et al, 2007), when only a subset of the variables are","acronyms":[[140,145],[36,40]],"long-forms":[[106,138],[9,34]]},{"text":"3.3 Participants and Task Eighty participants were recruited from Amazon?s Mechanical Turk2 (MTurk) for this between2http:\/\/www.mturk.com","acronyms":[[93,98]],"long-forms":[[75,91]]},{"text":"In Processdings of Sixth World Conference on  Language Resources and Evaluation (LREC),  pages 2961-2968, Marrakech, Morocco.","acronyms":[[89,93]],"long-forms":[[54,72]]},{"text":"terial with relevant events will be done along the 1MUMIS is an on-going EU-funded project within the Information Society Program (IST) of the European Union, section Human Language Technology (HLT).","acronyms":[[131,134]],"long-forms":[[102,121]]},{"text":"pendency and constituency tracks are shown in table 1. The label attachment score (LAS) was used by the organizer for evaluating the dependency versions,","acronyms":[[83,86]],"long-forms":[[59,81]]},{"text":"for the discrimination of akin languages: The DSL corpus collection. Onto Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. ","acronyms":[[145,149],[49,52]],"long-forms":[[106,143]]},{"text":"contributions to sentence similarity. During most cases, the longer common sequence (LCS) the deux sentences have, the higher similarity score the sentences","acronyms":[[81,84]],"long-forms":[[57,79]]},{"text":"local  cons t i tuents .  These i n c l u d e  Hsimplem noun expression (NPs) and  prepositional phrases (PPs), (\"simplen meaning 'up to the head noun but  not containing any modified clauses or phrases\"),  and verb grouped (VGs) ","acronyms":[[103,106],[70,73],[221,224]],"long-forms":[[80,101],[56,68],[208,219]]},{"text":"~ '1  procedural component  Q data structure  SSP = SemanticJSyntacticJPhonological  Figl~e 1: The SYNPHONICS Formulator ","acronyms":[[46,49],[99,109]],"long-forms":[[52,83]]},{"text":"NLP-Based  Index ing  in  During fo rmat ion   Ret r ieva l   In information retrieval (IR), a typical task is to  fetch relevant documents from a large files in ","acronyms":[[84,86],[0,3]],"long-forms":[[61,82]]},{"text":"Approach for Arabic-English Named Entity Translate, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages (ACL),  Academics of Michigan, Ann Arbor","acronyms":[[136,139],[73,76]],"long-forms":[[103,134]]},{"text":"Across this paper we shows how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank, with about 1 mil-","acronyms":[[100,103]],"long-forms":[[79,98]]},{"text":" 1 Introduction Information recoveries (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and sev-","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":" In our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both pieces of text.","acronyms":[[41,48]],"long-forms":[[50,57]]},{"text":"For aviation incidents, the advantages of the proposed prior is reflected in the placements (LO) and countries (CO) slots, which may confuse the various models as they both belong to the entity type loca-","acronyms":[[107,109],[90,92]],"long-forms":[[98,105],[80,88]]},{"text":"wqK ,   and the word sequence of the web page,                    A=WAH=wA1, wA2, ?, wAL,  ","acronyms":[[68,70]],"long-forms":[[71,74]]},{"text":" 246  AO = all objects  MO = matched objects ","acronyms":[[6,8],[24,26]],"long-forms":[[11,22],[29,43]]},{"text":"translate quality include the ridge regression (RR) and support vector regression (SVR) with RBF (radial based functions) kernel (Smola and Scho?lkopf, 2004).","acronyms":[[95,98],[50,52],[85,88]],"long-forms":[[100,122],[32,48],[58,83]]},{"text":"s+trsl Alhnd qmrA<STnAEyA <lY Almryx ? India will dispatch a satellite to Mars [in 2013]?. Throughout any trees node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB = verb, PRT = particle, PROP = adequate noun, NOM = nominal)and relating (MOD = modifier, SBJ = subject, OBJ = object). The terms under the line are the Buckwalter POS tag, thelemma and the gloss, respectively.","acronyms":[[181,184],[193,196],[209,213],[229,232],[257,260],[273,276],[288,291],[176,179],[147,152],[347,350]],"long-forms":[[187,191],[199,207],[216,222],[235,246],[263,271],[279,286],[294,300]]},{"text":"Table 4: Human expert assess accuracy (Acc.)  and full cluster accuracy (FAcc.) of models on","acronyms":[[76,81],[42,45]],"long-forms":[[53,74],[32,40]]},{"text":"events coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard informations extraction (IE) pipeline. ","acronyms":[[136,138]],"long-forms":[[112,134]]},{"text":"Table 1: Labelled attachment score on the two test sets of the best single parse, blends with weights set to PoS characterised attachment score (LAS) and blended with learning weights.","acronyms":[[141,144],[110,113]],"long-forms":[[114,139]]},{"text":"tation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as \/html[1]\/body[1]\/table[2]\/tr\/td[1]","acronyms":[[111,116]],"long-forms":[[101,109]]},{"text":"2115  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[74,76],[32,36]],"long-forms":[[54,72]]},{"text":" 1 Introduction  Statistical Machine Translation (SMT) is attracting more attentions than rule-based and example-","acronyms":[[50,53]],"long-forms":[[17,48]]},{"text":"In this work, we apply Dirichlet Process Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering.","acronyms":[[115,118],[57,62]],"long-forms":[[86,113],[23,55]]},{"text":"the source and target sides are lexicons (piers) 2) Unlexicalized (ULex): all leaf nodes in both the 46","acronyms":[[71,75]],"long-forms":[[56,69]]},{"text":"set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the","acronyms":[[140,142]],"long-forms":[[120,138]]},{"text":" To resolves this problem, we introduce  Document oriented Preference Sets(DoPS). The ","acronyms":[[72,76]],"long-forms":[[38,70]]},{"text":"= li), ? Hierarchical loss (H-Loss) function is defined as:","acronyms":[[28,34]],"long-forms":[[9,26]]},{"text":"scoring the candidate set of which class of anaphoric expression (DNOM = definite  NP, PER{I,2,3} = first\/second\/third person pronouns, POS{1,2,3} = first\/second\/third  person possessives, RELA = relative pronouns, REFL = reflexive\/reciprocal pronouns). ","acronyms":[[189,193],[215,219],[66,70],[83,85],[87,90],[136,139]],"long-forms":[[196,204],[222,231],[73,81]]},{"text":"on Artificial Intelligence (KI2002), volume 2479 of Lecture Notes in Artificial Intelligence (LNAI), pages 18?32, Aachen, Germans, September. ","acronyms":[[94,98],[28,34]],"long-forms":[[52,92]]},{"text":"guages: Briton sign language (BSL), Danish (DSL), Frenchman Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT). The data for the","acronyms":[[96,99],[31,34],[45,48],[67,71],[83,86],[113,116]],"long-forms":[[89,94],[8,29],[37,43],[51,65],[74,81],[106,111]]},{"text":"proach of (Liu et al, 2004), in which IDs (categories seeds) and instances are represented by vectors in a ordinary IR-style Vector Space Model (VSM), and similarity is measured by the cosine functions:","acronyms":[[140,143],[111,113]],"long-forms":[[120,138]]},{"text":"day(x, fri) ? during(x, night) Here the reasonable form (LF) is a lambda-calculus expression defined a set of entities that are flights","acronyms":[[54,56]],"long-forms":[[40,52]]},{"text":"Since the bilingual corpus is only aligned at the document level, we fulfilled sentence alignment using the Champollion Tool Kit (CTK).4 After removing punishments with no aligned sentence, a generals","acronyms":[[130,133]],"long-forms":[[108,128]]},{"text":" Entities On the level of entity extraction, Named Agency (NE) were defined as proper names and quantities of interest. ","acronyms":[[61,63]],"long-forms":[[45,59]]},{"text":"phrases (NPs) (representing 49.6% of the total number of words), verb phrases (VPs), prepositional phrases (PPs), adjectival phrases (ADJPs), and quantity words (QPs), representing 99.1% of","acronyms":[[136,141]],"long-forms":[[116,134]]},{"text":"larities of MSA. ARET has two subparts tools : the  Arabic Reading Facilitation Tool (ARFT) and the  Arabic Reading Assessment Tool (ARAT).","acronyms":[[86,90],[17,21],[12,15],[133,137]],"long-forms":[[52,84],[101,131]]},{"text":"We also wanted to determine if information about 6http:\/\/www.isi.edu\/?ravichan\/YASMET.html dialog acts (DA) helps the ranking task. If we","acronyms":[[104,106]],"long-forms":[[91,102]]},{"text":"Table 2: Overall scores of whole task as well as separately for each annotation format in terms of labeled precision (LP), recall (LR) and F 1","acronyms":[[118,120],[131,133]],"long-forms":[[99,116],[123,129]]},{"text":"structures. 4 Such an algorithm has to defining for every LFG a relation  F?(~,s)  (s is generable from (I)) between directed acyclic graphs (DAGs)  and terminal rope.","acronyms":[[140,144],[56,59],[72,74]],"long-forms":[[115,138]]},{"text":"Kearns (2002) distinguishes between two usages of light verbs in LVCs: what she calls a true light verb (TLV), as in give a groan, and what she calls a vague action verb (VAV), as in","acronyms":[[105,108],[65,69],[171,174]],"long-forms":[[88,103],[152,169]]},{"text":"tion of precise probability scores for partial hypotheses contain-  ing islands, in the context of  a Stochastic-Context-Free-Grammar  (SCFG) for Language Modeling (LM). The second issue is the ","acronyms":[[165,167],[136,140]],"long-forms":[[146,163],[102,133]]},{"text":"which allows POS tagged and chunked data to be represented (including recursion), and Shakti Standard Format (SSF)2. The editor allows","acronyms":[[110,113],[13,16]],"long-forms":[[86,108]]},{"text":" 3.1 The Beta Process and the Bernoulli treated The beta process(BP) (Thibaux and Amman, 2007; Paisley and Carin, 2009) and the related Indian buf-","acronyms":[[65,67]],"long-forms":[[52,64]]},{"text":" 1 Introduction Information Extraction (IE) refers to the problem of extracting structured information from unstructured","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"Table 4: Accuracies on a balanced testing set (random baseline: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?","acronyms":[[67,69],[84,88]],"long-forms":[[72,82],[93,100]]},{"text":"any hypotheses between frames and event.  (2) SameFrame (SF) is the second baseline system, which apply H1 over the results from AN-","acronyms":[[58,60]],"long-forms":[[47,56]]},{"text":"alignment G (both A and G can be split into two subsets AS ,AP and GS , GP , respectively representing Sure and Probable alignments) Precision (PT ), Recall (RT ), F-measure (FT ) and Alignment Error","acronyms":[[144,146],[56,58],[60,62],[67,69],[72,74],[158,160],[175,177]],"long-forms":[[112,131],[150,156],[164,173]]},{"text":"planes, the \"READ\"-units by AND-planes. The flip-  flops (FF) are simple register units and the shift  register is a simple PLA network of well  known ","acronyms":[[58,60],[124,127]],"long-forms":[[44,56]]},{"text":"direct analogy to Sidner's \\[26\\] potential local foci,  and assumes only one temporal referent in the  temporal focusing (TF). ","acronyms":[[120,122]],"long-forms":[[104,118]]},{"text":"the number of sentences in each inspected.  Latent Dirichlet Allocation (LDA) We also enforce Latent Dirichlet Allocation (Blei et al.,","acronyms":[[70,73]],"long-forms":[[41,68]]},{"text":"The input to the first block are the words of the TEXTCHUNK, represented by CW (Collobert and Weston, 2008) embeddings.","acronyms":[[76,78],[50,59]],"long-forms":[[80,100]]},{"text":" 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument ties in natural","acronyms":[[52,55]],"long-forms":[[28,50]]},{"text":"method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation unit (MTUs). Figure 1","acronyms":[[125,129]],"long-forms":[[98,123]]},{"text":"Abstract Data sparsity is one of the main factors that make word sense disambiguation (WSD) difficult.","acronyms":[[87,90]],"long-forms":[[60,85]]},{"text":"sentences. The third, following (Yates et al, 2006), is maximum recall (MR). MR simply predicts that all","acronyms":[[72,74],[77,79]],"long-forms":[[56,70]]},{"text":" In order to overcome these limitations, some  techniques like word sense induction (WSI) have  been suggests for discovering words?","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"formulation a ser ies of inc reas ing ly  soph is t icated natura l  parlance unders tand ing   systems which will serving as an in tegrated  in ter face  to severa l  faci l i t ies at the Pacif ci  F leet Commanding Center:  the In tegrated  Data Foundation (IDB), which conta ins  information  about  ships, the i r  read iness  s tates ,  the i rs  capabi l i t ies,  etc.;","acronyms":[[249,252]],"long-forms":[[225,247]]},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is considered one of the most important prob-","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech-  niques have been re-targeted to enable efficient ","acronyms":[[101,104],[43,45]],"long-forms":[[73,99],[21,42]]},{"text":"performs two vital functions in the case of our Japanese  processing:  1 CN = common noun; SN = sa-inflection noun (  nominalized .verb); VB = verb; VSUF = verb suffix; CM = ","acronyms":[[73,75],[138,140],[149,153],[169,171]],"long-forms":[[78,89],[143,147],[156,167]]},{"text":"We chose Gaussian distributions. If the parents of node X are Y, P (X|Y ) = N(m + W ?","acronyms":[[68,71]],"long-forms":[[56,63]]},{"text":"These features capture the context of the adverb and help in deciding the presence of the manner (MNR) component. ","acronyms":[[98,101]],"long-forms":[[90,96]]},{"text":"Data. We evaluate our model on forecasts paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009).","acronyms":[[85,91]],"long-forms":[[63,83]]},{"text":"and documents created by three or four New York Times columnists (TF = Thomas Friedman, PK = Paul Krugman, MD = Maureeen Dowd, GC = Gail Collins).","acronyms":[[88,90],[66,68],[107,109],[127,129]],"long-forms":[[93,105],[71,86],[112,125],[132,144]]},{"text":"2.1 Conditional Random Fields  Conditional random field (CRF) was an extension  of both Maximum Entropy Model (MEMs) and  Hidden Markov Models (HMMs) that was firstly ","acronyms":[[111,115]],"long-forms":[[88,109]]},{"text":"Sotelo, 2007).  4.3 Polarity Lexicon (LEX) We have built a polarity lexicon with both Positive","acronyms":[[38,41]],"long-forms":[[29,36]]},{"text":"many types of goodness measures, such as Description Length Gain (DLG) recommendations by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (","acronyms":[[126,128],[69,72]],"long-forms":[[108,124],[44,67]]},{"text":"1978. Longman Dictionary of  Contemporary lCnglish (LI)OCE). Long\\]nan, liar- ","acronyms":[[51,59]],"long-forms":[[6,50]]},{"text":" 1 In t roduct ion   Word Sense Disambiguation (WSD) is an open prob-  lem in Natural Language Processing.","acronyms":[[48,51]],"long-forms":[[21,46]]},{"text":"mapping all non-core argument labels in the guessed and correct labelings to NONE.  Coarse Modifier Argument Measures (COARSEARGM). Sometimes it is sufficient to","acronyms":[[119,129],[77,81]],"long-forms":[[84,117]]},{"text":" Across Section 3, we report on two Amazon Mechanical Turk (MTurk) experiment, which demonstrate that crowdsourcing is a feasible way","acronyms":[[56,61]],"long-forms":[[39,54]]},{"text":"Table 1). CC = coordinating cooperating; CD = cardinale number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; OF = to; VB =","acronyms":[[41,43],[79,81],[10,12],[63,65],[91,93],[111,114],[130,134],[189,191],[198,200],[176,178],[157,160]],"long-forms":[[46,54],[84,89],[15,39],[68,77],[105,109],[117,128],[137,155],[163,174],[181,187]]},{"text":" Parsed* Recalled t Prec\/Rec t MLP Prob t  Left Corner (LC) 21797 91.75 9000 .76399 .78156 .175928  KILOGRAMS o LC 53026 96.75 7865 .77815 .78056 .359828 ","acronyms":[[54,56],[29,32],[98,100],[103,105]],"long-forms":[[41,52]]},{"text":"Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein,  1998), latent semantic analyse (LSA) (Gong and  Liu, 2001).","acronyms":[[115,118],[50,53]],"long-forms":[[89,113],[22,48]]},{"text":"appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). ","acronyms":[[101,104]],"long-forms":[[77,99]]},{"text":"Taalkommissie van die Suid-Afrikaanse Akademie vir Wetenskap en Kuns. Centre for Text Technology (CTexT), North-West College, Potchefstroom, South Africa. ","acronyms":[[98,103]],"long-forms":[[70,96]]},{"text":" 1 Introduction  Relation Extraction (RE) aims to identify a set of  predefined relations between pairs of entities in ","acronyms":[[38,40]],"long-forms":[[17,36]]},{"text":" 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher informations (BALSAM)query-by-committee (SVE)random NLPBA","acronyms":[[60,62],[83,86],[107,110],[118,123]],"long-forms":[[39,58],[63,69]]},{"text":"In a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (SLU) is a crucial component aiming at capturing","acronyms":[[125,128]],"long-forms":[[94,123]]},{"text":"Topic: Short, routinely controversial statement that defines the subject of interest.   Context Dependent Claim (CDC): General, and concise declarations, that directly supports or contests  the given Topic.","acronyms":[[111,114]],"long-forms":[[86,109]]},{"text":"model for sequence classification. In Lawsuits of International Conference on Data Mining (ICDM). ","acronyms":[[94,98]],"long-forms":[[53,92]]},{"text":" 3.2 Structures Maps Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner","acronyms":[[76,79],[152,155]],"long-forms":[[50,74],[126,150]]},{"text":"cerd,jurafsky,manning@stanford.edu Abstract Minimum error rate training (MERT) is a widely used learning procedure for statistical","acronyms":[[73,77]],"long-forms":[[44,71]]},{"text":"given for each environment. For example, Q appeared eight  times in the context EH--EN ( E - n ,  and a check of the  reference list shows that all these occurrences were in the ","acronyms":[[84,86],[80,82]],"long-forms":[[89,94]]},{"text":"Abbreviations POS = Part of Rhetoric NE = Named Entity CE = Correlated Entity","acronyms":[[35,37]],"long-forms":[[40,52]]},{"text":"result. We have used the symbol Comp in that case (e.g., if  ANTI (A)=B and CONV(B)=C, then the relation result-  ing from the composition is simply ANTI(CONV(A))----C).","acronyms":[[61,65],[149,153],[76,80]],"long-forms":[]},{"text":" We use LibSVM (Chang and Lin, 2011), an implementation of Support Vector Machines (SVM) (Cortes and Vapnik, 1995), as the underlying tech-","acronyms":[[84,87],[8,14]],"long-forms":[[59,82]]},{"text":"toolkit that contains a suit of modules for generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis (NA). ","acronyms":[[146,148]],"long-forms":[[128,144]]},{"text":"location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT) ","acronyms":[[75,78],[92,95],[9,12],[28,31],[44,47],[57,61],[103,107],[115,118],[130,133]],"long-forms":[[64,73],[80,90],[0,8],[14,27],[34,43],[49,56],[98,102],[109,114],[120,129]]},{"text":"mentary and United Nations parallel corpora. The semantic phrase table (SPT) was extracted from the same corpora annotated with FreeLing (Carreras et","acronyms":[[72,75]],"long-forms":[[49,70]]},{"text":"pondence between proofs and addictions edifice.  Dependency grammar (DG) takes as fundamental  ~This approach of 'normal form parsing' has been ","acronyms":[[72,74]],"long-forms":[[52,70]]},{"text":"directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"mfly@sky.ru Abstract YARN (Yet Another RussNet) project started in 2013 aims at creating a large","acronyms":[[21,25]],"long-forms":[[27,46]]},{"text":"has been debated extensively in multiple formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.","acronyms":[[128,131],[62,65],[89,91]],"long-forms":[[104,126]]},{"text":"We use three classes of features? Crowd Grades (CG), Force Alignment features (FA) and Natural Language Processing features (NLP).","acronyms":[[79,81],[48,50],[125,128]],"long-forms":[[53,68],[34,46],[87,114]]},{"text":"In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 217?220, Va?xjo?. ","acronyms":[[73,76]],"long-forms":[[38,71]]},{"text":".  3.4 Stochastic Gradient Descent (SGD) Training With the opportunity gradients, we applied L2-norm regularized SGD training to iteratively learn the feature","acronyms":[[36,39],[110,113]],"long-forms":[[7,34]]},{"text":"the set of required domains. Various classification systems were considered, including the Dewey Decimal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, however, are","acronyms":[[164,167],[121,124]],"long-forms":[[130,162],[91,119]]},{"text":"ysis incorporating social networks. In Proceedings of Knowledge Discovery and Data Mining (KDD). ","acronyms":[[91,94]],"long-forms":[[54,82]]},{"text":"? P5E3N4S3, F W I International Language of Service and Maintenance (ILSAM) (Pym 1990) is an influential language similar to Caterpillar Fundamental English, from which it was derived","acronyms":[[69,74],[2,10]],"long-forms":[[18,67]]},{"text":"of two kind of data: One is the hand constructed seg-  mentation dictionary (HBSD)  and the other is the  simple noun dictionary for segmentation (SND). ","acronyms":[[142,145],[72,76]],"long-forms":[[101,123],[33,70]]},{"text":"a set of traits in the Sentence Scoring phase.  The Maximal Marginal Relevance (MMR) algorithm is then using in the Sentence Re-ordering","acronyms":[[82,85]],"long-forms":[[54,80]]},{"text":"For POS tagging, the three primary error categories are the confusion between adverbs (AD) and verbs with an  adverbial force, between measure expression (M) and ","acronyms":[[84,86],[4,7]],"long-forms":[[75,82],[132,139]]},{"text":" It is a Neo-Reichenbachian representation (Reichenbach,  1966) in that its s imple tense s t ructures  (STSs) re-  late the following three entities: the time of the event ","acronyms":[[105,109]],"long-forms":[[76,102]]},{"text":"4 Surveillance Named Entity Recognise In the first part of this work, we adopt a supervised named entity recognised (NER) framework for the attribute extraction problem from eBay listing titles.","acronyms":[[118,121]],"long-forms":[[92,116]]},{"text":"grammatical frameworks (HLG). Combinatory  Categorial Grammars (CCG) (Steedman, 1987;  Steedman, 1996; Steedman, 1998; Steedman and ","acronyms":[[64,67],[24,27]],"long-forms":[[30,62]]},{"text":" In this paper, we will explores how to adapts a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain.","acronyms":[[74,77],[85,87]],"long-forms":[[53,72]]},{"text":"Lexical features show a much more mixed result. Type?Token Ratio (TTR) is only important for document classification, whereas most of the","acronyms":[[66,69]],"long-forms":[[48,64]]},{"text":"1 ? Active Node List (ANL): a list that records all ?","acronyms":[[22,25]],"long-forms":[[4,20]]},{"text":"First, we render background on Open IE and how it relates to Semantic Role Labeling (SRL). Section 3 de-","acronyms":[[86,89],[37,39]],"long-forms":[[62,84]]},{"text":"formation recovering resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1. Scholar in","acronyms":[[136,140]],"long-forms":[[109,134]]},{"text":"Total 2,910 1,086 3,996 Table 1: Number of annotated elements per category in our gold standard (CR=controlled requirements, UR=uncontrolled requirements)","acronyms":[[97,99],[125,127]],"long-forms":[[100,123],[128,140]]},{"text":"itly mark objects of prepositions (POBJ), possessors in idafa construction (IDAFA), conjuncts (CONJ) and conjunctions (CC), and the accusative specifier, tamyiz (TMZ).","acronyms":[[119,121],[35,39],[76,81],[95,99],[162,165]],"long-forms":[[105,117],[10,33],[56,74],[84,93],[154,160]]},{"text":"? P5E3N4S3, F W I International Language of Servicing and Maintenance (ILSAM) (Pym 1990) is an influential language similar to Caterpillar Fundamental Francais, from which it was derived","acronyms":[[69,74],[2,10]],"long-forms":[[18,67]]},{"text":"375 that...). Extreme case formulations (ECF) are lexical patterns emphasizing extremeness (e.g., This is","acronyms":[[41,44]],"long-forms":[[14,39]]},{"text":"tion of the reference xamples takes place.  Translation Keepsakes (TMs) are such purely  memory based MT-systems.","acronyms":[[66,69],[101,103]],"long-forms":[[44,64]]},{"text":"At  the top level, >,sb,,~ denotes the basic relation for the  overall ranking of information structure (IS) patterns. ","acronyms":[[105,107]],"long-forms":[[82,103]]},{"text":"matic and paradigmatic associations on the results of the clustering step. We conduct two experiments on SemEval-2012 task 2 and Scholastic Assessment Test (SAT) analogy quizzes to measure relational similarity to evaluate our model.","acronyms":[[157,160]],"long-forms":[[129,155]]},{"text":"(Marcus et al., 1993) whereas IN = preposition or conjunction, subordinating; CC = Coordinating Conjunction; VBN = Verb, past participle; VBG = verb, gerund or present partici-","acronyms":[[78,80],[30,32],[109,112],[138,141]],"long-forms":[[83,107],[115,136],[144,175]]},{"text":" Since we planned to eventually test our algorithms in  word gratitude on the Resource Management (RM)  database, our phone classification experiments were moreover ","acronyms":[[101,103]],"long-forms":[[80,99]]},{"text":"istic conversational systems. During Proceedings of Intelligent User Interfaces 2001 (IUI-01), pages 1?8, Santa Fe, NM, January.","acronyms":[[82,88],[112,114],[108,110]],"long-forms":[[48,80]]},{"text":"labels.5 We simulate a user?s restriction by categorize words in the training split by their information gains (IG).6 After ranking the top 200 words for each class","acronyms":[[108,110]],"long-forms":[[90,106]]},{"text":"pathway representation formats: Systems Biology Markup Language (SBML)3 (Hucka et al, 2003) and Biological Pathway Exchange (BioPAX)4 (Demir et al, 2010).","acronyms":[[125,131],[65,69]],"long-forms":[[96,123],[32,63]]},{"text":"corpora for our experiments. The first is a novel corpus of 70 articles from New York Times (NYT) LDC corpus, each describe one or more terrorist events","acronyms":[[91,94],[96,99]],"long-forms":[[75,89]]},{"text":"Table 4: Accuracies on a balanced test set (random baseline: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?","acronyms":[[67,69],[84,88]],"long-forms":[[72,82],[93,100]]},{"text":"Abstract  This paper describes a reestimation method for stochastic parlance models uch as the  N-gram model and the Hide Maxkov Model(HMM) from ambiguous observations. It is ","acronyms":[[137,140]],"long-forms":[[117,135]]},{"text":"Artola Xo (1993)o \"ilIZTSUA: lIiztegi-sistema urDrt.lc  a(zipperhead~dunaren sorkuntza eta eraikuntza \/Conccption  d'un syst~,me smarter d'aide dictionnarialc (SIAl))\"  Ph.D. Thesis.","acronyms":[[158,162]],"long-forms":[[114,156]]},{"text":" School of Information Science Japan Advanced Institute of Science and Technology (JAIST), Japan nthnhung@jaist.ac.jp","acronyms":[[83,88]],"long-forms":[[31,81]]},{"text":" 3.1 Identifying verbal blocks (Vbs) Verbal block are composed of a head (Vb-H) and possibly accompanying dependents (Vb-D).","acronyms":[[75,79],[32,35],[119,123]],"long-forms":[[37,73],[17,30]]},{"text":"In the cross-validation process, Multinomial Naive Bayes (MNB) has shown better results than Aided Vector Machines (SVM) as a ingredients for AdaBoost.","acronyms":[[118,121],[58,61]],"long-forms":[[93,116],[33,56]]},{"text":"adapted to a new spheres.  Word sense disambiguation (WSD), on the other hand, is the closely bound task of assigning a sense","acronyms":[[53,56]],"long-forms":[[26,51]]},{"text":"LMs by perplexity (PPL). We use the Mur Street Journal (WSJ) portion of Penn Treebank (PTB). ","acronyms":[[88,91],[0,3],[19,22],[57,60]],"long-forms":[[73,86],[7,17],[36,55]]},{"text":"we oftentimes decompose the global probability of sequences utilise a directed graphical model (e.g., an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)).","acronyms":[[148,151],[98,101]],"long-forms":[[122,146]]},{"text":"results in Die event. Freshly improvement of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and","acronyms":[[76,80]],"long-forms":[[45,74]]},{"text":"For the discourse  structure analysis, we suggest a statistical model  with discourse segment boundaries (DSBs)  similar to the idea of gaps suggested for a ","acronyms":[[106,110]],"long-forms":[[76,104]]},{"text":"ining both BLEU and NIST scores? relationship to their Unlabeled Accuracy Score(UAS). ","acronyms":[[80,83],[11,15],[20,24]],"long-forms":[[55,79]]},{"text":"triplet model since it is based on word triplets, is not trained discriminatively but uses the classical maximum likelihood approach (MLE) instead. ","acronyms":[[134,137]],"long-forms":[[105,123]]},{"text":" scores cw(ei) are combined: MEAN CENTIMETRES (cM (eI1)) is calculate as the geometric mean of the confidence scores of the","acronyms":[[34,36]],"long-forms":[[38,40]]},{"text":"5.2.1 Query Focussed Rewards We have suggests an extension to both reward functions to allow for query focused (QF) summarization.","acronyms":[[111,113]],"long-forms":[[96,109]]},{"text":" 1 Introduction  When a natural language processing (NLP) system  is created in a modular fashion, it can be relatively ","acronyms":[[53,56]],"long-forms":[[24,51]]},{"text":"tleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing.","acronyms":[[131,136]],"long-forms":[[114,129]]},{"text":"lion tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter ?","acronyms":[[82,84]],"long-forms":[[66,80]]},{"text":" The particular case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced ?","acronyms":[[69,75]],"long-forms":[[55,67]]},{"text":" respectively display the results obtained without and with the use of subcat peculiarities (SF). The sec-","acronyms":[[88,90]],"long-forms":[[71,86]]},{"text":" 2 System Overview Our system, named PML Tree Query (PML-TQ), consists of three main components (deliberated fur-","acronyms":[[53,59]],"long-forms":[[37,51]]},{"text":"ment, we compares the following three techniques for word  similarity measure:  * the Bunruigoihyo thesaurus (BGH): the similarity  between case fillers is measured by a function be- ","acronyms":[[107,110]],"long-forms":[[83,105]]},{"text":"contribution: Functional GENDER and NUMBER features contribute more than their form-based counterparts, in both gold and predicted conditions; rationality (RAT) as a single idiosyncratic on top of the POS tag set helps in gold (and with Easy-First Parser, further in scheduled conditions)?but when used in jumpsuit with","acronyms":[[156,159],[195,198]],"long-forms":[[143,154]]},{"text":"Figure 5: Example GMM fitting 2. Gaussian mixture model (GMM)-based POI probability (prior) calculation","acronyms":[[57,60],[18,21],[68,71]],"long-forms":[[33,55]]},{"text":"Method (METH) the methods used Result (RES) the results achieved Conclusion (CON) the authors? result","acronyms":[[77,80],[8,12],[39,42]],"long-forms":[[65,75],[0,6],[31,37]]},{"text":" Its data-driven approach learns a sub-word lexicon from a training corpus of words by using a Minimum Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with","acronyms":[[123,126]],"long-forms":[[95,121]]},{"text":"ZC05 (Zettlemoyer and Corinth 2005) 79.3 ?  ZC07 (Zettlemoyer and Collins 2007) 86.1 ? ","acronyms":[[44,48],[0,4]],"long-forms":[[50,78]]},{"text":"1 Introduction After two decades of steady advance, research in statistical machine translation (SMT) started to cross its camino with translation industry with tan-","acronyms":[[98,101]],"long-forms":[[65,96]]},{"text":"presidential nomination to seek the backing of the {{w|Libertarian Party (Unidos States)|Libertarian Party}} (LP). ","acronyms":[[110,112]],"long-forms":[[89,107]]},{"text":"10-best(+AG) 4.35 90.0 Table 1: Task terminating rate pursuant to using the AG (Agenda Graph) and n-best hypotheses for n=1 and n=10.","acronyms":[[76,78],[9,11]],"long-forms":[[80,92]]},{"text":"Sagae and Tsujii (2008) presents a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of tallest span entering nodes.","acronyms":[[123,127]],"long-forms":[[98,121]]},{"text":"of Excellence and the Defense Advanced Research Projects Agency (DARPA) Appliance Reading Program under Air Force Research Darkroom (AFRL) prime contract no.","acronyms":[[133,137],[65,70]],"long-forms":[[102,131],[22,63]]},{"text":"TH + DR + EG + LC 56.51 TH + EG + LC 56.50 Character Type (CT) 51.96 Word Familiarity (WF) 51.50","acronyms":[[59,61],[0,2],[5,7],[10,12],[15,17],[24,26],[29,31],[34,36],[87,89]],"long-forms":[[43,57],[69,85]]},{"text":"and the parameters ? can be estimated using maximum opportunity estimation(MLE) on a training corpus(Och and Ney, 2003).","acronyms":[[74,77]],"long-forms":[[44,73]]},{"text":"other synthesis techniques.  The TD-PSOLA (Time Domain Pitch Synchronous Overlap-add) developed by CNET is a very simple  but ingenious method which assures high voice quality, the only disadvantage is that it is based on a time: ","acronyms":[[33,41],[99,103]],"long-forms":[[43,84]]},{"text":"tice to both specifics cite above. The central construct in this framework is  that of context factor (CF). A CF is define by a scope, which is a collection of individ- ","acronyms":[[114,116],[121,123]],"long-forms":[[98,112]]},{"text":"92\\]. This selective approach culminated to significant  results in some restricted applications (ATIS...). ","acronyms":[[91,98]],"long-forms":[[77,89]]},{"text":"GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters ? Indotag (Indonesian): Conditional Haphazard Field (CRF) POS tagger. ","acronyms":[[136,139],[141,144]],"long-forms":[[110,134]]},{"text":" They use Solely Word-Seg (OWS), Whole Layer Weight (WLW), SC (SC) and FeedBack mechanism (FB) separately.","acronyms":[[51,54],[25,28],[57,59],[61,63],[89,91]],"long-forms":[[43,49],[10,23],[69,77]]},{"text":"3.1 Div is aeon  and  L inear l i za t ion  o f   Lawsuits   At first, we define a translation pattern (TPi) as fol-  lows.","acronyms":[[100,103]],"long-forms":[[79,98]]},{"text":"The s t ruc ture  11ke a combinat ion of  a verb (V) and  an adverb ia l  par t tc le  (ADVPART) tn th ts  sequence  wtth or w i thout  a pronoun (PRON) tn between tn  Engltsh t swr t t ten  as fo l lows .","acronyms":[[147,151],[88,95]],"long-forms":[[138,145],[44,48],[61,85]]},{"text":"This method is very simpler than the ILP method, while it can achieve comparable result on the CLANG (Coach Vocabulary) and Query corpus. ","acronyms":[[95,100],[37,40]],"long-forms":[[102,116]]},{"text":" 1 Introduction  Relation Extraction (RE) aims to detecting a set of  predefined relations between pairs of entities in ","acronyms":[[38,40]],"long-forms":[[17,36]]},{"text":" The concentrations of this paper is a debated of various methods used to create a set of acoustic models for  characterizing the PLU's use in large vocabulary recognition (LVR). The set of context independent ","acronyms":[[168,171],[124,129]],"long-forms":[[138,166]]},{"text":"One is manually typing the text transcriptions which we regarded as the Correct Recognition Result (CRR) transcription, and another is the ASR result which","acronyms":[[100,103],[139,142]],"long-forms":[[72,98]]},{"text":" 1 Introduction The approach to factoid question answering (QA) that we adopt was first described in (Whittaker et","acronyms":[[60,62]],"long-forms":[[40,58]]},{"text":" Longer commonly, recurrent neural networks are trained with stochastic gradient ancestry (SGD), where the gradient of the training criterion is com-","acronyms":[[88,91]],"long-forms":[[59,86]]},{"text":"nym, Instance Hypernym, Part Holonym, Member Holonym, Substance Meronym, Entailment Table 1: Similarity features using WordNet (WN). ","acronyms":[[128,130]],"long-forms":[[119,126]]},{"text":"ambiguation as a binary classification problem. Li  then uses Support Vector Machine (SVM) with  mutual information between each Chinese charac-","acronyms":[[86,89]],"long-forms":[[62,84]]},{"text":"  We extracted bag-of-word features and trained a  Support Vector Machine (SVM) classifier (Burges, 1998) using the above dataset.","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"translate architecture, was developed under sponsorship from  Swedish Telecom by a worked between SRI International,  the Swedish Institute of Computer Science (SICS), and Telia  Research.","acronyms":[[170,174],[107,110]],"long-forms":[[131,168]]},{"text":"5, we discuss our approach to increase the coverage of the paragon by used synset ID?s from the English WordNet (EWN). Section 6 describes our","acronyms":[[112,115],[81,85]],"long-forms":[[95,110]]},{"text":"In our development of 60 Japanese predicates (verb and verbal noun) frequently appearing in Kyoto University Text Corpus (KTC) (Kurohashi and Nagao, 1997) , 37.6% of the frames included","acronyms":[[122,125]],"long-forms":[[92,120]]},{"text":"above.  Iconic Inflectional Classes (IICs) are ICs that are manually fully annotated, i.e., they have all the tem-","acronyms":[[37,41]],"long-forms":[[8,35]]},{"text":"notation is illustrated in Figure 3.  3.3 Positional Anonymous Model (PosUnk) The main ineptitude of the PosAll model is that","acronyms":[[68,74]],"long-forms":[[42,60]]},{"text":"The types of links  traversed in the scoured (in the first case) or the verified slots (in the second case) are a  operation of the semantic lass (SEM-C) of the first constituent. This function assigns to each ","acronyms":[[144,149]],"long-forms":[[129,137]]},{"text":"tor machines: learning with many germane features. In European Conference on Machine Learning (ECML). ","acronyms":[[96,100]],"long-forms":[[55,94]]},{"text":"726  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"with one object punishments (IMP_VNP), V_NP_PP  agentless passive sentences (PAS_VNPP), V_NP bypassives (BYPAS_VN), and N_PP clauses (NPP) and  these are all rulings that happen in the realiser, ","acronyms":[[132,135],[27,34],[37,44],[75,83],[103,111]],"long-forms":[[118,122],[86,101],[46,73]]},{"text":"4.1 Syntactic template picks PERSONAGE?s entrances generation dictionary is made of 27 Deep Syntactic Structures (DSyntS): 9 for the recommendation claim, 12 for the comparison","acronyms":[[114,120]],"long-forms":[[87,112]]},{"text":"The generator operates from a declarative know-  ledge base of linguistic knowledge, common to that used  by PHRAN (PHRasal ANalyzer; Wilensky and Arens,  1980).","acronyms":[[109,114]],"long-forms":[[116,132]]},{"text":"nique was used in The MAYO Clinic Vocabulary  Server (MCVS)5, which encodes clinical expressions into medical ontology (SNOMED-CT) and  identifies whether the event is supportive or negatives.","acronyms":[[120,129],[22,26],[54,58]],"long-forms":[]},{"text":"that combining additional knowledge origin, including lexical features (LX1) and the non-verbal features, prosody (PROS), motion (MOT), and context (CTXT), yields a further improve (of 8.8%","acronyms":[[116,120],[131,134],[73,75],[150,154]],"long-forms":[[107,114],[123,129],[55,70],[141,148]]},{"text":"All experiments carried out in this study are for the English (EN) - French (FR) language pair. ","acronyms":[[77,79],[63,65]],"long-forms":[[69,75],[54,61]]},{"text":"716 Figure 5: The NUMBERS System Architecture (CA = communicative legislation) The module network topology of the system is","acronyms":[[47,49],[18,25]],"long-forms":[[52,69]]},{"text":"used throughout the paper. The notation is that  used in the Core Language Engine (CLE) devel-  oped by SKI's Cambridge Computer Science Re- ","acronyms":[[83,86],[104,107]],"long-forms":[[61,81]]},{"text":"and reranking for the statistical parsing of spanish. In Lawsuits of Human Language Technology (HLT) and the Conference on Empirical Processes in Natural","acronyms":[[99,102]],"long-forms":[[72,97]]},{"text":"derived from these MDPs: (1) Diff?s: the number of states whose policy differs from the Baseline 2 policy, (2) Percent Policy change (P.C.): the weighted amount of change between the two policies (100%","acronyms":[[134,138],[19,23]],"long-forms":[[119,132]]},{"text":" minimizes the multiway normalized cut(MNCut): MNCut(I) = K ?","acronyms":[[39,44],[47,52]],"long-forms":[[15,37]]},{"text":"2 Pivot Translation Pivot translation is a translation from a source language (SRC) to a target language (TRG) through an intermediate pivot (or bridging) language (PVT).","acronyms":[[106,109],[79,82],[165,168]],"long-forms":[[89,104],[62,77],[135,163]]},{"text":"term t appears in position i around the entity.  Bigram Context (BCON): The bigram-based  context model was built in a similar way to UCON, ","acronyms":[[65,69],[134,138]],"long-forms":[[49,63]]},{"text":"All the words were graded  into three types: Lexicon words (LWs), Factoid  words (FTs), Named Entity (NEs). Accordingly, ","acronyms":[[107,110],[65,68],[87,90]],"long-forms":[[93,105],[50,63],[71,85]]},{"text":"future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from la-","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":"New in three aspects. Fiirst, the baseline units of their model are elementary discourse units (EDUs) from Rhetorical Structure Theory (RST) (Mann","acronyms":[[92,96],[132,135]],"long-forms":[[64,90],[103,130]]},{"text":"and Ripper, on the other hand, appear to take more advantage of some feature types than others. For the third task, lexical (LX) and discourse (DS) features apparently have more predictive power for both C4.5 and SVM than the other types.","acronyms":[[125,127],[144,146],[213,216]],"long-forms":[[116,123],[133,142]]},{"text":"run in parallel. This kind of parallelism is a good fit for the Same Instruction Multiple Thread (SIMT) hardware paradigm implemented by modern GPUs.","acronyms":[[98,102],[144,148]],"long-forms":[[64,96]]},{"text":"U, x \/y \/z ,  T, V ~ VYw\\[Y \/X\\ ]   (14)  5Here the 'full' version of (VR) is being utilise, incorpo-  rating a amended of bound variable.","acronyms":[[71,73]],"long-forms":[[59,66]]},{"text":"SC is mainly employs in  mainland China while TC is mainly used in Taiwan  and Hong Kong (KONG). In this experiment, we further ","acronyms":[[87,89],[0,2],[43,45]],"long-forms":[[76,85]]},{"text":"sulting matrices be M1 and M2, respectively. In stepping (3), SentIDs (sentences where the two words appear with the specified relation) are acquired by","acronyms":[[58,65]],"long-forms":[[67,96]]},{"text":"Pour each  model sentence MSij,, the model constructor selects  the Required Lexicon (RLijk), a set of the most  essential lexical entities required to appear in a ","acronyms":[[81,86],[25,29]],"long-forms":[[63,79]]},{"text":"Score(E), where E is an example of Pat To improve ranking, we also try to find the longest similar subsequence (LSS) between the user input, Sent and retrieved example, Exm","acronyms":[[112,115]],"long-forms":[[83,110]]},{"text":"account for these generalizations by decom-  posing the grammar regulation to Immediate Dom-  inance(ID) ordinances and Linear Preeedence(LP)  rules.","acronyms":[[128,130],[96,98]],"long-forms":[[110,126],[73,95]]},{"text":"RLP = rule  learner prediction. RS = Reference Norms   ","acronyms":[[32,34],[0,3]],"long-forms":[[37,55],[6,30]]},{"text":"1 will refer to pa~rs   primarily orientated towards the former goal as Practical Parsers (PP) and refer  to the others as Performance Paragon Parsers (PMP). With these distinctions ","acronyms":[[148,151],[89,91]],"long-forms":[[121,146],[70,86]]},{"text":"REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78","acronyms":[[40,45],[0,3],[13,17],[69,75]],"long-forms":[[48,68],[20,39],[78,100],[6,12]]},{"text":"124   Proceedings of the Ninth International Workshop on Parsing Techs (IWPT), pages 202?203, Vancouver, October 2005.","acronyms":[[79,83]],"long-forms":[[31,77]]},{"text":"ric of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles utilize to construct the language model (LM), the type of LM, the type of grammar bylaws in the Phoenix book","acronyms":[[134,136],[151,153],[60,62]],"long-forms":[[118,132]]},{"text":"Abstract  This paper proposes a novel reordering model  for statistic machine translate (SMT) by  means of modeling the translation orders of ","acronyms":[[93,96]],"long-forms":[[60,91]]},{"text":"the parameters ? = (s,W,b,x) via backpropagation with stochastic gradient lineage (SGD). ","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"interface.  The PRIDES User Interface Layer (PUI) is  responsible for creating and managing the screen ","acronyms":[[45,48]],"long-forms":[[16,37]]},{"text":"of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP). ","acronyms":[[117,120],[29,32],[110,115]],"long-forms":[[120,121]]},{"text":"cessing information from a structured database ? a natural parlance interface to a database (NLIDB) (Kapetanios et al, 2010).","acronyms":[[93,98]],"long-forms":[[51,91]]},{"text":" 3 Throughout this work, we use HL (Hu and Liu, 2004), MPQA (Wilson et al.,","acronyms":[[24,26],[47,51]],"long-forms":[[28,38]]},{"text":"                                                    2  In our system, we define ten kinds of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, telephoned  number, and WWW.","acronyms":[[148,151],[114,118],[190,193]],"long-forms":[[140,146],[108,112]]},{"text":"UMLS Metathesaurus version 2003AC, the chaining mammectomy has been awarded the concept-unique identifier C0024881 (CUI), the lemma-unique identifier L0024669 (LUI), and the string-unique identifier S0059711 (SUI).","acronyms":[[115,118],[0,4],[27,33],[159,162],[208,211]],"long-forms":[[105,113],[125,157],[173,206]]},{"text":"3.4 The  NATO Research  Study  Group  on  Speech  Process ing   The North Atlantic Treaty Organisations (NATO) Re-  search Studies Group on Speech Processing (RSG10) \\[87\\], ","acronyms":[[104,108],[9,13],[156,161]],"long-forms":[[68,102],[110,154]]},{"text":"1408 then apply our model to the well-established sequence tagging task: noun phrase (NP) chunking. ","acronyms":[[87,89]],"long-forms":[[74,85]]},{"text":"of-the-art in more detail.  The field of Information Extraction (IE) has been heavily influences by the Information Retrieval (IR)","acronyms":[[65,67]],"long-forms":[[41,63]]},{"text":"In my semantic role labeling research, I used the Tilburg Memory Learner (TiMBL) for this purpose.","acronyms":[[74,79]],"long-forms":[[50,72]]},{"text":"Parse tree of tagged sentence in Box 1  3 Geographic Information Retrieval  3.1 Propositional logic of context (PLC)  As previously discussed, candidate named entities ","acronyms":[[112,115]],"long-forms":[[80,110]]},{"text":"to say\" and the \"what to say\" is still an open  question. RAGS (rags, 1999) proposes astandard  architecture for the data, but leaves the ","acronyms":[[58,62]],"long-forms":[[64,68]]},{"text":"Recap This cooperation looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and","acronyms":[[72,76]],"long-forms":[[49,70]]},{"text":"More common approaches to language model adaptation, such as counting merging and model interpolation, are special cases of maximum a posteriori (MAP) estimation (Bacchiani and Roark, 2003).","acronyms":[[143,146]],"long-forms":[[121,141]]},{"text":"We encode the target state in the  similar way. Like the Vector Space Model(VSM),  we use a label matrix to represent each class as in ","acronyms":[[76,79]],"long-forms":[[57,74]]},{"text":"particular components - -  immediate dominance (ID) rules, meta-  ordinances, linear precedence (LP) statements, feature co-occurrence  restrict (FCRs), and features specification defaults (FSDs)  - -  and four universal components - - a theory of syntactic fea- ","acronyms":[[188,192],[48,50],[92,94],[145,149]],"long-forms":[[156,186],[27,46],[73,90],[108,143]]},{"text":" 2.3 Linear Programming A Linear Program (LP) is an optimization difficulty where a linear aiming function is minimized (or maximized) under linear constraints.","acronyms":[[42,44]],"long-forms":[[26,40]]},{"text":"Table 1 gives an overview of all entity classes and relations. The workflow consists of two steps: Firstly, rule- and ontology-based named entity acknowledging (NER) is performed (cf. Section","acronyms":[[162,165]],"long-forms":[[136,160]]},{"text":"  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33?37, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[70,72],[28,32]],"long-forms":[[50,68]]},{"text":"the documentaries. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree sys-","acronyms":[[69,71]],"long-forms":[[52,67]]},{"text":"The following are  typlcal relation names: NT (fatter term); PT (part); FUN (function);  SYN (syntax); EG (example). ","acronyms":[[91,94],[43,45],[47,60],[63,65],[74,77],[105,107]],"long-forms":[[96,102],[67,71],[79,87],[109,116]]},{"text":"In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI?09), page 1,058?1,064, Pasadena, CA.","acronyms":[[81,89],[121,123]],"long-forms":[[22,79]]},{"text":"experiments. For Hindustani, we worked with the Hindi Dependency Treebank (HDT) liberated as part of Coling 2012 Shared Task (Bharati et","acronyms":[[70,73]],"long-forms":[[43,68]]},{"text":"Social media in general exhibit a rich variety of  information wellspring. Issue answering (QA) has  been particularly amenable to social media, as it ","acronyms":[[92,94]],"long-forms":[[72,90]]},{"text":" This  suspens ion takes place dur ing un i f icat ion  of  the Apartment Concurrent  Pro log (FCP) pred icate   (seeing below), into which expert  rout ines are ","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":"........................................ LTH  ........... Link between STH and LTHs  TLink (Translation Link) between language LTHs  Figure 2: Example of an STH linked to a Fragment ","acronyms":[[85,90],[71,74],[79,83],[127,131],[157,160],[41,44]],"long-forms":[[92,108]]},{"text":"interchange of LRs. It also demonstrate how MLI  can be applied to Asian Linguistics Resource (ALR)  through making the result of collaborative en-","acronyms":[[92,95],[15,18],[44,47]],"long-forms":[[67,90]]},{"text":"less than linear is the sample size, m. We formalize  this as a variant of \"Set Cover\" problem which we call  \"Weighted Set Cover~(WSC), and prove the existence of  an approximation algorithm with a performance guar- ","acronyms":[[131,134]],"long-forms":[[111,129]]},{"text":"   First Paragraph (FPAR): We examined several  hundred pages, and observed that a human could ","acronyms":[[20,24]],"long-forms":[[3,18]]},{"text":" What we described is part of a theory of language (knowledge and treatment)  called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge ","acronyms":[[99,101]],"long-forms":[[85,97]]},{"text":" The growing need to  manage and search wide video collections presents  a challenge to traditional information retrieval (IR)  technologies.","acronyms":[[124,126]],"long-forms":[[101,122]]},{"text":"  Here the parameters are set using an algorithm  whose uniform resource name (URN),  xyz.edu\/algo-1, is declared as an attribute of the ","acronyms":[[79,82]],"long-forms":[[56,77]]},{"text":"Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and telecast news (BNews). The number of organizations","acronyms":[[116,121]],"long-forms":[[100,114]]},{"text":"ber of occurrence for this feature per patients narrative is obtained bases on the frequency of the coordinating conjunction PoS tag (CC) detected in the parse tree structure.","acronyms":[[133,135]],"long-forms":[[112,123]]},{"text":"text corpus, to be made available without royalties for scientific research. The text will  be formatted using SGML (the Standard Generalized Markup Language). To date ","acronyms":[[111,115]],"long-forms":[[121,157]]},{"text":"  Lawsuits of the 2014 Conference on Empirical Methods in Naturel Language Processing (EMNLP), pages 477?487, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"A Named Entity Labeler for German: Exploiting Wikipedia and Distributional Clusters. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 552?556, La Valletta, Malta.","acronyms":[[156,160]],"long-forms":[[121,139]]},{"text":"six different domains: Newswire (NW), Disseminated News (BN), Circulated Conversations (BC), Webblog (WL), Usenet (UN), and Conversational Telephone Speech (CTS).","acronyms":[[111,113],[33,35],[54,56],[84,86],[98,100],[153,156]],"long-forms":[[103,109],[23,31],[38,52],[59,82],[89,96],[120,151]]},{"text":"tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s controlled vocabulary dictionary, Medicinal Subject Headings (MeSH),2 contains about 23,000 descriptors arranged in a hierarchical structure and more than 151,000","acronyms":[[142,146],[77,82]],"long-forms":[[116,140]]},{"text":"equivalent m Dutch For a sample of 59 Ital,an noun  s)nsets there is at least an overlap of 30% (20) with  Dutch Examples are Arbeltszeitverkurzung (DE)  = arbeidstijdverkortmg (NL) = (reduction of work- ","acronyms":[[149,151],[178,180]],"long-forms":[[107,121]]},{"text":"(Associativity)  \\[una\\[BC\\]\\] = \\[\\[AB\\]C\\]  (A(BC)) = ((AB)C)  (L, -singleton bidirectionality) ","acronyms":[[56,60]],"long-forms":[[45,49]]},{"text":"For ex-  ample, lobe PPs, such as \"in 1959\", where  the prepositional object is labeled CD (cardi-  nal), favor attachment to the VP, because tile ","acronyms":[[91,93],[133,135],[25,28]],"long-forms":[[95,106]]},{"text":"and normalizing it with a visualize to discriminate the significance of words across documents and then approximating it using singular value decomposition(SVD) in R dimensions (Bellegarda, 2000).","acronyms":[[149,152]],"long-forms":[[120,147]]},{"text":"Apple Happenings  1 Introduction  The SlmSum (Slmulatmn of Summarizing)  system does what its name pronuses It simu- ","acronyms":[[34,40]],"long-forms":[[42,66]]},{"text":"guages: British sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT). The data for the","acronyms":[[96,99],[31,34],[45,48],[67,71],[83,86],[113,116]],"long-forms":[[89,94],[8,29],[37,43],[51,65],[74,81],[106,111]]},{"text":"Advanced Research and Development Activity (ARDA)?s Advanced Question Answering for Intelligence (AQUAINT) Program, a DOI grant under the Reflex","acronyms":[[98,105],[44,48],[118,121]],"long-forms":[[0,42],[52,96]]},{"text":"   Additionally, Mean Reciprocal Rank (MRR) is  also reported.","acronyms":[[39,42]],"long-forms":[[17,37]]},{"text":" 5.4 Nonshared Concept Activation with  No Identif ication Intention (NSNI). ","acronyms":[[70,74]],"long-forms":[[40,68]]},{"text":"Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al (2006) have shown that a shift-reduce parser can give","acronyms":[[72,75]],"long-forms":[[54,70]]},{"text":"1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Consequences for parsing section 0 ( \u0000 40 mots) of the WSJ Penn Treebank: OP = overparsing, LP\/LR = tagged accuracy\/recall.","acronyms":[[137,139],[118,121],[155,160]],"long-forms":[[142,153],[163,188]]},{"text":"in the original English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al 2006) allows us to recover super-","acronyms":[[91,94]],"long-forms":[[75,89]]},{"text":"inference of lexical semantic roles After the training phase, a testing procedure using the Markov Chain Monte Carlo (MCMC) inference engine can be used to infer role labels.","acronyms":[[118,122]],"long-forms":[[92,116]]},{"text":"Again, even the Table 5 Precise (PD), recall (RD), and F-measure (FD) for malapropism detection with five actions of semantic relatedness, various the scope of the search for related words to 1, 3, or 5 paragraphs","acronyms":[[35,37],[48,50],[68,70]],"long-forms":[[24,33],[40,46],[57,66]]},{"text":"denote Run1, Run2, and Run3, respectively, our submissions to the exchanged task; FL=Flagging, FS=Featuring stacking, DS=Domain stacking. ","acronyms":[[92,94],[113,115],[79,81]],"long-forms":[[95,111],[116,131],[82,90]]},{"text":"The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format (SLF), consisting of a set of vertices and a set of edges.","acronyms":[[102,105],[25,30],[73,76]],"long-forms":[[77,100]]},{"text":"For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as common nouns (NN) if they  are not capitalised inside a sentence and proper ","acronyms":[[130,132]],"long-forms":[[123,128]]},{"text":"here we only consider cluster which contain at least one ? Person (PER)? entity.","acronyms":[[68,71]],"long-forms":[[60,66]]},{"text":" The third thread is the sponsorship of the international  Message Understanding Conferences (MUC's) and Text  Retrieval Lectures (TREC's).","acronyms":[[94,99],[134,140]],"long-forms":[[59,92],[105,132]]},{"text":"References TREC (Text REtrieval Conference) : http:\/\/trec.nist.gov\/ NTCIR (NII-NACSIS Proof Collate for IR Systems) project: http:\/\/research.nii.ac.jp\/ntcir\/index-en.html","acronyms":[[75,85],[11,15],[68,73],[106,108]],"long-forms":[[17,42]]},{"text":"2 Algorithms and Data  2.1 Task Definition and Data  The named entity (NE) task used for this  evaluation requires the system to identify all ","acronyms":[[71,73]],"long-forms":[[57,69]]},{"text":"Task (Pradhan et al 2011), one text from each of the five represented genres: Broadcast Conversations (BC), Broadcast News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups","acronyms":[[124,126],[139,141]],"long-forms":[[108,122],[129,137]]},{"text":"Finance and economics (FAE) 100  Education (EDU) 100  Entertainment (ENT) 100  Computer (COM) 100 ","acronyms":[[69,72],[23,26],[44,47],[89,92]],"long-forms":[[54,67],[0,20],[33,42],[79,87]]},{"text":"email: adam@itri.bton.ac.uk  People have been writing curriculum for auto-  matic Word Sense Disambiguation (WSD) for  forty ages now, yet the validity of the task has ","acronyms":[[107,110]],"long-forms":[[80,105]]},{"text":"derivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and less Bayes endangerment system combination (MBR-SC) combining three systems.","acronyms":[[137,143],[26,29],[89,92]],"long-forms":[[98,135],[55,73]]},{"text":"c?2008 Association for Computational Linguistics Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information   Rahul Chitturi, John. H.L. Hansen1 Center for Robust Speech Systems(CRSS) Erik Jonsson School of Engineering and Computer Science University of Texas at Dallas Richardson, Texas 75080, U.S.A {rahul.ch@student, john.hansen@}utdallas.edu Abstract  The variation in speech due to dialect is a factor which significantly impacts speech system per-formance.","acronyms":[[233,237],[350,355]],"long-forms":[[200,231]]},{"text":"common view of the semantics of time. Since the target application domain is an historical database, we  present the important trait of the Historical Relational Database Model (HRDM), an extension to the  relational model motivated by the desiring to integration more \"real world\" semantics into a database at ","acronyms":[[181,185]],"long-forms":[[143,179]]},{"text":"BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList). The t-test re-","acronyms":[[105,110],[40,45]],"long-forms":[[80,103]]},{"text":"     1 Introduction  Most of the natural language generation (NLG)  components in current dialog systems are imple-","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"proposed two novel features, Intra-sentence positional term weighting (IPTW) and the Patched language model (PLM), and showed their effectiveness by conducting automatic","acronyms":[[109,112],[71,75]],"long-forms":[[85,107],[29,69]]},{"text":"natural language utterances. It accesses a database typi-  cal for the information recapture task (CORDIS). ","acronyms":[[99,105]],"long-forms":[[59,97]]},{"text":"bbai@nec-labs.com Abstract We develops a recursive neural network (RNN) to extract answers to arbitrary natural language","acronyms":[[66,69]],"long-forms":[[40,64]]},{"text":" TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding","acronyms":[[58,60],[36,39],[1,5]],"long-forms":[[45,56],[15,34]]},{"text":" Its data-driven approach learn a sub-word lexicon from a training corpus of words by utilizes a Lesser Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with","acronyms":[[123,126]],"long-forms":[[95,121]]},{"text":"(1992). Grammars  are identified over typewritten fea-  twre .structures (TFSs) which can be viewed as  generalizations of first-order terms (Carpenter, ","acronyms":[[65,69]],"long-forms":[[35,63]]},{"text":"the fellas) or part of a knotty coordinating conjunction (both boys and girls), the Penn  Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter-  miner), RB (adverb), NNS (plural common noun) and coordinating cooperated (CC),  respectively.","acronyms":[[251,253],[159,162],[183,185],[196,199]],"long-forms":[[225,249],[164,173]]},{"text":"lated by the preliminary Penn Treebank grammar to utter PCFG surprisal calculated by the Nguyen et al (2012) Generalized Categorial Grammar (GCG). ","acronyms":[[138,141],[53,57]],"long-forms":[[106,136]]},{"text":" ? Research Question 1 (RQ1): How do we define suggestions in suggestion mining?","acronyms":[[24,27]],"long-forms":[[3,22]]},{"text":"data as describes in Niehues and Vogel (2008).  The phrase table (PT) is built utilizes the Moses toolkit (Koehn et al.,","acronyms":[[66,68]],"long-forms":[[52,64]]},{"text":"before the start of the current utterance.  Overlapping label (OL) an utterance on another channel with a particular DA tag overlaps the","acronyms":[[63,65],[117,119]],"long-forms":[[44,61]]},{"text":"It  is at this critical point, when care is being trans-  ferred from the Operating Room (OR) to the ICU  and monitoring is at a minimum, that the pa- ","acronyms":[[90,92],[101,104]],"long-forms":[[74,88]]},{"text":"ically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al 2013).","acronyms":[[119,122]],"long-forms":[[90,117]]},{"text":"(grammatical knowledge and lexical knowledge respectively). The insights obtained from our collaborated  with machine readable dictionaries (MRDs) the Longman Vocabulary of Contemporary English,  henceforth LDOCE, and the Van Dale dictionary of contemporary Dutch \"Groot Woordenboek van ","acronyms":[[131,135],[197,202]],"long-forms":[[100,129]]},{"text":"mid method in DUC 2005. Proceedings of the 5th Document Understanding Conference (DUC). Van-","acronyms":[[82,85],[14,17]],"long-forms":[[47,80]]},{"text":" 4KEY: E1S=singular first person ergative, INC=incompletive, PART=particle, PREP=preposition, PRON=pronoun, NEG=negation, 37","acronyms":[[61,65],[76,80],[94,98],[108,111],[7,10],[43,46]],"long-forms":[[66,74],[81,92],[99,106],[112,120],[47,59],[11,41]]},{"text":"on the base. ( Code-a-phone, 1989)  (2d) In the STBY (standby) position, the phone  will ring whether the handset .is on the base or ","acronyms":[[48,52]],"long-forms":[[54,61]]},{"text":" In Proceedings of the 14th Globally Conference on World Wide Web (WWW), pages 342?351, Chiba.","acronyms":[[72,75]],"long-forms":[[56,70]]},{"text":" 2 Normalized Compression Distance Normalized compression distance (NCD) is a similarity measure based on the idea that a string x is","acronyms":[[68,71]],"long-forms":[[35,66]]},{"text":"word and sentence level QE. In this work we describe the Fondazione Bruno Kessler (FBK), Universitat Polit`ecnica de Val`encia (UPV) and Uni-","acronyms":[[83,86],[24,26],[128,131]],"long-forms":[[57,81],[89,126]]},{"text":"In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels working on mereotopological","acronyms":[[62,64]],"long-forms":[[41,60]]},{"text":"symbols as well as full names. Group such as the  Human Genome Organisation (HUGO), Mouse Genome  Institute (MGI), UniProt, and the National Centre for ","acronyms":[[78,82],[110,113]],"long-forms":[[51,76],[85,108]]},{"text":"To our knowledge there exist two off the shelf Frenchman Arabic Machine Translation (MT) systems: Tarjim and Almisbar.3 We using both MT systems to translate","acronyms":[[83,85],[130,132]],"long-forms":[[62,81]]},{"text":"Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, SDE=Software Development Engineer, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer, WN-lemma=WordNet lemmatization, McCCJ=McClosky-Charniak-Johnson","acronyms":[[138,141],[73,75],[94,97],[173,180],[199,205],[222,230],[249,257],[281,286]],"long-forms":[[142,171],[76,92],[98,125],[181,197],[206,220],[231,247],[258,279],[287,312]]},{"text":"\/NN ?? \/NN ]   Input: wi: word index (ID) in a given sentence. ","acronyms":[[38,40]],"long-forms":[[31,36]]},{"text":" 2003. Voice extensible markup linguistics (VoiceXML) version 2.0.","acronyms":[[41,49]],"long-forms":[[7,39]]},{"text":"The frst step is the identify of the presence of a graphical image in the web page by a  Browser Helper Object (BHO) (Elzer et al., 2007).","acronyms":[[119,122]],"long-forms":[[96,117]]},{"text":"  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120?125, Boulder, Colorado, June 2009.","acronyms":[[87,92]],"long-forms":[[46,85]]},{"text":"1 Introduction Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks on both general (e.g. newspaper text) and","acronyms":[[108,110]],"long-forms":[[87,106]]},{"text":"This paper explores the processing predictions of the ERH on a systematic class of relative clause types, the Accessibility Hierarchy (AAAAH) shown in figure 1.","acronyms":[[135,137],[54,57]],"long-forms":[[110,133]]},{"text":"make assertions that personal pronouns likes \\she\" cannot co-refer with \\company\".  In MUC-7, we developed a word sense disambiguation (WSD) module, which expunge some of the implausible senses from the list of prospective senses.","acronyms":[[135,138],[86,91]],"long-forms":[[108,133]]},{"text":"TC 59% 59% 79% 85% BTC 86% 86% 86% 92% Table 1: Task Completion (CT) and Binary Task Completion (BTC) prediction results, utilize auto-","acronyms":[[65,67],[0,2],[19,22],[97,100]],"long-forms":[[48,63],[73,95]]},{"text":"the firstly reference in this study. ( 3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluating to topics beyond the","acronyms":[[79,83]],"long-forms":[[59,77]]},{"text":"rately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers","acronyms":[[135,138]],"long-forms":[[106,133]]},{"text":"sentence length. This gives usa five metrics of complexity: sentence length (SL), tree depth (TD), branching factor (BF), normalized tree depths","acronyms":[[76,78],[93,95],[116,118]],"long-forms":[[59,74],[81,91],[98,114]]},{"text":"Abstract We experiment with using different sources of distant supervision to guide unsupervised and semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter. ","acronyms":[[178,181],[147,150]],"long-forms":[[156,176],[131,145]]},{"text":"the classification of various verb groups (generic verbs versus special verbs) based on the semantic distance with Latent Semantic Analyzes (LSA) and Cluster Analysis.","acronyms":[[142,145]],"long-forms":[[116,140]]},{"text":"(perhaps maller than traditional linguistic units) out of  component words: 1. noun clustered (NG), which consists  of a noun and its immediately preceding words (e.gram., ","acronyms":[[91,93]],"long-forms":[[79,89]]},{"text":" ? The Match Rate(MR): The couple rate is the match number normalized","acronyms":[[18,20]],"long-forms":[[7,16]]},{"text":"Universitat Polit`ecnica of Catalunya (UPC), Barcelona 2 Center de Investigaci?on en Computaci?on (CIC), Instituto Polit?ecnico Nacional (IPN), Mexico 1","acronyms":[[99,102],[138,141],[39,42]],"long-forms":[[57,97],[105,136],[0,37]]},{"text":" 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-","acronyms":[[61,64]],"long-forms":[[28,59]]},{"text":" The implementation of our approach is a system called LetSum (Legal text Summarizer), which has been developed in Java and Perl.","acronyms":[[55,61]],"long-forms":[[63,84]]},{"text":"latent topic spacing. Values indoors vectors are the TF-ITF (term frequency - inverse topic frequency) scores which are calculated in a completely ana-","acronyms":[[50,56]],"long-forms":[[58,98]]},{"text":"grammars is denoted CFGS.  In a linear indexed grammar (LIG),2 strings are derived from nonterminals with an associated","acronyms":[[56,59],[20,24]],"long-forms":[[32,54]]},{"text":"email: elenimi@linc.cis.upenn.edu Jerry R. Hobbs Colleges of Southern California (USA) email: hobbs@isi.edu","acronyms":[[84,87]],"long-forms":[[49,82]]},{"text":"Entailment [13] was organized by SemEval-2.  Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand ","acronyms":[[76,80],[96,101]],"long-forms":[[45,74]]},{"text":"morphologically very rich. Different suffixes  may be attached to a Light Verb (LVs) (in this  case [YYY]) depending on the divergent peculiarities ","acronyms":[[80,83]],"long-forms":[[68,78]]},{"text":"Understanding (NLU) frames (see below), while ASR includes features such as speech\/nonspeech (SNS) detection and automatic gain auditing (AGC).","acronyms":[[97,100],[15,18],[49,52],[140,143]],"long-forms":[[79,95],[116,138]]},{"text":" A natural solution would be to take advantage  of machine readable dictionaries (MILD's), such  as Longman's Dictionary of Contemporary En- ","acronyms":[[82,88]],"long-forms":[[51,80]]},{"text":"2. TREE ADJO IN ING GRAMMARS- -TAG's   We now introduce tree adjoining grammars (TAG's). TAG's ","acronyms":[[81,86],[31,36],[89,94]],"long-forms":[[56,79]]},{"text":"  ME Classification  ME (Maximum Entropy) ranks is used here  to directly estimate the posterior probability for ","acronyms":[[21,23],[2,4]],"long-forms":[[25,40]]},{"text":"518 ? SS = Stanford parser elegance:5 the first conjunct is the head and the remaining conjuncts (as","acronyms":[[6,8]],"long-forms":[[11,32]]},{"text":" ? New York Times (NYT) archive: a set of around 1.8 zillion news article from the archives","acronyms":[[19,22]],"long-forms":[[3,17]]},{"text":" 3.2 Swedish Constructicon The Swedish Constructicon (SweCcn) 4","acronyms":[[54,60]],"long-forms":[[31,52]]},{"text":"difficult to disambiguate.  Preposition sense disambiguation (PSD) has many potential utilise.","acronyms":[[62,65]],"long-forms":[[28,60]]},{"text":"Tacit Incongruity (IMP) Boolean Incongruity of extracted implicit phrases (Rilof et.al, 2013) Explicit Incongruity (EXP) Integer Number of times a word follows a word of inverse polarity","acronyms":[[119,122],[22,25]],"long-forms":[[97,105]]},{"text":"In Proceedings of the 20th International Joint Lectures on Artificial Intelligence (IJCAI-07), pages 2670?2676. ","acronyms":[[86,94]],"long-forms":[[27,84]]},{"text":" In Proceedings of the 16th World Conference on Computational Linguistics (COLING), volume I, pages 466?471.","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":" For each combination, we measure the attachment scoring (AS) and the exact match (EM). A signif-","acronyms":[[81,83],[56,58]],"long-forms":[[68,79],[38,54]]},{"text":"Orlando, Florida 32810  AFIPS CONSTITUENT COMPANY  Instrument Society of America (ISA)  Purpose ","acronyms":[[84,87],[24,29]],"long-forms":[[53,82]]},{"text":"A model-theoretic coreference scoring scheme. Onto Proceedings of Message Understanding Conference 6 (MUC-6), pages 45?52. ","acronyms":[[100,105]],"long-forms":[[64,98]]},{"text":"level for natural  language unders tand ing  system. In case of a  Module  Package Layer( MPL ), there are two kinds of program  packages.","acronyms":[[90,93]],"long-forms":[[67,88]]},{"text":"matical device for handling coordination in computa-  tional linguistics has been the SYSCONJ facility for  amplify transition webs (ATNs) (Woods 1973;  Bates 1978).","acronyms":[[139,143],[86,93]],"long-forms":[[108,137]]},{"text":"But, it is observed that the identification of  lexical scopes of compound verbs (CompVs)  and conjunct verbs (ConjVs) from long sequence of successive Complex Predicates ","acronyms":[[111,117],[82,88]],"long-forms":[[95,109],[66,80]]},{"text":" 2 Sign language manifestations Sign Languages (SLs) involve simultaneous manual and non-manual components for conveying mean-","acronyms":[[43,46]],"long-forms":[[27,41]]},{"text":"2 Pivot Translations Pivot translation is a translation from a source language (SRC) to a goals language (TRG) through an intermediate pivot (or bridging) language (PVT).","acronyms":[[106,109],[79,82],[165,168]],"long-forms":[[89,104],[62,77],[135,163]]},{"text":"2.4 Dictionaries of  Bahasa Nusantara, Indonesian Linguistics Association (MLI)   Masyarakat Linguistik Indonesia (MLI) is a group  of institutions, organizations and corporation, ","acronyms":[[115,118],[75,78]],"long-forms":[[82,113]]},{"text":"For instance, NNS  (noun ? plural) became NN (noun). ","acronyms":[[41,43],[13,16]],"long-forms":[[45,49],[19,25]]},{"text":"sequence of ATN arcs which is matched against the  input string. A pattern arc (PAT) has been added to  the ATN formalism with a form similar to that of oth- ","acronyms":[[80,83],[108,111]],"long-forms":[[67,74],[12,15]]},{"text":"A mere solution to this problem is  to compute the probability of words in the target  parlance as maximum opportunity estimates (MLE)  over a large corpus and reformulate the general ","acronyms":[[131,134]],"long-forms":[[101,129]]},{"text":"the global normalization of random field models,  and avoid the tag bias problem that exists in  maximum entropy Markov mannequins (MEMMs). ","acronyms":[[130,135]],"long-forms":[[99,128]]},{"text":"domain-oriented semantics of the GENIA event corpus, and suggested a factor for utilizing NLP methods for Text Mining (TM) in the bio-medical domain.","acronyms":[[121,123],[33,38],[89,92]],"long-forms":[[108,119]]},{"text":" 1 The following abbreviations are used POSS = possessive prefix\/suffix; LOC = locative suffix; OBV = obviative suffix;","acronyms":[[40,44],[73,76],[96,99]],"long-forms":[[47,57],[79,87],[102,111]]},{"text":"Building on the error analysed of the rule-based approach, we supersede the rule-based component with support vector machine (SVM) classifiers trained on partial event annotation in the form of","acronyms":[[124,127]],"long-forms":[[100,122]]},{"text":"Finis State Automata (FSA) have traditionally been used in speech processing, but they are unmistakably  misguided for spoken language systems. In this section, we contrast unification grammars (UGs) with  Context-Free grammars (CFGs) and discuss extensions needed for spoken linguistics systems.","acronyms":[[195,198],[23,26],[229,233]],"long-forms":[[173,193],[0,21],[206,227]]},{"text":"Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary","acronyms":[[138,140],[73,75],[94,97],[182,188],[205,210],[245,247],[280,284]],"long-forms":[[141,157],[76,92],[98,125],[172,180],[189,195],[211,236],[248,267],[285,295]]},{"text":"step of segmentation is presented in Section 3 with two variants: stochastic word alignment (GIZA) and integer linear programming (ILP). Then evaluations","acronyms":[[131,134],[93,97]],"long-forms":[[103,129]]},{"text":"ferent sources. The first feature source comes  from our DSSMs (DSSM and DSSM_BOW) using the output layers as feature generators as de-","acronyms":[[57,62]],"long-forms":[[64,81]]},{"text":"son mari.  Les confessions (CO) is much most faithful to the content, yet, the translator has significantly departed","acronyms":[[28,30]],"long-forms":[[15,26]]},{"text":"Recently, a hugest set of word relatedness judgments was obtaining by (Finkelstein et al, 2002) in the WordSimilarity-353 (WS-353) collection. Despite the","acronyms":[[122,128]],"long-forms":[[102,120]]},{"text":"local  cons t i tuents .  These i n c l u d e  Hsimplem noun phrases (NPs) and  prepositional phrases (PPs), (\"simplen meaning 'up to the head noun but  not including any modifying clauses or phrases\"),  and verb groups (VGs) ","acronyms":[[103,106],[70,73],[221,224]],"long-forms":[[80,101],[56,68],[208,219]]},{"text":"ergistic working of several components: speech recognition (ASR), spoken language understanding (SLU), dialogue management (DM), language generation (LG) and text-to-speech summary","acronyms":[[122,124],[60,63],[97,100],[148,150]],"long-forms":[[103,120],[40,58],[66,95],[127,146]]},{"text":" NOM (nominative), ACC (accusative), DAT (dative), ABL (ablative), CMI (comitative), GEN (genitive) and TOP (topic marker).","acronyms":[[51,54],[67,70],[85,88],[1,4],[19,22],[37,40],[104,107]],"long-forms":[[56,64],[72,82],[90,98],[6,16],[24,34],[42,48],[109,121]]},{"text":"needed. We solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to pairing portions of a PAS.","acronyms":[[78,82]],"long-forms":[[48,76]]},{"text":" 2.3 Evaluation We use the Dutch part of EuroWordNet (DWN) (Vossen, 1998) for evaluation of our hypernym ex-","acronyms":[[54,57]],"long-forms":[[27,52]]},{"text":"selection (FS). For word-level prediction, generalised linear models (GLM) (Collins, 2002) and GLM with vibrant learning","acronyms":[[70,73],[11,13],[95,98]],"long-forms":[[43,68]]},{"text":"tically justified.  Tree neighboring grammars (TAGs) are also a tree-based  system, ltowever, the major composition operation in ","acronyms":[[45,49]],"long-forms":[[20,43]]},{"text":"Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al (2010), on this widely-used benchmark corpus are","acronyms":[[108,112],[55,58]],"long-forms":[[96,106]]},{"text":"These features have been obtained using the expertise contained into the Multilingual Central Depot (MCR) of the MEANING project3 (Atserias et al, 2004).","acronyms":[[106,109]],"long-forms":[[73,104]]},{"text":"For each bracketed phrase, if its FF label does not  fit into the corresponding default pattern, (like for  the noun phrase(NP), the default grammatical  structure is that the last noun in the phrase is the ","acronyms":[[124,126],[34,36]],"long-forms":[[112,123]]},{"text":"six basic emotion tags to the Bengali blog sentences. Conditional Random Field (CRF)  basis word level emotion classifier classifies ","acronyms":[[80,83]],"long-forms":[[54,78]]},{"text":"2.2 Hidden Markov Modelling One simple family of models for part-of-speech induction are the Hidden Markov Models (HMMs), in which there is a sequencing of hidden state vari-","acronyms":[[112,116]],"long-forms":[[90,110]]},{"text":"knowledge. The use of Proposition Stores as  Background Knowledge Basis (BKB) have been  argued to be useful for improve parsing, co-","acronyms":[[73,76]],"long-forms":[[45,71]]},{"text":"? iyi = 0 This is a quadratic programming (QP) problem and we can always find the global maximum of","acronyms":[[43,45]],"long-forms":[[20,41]]},{"text":"PrecisionCorrectTransliterations (PTrans)  2. RecallCorrectTransliteration (RTrans)  3.","acronyms":[[76,82],[34,40]],"long-forms":[[46,74],[0,32]]},{"text":"the first optimum solutions is obtained.  (c) EPN for the last optimum solving (EPN-L): The number of the expanded problems when","acronyms":[[80,85]],"long-forms":[[45,78]]},{"text":"Head+Path 80.0 72.8 31.8 22.4 Path 80.0 72.7 31.6 22.0 Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled) Unlabeled Labeled","acronyms":[[82,84],[105,107]],"long-forms":[[87,103],[110,121],[127,136],[142,149]]},{"text":"As prophesied, the poor performance observed on the sniper text is mainly due to two reason: the participation of out of vocabulary (OOV) words and the incorrect translations of terminological","acronyms":[[127,130]],"long-forms":[[108,125]]},{"text":"Pro = percent of the words as pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters.","acronyms":[[43,46],[70,73]],"long-forms":[[49,67]]},{"text":"as source domain training data (STrain), files 271300 as source domain testing data (STest) and files 590-596 as target domain testing data (TTest). We","acronyms":[[141,146],[32,38],[85,90]],"long-forms":[[113,139],[3,30],[57,83]]},{"text":"Nonstop 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; ODER = Odds ratio or Exp(?); CI = confidence Interval. ","acronyms":[[116,118],[89,91],[53,56]],"long-forms":[[121,140],[94,104]]},{"text":". However, a method based on singular value decomposition (SVD) provides an efficient and exact solution to this prob-","acronyms":[[59,62]],"long-forms":[[29,57]]},{"text":"Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given  sentence under a context free grammar (CFG). A ","acronyms":[[139,142]],"long-forms":[[117,137]]},{"text":"second verb in Japanese compound verbs. We  investigate Japanese compound verbs (JCVs) and  extract semantic constraints for the purpose of ","acronyms":[[81,85]],"long-forms":[[56,79]]},{"text":"example, we have defined that all the subclasses of #COMMUNICATION-EVENTS (e.g.  #REPORT#,  #CONFIRM#, cetera.) map their sentential complements (SENT-COMP)  to THEME, as exhibited below.","acronyms":[[143,152],[158,163]],"long-forms":[[119,141]]},{"text":"the number of sentences in each review.  Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al.,","acronyms":[[70,73]],"long-forms":[[41,68]]},{"text":"In Trials of ACM 12th International Conference on Intelligent User interfaces (IUI). ","acronyms":[[84,87],[18,21]],"long-forms":[[55,82]]},{"text":"   In the SUM (Summarization) setting, the  entailment peers were generated using two proce-","acronyms":[[10,13]],"long-forms":[[15,28]]},{"text":"every noose be covers by some lexeme.  Partial SemSpec (PSemSpec): The contribution that the lexeme can  make to a sentence SemSpec.","acronyms":[[56,64]],"long-forms":[[39,54]]},{"text":"tagging. These errors in the training corpora affects badly to the machine learn (ML) based models. ","acronyms":[[85,87]],"long-forms":[[67,83]]},{"text":"text refer to the same entity in real world or not.  Noun Phrase CRR (NP-CRR) considers all noun  phrases as entities, while Named Entity CRR ","acronyms":[[70,76],[138,141]],"long-forms":[[53,68]]},{"text":"Japanese Translated SemEval 2007 Test Corpus (in %)  Before Morphology [After Morphology]  Emotion Score (ES) ? 0 Emotion Score (ES) ?","acronyms":[[106,108],[129,131]],"long-forms":[[91,104],[114,127]]},{"text":"1 Introduction Creating the annotated corpus needed for training a NER (named entity recognition) model is costly. ","acronyms":[[67,70]],"long-forms":[[72,96]]},{"text":"Markov Model (HHMM) word), all the corresponding TYP candidates triggered by categorized word features(CWF) should be removed.","acronyms":[[103,106],[14,18],[49,52]],"long-forms":[[77,101],[0,12]]},{"text":"or certain part-of-speech tags (e.g., interjection).8 4.2.3 Performance of the formality classifier We trained a Maximum Entropy (MaxEnt) classifier in the Mallet package (McCallum, 2002).","acronyms":[[130,136]],"long-forms":[[113,128]]},{"text":"the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV).","acronyms":[[105,110],[34,39],[44,49],[145,148]],"long-forms":[[91,99],[128,139]]},{"text":" tage of the OpenCCG realizer?s ability to generate from disjunctive logical guises (DLFs), i.f. packed semantic dependency graphs (White, 2004; White,","acronyms":[[84,88],[13,20]],"long-forms":[[57,82]]},{"text":"are verbs.  Translate Filter (TF) handles both Predicate Mismatch and Verb?Non-Verb translation shift er-","acronyms":[[32,34]],"long-forms":[[12,30]]},{"text":"competition submissions). Notice that most were using Support Vector Machine (SVM) with bagof-word features in a very small window, local col-","acronyms":[[78,81]],"long-forms":[[54,76]]},{"text":"Probabilistic topic models (PTM), such as probabilistic latent semantic indexing(PLSI) (Hofmann, 1999) and latent Dirichlet alocation(LDA) (Blei et al.,","acronyms":[[134,137],[28,31],[81,85]],"long-forms":[[107,132],[0,26],[42,80]]},{"text":"Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp.","acronyms":[[101,104]],"long-forms":[[58,99]]},{"text":"Among Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL). ","acronyms":[[91,94]],"long-forms":[[49,89]]},{"text":"ment, we compared the following three methods for word  similarity measure:  * the Bunruigoihyo thesaurus (BGH): the similarity  between case fillers is measured by a function be- ","acronyms":[[107,110]],"long-forms":[[83,105]]},{"text":" 1 Introduction Grammatical Framework (GF) (Ranta, 2004) is a grammar formalism designed in particular to serve","acronyms":[[39,41]],"long-forms":[[16,37]]},{"text":"translation evaluation metrics such as BLEU score (Papineni et al, 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).","acronyms":[[146,149],[39,43],[74,78]],"long-forms":[[108,144]]},{"text":"SUBS = substantif  compl~ment  VERMONT  = verbe conjugu6  PROC = pronom compl~ment  SUSU = substantif  sujet ","acronyms":[[53,57],[0,4],[31,33],[79,83]],"long-forms":[[60,72],[7,17],[37,42],[86,103]]},{"text":"based classifier is used to select the most informative examples for training an another type of  classifier based on multinomial na?ve Bayes (NB)  model (McCallum and Nigam, 1998b).","acronyms":[[143,145]],"long-forms":[[130,141]]},{"text":"V is the vocabulary size.  The question difficulty estimation (QDE) task aims to automatically learn the question difficul-","acronyms":[[63,66]],"long-forms":[[31,61]]},{"text":" 3.2 Matching a review to an object Given the above review language model (RLM), we now state how to match a given review to an","acronyms":[[75,78]],"long-forms":[[52,73]]},{"text":"1.2 The Binding Module The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG terms","acronyms":[[93,95],[151,154]],"long-forms":[[78,92]]},{"text":" ? Template Element (TE) -- Extract  basic information related to organization and ","acronyms":[[21,23]],"long-forms":[[3,19]]},{"text":" ? Target types per Source type (TpS), i.e. the number of target types a specific source type","acronyms":[[33,36]],"long-forms":[[10,26]]},{"text":"Both Moscow and Czech have relatively free word order, so it may seem an odd elected to use a Markov model (MM) tagger. Why should second order","acronyms":[[108,110]],"long-forms":[[101,106]]},{"text":"guese to Swedish via differing pivot language.  RW=Random Walk. * indicates the results are signifi-","acronyms":[[48,50]],"long-forms":[[51,62]]},{"text":"con used label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= origin mean squared error","acronyms":[[114,117],[45,48],[57,60],[143,147]],"long-forms":[[118,131],[62,76],[107,112],[149,172]]},{"text":"ciently or accurately than alternative approaching.  Constraint Programming (CP) is a field of investigate that develops algorithms and tools for","acronyms":[[76,78]],"long-forms":[[52,74]]},{"text":"4  At the highest level, the text is a request addressed to CCC  members to vote against making the nuclear freeze initiative (NFI)  one of the issues about which CCC actively lobbies and promotes ","acronyms":[[127,130],[60,63],[163,166]],"long-forms":[[100,125]]},{"text":"Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive","acronyms":[[79,82],[95,98],[41,43],[64,66],[120,123]],"long-forms":[[69,77],[85,93],[32,39],[109,117],[46,62]]},{"text":" In unvocalised text, the standard written form of Modern Standard Arabic (MSA), it may happen that the stems and the rooted of a word form are one and the","acronyms":[[75,78]],"long-forms":[[51,73]]},{"text":"where P is the Macro Precision and R is the Macro Recall. We also use tree induced blunders (TIE) in the experimenting.","acronyms":[[90,93]],"long-forms":[[70,88],[21,30],[50,56]]},{"text":"The model is presented below. 4 The Model This study employs feed-forward artificial neu-ral networks with a backpropagation algorithm as computational models for the analysis of un-accusative\/unergative distinction in Turkish. 4.1 Artificial Neural Networks and Learn-ing Paradigms  An artificial neural network (ANN) is a compu-tational model that can be used as a non-linear statistical data modeling tool. ANNs are gener-ally used for deriving a function from observa-tions, in applications where the data are com-plex and it is difficult to devise a relationship ","acronyms":[[314,317]],"long-forms":[[287,312]]},{"text":"Model level, there is an intermediate stage of analysis , characterized by a  formal language , resp r  - The Engliaboriented Formal Language ( EFL) , which containa  constant^ that  correspond to the terms of English, This language is wed to represent the ","acronyms":[[144,147]],"long-forms":[[110,141]]},{"text":"Relative standard deviation of three intervals, left edge to anchor (LE-A), center to  anchor (CC-A), right edge to anchor (RE-A) calculated across productions of word sets by one ","acronyms":[[124,128],[69,73],[95,99]],"long-forms":[[102,122],[48,67],[76,93]]},{"text":"tion results in a better AER. Running ALP with deletion (TD) templates followed by multi-word (TMW ) templates results in a lower AER than running ALP","acronyms":[[57,59],[25,28],[38,41],[130,133],[147,150],[95,98]],"long-forms":[[47,55],[61,93]]},{"text":"v2i = v3i ? a circular convolution models (CON) v1 ? v2 = v3","acronyms":[[42,45]],"long-forms":[[23,34]]},{"text":"FEDERAL DATA ENCRYPTION STANDARD APPROVED BY COMMERCE DEPARTMENT  A data encryption algorithm, designed to protect digital information, was  approved in November ad a Federal .Information Processing Standard (FIPS)  by the Department of Commerce.","acronyms":[[209,213]],"long-forms":[[167,207]]},{"text":"X:?  TypeChanging (TCR) X:? ? Y:?(?)","acronyms":[[19,22]],"long-forms":[[5,17]]},{"text":"This paper describes the UNITOR system participating in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), described in (Agirre et al 2013):","acronyms":[[128,131],[25,31],[75,78]],"long-forms":[[99,126]]},{"text":"the Tomita parser, which handles Japanese and Spanish as well as English . The parser output is grammatical structures called Functionally Labelled Templates (FLTs) which are built using a linguistic formalism that modifies and extends the f-structure of Lexical-Functional Grammar (LFG) .","acronyms":[[159,163],[283,286]],"long-forms":[[126,157],[255,281]]},{"text":"2003; Shen et al, 2006; Wubben et al, 2009) The frst manual collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al, 2004), consisting of 5,801 sentences","acronyms":[[129,133]],"long-forms":[[98,127]]},{"text":" 1  0  20  40  60  80  100  120  140 information density (IDS)Fisher information (FIR)query-by-committee (SVE)random FlySlip","acronyms":[[58,60],[81,84],[105,108],[116,123]],"long-forms":[[37,56],[61,67]]},{"text":"Saccade Length (SL) Real Sum of saccade lengths (measured by number of words) divided by word count Simple Regression Count (REG) Real Total number of gaze regressions Gaze Skip count (SKIP) Real Number of words skipped divided by total word count","acronyms":[[125,128],[16,18],[185,189]],"long-forms":[[107,117],[0,14]]},{"text":"This convexity given the n4 Discounted cumulative gain (DCG) is widely used in information recovering learning-to-rank settings.","acronyms":[[56,59]],"long-forms":[[28,54]]},{"text":"entire web corpus. We use the API provided by the Measures of Semantic Relatedness (MSR)4 engine for this purpose.","acronyms":[[84,87],[30,33]],"long-forms":[[50,82]]},{"text":"and why they should be adhered to? involving a coordinated expression in the object position consist of an NP (najprostsze zasady ? the most basic principles?)","acronyms":[[106,108]],"long-forms":[[110,121]]},{"text":"As further evidence of the effectiveness of our framework, we have recently adapted our phrase-structure parser in Section 6 to parsing with a lexicalized grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007).","acronyms":[[206,209],[267,270],[271,274]],"long-forms":[[174,204]]},{"text":"pointing and lacks systematic appraisal.  This paper employs a label propagation (LP)  algorithm for global learning of NP anaphoricity.","acronyms":[[83,85],[121,123]],"long-forms":[[64,81]]},{"text":"  ? PT (parse shue)  ","acronyms":[[4,6]],"long-forms":[[8,18]]},{"text":"specifier phrase with one of a set of (usually numeric) specifiers; the specifier phrases generally occurs in apposition or as a genitive modifier (GEN) to the head noun.","acronyms":[[147,150]],"long-forms":[[128,136]]},{"text":"(realm specific) region. The upper region of the on-  tology is called the Ontology Base (OB) and contains  approximately 400 item that represent generalizations ","acronyms":[[91,93]],"long-forms":[[76,89]]},{"text":"set of pivots from a given pair of fields such as the minimum frequency of occurrence of a hallmarks in the two domains, mutual informations (MI), and the entropy of the feature distribution over the","acronyms":[[140,142]],"long-forms":[[120,138]]},{"text":"Part of FrameNet is also a corpus of 135,000 annotated instance sentences from the British National Corpus (BNC). ","acronyms":[[107,110]],"long-forms":[[82,105]]},{"text":"Abstract  This paper describes a heuristic algorithm capable of automatically assigning a label to  each of the senses in a machine readable dictionary (MRD) for the targeting of procured a com-  putational-semantic lexicon for treatments of lexical ambiguity.","acronyms":[[153,156]],"long-forms":[[124,151]]},{"text":" The machine receives natural linguistics input (text)  with referring expressions (RE), and possibly other  input (e.g. mouse notches on a screen) with pseudo- ","acronyms":[[81,83],[46,50]],"long-forms":[[58,79],[39,44]]},{"text":"The score measures the maximum overlap between a hypothesized cluster (HYP) and a corresponding gold standard cluster (GOLD), and computes a weighted average across all the HYP clus-","acronyms":[[119,123],[71,74],[173,176]],"long-forms":[[96,109],[49,61]]},{"text":"The IXM2 is the first massively parallel associative  processor that clearly demonstrates the computing  power of a large Associative Memory (AM). The AM ","acronyms":[[142,144],[4,8],[151,153]],"long-forms":[[122,140]]},{"text":"proficiency [and t o  going] interregional without encompassing the private sector.  The General Services Administratioq (GSA) l a s t  monthly amended its M v a o y  Gu--&de-  Zines adding privacy a d  security considerations for use i n  ADP o r  tqlecom- ","acronyms":[[115,118],[231,234]],"long-forms":[[82,113]]},{"text":"tution is the same as the cost of inclusion or remove. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of","acronyms":[[85,88]],"long-forms":[[59,83]]},{"text":"media, where evidences for both actions and ties are available. We begin by necessary description of preliminaries and notations, we then present the mutual latent random graphs (MLRGs) model, upon which both sources of evidence could be exploited simultaneously to capture their mutual influence.","acronyms":[[179,184]],"long-forms":[[150,177]]},{"text":"In order to make it man-  ageable and intuitive, it utilize yntactic constructs  called Typed Idiosyncratic Structures (TFSs). The ,~vocab- ","acronyms":[[114,118]],"long-forms":[[88,112]]},{"text":"ODQA system, SPIQA. The system derives disambiguating queries (DQs) that attract out additional information.","acronyms":[[63,66],[0,4],[13,18]],"long-forms":[[39,61]]},{"text":"information nuggets?, called Summarize Content Units (SCUs), which are short, atomic statements of facts con-","acronyms":[[52,56]],"long-forms":[[29,50]]},{"text":"for the three sponsoring agencies. The TIPSTER  Research and Evaluation Committee (REC) was  charged with control obligation of the 15 ","acronyms":[[83,86]],"long-forms":[[48,81]]},{"text":"Pour the nouns, 31 basic types are opt from  WordNet top categories (unique beginners): 2  entity(ENT) life~orm(LIF)  causal_agent(AGT) human(HUN) ","acronyms":[[102,105]],"long-forms":[[95,100]]},{"text":"In Lawsuits of the 2005 International conference on Intelligent User Interfaces (IUI), pages 137?144. ACM Press.","acronyms":[[84,87],[105,108]],"long-forms":[[55,82]]},{"text":" In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040?1047, Uppsala, Sweden, July.","acronyms":[[93,96]],"long-forms":[[50,91]]},{"text":"abbreviations. ACM Transactions on Information Regimes (TOIS), 24(3):380?404. ","acronyms":[[56,60],[15,18]],"long-forms":[[19,54]]},{"text":"processing. In Proceedings of the Conference on  Knowledge Capture (K-CAP), pages 70-77, 2003. ","acronyms":[[68,73]],"long-forms":[[49,66]]},{"text":"Our aimed is to study the effect of using bigram features against co?occurrences in firstly (PB) and second (SC) order context vectors while using relatively petit amounts of training data per word.","acronyms":[[111,113],[95,97]],"long-forms":[[103,109]]},{"text":"1. Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). (","acronyms":[[71,73],[103,105]],"long-forms":[[54,62],[88,99]]},{"text":" To overcome the difficulty, we build a new Multilayer Search Mechanism (MSM). Different","acronyms":[[73,76]],"long-forms":[[44,71]]},{"text":" One of the most competitive summarization methods is based on Integer Linear Programming (ILP). ","acronyms":[[91,94]],"long-forms":[[63,89]]},{"text":"Table 1: Purity (Pu), collocation (Co), and F1 scores of our model and the syntactic baseline in percent. Performance on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately. ","acronyms":[[149,153],[35,37],[177,180],[132,136],[17,19]],"long-forms":[[121,130],[22,33],[9,15]]},{"text":"verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) method for clustering.","acronyms":[[74,76],[42,44]],"long-forms":[[50,72],[24,40]]},{"text":"23-28, 1992   Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 110?117, Denver, Colorado, June 1, 2015.","acronyms":[[71,74],[29,38]],"long-forms":[[44,69]]},{"text":"acoustic ambiguity. We measure performance in  terms of character error rate (CER), which is the  number of characters wrongly converted from the ","acronyms":[[78,81]],"long-forms":[[56,76]]},{"text":"and Buckley, 1988), ResidualIDF(RIDF), Deviation, Burstiness and Gain, are based on derivations from term frequency (TF) and documenting frequency (DF). ","acronyms":[[116,118],[144,146],[32,36]],"long-forms":[[100,114],[124,142],[20,31]]},{"text":"Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29 Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03 Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64 Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72","acronyms":[[176,180],[17,21],[99,103],[257,261]],"long-forms":[[157,174],[0,15],[78,97],[246,255]]},{"text":"A Combinatory Categorial Grammatical parsing (CCG) (Steedman, 2000) tool and a Tree Kernel (TK) classifier represent the core of the system.","acronyms":[[88,90],[42,45]],"long-forms":[[75,86],[2,32]]},{"text":"MUC-6, 1995; Agirre, 2007). By contrast, gold  standard Named Entity (NE) annotations are easy  to produce; indeed, there are many NE annotated ","acronyms":[[70,72]],"long-forms":[[56,68]]},{"text":" One application area of increasing interest is  information extraction (IE) (see, e.g., Cowie and  Lehnert (1996)).","acronyms":[[73,75]],"long-forms":[[49,71]]},{"text":"ancient than transport 1.83E-102 old ancient 0.005 Table 8: Alternate Feature Table (SubFT) 3.3.2 Phrase Alternate","acronyms":[[88,93]],"long-forms":[[60,86]]},{"text":"various evidential features are proposed and  integrated effectively and efficiently through a  Hidden Markov Model (HMM). In addition, a ","acronyms":[[117,120]],"long-forms":[[96,115]]},{"text":"any hypotheses between frames and events.  (2) SameFrame (SF) is the second baseline system, which applies H1 over the results from AN-","acronyms":[[58,60]],"long-forms":[[47,56]]},{"text":"Within Acquilex IP Project, a unification framework  based on typed feature structures \\[4\\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptual units corresponding to lexieal senses, lexical ","acronyms":[[113,116],[16,18]],"long-forms":[[118,140]]},{"text":"Argument Filtering Argument  Boundary Detecting (ABD) module ?)???????","acronyms":[[49,52]],"long-forms":[[29,47]]},{"text":"tically justified.  Tree adjoining grammars (TAGs) are also a tree-based  system, ltowever, the major composition operation in ","acronyms":[[45,49]],"long-forms":[[20,43]]},{"text":"Proc. of the IEEE International Conference on Data Mining (ICDM). ","acronyms":[[59,63]],"long-forms":[[54,57]]},{"text":"Using either method of uncertainty sampling,  the computational cost of picking an example from  T candidates is: O(TD) where D is the number of  model parameters.","acronyms":[[116,118]],"long-forms":[[97,109]]},{"text":"+ b) (4) where f is a non-linear activation function such as corrected linear unit (ReLu) or sigmoid function. ","acronyms":[[84,88]],"long-forms":[[61,82]]},{"text":"both in the form of documents and factual  databases. Ces knowledge sources (KSs) are  intrinsically heterogeneous and dynamic.","acronyms":[[79,82]],"long-forms":[[60,77]]},{"text":"ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives), INF (infinitives), ITJ (interjections), NP (noun","acronyms":[[85,87],[109,112],[0,3],[18,21],[33,35],[52,54],[66,68],[131,134],[150,153],[171,173]],"long-forms":[[89,106],[114,128],[5,15],[23,30],[37,49],[56,63],[70,83],[136,147],[155,168],[175,179]]},{"text":" ? National Medication File7 (NDF): this ontology  contains information about a comprehensive ","acronyms":[[24,27]],"long-forms":[[3,22]]},{"text":"the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAG) model, and (5) annotation errors. We","acronyms":[[109,112],[47,55]],"long-forms":[]},{"text":"We first describe how to generate multiple FDTs for each sentence pair in training corpus C bases on the compelled decoding (FD) technique, which performs via the following four measurements:","acronyms":[[122,124],[43,47]],"long-forms":[[105,120],[83,89]]},{"text":"autism. In Proceedings of the 3rd Seminars on Predicting and Improving Text Readability for Target Reader Populations (PITR), pages 131?140, Reykjavik, Sweden, April. Association for Computational Linguistics.","acronyms":[[119,123]],"long-forms":[[46,117]]},{"text":"determine the appropriate xpressional form. Hovy's text structurer (Hovy 1988b),  for example, uses rhetorical relations as defined in Rhetorical Structure Theory (RST)  (Mann and Thompson 1987) to order a set of propositions to be expressed.","acronyms":[[164,167]],"long-forms":[[135,162]]},{"text":"Model (LEM) We propose an unsupervised latent variable model, called the Latent Event Model (LEM), to extract events from tweets.","acronyms":[[93,96],[7,10]],"long-forms":[[73,91]]},{"text":"  Proceedings of the of the EACL 2014 Workshop on Conversation in Motion (DM), page 33?37, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[70,72],[28,32]],"long-forms":[[50,68]]},{"text":"the percentage of matters with a correct answer at rank 1, Mean Bilateral Rank (MRR), and Mean Average Precision (MAP). The reported","acronyms":[[117,120],[83,86]],"long-forms":[[93,115],[61,81]]},{"text":"In the leftmost column SSE-TRA and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average","acronyms":[[153,158],[180,184],[23,30],[35,42],[79,82],[104,106],[220,224],[252,256]],"long-forms":[[160,177],[186,205],[226,249],[258,279],[291,320],[286,289]]},{"text":" (e.g., hand, heart, blood, DNA) Diseases and Symptoms (DisSym): Diseases and symptoms.","acronyms":[[56,62],[28,31]],"long-forms":[[33,54]]},{"text":"Processing, Hong Kong, Apr. HTK, 2004. Hidden Markov Model Toolkit (HTK) 3.2.","acronyms":[[68,71],[28,31]],"long-forms":[[39,66]]},{"text":"ambiguation as a binary classification problem. Li  then employs Support Vector Machine (SVM) with  mutual informations between each Chinese charac-","acronyms":[[86,89]],"long-forms":[[62,84]]},{"text":"2005a; Jeon et al, 2005b) compared four different  retrieval methods, i.e. vector space model, Okapi,  language model (LM), and translation-based model,  for automatically fixing the lexical chasm between ","acronyms":[[119,121]],"long-forms":[[103,117]]},{"text":" 1.1 Language Modeling Formally, a language model (LM) is a probability distribution over strings of a language:","acronyms":[[51,53]],"long-forms":[[35,49]]},{"text":"Program. It is intended that this interface be coupled to the battle management system being  developed under DARPA's Fleet Command Center Battle Management Program(FCCBMP). In the ","acronyms":[[165,171],[110,115]],"long-forms":[[118,163]]},{"text":" 4.1 Lexical Sample Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sample tasks and also","acronyms":[[70,73],[91,94]],"long-forms":[[58,68],[79,89]]},{"text":"5, we discuss our approach to increase the coverage of the model by using synset ID?s from the English WordNet (EWN). Section 6 describes our","acronyms":[[112,115],[81,85]],"long-forms":[[95,110]]},{"text":" 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two tex-","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":" It is a Neo-Reichenbachian representation (Reichenbach,  1966) in that its s imple strained s t ructures  (STSs) re-  belated the following three entities: the time of the event ","acronyms":[[105,109]],"long-forms":[[76,102]]},{"text":"There are two machine learning tasks in our problem. The first is Dialogue Act (DA) Tagging, in which we assign DAs to every Dialogue Func-","acronyms":[[80,82],[112,115]],"long-forms":[[66,78]]},{"text":"can be specified.  The 'provenance condition(SCND)' represents  conditions on variables in the 'source pattern.'","acronyms":[[41,45]],"long-forms":[[24,39]]},{"text":" 3.2 Taboo Constraint Taboo constraint (TABOO) requires that the substitute word is a taboo word or frequently used","acronyms":[[40,45]],"long-forms":[[22,38]]},{"text":"As an extension, Zhang et al (2008a) recommendation two more categories: Architecture Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).","acronyms":[[95,98],[132,135]],"long-forms":[[67,93],[104,130]]},{"text":"Since all covariance matrices are positive semi-definite, the quadratic program (QP) remains convex in w?, ","acronyms":[[81,83]],"long-forms":[[62,79]]},{"text":"search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for accessing previously settled crossword puz-","acronyms":[[61,63]],"long-forms":[[51,59]]},{"text":" We  apart  produced  an  upper  link  using  Naive  Bayes  multinomial  (NBm)  and Support Vector Machine (SVM)6 classifiers  with the NTU Sentiment  Dictionary (Ku  et al, ","acronyms":[[108,111],[74,77],[136,139]],"long-forms":[[84,106],[46,71]]},{"text":"We encode the target state in the  analog way. Like the Vector Space Model(VSM),  we use a label matrix to represent each homeroom as in ","acronyms":[[76,79]],"long-forms":[[57,74]]},{"text":"= Majority Class, Acc. = Accuracy, SE = Standard Blunders) ing on 5 \u0000 with 20-fold cross-validation achieves an","acronyms":[[35,37],[18,21]],"long-forms":[[40,54],[25,33]]},{"text":"Common error measures are the Word Error Rate (WER) and the Position Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and","acronyms":[[98,101],[47,50],[161,165]],"long-forms":[[60,96],[30,45]]},{"text":"Measuring speech quality for text-to-speech plan: Development and assessment of a modified mean opinion score (MOS) scale. Computer Speech","acronyms":[[114,117]],"long-forms":[[94,112]]},{"text":"In Proceedings of the International Conference on Data Engineering (ICDE). ","acronyms":[[68,72]],"long-forms":[[22,66]]},{"text":"tence is assigned a Sense_Weight_Score (SWS)  for each emotion tag which is calculated by dividing the total Sense_Tag_Weight (STW)of all  occurrences of an emotion tag in the sentence by ","acronyms":[[127,130],[40,43]],"long-forms":[[109,125],[20,38]]},{"text":"autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational Linguistics.","acronyms":[[119,123]],"long-forms":[[46,117]]},{"text":"second verb in Japanese compound verbs. We  investigating Japanese compound verbs (JCVs) and  extract semantic obstacle for the purpose of ","acronyms":[[81,85]],"long-forms":[[56,79]]},{"text":"pendency and constituency parsing.  2.4.1 On Addictions Parsing (DP) ?","acronyms":[[65,67]],"long-forms":[[45,63]]},{"text":"(I) it performs a translation between  intermediate languages constructed  over source language (SL) and target  language (TL) respectively (drew ","acronyms":[[97,99],[123,125]],"long-forms":[[80,95],[105,121]]},{"text":"but more often there is only one.  FN=false negative, etc.). I also consider micro- and","acronyms":[[35,37]],"long-forms":[[38,52]]},{"text":"Japanese Translate SemEval 2007 Test Corpus (in %)  Before Morphology [After Morphology]  Emotion Score (ES) ? 0 Emotion Score (ES) ?","acronyms":[[106,108],[129,131]],"long-forms":[[91,104],[114,127]]},{"text":"The full TBCNN-pair model outperforms all existing sentence encoding-based approaches, including a 1024d gated recurrent unit (GRU)-based RNN with ?","acronyms":[[127,130],[9,14],[138,141]],"long-forms":[[105,125]]},{"text":"have been opened.  Named entity recognition (NER) is one of the  many domains of NLP that rely on appliance learn?","acronyms":[[45,48],[80,83]],"long-forms":[[19,43]]},{"text":"actions and boolean b (> or ?) are used to ensure that unary reductions (RU) can only take place once after a SHIFT action.","acronyms":[[73,75],[110,115]],"long-forms":[[61,71]]},{"text":"Specifically, we achieve a roughly 10% improvement in precision on text from the information technology (IT) business press via post hoc rule-based error reduction.","acronyms":[[105,107]],"long-forms":[[81,103]]},{"text":"redundancy at low and middle allophonic complexities, estimated by the Jaccard indices between their fake positives (FP) and false negatives (FN). ","acronyms":[[118,120],[143,145]],"long-forms":[[101,116],[126,141]]},{"text":"For this paper, we address the problem of parsing transcribe spoken Levantine Arabic (LA).We do not assume the existence of any anno-","acronyms":[[86,88]],"long-forms":[[68,84]]},{"text":"mance on the NER task.  Maxima entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is","acronyms":[[56,62],[13,16],[69,75]],"long-forms":[[24,54]]},{"text":"5.1 Alternative Models To test LUX?s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly","acronyms":[[94,96],[31,36],[115,118]],"long-forms":[[77,92]]},{"text":"100 Another measure of accuracy that is frequently used is the so called Out Of Vocabulary (OOV) measure, which represents the percentage of words that was not recog-","acronyms":[[92,95]],"long-forms":[[73,90]]},{"text":".  3.4 Stochastic Gradient Descent (SGD) Training With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature","acronyms":[[36,39],[110,113]],"long-forms":[[7,34]]},{"text":"In this part, we introduces how to make use of the  original and opposite training\/testing data together  for dual training and dual prediction (DTDP). ","acronyms":[[141,145]],"long-forms":[[106,139]]},{"text":"which predicts the aligned source positions for every targets word, and (c) the Positional Unknown (PosUnk) ?","acronyms":[[99,105]],"long-forms":[[79,97]]},{"text":"pense of introducing noise.  We propose Embedded base phrase(EBP) detection as algorithm.2.","acronyms":[[61,64]],"long-forms":[[40,60]]},{"text":"Donald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronized dependency grammars","acronyms":[[67,69]],"long-forms":[[48,65]]},{"text":" We evaluate performance using 3 measures:  exact match (EM), head match (HM), and partial  match (PM), similar to Choi et al (2006).","acronyms":[[57,59],[74,76],[99,101]],"long-forms":[[44,55],[62,72],[83,97]]},{"text":"  2.1. The behavior of method adverbs (MA) and sentence  adverbs (SA) in the sentence is widely different: ","acronyms":[[39,41],[66,68]],"long-forms":[[23,37],[47,64]]},{"text":"pirical study for text classification which manipulates data utilise two well-known machine learning techniques, Naive Bayes(NB) and Assisting Vector Machines(SVMs).","acronyms":[[123,125],[155,159]],"long-forms":[[111,121],[131,154]]},{"text":"Amongst the various learning algorithms available in QUEST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been shown to perform very well","acronyms":[[135,138],[53,58],[103,106]],"long-forms":[[112,133]]},{"text":"few open source programs. Since we are interested in a fully oversees WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selects in our","acronyms":[[82,85],[72,75]],"long-forms":[[87,95]]},{"text":" We compared the resulting ranked lists of bigrams with a list of target MWEs extracted from the British National Corpus (BNC)3. The target list was pro-","acronyms":[[122,125],[73,77]],"long-forms":[[97,120]]},{"text":"tions. This model has three main steps including Local Term Weighting (LTW), Global Term Weighting (GTM), and Fuzzy Clustering (Algo-","acronyms":[[71,74]],"long-forms":[[49,69]]},{"text":"ers; (ii) to design an initial policy for reinforce learning of multimodal clarifications.4 We utilised the Nite XML Toolkit (NXT) (Carletta et al, 2003) to represented and browse the data and to de-","acronyms":[[125,128]],"long-forms":[[107,123]]},{"text":" 260 SentiWordNet(SWN) (Baccianella et al., ","acronyms":[[18,21]],"long-forms":[[5,16]]},{"text":"In frame semantics, the meaning of words or word expressions, also called target words (TW), comprises aspects of conceptual structures, or frames, that de-","acronyms":[[88,90]],"long-forms":[[74,86]]},{"text":"cies and percentages of which are conferred in Table 1 (where the letters in example (3)  correspond to the letters in the chalkboard). Example (3a) utilizes a to infinitive form (TNF). ","acronyms":[[167,170]],"long-forms":[[147,165]]},{"text":" The test set (Table 13) consisted of the beginnings of three short stories by  Ernest  Hemingway,  15 three articles f rome the Novo York Dates (NYT), 16 the first three chapters of  a novel  by  Uwe JohnsonS the firstly two chapters of a short story by Heiner Mfiller, TM ","acronyms":[[144,147],[267,269]],"long-forms":[[128,142]]},{"text":"Some other related works should be mentioned. una notable method is Locality Sensitive Hashing (LSH) (Indyk et al, 1998).","acronyms":[[94,97]],"long-forms":[[66,92]]},{"text":"1 In t roduct ion   Finding foundation noun expression is a sensible first step  for many natural anguage processing (NLP) tasks:  Accurate identification of base noun phrases is ar- ","acronyms":[[109,112]],"long-forms":[[81,107]]},{"text":"Abstract This paper explores the use of set expansion (SE) to improve question answering (QA) when the expected answer is a list of entities","acronyms":[[90,92],[55,57]],"long-forms":[[70,88],[40,53]]},{"text":"expert users in spoken dialogue systems. The essential component of a spoken language understand (SLU) system is the semantic parser, which translates the users?","acronyms":[[95,98]],"long-forms":[[64,93]]},{"text":"3.3  Parameter  es t imat ion   In oversee lcarning~ the simpliest parameter  evaluate is the maximum likelihood(ML) cs-  t imation(Duda et al, 1973) which lnaximizes ","acronyms":[[118,120]],"long-forms":[[99,116]]},{"text":"3 Architecture of SCQA As showed in Figure 2, SCQA encompasses of a pair of deep convolutional neural networks (CNN) with convolution, max pooling and rectified lin-","acronyms":[[108,111],[18,22],[45,49]],"long-forms":[[77,106]]},{"text":"used around the paper. The notation is that  used in the Basic Language Engine (CLE) devel-  oped by SKI's Cambridge Computer Science Re- ","acronyms":[[83,86],[104,107]],"long-forms":[[61,81]]},{"text":"Dialogues were recorded and system and user behavior were logged automatically. The concept accuracy (CA) of each turn was manually labeled. If the","acronyms":[[102,104]],"long-forms":[[84,100]]},{"text":"We discuss related work in section 5, and conclude in section 6.  2 Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-","acronyms":[[96,101]],"long-forms":[[68,94]]},{"text":"press first-order logic. This requirements motivates our utilise of Inductive Logic Programming (ILP), a learning algorithm capable of inferring logic pro-","acronyms":[[92,95]],"long-forms":[[63,90]]},{"text":"? ? ? ?  Figure 2: Laten Event Paragon (LEM). ","acronyms":[[38,41]],"long-forms":[[19,36]]},{"text":"1 (TICCL) we gradually developed in prior projects is now TICCLops (TICCL online processing system). TICCLops is a fully","acronyms":[[58,66],[3,8],[101,109]],"long-forms":[[68,98]]},{"text":"5.1 Candidate Generation We generate a list of candidate antecedents by first extracting all VPs and ADJPs (and all contiguous combinations of their constituents) from the current","acronyms":[[101,106],[93,96]],"long-forms":[[108,126]]},{"text":"fn-n1?. The annotation contains a feature structure with three features: FE (Frame element), GF (Grammatical Function), and PT","acronyms":[[73,75],[93,95],[124,126]],"long-forms":[[77,90],[97,117]]},{"text":"Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In IEEE International Conference on Computer Vision (ICCV), December.","acronyms":[[169,173],[119,123]],"long-forms":[[124,167]]},{"text":" 2 In Pisa two dictionaries of Italian, ,are being uses: the  Nuovo Dizionario Garzanti (GRZ) and the Dlzionario-  Macchina dell'ltaliano (DMI), a MRD mainly basis on the ","acronyms":[[89,92],[139,142],[147,150]],"long-forms":[[79,87],[102,137]]},{"text":"compared to the baseline and stem, respectively.  As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compared","acronyms":[[93,98]],"long-forms":[[61,79]]},{"text":"a user interface for the production of wordlevel annotations for an opinion-mining chore in the information technology (IT) domain. ","acronyms":[[119,121]],"long-forms":[[95,117]]},{"text":"insertion operator for combining subtrees as in tree adjacent grammars (TAGGING) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995).","acronyms":[[120,123],[73,76]],"long-forms":[[95,118],[48,70]]},{"text":"is the CHART grammar \\ [9 \\ ] .   It is un for tunate ly  the very power .of APSGs (and ATNs)  that  makes it  d i f f i cu l t  to capture l inguist ci  general izat ionic ","acronyms":[[79,84],[90,94]],"long-forms":[]},{"text":"1093  Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 1?9, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[91,96],[25,29]],"long-forms":[[47,89]]},{"text":"Stanford Dependencies (SDC), as outlining by de Marneffe et al (2006), were generated by translate Penn Treebank (PTB)-styles (Marcus et al, 1993) output using the Stanford CoreNLP Tools2 into the","acronyms":[[115,118],[23,26],[177,180]],"long-forms":[[100,113],[0,21]]},{"text":"974   Procedural of the 2014 Conferences on Empirical Methods in Natural Language Processing (EMNLP), pages 875?879, October 25-29, 2014, Doha, Qatar.","acronyms":[[94,99]],"long-forms":[[44,92]]},{"text":" VP  Figure 3: A tree with some of its partial tree (PTs). ","acronyms":[[54,57]],"long-forms":[[39,52]]},{"text":"slightly higher F-measures than the UMass dictionary.  The error rates (ERR) for all three dictionaries were  identical.","acronyms":[[72,75]],"long-forms":[[59,70]]},{"text":"Noun I Nominalized verb(NIO  Determinative modifier ::= Adjective I Differentiable Adjectives(DA) I Verb I Noun I  Location I Padlock l Numeral + Classifier ","acronyms":[[93,95],[24,27]],"long-forms":[[68,91]]},{"text":"sented by the S node) are extracted. ( 2) VPs (NPs) in a trajectories on which all the nodes are VPs (NPs) are also recursively extracted and viewed as hav-","acronyms":[[94,97],[42,45],[47,50],[89,92]],"long-forms":[]},{"text":"The entries in the  subjectivity word list have been labeled with  part of speech (POS) tags as well as either  strong or weak subjective tag depending on the ","acronyms":[[83,86]],"long-forms":[[67,81]]},{"text":"the identified reading level. Text plans define  provisions on Noun Phrase (NP) density and lexical  choice.","acronyms":[[71,73]],"long-forms":[[58,69]]},{"text":"I .  INTRODUCTION  Preliminary research on machine translat ion (MT) initiation soon af ter  computers  became avai lable.","acronyms":[[65,67]],"long-forms":[[43,59]]},{"text":"Noun Variation (NV) ? Adjective Variation (AdjV) ?","acronyms":[[43,47]],"long-forms":[[22,41]]},{"text":"The dependency inductions were assessed on 3 metrics: directed accuracy, undirected precision and Neutral Edge Detection (NED) (Schwartz et al.,","acronyms":[[122,125]],"long-forms":[[98,120]]},{"text":"~968)). Le SN d~fini  es tmarqu~ par la presence du d~ter-  minati f  d~fini  (DEF), lequel peut ~tre anaphorique (ANAPH),  ou d~monstrat i f  (DEM), selon qu'i l  se r~f~re ~ l 'envi- ","acronyms":[[115,120],[11,13],[79,82],[144,147]],"long-forms":[[102,113],[49,76],[127,141]]},{"text":"Final step in treebuilding.  The English Destressin8 Rule (EDR) is used to  determ\/ne which vowels should be downsize.","acronyms":[[59,62]],"long-forms":[[33,57]]},{"text":"are defined:    (1)  VSM-based (Vector Space Model base)  trigger word similarity: the trigger words ","acronyms":[[21,30]],"long-forms":[[32,50]]},{"text":"Construct word representation model for  corpus in the groundwork time, D(TB), and in the  objectives time, D(TT). ( Section 2.1) ","acronyms":[[100,102]],"long-forms":[[85,96]]},{"text":"harmonizing G (both A and G can be split into two subsets AS ,AP and GS , GP , respectively representing Sure and Probable alignments) Precision (PT ), Recalling (RT ), F-measure (FT ) and Alignment Errors","acronyms":[[144,146],[56,58],[60,62],[67,69],[72,74],[158,160],[175,177]],"long-forms":[[112,131],[150,156],[164,173]]},{"text":"5 Conclusion and Further Works   In this paper we have proposed a reestimation algorithm and a best-first parsing algorithm  for probabilistic dependency grammars(PDG). The reestimation algorithm is a variation of ","acronyms":[[163,166]],"long-forms":[[129,161]]},{"text":"lines are published results by Hall and Nivre 2008 (HN08), Maier et al 2012 (M12), van Cranenburgh 2012 (C12), Kallmeyer and Maier 2013 (KM13), van Cranenburgh and Bod 2013 (CB13), and Versley 2014a, 2014b (V14a, V14b).","acronyms":[[137,141],[52,56],[77,80],[105,108],[174,178],[207,217]],"long-forms":[[111,135],[31,50],[60,75],[87,103],[148,172],[185,205]]},{"text":"distribution which underlies natural vocabulary text   -- which is if not a pure Zipfian distribution at least  an LNRE (gargantuan number of rare events, cf. Baayen ","acronyms":[[113,117]],"long-forms":[[119,146]]},{"text":"The most successful stochastic language models  have been based on finite-state descriptions such  as n-grams or hidden Markov models (HMMs)  (Jelinek et al, 1992).","acronyms":[[135,139]],"long-forms":[[113,133]]},{"text":"based on the design the Princeton English Wordnet.  Arabic Wordnet (AWN) (Elkateb, 2006; Black and Fellbaum, 2006; Elkateb and Fellbaum, 2006) has","acronyms":[[68,71]],"long-forms":[[52,66]]},{"text":"We evaluated our Chinese word segmenter in the open track, on all 4 corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and","acronyms":[[101,103],[136,141],[164,167]],"long-forms":[[84,99],[106,121],[144,162]]},{"text":"on the gender of the user.  User ID: The user ID (UID) label are inspired by research on Arabic Twitter proving that a consid-","acronyms":[[50,53]],"long-forms":[[41,48]]},{"text":"the  locat ions  illustrated in F ig .4 .  The s lo t  SH~P constituted  whether  the  par t   cor answering ing  to  th i s  frame is  a reg ionic(~EG)  or a branch(BR una) The  SU~P s lo t  record  i t s  subpar ts  and the i r  locat ions  of or re ia t ions  to ","acronyms":[[154,158],[49,53],[136,139],[165,169]],"long-forms":[[147,152]]},{"text":"1.2 The Binding Module The output of grammatical modules is then nurtured onto the Binding Module(BM) which activates an algorithm for anaphoric link in LFG terms","acronyms":[[93,95],[151,154]],"long-forms":[[78,92]]},{"text":"5.2.1 Query Focused Rewards We have proposed an extension to both reward functions to allow for query focused (QF) summarization.","acronyms":[[111,113]],"long-forms":[[96,109]]},{"text":"knowledge about the structuf.e of the global into account.  - The Data Base Parlance ( DBL ) , which contains conatants that correspond  to data base primitives . (","acronyms":[[86,89]],"long-forms":[[65,83]]},{"text":"Abstract  We present a novel method for evaluating  the output of Machine Translation (MT),  based on comparing the dependency ","acronyms":[[87,89]],"long-forms":[[66,85]]},{"text":"get node and another node on the dependency parsed tree: ANC (ancestor), DES (descendant), SIB (sibling), and GOALS (target word). Figure 5 displays","acronyms":[[110,116],[57,60],[73,76],[91,94]],"long-forms":[[118,124],[62,70],[78,88],[96,103]]},{"text":"served as the founding Editor-in-Chief of ACM  Transactions on Knowledge Discovery from Data (TKDD). He has received ACM SIGKDD In-","acronyms":[[94,98],[42,45],[117,120],[121,127]],"long-forms":[[47,92]]},{"text":"al., 2007) utilizing the document-level themes extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambigua-","acronyms":[[85,88]],"long-forms":[[56,83]]},{"text":"mantic feature called the latent topic feature, which is extracted by exploiting the Latent Dirichlet Allocation (LDA) algorithm. Unlike syntactic fea-","acronyms":[[114,117]],"long-forms":[[85,112]]},{"text":"Currently a immense proportion of languageindependent MT approaches are based on the  statistical machine translation (SMT) paradigm  (Koehn, 2010).","acronyms":[[117,120],[52,54]],"long-forms":[[84,115]]},{"text":"2.1 Download Initial Collection        The Yahoo Full Coverage Collection (YFCC) was  unload from http:\/\/fullcoverage.yahoo.com during ","acronyms":[[75,79]],"long-forms":[[43,73]]},{"text":"Y? >l LY?l, s.t. SYl = SY?l (1) 1","acronyms":[[17,20]],"long-forms":[[23,27]]},{"text":" ? ALGN (alignment-based): We ran a sentence alignment algorithm (Gale and Church, 1993)","acronyms":[[3,7]],"long-forms":[[9,18],[45,54]]},{"text":"These two representations are associated in an abstract type map, called AAM (Abstract Associative Map). This","acronyms":[[73,76]],"long-forms":[[78,102]]},{"text":"In order to test the ranked regulatory for the extraction of part?overall relations, we selected two different text collections: the LA Times news clauses from TREC 9 and the Wall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomly elect 10,000 sentences that formed two distinct test corpora.","acronyms":[[198,201],[162,166],[134,136]],"long-forms":[[177,196]]},{"text":"mance figures are the standard measures used for this task: F-measure (harmonic mean of recall and precision) and slot error rate (SER), where separate type, extent and content error measures are averaged to get the reported result.","acronyms":[[131,134]],"long-forms":[[114,129]]},{"text":"We implementing the combination patterns to the training corpus, and count pairs of True Positives (TP) and Bogus Positives (FP). The scores are calculated","acronyms":[[118,120],[93,95]],"long-forms":[[101,116],[77,91]]},{"text":"the percentage of tokens that are assigned the correct head and dependency label, as well as the unlabeled attachment score (UAS), that is, the percentage of tokens with the correct head, and the label accuracy (LA), that is, the percentage of tokens with the correct dependency label.","acronyms":[[212,214],[125,128]],"long-forms":[[196,210],[97,123]]},{"text":" The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al, 2012).","acronyms":[[86,89]],"long-forms":[[57,84]]},{"text":"POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11 Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24 Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00 Table 1: Correlations between (mean-centered) predictors.","acronyms":[[163,167],[13,17],[90,94]],"long-forms":[[150,161],[0,11],[83,88]]},{"text":"Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3. Student's Priming Percentages aggregated by task set (TS = Task Set)  Differing significant relationship emerged within the models. We discuss a subset of these here.","acronyms":[[123,125],[46,48]],"long-forms":[[128,136],[51,59]]},{"text":"boundaries. Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is utilised to offering word boundaries as well as inaccu-","acronyms":[[89,96]],"long-forms":[[56,87]]},{"text":" 3.1 Evaluation methods We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries.","acronyms":[[53,55]],"long-forms":[[35,51]]},{"text":"for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and","acronyms":[[131,135],[19,23],[81,84]],"long-forms":[[95,129],[51,79],[0,17]]},{"text":" 3 The Discourse Model Informally, a DiscourseModel (DM)may be described as the set of entities \"specified\" in a discourse, linked together by the relations they participate in.","acronyms":[[53,55]],"long-forms":[[37,51]]},{"text":"outer: the perceived concrete or abstract source, goal, or  location of the action,event, or state  Correspondent (CAR):  inner: the entity perceived as being in correspondence with ","acronyms":[[115,118]],"long-forms":[[100,113]]},{"text":" 1 Introduction Information extraction (IE) systems recover structured information from text.","acronyms":[[40,42]],"long-forms":[[16,38]]},{"text":"we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)).","acronyms":[[148,151],[98,101]],"long-forms":[[122,146]]},{"text":"3.2 Structural modeling We go beyond traditional feature vectors by employing structured models (STRUCT), which encode each comment into a shallow syntactic tree.","acronyms":[[94,100]],"long-forms":[[75,85]]},{"text":"As further testimonial of the effective of our framework, we have freshly adjusting our phrase-structure parser in Section 6 to parsing with a lexicalized grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007).","acronyms":[[206,209],[267,270],[271,274]],"long-forms":[[174,204]]},{"text":" ? Unlabeled Elementary Dependencies (UED) identical to LED, except ignoring all labeling","acronyms":[[38,41],[56,59]],"long-forms":[[3,36]]},{"text":"grading.  The Content Rating Module (CAM) presented in Bailey (2008) and Bailey and Meurers","acronyms":[[41,44]],"long-forms":[[14,39]]},{"text":"The bacteria track consists of two functions, BB and BI.  2.4.1 Bacteria biotope chore (BB) The objective of the BB task (Bossy et al, 2011) is to ex-","acronyms":[[83,85],[102,104],[42,44],[49,51]],"long-forms":[[60,76]]},{"text":"2.85 0.001 8 29. ( VP (VBP ? NEED?) (","acronyms":[[19,21]],"long-forms":[[23,26]]},{"text":"{zhongzhi, nght}@comp.nus.edu.sg Abstract Word sense disambiguation (WSD) systems based on supervised learning","acronyms":[[69,72]],"long-forms":[[42,67]]},{"text":"{wangruibo,gaoyahui}@sxu.edu.cn Abstract In this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the","acronyms":[[79,82]],"long-forms":[[56,78]]},{"text":"unigram model (BoW)+HI, where in addition to representing what words occur in a text, we also represent what Harvard Inquirer (HI)3 word class emerge in it.","acronyms":[[127,129],[15,18],[20,23]],"long-forms":[[109,125]]},{"text":" 1 Introduction Named entity acknowledgement (NER) is the task of identifying and classifying phrases that denote certain kinds of named","acronyms":[[42,45]],"long-forms":[[16,40]]},{"text":"ically measures the semantic similarity between two texts, which was the goals of the 2013 Semantic Textual Similarity (STS) task (Agirre et al 2013).","acronyms":[[119,122]],"long-forms":[[90,117]]},{"text":"adaptation led to a considerable improvement of +4.1 BLEU and grande enhancements in terms of METEOR and Translation Edit Rate (TER). We","acronyms":[[127,130],[53,57],[93,99]],"long-forms":[[104,125]]},{"text":"The typical way to address these situations is to jointly model these relations, e.g., using Markov logic networks (MLN) (Poon and Vanderwende, 2010).","acronyms":[[116,119]],"long-forms":[[93,114]]},{"text":" 1356 Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data chooses Annealing","acronyms":[[22,26],[45,47]],"long-forms":[[6,20]]},{"text":"to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree.","acronyms":[[90,93],[41,44]],"long-forms":[[54,88],[20,39]]},{"text":"its nondecomposability, as well as a croix between B??? and word error rate (WER) that is decomposable down to the subsentential tiers (in a sense to be","acronyms":[[77,80]],"long-forms":[[60,75]]},{"text":"Domains: HT = human transcription factors in blood cells, TCS = two-component systems, BB = bacteria biology, BS = Bacillus subtilis","acronyms":[[87,89],[9,11],[58,61],[110,112]],"long-forms":[[92,108],[14,33],[64,85],[115,132]]},{"text":"In addition, for some nodes it is necessary to insist that adjunction is mandatory at  a node. In such a case, we say that the node has an Obligatory Adjoining (OA)  constraint.","acronyms":[[161,163]],"long-forms":[[139,159]]},{"text":"means, rightness  sentences could then be  computat iona l ly  produced from the lo-  g ical  patters  ment ioned  in (II). ","acronyms":[],"long-forms":[]},{"text":"topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we use a Latent Dirichlet Allocation (LDA) to assessing their themes. Figure 1 demonstrate an example, where","acronyms":[[136,139]],"long-forms":[[107,134]]},{"text":"Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR).","acronyms":[[106,108],[85,87],[142,144]],"long-forms":[[90,104],[59,83],[115,140]]},{"text":"jective U-shaped is an example of gesture enriching an adjectival meaning through the interface default Adjective meaning extended (AdjMExt) AdjMExt: Adjective(u), sem(u) is ?","acronyms":[[132,139],[141,148]],"long-forms":[[104,130]]},{"text":")( bwN We use the corpus provided by IR task of  NTCIR2 (NTCIR 2002) as the training set to  compute the mutual information of words.","acronyms":[[49,55],[37,39]],"long-forms":[[57,67]]},{"text":"In all other tests, including all closed  tests, City University of Hong Kong (CityU)  open test and Microsoft Research (MSR) open  test, we trained our system using the relevant ","acronyms":[[121,124],[79,84]],"long-forms":[[101,119],[49,64]]},{"text":"Tables 4: Reranking results (%BLEU on TEST).  Discriminative Word\/Tag LMs (DISC): Onto each language pair, we generated 10,000-best lists for","acronyms":[[74,78],[69,72],[37,41],[29,33]],"long-forms":[[45,59]]},{"text":" ? Reduce Left - X (RL) : Pops the top two nodes from the stacked, combines them into a new node","acronyms":[[20,22]],"long-forms":[[3,14]]},{"text":" 1 Introduction  Open domain question replying (QA), as defined  by the TREC competitions (Voorhees, 2003), ","acronyms":[[49,51],[73,77]],"long-forms":[[29,47]]},{"text":" SYSTEM ARCHITECTURE The TIA used for MUC-3 was developed from the AD-TIA (Alternate Domain TIA) . This system, shown","acronyms":[[67,73],[38,43],[25,28]],"long-forms":[[75,95]]},{"text":"Noun Variation (NV) ? Adjective Variant (AdjV) ?","acronyms":[[43,47]],"long-forms":[[22,41]]},{"text":"guese to Swedish via different pivot language.  RW=Random Walk. * indicates the results are signifi-","acronyms":[[48,50]],"long-forms":[[51,62]]},{"text":"quency weighted recall evaluation. We used a  Japanese frequency dictionary (FD) generated  from the Japanese EDR corpus (Isahara, 2007) to ","acronyms":[[77,79]],"long-forms":[[55,75]]},{"text":"pendencies is only context-free (section 2.1). Our argument is groundwork on our desire to use a discourse grammar in naturel language generation (NLG). It is well-known that","acronyms":[[142,145]],"long-forms":[[113,140]]},{"text":"Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al, 2004).","acronyms":[[90,93]],"long-forms":[[66,88]]},{"text":"The Text Encoding Initiative (TEI) is a cooperative undertaking of the Association  for Computers and the Humanities (ACH), the Association for Computational Lin-  guistics (ACL), and the Association for Literary and Linguistic Computing (ALLC). ","acronyms":[[239,243],[30,33],[118,121],[174,177]],"long-forms":[[188,237],[4,28],[71,116],[128,172]]},{"text":"J = Japanese ????? S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics ","acronyms":[[32,34],[62,64]],"long-forms":[[37,50],[4,12],[67,83]]},{"text":"There are two machine learning tasks in our problem. The first is Dialogue Act (DA) Tagging, in which we assigns DAs to every Talks Func-","acronyms":[[80,82],[112,115]],"long-forms":[[66,78]]},{"text":"the first optimum solution is obtained.  (c) EPN for the last optimum solution (EPN-L): The number of the expanded problems when","acronyms":[[80,85]],"long-forms":[[45,78]]},{"text":"Two document collections are used in this study.  CE (Chinese Encyclopedia): This is from the electronic version of the Chinese Encyclopedia.","acronyms":[[50,52]],"long-forms":[[54,74]]},{"text":" 2.2 Hierarchical Softmax Hierarchical Softmax (HSM) organizes the output terminology into a tree where the leaves are","acronyms":[[48,51]],"long-forms":[[26,46]]},{"text":"these road. This corresponds to the Viterbi approximation i speech recognition or in  other related areas for which concealing Markov models (HMM's) are used. In all such ","acronyms":[[139,144]],"long-forms":[[117,137]]},{"text":"relates only loosely to the semantics of natural language. The works we present in  this paper differs from all previous cooperated in natural anguage treat (NLP) in at  least two respects.","acronyms":[[156,159]],"long-forms":[[128,154]]},{"text":"passenger vessel.  (5) Multiple mentions (MENTION): These alignments link one word to multiple occur-","acronyms":[[42,49]],"long-forms":[[32,40]]},{"text":"lines and web-gathered word lists. Theses grammars are represented by Finite State Machine (FSMs) (thanks to the INTO&T GRM\/FSM toolkit (Allauzen et","acronyms":[[93,97],[114,118],[119,126]],"long-forms":[[70,91]]},{"text":"2011.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.","acronyms":[[44,46],[56,62]],"long-forms":[[23,42]]},{"text":"We first experiment with the separately trained supertagger and parser, which are then combined using belief propagation (BP) and dual decomposition (DD).","acronyms":[[122,124],[150,152]],"long-forms":[[102,120],[130,148]]},{"text":"in the preliminary English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al 2006) allows us to recaptured super-","acronyms":[[91,94]],"long-forms":[[75,89]]},{"text":"neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on bounds words to predict the order of adjacent","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":" 2 Forced Derivation Shue for SMT A coerced derivation tree (FDT) of a sentence pair {f, e} can be defined as a pair G =< D,A >:","acronyms":[[60,63],[30,33]],"long-forms":[[36,58]]},{"text":"Introduction  The pro jec t  note  presents  the  computer  program  GECO (GEometry COnsu l te r ) ,  wh ich  generates   exp lanat ions  (descr ip t ions )  o f  geometr i ca l  ","acronyms":[[69,73]],"long-forms":[[75,96]]},{"text":"In Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pages 282?289. ","acronyms":[[79,83]],"long-forms":[[33,77]]},{"text":"726  Proceedings of the 2014 Conference on Experimental Methodology in Natural Language Processing (EMNLP), pages 657?669, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":" 1 Introduction Many problems in natural language processing (NLP) involve optimizing some objective function over a set of","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"IGT-XML. At the heart of the model is a representation of interlinearized glossed text (IGT). Building","acronyms":[[88,91],[0,7]],"long-forms":[[58,86]]},{"text":"( NN?? ) discourses 1:1 ( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1 ( NP ( NR?? ) (","acronyms":[[34,36],[2,6],[59,61],[64,68]],"long-forms":[[22,29]]},{"text":"3.1 The Information Extraction Channeling  The extraction task we are addressing is that of the  Automatic Content Extraction (ACE)1 assessed. ","acronyms":[[125,128]],"long-forms":[[95,123]]},{"text":"+ raising verb - non-raising verb KEYAGR (key agreement) ?","acronyms":[[34,40]],"long-forms":[[42,55]]},{"text":"oracle? which determining the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data fully, and","acronyms":[[72,75],[98,101]],"long-forms":[[51,70]]},{"text":"nodes  where  the re lat ion cor responds  to a verb .   Verb  nodes  (VERBSTR)  conta in  a po in ter  to the  RE-  LATION represented  by  the  verb ?","acronyms":[[71,78]],"long-forms":[[57,61]]},{"text":"uation contained in the RST Discourse Treebank (RST-DTB)(Carlsson et al 2001) aired by the Linguistic Data Consortium (LDC)3. The RST-DTB","acronyms":[[124,127],[48,55],[135,142]],"long-forms":[[96,122],[24,46]]},{"text":"the global normalization of random field models,  and avoid the label bias problem that exists in  maximum entropy Markov models (MEMMs). ","acronyms":[[130,135]],"long-forms":[[99,128]]},{"text":"120 compared to approximate maximum likelihood estimation (MLE). However, this method has not been","acronyms":[[59,62]],"long-forms":[[28,57]]},{"text":"can be specified.  The 'source condition(SCND)' represents  conditions on variables in the 'source pattern.'","acronyms":[[41,45]],"long-forms":[[24,39]]},{"text":" ITSPOKE-WOZ is a semi-automatic version of ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), which is a speech-enhanced ver-","acronyms":[[44,51],[1,12]],"long-forms":[[53,80]]},{"text":" 3.2 Latent Structure SVM We employ the latent structural SVM (LS-SVM) model for learned the discriminative model of query","acronyms":[[64,70],[23,26]],"long-forms":[[41,62]]},{"text":"Table 3: The effect of new features on the development set for English. UAS = unlabeled attachment score; UEM = unlabeled exact match.","acronyms":[[72,75],[106,109]],"long-forms":[[78,104],[112,133]]},{"text":"(approx. 68,000 sentences, 1.4 trillion tokens), (2) Brown Corpus (BROWN) (approx. 60,000","acronyms":[[66,71]],"long-forms":[[52,57]]},{"text":"knowledge from a corpus\\[2\\]\\[6\\]\\[91.  Machine translation (MT) systems are no  exception.","acronyms":[[61,63]],"long-forms":[[40,59]]},{"text":"by the connectives will yield better readability.  Entity Grid (EG) Along with the previous work (Pitler and","acronyms":[[64,66]],"long-forms":[[51,62]]},{"text":"disasters or politician crises in the media. There are two main approaches to audio hotspotting; one involves speech-to-text (STT), also known as large vocabulary continuous speech acknowledging (LVCSR),  and the other employs phonetic speech gratitude.","acronyms":[[193,198],[125,128]],"long-forms":[[145,191],[109,123]]},{"text":"229  Proceedings of the Atelier on Discourse in Machine Translation (DiscoMT), pages 19?26, Sofia, Bulgaria, August 9, 2013.","acronyms":[[70,77]],"long-forms":[[36,68]]},{"text":"tions\" as per the gold standard.  D = Truthful Negatives (TN) = Pairs that were identified as \"Untrue Transliterations\" by the par-","acronyms":[[54,56]],"long-forms":[[38,52]]},{"text":" ? Template Element (TE) -- Extract  basic information related to organizing and ","acronyms":[[21,23]],"long-forms":[[3,19]]},{"text":"This paper examines the processing predictions of the ERH on a systematic class of relative clause types, the Accessibility Hierarchy (AH) shown in figure 1.","acronyms":[[135,137],[54,57]],"long-forms":[[110,133]]},{"text":"(V = main verb, N = noun, AUXV = ancillary verb, COMPL = completive, ccomp_obj = clausal supplement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP,  &MAINV = main sneeze, &<AUXMOD = verbal auxiliary modifier). ","acronyms":[[235,240],[26,30],[49,54],[108,111],[69,78],[138,144],[158,164],[188,192],[212,216],[256,262]],"long-forms":[[243,252],[5,14],[20,24],[33,47],[57,67],[115,123],[81,107],[128,136],[147,156],[167,186],[195,210],[219,231],[265,292]]},{"text":"(Eisner, 1996). We define role value labeled precision (RLP) and role value labeled recall (RLR) on dependency links as follows:","acronyms":[[92,95],[56,59]],"long-forms":[[65,90],[26,54]]},{"text":"Across  * This work has been sponsored by the Fonds zur  FSrderung der wissenschaftlichen Forschung (FWF),  Grant No.","acronyms":[[97,100]],"long-forms":[[53,95]]},{"text":"1992. 100 million words of  English: the British National Corpus (NLC). ","acronyms":[[66,69]],"long-forms":[[41,64]]},{"text":"  Definition: Asymmetric nearest common  ancestor (ANCA)  The asymmetric nearer common ancestors from ","acronyms":[[51,55]],"long-forms":[[14,49]]},{"text":"1998. Protein folding in the hydrophobic-hydrophilic(HP) modeling is NPcomplete.","acronyms":[[53,55],[66,68]],"long-forms":[[29,52]]},{"text":"languages.  Mutual Information (MI)  For the purpose of this experiment we utilized a ","acronyms":[[32,34]],"long-forms":[[12,30]]},{"text":"In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25?32, Rochester, NY.","acronyms":[[70,74],[3,8],[101,103]],"long-forms":[[45,68]]},{"text":"We use three classes of features? Crowd Grades (CG), Force Alignment features (FA) and Natural Language Processing traits (NLP).","acronyms":[[79,81],[48,50],[125,128]],"long-forms":[[53,68],[34,46],[87,114]]},{"text":"Cognitive Science Department at Xiamen University (XMU) ? ?  Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)    National Taipei University of Technology (NTUT)   ","acronyms":[[118,125],[51,54],[172,176]],"long-forms":[[61,116],[32,49],[130,170]]},{"text":"search excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz (LOEWE) as part of the research center Digitally Humanities.","acronyms":[[101,106]],"long-forms":[[26,99]]},{"text":"We conducts an extrinsic rating to compare  the different versions of ArSenL on the task of  subjectivity and sentiment analysis (SSA). We ","acronyms":[[133,136],[73,79]],"long-forms":[[96,131]]},{"text":"The next four columns show the number of true positives (TP)--verbs judged +S both  by machine and by hand; false positives (FP)--verbs judged +S by machine, -S  by  hand; true negatives (TN)--verbs judged -S  both by machine and by hand; and false  negatives (FN)--verbs judged -S  by machine, +S by hand.","acronyms":[[188,190],[57,59],[125,127],[261,263]],"long-forms":[[172,186],[41,54],[108,122],[243,258]]},{"text":" Experiments are performed on two datasets, the English Penn Treebank (PTB) dataset using the standard train, dev and test splits, and the ARK","acronyms":[[71,74]],"long-forms":[[56,69]]},{"text":"2007). The practical NLP application based evaluations are automatic speech recognition (ASR), information retrieval (IR) and statistical machine","acronyms":[[89,92],[21,24],[118,120]],"long-forms":[[59,87],[95,116]]},{"text":"For the nouns, 31 basic types are selected from  WordNet top categories (unique beginners): 2  entity(ENT) life~orm(LIF)  causal_agent(AGT) human(HUN) ","acronyms":[[102,105]],"long-forms":[[95,100]]},{"text":"2.1 Knowledge Source We employ BabelNet 2.5.1 as our reference knowledge base (KB). BabelNet is a multilingual","acronyms":[[79,81]],"long-forms":[[63,77]]},{"text":"matic speech recognizers in Speech Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output. For the","acronyms":[[128,131],[75,79]],"long-forms":[[97,126],[28,73]]},{"text":"The second and last step to generate qwn-ppv(s) consists of propagating over a WordNet graph to obtain a Personalized PageRanking Vector (PPV), one for each polarity.","acronyms":[[138,141],[37,44]],"long-forms":[[105,136]]},{"text":"2003). Another kind of verb-based multiword expression is light verb constructions (LVCs), such as the examples in (1).","acronyms":[[84,88]],"long-forms":[[58,82]]},{"text":"tegrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. ","acronyms":[[144,146]],"long-forms":[[123,142]]},{"text":"2   TASK A: Question Generation from Paragraphs  1.1   Task Definition  The Question Generation from Paragraphs (QGP) task challenges participants to  generate a list of 6 questions from a given input paragraph.","acronyms":[[113,116]],"long-forms":[[76,111]]},{"text":"  The similarity of two words is the least common  ancestor information content(IC), and hence, the  higher the information content is, the more similar ","acronyms":[[80,82]],"long-forms":[[60,78]]},{"text":"an efficient bottom-up algorithm. The asymptotic time complexity of this search is O(SR) where S is the number of source nodes andR is the number","acronyms":[[85,87]],"long-forms":[[73,79]]},{"text":"3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Supports Vector Machines (SVM), is used for the categorize task.","acronyms":[[105,108],[31,40]],"long-forms":[[80,103]]},{"text":"Figure 4: Change of global network properties with incremental addition of edges to the directed network of news genre. SCC = Strongly Connected Component, CC = Connected Component. By ?","acronyms":[[120,123],[156,158]],"long-forms":[[126,154],[161,180]]},{"text":"They thus luring researching from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that ut-","acronyms":[[89,92]],"long-forms":[[76,87]]},{"text":"by adapt the baseline model to four adaptation domains. In particular, we attempt to interpret the results given in terms of the character error rate (CER) by  correlating them with the featured of the adaptive domain measured us-","acronyms":[[154,157]],"long-forms":[[132,152]]},{"text":"gramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050?1055. ","acronyms":[[73,77]],"long-forms":[[13,71]]},{"text":"Note Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity). ","acronyms":[[85,87]],"long-forms":[[88,98]]},{"text":"cannot co-exist on nouns. Next comes the class of particle proclitics (PART+): +  l+ ?","acronyms":[[71,76]],"long-forms":[[50,69]]},{"text":"as a statistical model of natural anguage, t and weak-  eus Jelinek et al's contention that \"in an ambiguous  but suitably chose probabilistic CFG (PCFG),  correct parses are Ifigh likelihood parses\" (p. 2).","acronyms":[[154,158]],"long-forms":[[135,152]]},{"text":"that are of interest o specific users. An example of IE is the  Named Ent i ty  (NE) task, which has become established  as the important first step in many other IE tasks, provid- ","acronyms":[[81,83],[53,55],[163,165]],"long-forms":[[64,78]]},{"text":"2 values (ARG2-4, ARGM-DIS (discourse), ARGM-LOC (locative), ARGM-MNR (manner), and ARGM-TMP (temporal)), but given the large number of degrees of free-","acronyms":[[84,92],[40,48],[61,69]],"long-forms":[[94,102],[50,58],[71,77]]},{"text":" Given the good outcomes of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd provenance the annotation task.","acronyms":[[94,97]],"long-forms":[[70,92]]},{"text":"most u n e x p a n d e d  node o f  TI: for  3 b t h i s  r e s u l t s  i n :   3 e .  (S ( V  m a i l )  (NP ( N P  ( N  B o x e s )  (N*)) P P * )  ( P P * ) ) .  ","acronyms":[[108,110],[142,145]],"long-forms":[[113,116]]},{"text":"and labeled by the people on Amazon Mechanical Turk, a web service. Amazon Mechanical Turk (MTurk) allows individuals to post jobs on MTurk with a set fee that are","acronyms":[[92,97]],"long-forms":[[75,90]]},{"text":" 4 Active Learning Active Learning (AL) is a machine learning paradigm that let the learner decided which data it","acronyms":[[36,38]],"long-forms":[[19,34]]},{"text":" 3 Baseline SMT system The goal of statistical machine translation (SMT) is to produce a objectives sentence e from a provenance sen-","acronyms":[[68,71],[12,15]],"long-forms":[[35,66]]},{"text":"CoTrain v. TSVM(EN) 1.41E-05 0.00311 2.17E-08 CoTrain v. TSVM(ENCN) 1.37E-08 0.0113 0.0396 CoTrain v. SelfTrain(CN) 7.07E-18 2.79E-11 6.53E-07 CoTrain versus. SelfTrain(EN) 1.01E-07 0.0192 1.35E-07","acronyms":[[115,117],[12,16],[17,19],[59,63],[64,68],[168,170]],"long-forms":[[93,100]]},{"text":"means, correct  sentences could then be  computat iona l ly  generated from the lo-  g ical  patters  ment ioned  in (II). ","acronyms":[],"long-forms":[]},{"text":"respect to phonology, morphology, syntax and the vocabulary. Linguistic resources (lexica, corpora) and naturel language processing (NLP) tools for such dialects (parsers) are very rare. ","acronyms":[[130,133]],"long-forms":[[101,128]]},{"text":"CRF) A wide range of contextual information, such as surrounding phrase (GREF), dependency or case structure (GTREF), and dependency chemin (GREF ), has been utilized for similarity computations, and achieved considerable success.","acronyms":[[138,142],[0,3],[72,76]],"long-forms":[[109,114]]},{"text":" ? Lexical Overlapped and Length (LO): This set of features represents the lexical overlap between","acronyms":[[31,33]],"long-forms":[[3,18]]},{"text":"Arabic Penn TreeBank POS tagset. Base Phrase (BP) Chunking is the process of creating non-recursive base phrases such","acronyms":[[46,48],[21,24]],"long-forms":[[33,44]]},{"text":"tasks. The latter three correspond to the three Gene Ontology (GO) (Ashburner et al, 2000) toplevel sub-ontologies, and terms of these kind were","acronyms":[[67,69]],"long-forms":[[52,65]]},{"text":"tical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random domains (CRF) as discriminative templates.","acronyms":[[125,128],[31,35],[89,92]],"long-forms":[[98,123],[0,29],[65,87]]},{"text":"mentary ASR systems, a technique first proposed in the context of NIST?s ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR","acronyms":[[138,141],[8,11],[185,188]],"long-forms":[[112,136]]},{"text":"semantically interpreted.  We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared","acronyms":[[63,67]],"long-forms":[[36,61]]},{"text":"steels@arti.vub.ac.be Abstract Fluid Construct Grammar (FCG) is a new linguistic formalism designed to ex-","acronyms":[[59,62]],"long-forms":[[31,57]]},{"text":"Additionally, we have acquired gazetteer lists for Hindustani and used these gazetteers in the Maximum Entropy (MaxEnt) founded Hindi NER system.","acronyms":[[107,113],[127,130]],"long-forms":[[90,105]]},{"text":"of binary relations that vary in length and since a  question representation (SRq) can be answered by a  sentence candidate (SRc) that includes more information  than the question specified, the Arity constraint i~ revised ","acronyms":[[125,128],[78,81]],"long-forms":[[105,123],[53,76]]},{"text":"the feature structures which are associated with each node, which prohibit certain compositions, are not shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intend, as","acronyms":[[158,162]],"long-forms":[[141,156]]},{"text":"See Table 3 for the complete list of non-predicate filtering describing restrictions on the role text (RT), role spanning (RS), and predicate fabric (PF) in terms of the semantic type","acronyms":[[101,103],[117,119],[143,145]],"long-forms":[[90,99],[106,115],[126,141]]},{"text":"there are two ways of feeding the context vector into the main recurrent language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec.","acronyms":[[113,115],[138,140],[89,92]],"long-forms":[[99,111],[125,136],[63,87]]},{"text":"the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAGGING) model, and (5) annotation errors. Ours","acronyms":[[109,112],[47,55]],"long-forms":[]},{"text":"Figure 4: Change of global network properties with incremental addition of edges to the directed network of news types. SCC = Strongly Connected Component, CC = Connected Elements. By ?","acronyms":[[120,123],[156,158]],"long-forms":[[126,154],[161,180]]},{"text":" We ran three parsing experiments: (i) replacing the value of the surface form (FORM) of pronominal prepositions with their lemma form (LEMMA), for","acronyms":[[80,84],[136,141]],"long-forms":[[74,78],[124,129]]},{"text":"Each accuracy measure is shown in a column, including the segmentation F-score (SF ), the holistic tagging 894","acronyms":[[80,82]],"long-forms":[[58,72]]},{"text":"54  Proceedings of the 2014 Conference on Empirical Modes in Natural Language Treatments (EMNLP), pages 325?335, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"workbench (Hall et al, 2009). For SVM, we employing the radial basis function kernel (RBF) and we utilize the wrapper provided by Weka for","acronyms":[[83,86],[34,37]],"long-forms":[[53,74]]},{"text":"fn-n1?. The annotation contains a feature edifice with three features: FE (Frame element), GF (Grammatical Function), and PT","acronyms":[[73,75],[93,95],[124,126]],"long-forms":[[77,90],[97,117]]},{"text":"are also kernel methods that directly take into account the multiclass nature of the problem such as the kernel partial least squares regression (KPLS). ","acronyms":[[146,150]],"long-forms":[[105,144]]},{"text":"\t\u000f\u000e \u0010\u0011\t\u0013\u0012\u0014\u0010\u0016\u0015\u0016\u0015\u0016\u0015\u0017\u0010\u0011\t\u0019\u0018\u001a\u0007 involves of a directed acyclic graph (DAG) that encodes a set of conditional independence assertions about vari-","acronyms":[[64,67]],"long-forms":[[40,62]]},{"text":"eling approaches. Table 3 shows the corresponding letter blunders rates (LER). LERs are more compa-","acronyms":[[70,73],[76,80]],"long-forms":[[50,68]]},{"text":"propose a new inference method ? collective iterative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities","acronyms":[[70,73],[110,113]],"long-forms":[[33,68],[88,108]]},{"text":" The application will finally be deployed using a Software as a Service (SaaS) model. It will","acronyms":[[76,80]],"long-forms":[[53,74]]},{"text":"to do the testing on real emotions. The Berlin Emotional Database (EMO-DB) contains the set of emotions from the MPEG-4 standard (anger,","acronyms":[[67,73],[113,119]],"long-forms":[[47,65]]},{"text":"The focus of the robust CSR technologies  on SLS applications is being facilitated by development and implementation of a well-structured  interface between a CSR and a natural language processor (NLP), authorizes cooperates with other  groups formulating NLPs for SLS applications.","acronyms":[[195,198],[24,27],[43,46],[157,160],[254,258],[263,266]],"long-forms":[[167,193]]},{"text":"derivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems.","acronyms":[[137,143],[26,29],[89,92]],"long-forms":[[98,135],[55,73]]},{"text":"VNMT 32.25 34.50++ 33.78++ 36.72?++ 30.92?++ 24.41?++ 32.07 Table 1: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We","acronyms":[[141,145],[0,4],[69,73],[88,92],[127,130]],"long-forms":[[133,140]]},{"text":"Tables 79 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets.","acronyms":[[98,100]],"long-forms":[[82,96]]},{"text":"To allow for portability, the SABA parser translates its natural  language input into an ~mtermediate\" s mantic network formalism  called SF (for \"Sentence Formalism'), presented in details in (Binot,  1984, 1985).","acronyms":[[138,140],[30,34]],"long-forms":[[147,165]]},{"text":" 2 Changes to the AZ Scheme Argumentative Zoning II (AZ-II) is a new annotation scheme, which is an elaboration of the orig-","acronyms":[[53,58],[18,20]],"long-forms":[[28,51]]},{"text":"To explore the impact of the quality of annotation resources, we also use a Chinese language analysis tool: Language Technology Platform (LTP) (Che et al, 2010).","acronyms":[[138,141]],"long-forms":[[108,136]]},{"text":"After grammatical ambiguities are deleted  by the stochastic parser, the phrases is divided  into noun phrases(NP) and verb phrases(VP),  giving, ","acronyms":[[110,112],[131,133]],"long-forms":[[97,108],[118,129]]},{"text":" 1 Introducing  Lexical Acquisition (LA) processes strongly rely on  basic assumptions embodied by the source informa- ","acronyms":[[38,40]],"long-forms":[[17,36]]},{"text":" ? Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for","acronyms":[[24,30]],"long-forms":[[3,22]]},{"text":"Brighton, BN1 9QN, England  Abstract  Generalised phrase structure grammars (GPSG's)  appear to offer a means by which the syntactic ","acronyms":[[77,83],[14,17],[10,13]],"long-forms":[[38,75]]},{"text":"phrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we","acronyms":[[116,119],[80,83]],"long-forms":[[94,114],[58,78]]},{"text":"1. Introduction  In a Spoken Language System (SLS) we must use all avail-  able knowledge sources (KSs) to decide on the spoken sen- ","acronyms":[[46,49],[99,102]],"long-forms":[[22,44],[80,97]]},{"text":"services (person or company information bureau), ht this  situation, the event aml the attitude of file inforumtion  possessor (IP) is transported to a speaker (SP);journalist. ","acronyms":[[161,163],[128,130]],"long-forms":[[152,159],[104,126]]},{"text":"1. Introduction Semantic role labeling (SRL) is the task of identifying arguments for a predicate and assigning semantically meaningful labels to them.","acronyms":[[40,43]],"long-forms":[[16,38]]},{"text":"ble. The model makes heavy use of single-category Ambiguity Classes (AC)3, which (being independent on the tagger?s intermediate decisions) can be","acronyms":[[69,71]],"long-forms":[[50,67]]},{"text":"eal papers. Ill PTvcccdings of thc Pac'lific Sym-  posium on Biocomp'uting'98 (PSB'98), .Jan-  1uAYy.","acronyms":[[79,85],[16,27]],"long-forms":[[51,77]]},{"text":"documents. Thus, HTMM can be seen both as a variant of Hidden Markov Model (MMM) and a variation of LDA.","acronyms":[[78,81],[17,21],[102,105]],"long-forms":[[57,76]]},{"text":"2: boston sweep colorado to win world series 3: rookies respond in first crack at the big time C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 ranked summaries of the redsox cluster","acronyms":[[111,114],[95,99]],"long-forms":[[115,145],[100,109]]},{"text":"Abstract We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the","acronyms":[[90,92]],"long-forms":[[73,88]]},{"text":"(wine) is localized to exterior locus (barrel) and crosses the intermediary locus IME(LOC) to be localized to the hinterland INT(LOC) (the_bottle). ","acronyms":[[127,130],[86,89],[82,85],[123,126]],"long-forms":[[97,106],[76,81]]},{"text":"besides included as features.  Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al (2007) to","acronyms":[[56,60]],"long-forms":[[28,54]]},{"text":"mentary and United Nations parallel corpora. The semantic expressions chalkboard (SPT) was extracted from the same corpora annotated with FreeLing (Carreras et","acronyms":[[72,75]],"long-forms":[[49,70]]},{"text":" In Proceedings of the 24th International Conference on Computational Linguistics (COLING). ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"Computational Linguistic Volume 40, Number 1 of the 22nd International Conference on Computational Linguistic (COLING?08), pages 71?84, Manchester.","acronyms":[[113,122]],"long-forms":[[86,111]]},{"text":" 3 The TMop framework TMop (Translation Souvenir open-source purifier) is an open-source TM cleaning software written","acronyms":[[22,26],[87,89]],"long-forms":[[28,46]]},{"text":"with their scope and corresponding negated events is an important task that could benefit other natural language processing (NLP) tasks such as extraction of factual information","acronyms":[[125,128]],"long-forms":[[96,123]]},{"text":"Classification, Word Sensing Disambiguation, etc. Across  Natural Language Processing (NLP), one of the  most used resources for WSD and other tasks is ","acronyms":[[81,84],[123,126]],"long-forms":[[52,79]]},{"text":"Table 4: Evaluation results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order Language Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector). ","acronyms":[[113,115],[127,129],[44,47],[78,82],[149,151]],"long-forms":[[117,124],[131,146],[84,110],[153,178]]},{"text":"ability of reordering models to catching this tag sequence in system translations. Popovic et al (2006) use the relative disparity between WER (word errors rate) and PER (position independent word error rate) to indicate reordering errors.","acronyms":[[139,142],[165,168]],"long-forms":[[144,159],[170,206]]},{"text":"CONN =  nil;  konj( KONJ )  FUNDF = fundf n( NOMINAL ); \/* No nil *\/ ","acronyms":[[20,24],[0,4],[28,33]],"long-forms":[[14,18]]},{"text":" We perform our analyzing on data from the 20082011 Text Analysis Conference (TAC)1 organized by the National Institute of Standard and Technol-","acronyms":[[77,80]],"long-forms":[[51,75]]},{"text":"1969)).  4 Ia the following \\] will use the term Unification Grammar (UG) aa hyperonym for  GPSG, LFG, FUG, IIPSG etc.,","acronyms":[[70,72],[92,96],[98,101],[103,106],[108,113]],"long-forms":[[49,68]]},{"text":" ? Because Dependency Grammar (DG) directly describes the functional relations between  words, and s dependency tree has not any non-terminal nodes, DG is suitable for our ","acronyms":[[31,33],[149,151]],"long-forms":[[11,29]]},{"text":"exp (14)    Short word difference penalty (SWDP): a  good translation should have roughly the same ","acronyms":[[43,47]],"long-forms":[[12,41]]},{"text":"Tableau 4. WSD precision recall and F-measure for  the algorithm groundwork on aligned wordnets (AWN),  for AWN with clustering (AWN+C) and for ","acronyms":[[90,93],[9,12],[122,127],[101,104]],"long-forms":[[72,88]]},{"text":" 5 for SK with the use of POS information (SK-POS). ","acronyms":[[43,49]],"long-forms":[[7,29]]},{"text":"Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment of letter","acronyms":[[118,123],[29,32],[56,59],[75,77],[153,157]],"long-forms":[[111,116],[9,26],[35,53],[66,72]]},{"text":"compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000).","acronyms":[[119,122],[73,75],[91,93],[157,160]],"long-forms":[[96,117],[50,71],[78,89],[128,155]]},{"text":"induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic","acronyms":[[116,119]],"long-forms":[[84,114]]},{"text":"Table 1: First five SentiWordNet entries for cold#a In our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWord-","acronyms":[[127,131],[104,107],[155,158]],"long-forms":[[109,125]]},{"text":"selection (FS). For word-level prediction, generalised linear models (GLM) (Collins, 2002) and GLM with dynamic learning","acronyms":[[70,73],[11,13],[95,98]],"long-forms":[[43,68]]},{"text":"Best-Scoring-Choice Fulfilment Pablo Gerva?s, Rachel Herva?s, Carlos Leo?n Natural Interaction based on Language (NIL) Universidad Complutense de Madrid","acronyms":[[115,118]],"long-forms":[[76,113]]},{"text":"and include them in the training data for the SMT.  Corpus Tandem (CComb) The easiest method is to using these n newly created paral-","acronyms":[[72,77]],"long-forms":[[52,70]]},{"text":"The rules, which maggio be varied for each  dictionary, are written uses a formalism in the spirit of  the \"definite clause grammars\" (DCG's) of Pereira and  Warren (1980) and \"modular logic grammars\" (MLG's) ","acronyms":[[136,141],[203,208]],"long-forms":[[109,133],[178,200]]},{"text":"Measuring and estimates post-editing effort is therefore a growing concern addressed by Confidence Estimation (CE) (Specia, 2011). ","acronyms":[[112,114]],"long-forms":[[89,110]]},{"text":"In this paper, I presents a lexical representation  of the light  verb  ha  'do'  used in two types of  Korean light verb constructing (LVCs). These ","acronyms":[[136,140]],"long-forms":[[110,134]]},{"text":"ambiguous word are included as features.  Medications Subject Headings (MeSH): The final feature is furthermore specific to the biomedical do-","acronyms":[[68,72]],"long-forms":[[42,66]]},{"text":"the sentences produced. Additionally, automated machine translation (MT) metrics are analysed to quantify the amount of information missing from","acronyms":[[69,71]],"long-forms":[[48,67]]},{"text":"We use 1300 texts (DEV) as our training set, 200 texts (TST1+TST2) for adjust, and 200 texts (TST3+TST4) as a test set. All 1700 docu-","acronyms":[[95,104]],"long-forms":[[88,93]]},{"text":"In (Raymond and Riccardi, 2007), the SFST-based models is compared with Support Vector Machines (SVM) (Vapnik, 1998) and Conditional Random Minefields (CRF) (Laf-","acronyms":[[96,99],[37,41],[147,150]],"long-forms":[[71,94],[120,145]]},{"text":"ghar.  Parse: The modern town of [NP (np Mumbai)  (punct ,) (advP about 50 km south of Navi ","acronyms":[[34,36],[61,65]],"long-forms":[[38,40]]},{"text":"3.2 Coordination Structures Among the most contentious annotation schemes are those of coordination structures (CS), which are groupings of two or more tokens that are in coordina-","acronyms":[[114,116]],"long-forms":[[89,112]]},{"text":"4.2 Preprocessing Parties?Whole Lexico-Syntactic Patterns Since our discovering procedure is based on the semantic information provided by WordNet, we need to preprocess the noun phrases (NPs) extracted by the three clusters considered and detects the potential portions and the whole concepts.","acronyms":[[183,186]],"long-forms":[[169,181]]},{"text":"ity. We presented an evaluation of these parameters for preposition sense disambiguation (PSD). ","acronyms":[[90,93]],"long-forms":[[56,88]]},{"text":"Into the former, they include  personal pronouns, sentential \" i t ,\"  and null-comple-  ment anaphora, and under the latter, verb phrase (VP)  ellipsis, sluicing, gapping, and depriving.","acronyms":[[140,142]],"long-forms":[[127,138]]},{"text":"Abstract  This paper provides a description of the Hong  Kong Polytechnic Campus (PolyU) System  that participated in the chore #5 of SemEval-2, ","acronyms":[[86,91]],"long-forms":[[62,84]]},{"text":"hardware) but also not highly specialized (implying that it is not limited too severely in scope of use). We believe  an architecture composed of a distributed set of processing elements (PE's), each containing local memory and high  speed DSP processors, with a limited interconnecfion a d communication capability may suit our needs.","acronyms":[[188,192],[240,243]],"long-forms":[[167,186]]},{"text":"it can therefore mean ? prostrated on the threshold  and respectfully (AD) paid visits three period? or ","acronyms":[[71,73]],"long-forms":[[53,56]]},{"text":"Figure 3: A Graphical Representation of the Infinite Tree Model than a simple Dirichlet process (DP)2 (Ferguson, 1973) is that we have to introduce coupling across","acronyms":[[97,99]],"long-forms":[[78,95]]},{"text":"1991), can be characterized asknowledge-rich  in that they presuppose that known lexical  items possess Conceptual Dependence(CD)-  like descriptions.","acronyms":[[126,128]],"long-forms":[[104,124]]},{"text":"are verbs.  Translation Filter (TF) handles both Predicate Mismatch and Verb?Non-Verb translation shift er-","acronyms":[[32,34]],"long-forms":[[12,30]]},{"text":"1125 VNC token expressions (CFS07 has 1180).  We then split them into a development (DEV) set and a test (TEST) set.","acronyms":[[85,88],[5,8],[28,33],[106,110]],"long-forms":[[72,83],[100,104]]},{"text":" 2.1 Modeling Votes Ideal point (IP) models are a mainstay in quantitative political science, often applied to voting records to","acronyms":[[33,35]],"long-forms":[[20,31]]},{"text":" 1 Introduction Question replies (QA) from a knowledge base (KB) has a long history within natural language","acronyms":[[36,38],[63,65]],"long-forms":[[16,34],[47,61]]},{"text":" ? Domain communicat ion knowledge (DCK). This is expertise about how to communi- ","acronyms":[[36,39]],"long-forms":[[3,34]]},{"text":"The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al, 2004) is adopted as the unsupervised segmentation crite-","acronyms":[[112,114]],"long-forms":[[94,110]]},{"text":"constructions. The notion of construction is similar to the one in Construction Grammar (CxG)4, as in (Goldberg, 1995), where:","acronyms":[[89,92]],"long-forms":[[67,87]]},{"text":"maps all non-core argument labels in the guessed and correct labelings to NONE.  Coarse Modifier Argument Actions (COARSEARGM). Sometimes it is sufficient to","acronyms":[[119,129],[77,81]],"long-forms":[[84,117]]},{"text":"query, some form of translation is need. One likely conjecture that a conjunction of two existing fields, IR and machine translation (MT), would be satisfactory for accomplishing the combined translation and retrieval task.","acronyms":[[137,139]],"long-forms":[[116,135]]},{"text":"605  NP- - - -NP:NP NP=S:N I '  VP~-VP:VP  S~---S:S  NP=NEITHER ' :PP  NP~-PP :NP  VP=VP:NP  S=S:N I '   NP  ~.-~NP :VP  PP -~-PP :PP  VP=VP:P I  ) S~S:  ","acronyms":[[56,58],[5,7],[14,16],[17,19],[20,22],[32,38],[39,41],[53,55],[62,64],[66,72],[74,76],[78,80],[100,102],[112,114],[116,118],[126,128],[130,132]],"long-forms":[[59,62],[81,86],[90,93],[133,137]]},{"text":"837 (a) (b) Figure 1: Deep recurrent neural network (DRNN) architectures: arrows represent connection matrices; white, black, and grey circles represent input frames, hidden states, and output frames, respectively; (a): L intermediate layer DRNN with recurrent connections","acronyms":[[53,57],[241,245]],"long-forms":[[22,51]]},{"text":"CFG. The proof is based on a lexicalization procedure related to the lexicalization  procedure used to create Greibach normal form (GNF) as presented in Harrison 1978. ","acronyms":[[132,135],[0,3]],"long-forms":[[110,130]]},{"text":"against the human annotation.  2.3 Disseminated Tree Kernel (DTK) Distributed Tree Kernel (DTK) (Zanzotto and","acronyms":[[60,63],[90,93]],"long-forms":[[35,58],[65,88]]},{"text":"Table 1: First five SentiWordNet entries for refrigerated#a Onto our experiments we use two differing versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWord-","acronyms":[[127,131],[104,107],[155,158]],"long-forms":[[109,125]]},{"text":"1 Introduction A fairly novel area of retrieval called topic detection and tracking (TDT) attempt to design procedures to automatically (1) spot new, previously unreported events, and (2)","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":" OOV Handling Techniques and their Combination We comparisons our baseline system (BASELINE) to each of our fundamental techniques and their full combi-","acronyms":[[79,87],[1,4]],"long-forms":[[62,77]]},{"text":"4.2 Cognate based features Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology.","acronyms":[[102,105]],"long-forms":[[86,100]]},{"text":"Gimenez and Marquez (2008) measure overlapping grammatical dependency relations (DP), semantic roles (SR), and discourse representations (DR).","acronyms":[[106,108],[85,87],[142,144]],"long-forms":[[90,104],[59,83],[115,140]]},{"text":"the Natural Linguistics Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to converted the text to spoken output.","acronyms":[[119,122],[33,36]],"long-forms":[[103,117],[4,31]]},{"text":" 2. Computer  Aided Wri t ing (CAW)  A computer system for a novelist is basically a ","acronyms":[[31,34]],"long-forms":[[4,29]]},{"text":"Table 2: Participants and summarized of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Parlance Processing investigators, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary","acronyms":[[138,140],[73,75],[94,97],[182,188],[205,210],[245,247],[280,284]],"long-forms":[[141,157],[76,92],[98,125],[172,180],[189,195],[211,236],[248,267],[285,295]]},{"text":"C, N, X, Y } (V = verb, AV = auxiliary verb, EV = sneezing with Ersatzinfinitiv, Vfin = finite verb, Vinf","acronyms":[[24,26],[45,47],[77,81]],"long-forms":[[29,43],[50,75],[84,95],[18,22]]},{"text":"Pivot, RHS),  generating all subconstituents  generate _rhs ( RHS ),  generate material on path to root ","acronyms":[[60,63],[7,10]],"long-forms":[[54,57]]},{"text":"Litkowski, K. C.: Syntactic Clues and Lexical Resources in Question-Answering. In f. M. Voorhees & D. K. Harman (eds.), The Ninth Text Convalescence Conference (TREC-9). ( 2001) 157-166 10.","acronyms":[[157,163]],"long-forms":[[120,155]]},{"text":"The target set is built using the ? 88-?89 Wall Street Journal Corpus (WSJ) tagged using the (Ratnaparkhi, 1996) tagger and","acronyms":[[71,74]],"long-forms":[[43,62]]},{"text":"The entry to the first blocking are the words of the TEXTCHUNK, represented by CW (Collobert and Weston, 2008) embeddings.","acronyms":[[76,78],[50,59]],"long-forms":[[80,100]]},{"text":"to facilitate understanding. We used latent Dirichlet alocation (LDA) (Blei, Ng, and Jordan, 2003) as our exploratory tool.","acronyms":[[65,68]],"long-forms":[[37,63]]},{"text":"tion algorithm. We use an existing unsupervised methods, called Double Propagation (DP) (Qiu et al, 2011), for extraction.","acronyms":[[83,85]],"long-forms":[[63,81]]},{"text":"and round corners for processes. Lexical access is applied to the input string to produce  (nondeterministically) the extended lexical item (ELI) of each word. Its output is split ","acronyms":[[141,144]],"long-forms":[[118,139]]},{"text":"I.e., we consider three increasingly constrained conditions: (1) substitute conforming only to the form constraints (FORM), (2) substitution according to both form and taboo","acronyms":[[118,122]],"long-forms":[[100,104]]},{"text":"difficult to disambiguate.  Preposition sense disambiguation (PSD) has many potential uses.","acronyms":[[62,65]],"long-forms":[[28,60]]},{"text":"2 We use a new related measure, which we call the overall percentage error reduction (OPER), that uses the entire area under the curves given by","acronyms":[[86,90]],"long-forms":[[50,84]]},{"text":"was supported in part by JSPS Research Fellowships for Young Scientists and in part by CREST, JST (Japan Science and Technology Agency). ","acronyms":[[94,97],[25,29],[87,92]],"long-forms":[[99,127]]},{"text":"understood as follows: ? if the POS-tag of current word  is VB (Verb) and  its word-form  is ? can?","acronyms":[[60,62],[32,39]],"long-forms":[[64,68]]},{"text":"els on a previously unseen test set ? the test split of part 3 of the PATB (PATB3-TEST). Table 6 shows","acronyms":[[76,86]],"long-forms":[[56,74]]},{"text":"Our model can now be represented like this:  241  Database (DB)  Facts about hotels ","acronyms":[[60,62]],"long-forms":[[50,58]]},{"text":"i. The Eng!\\[sh t_qMal_a~franslationSsSSSSSSSS_2~trm  Baak~reusd  Computer Aided T~anslation (CAT) research at Universiti  Sa~m MalsysL~ (USM) began in 1976 as an individual research ","acronyms":[[94,97],[138,141]],"long-forms":[[66,82],[111,136]]},{"text":"For each disjunetiort in indef.\"  Let compatible-disjuncts = CHECK-DISJ (disjunction, cond). ","acronyms":[[61,71]],"long-forms":[[73,84]]},{"text":"Although there is a unassuming cost associated with annotating data, we exhibit that a reduction of 40% relative in align error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003).","acronyms":[[126,129],[152,158]],"long-forms":[[109,124]]},{"text":" 3.2 Query by Committee  Query by Committee (QBC) was introduced by  Seung, Opper, and Sompolinsky (1992).","acronyms":[[45,48]],"long-forms":[[25,43]]},{"text":"LOC(at. IN) The ACT (Actor) can be any noun in the subjective case (the abbreviation n), the PAT (Patient)","acronyms":[[16,19],[0,3],[93,96]],"long-forms":[[21,26],[98,105]]},{"text":" Temporal Types Possible Values (tags) Timeline (TL) past, present, future Day of Week (DOW) Mon, Tue, . . . ,","acronyms":[[49,51],[88,91]],"long-forms":[[39,47],[75,86]]},{"text":" 1 Introduction Referring expressions (REs) are expressions intended by speakers to identify entities to hearers.","acronyms":[[39,42]],"long-forms":[[16,37]]},{"text":" Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors.","acronyms":[[85,88]],"long-forms":[[62,83]]},{"text":"We however noticed the relative degradation of quality in coordinating conjunctions (CC), determiners (DT) and personal pronouns (PRP). ","acronyms":[[130,133],[85,87],[103,105]],"long-forms":[[111,128],[58,83],[90,101]]},{"text":"1 Introduction In this paper, we describe our submission to the Genia Event (GE) information extracting subtask of the BioNLP Shares Task.","acronyms":[[77,79],[119,125]],"long-forms":[[64,75]]},{"text":"Inspired by the work of web search (Gao et al2010) and question retrieval in community question answer (Q&A) (Zhou et al2011), we assume the following generative","acronyms":[[104,107]],"long-forms":[[87,102]]},{"text":"In the cross-validation process, Multinomial Naive Bayes (MNB) has shown better results than Support Vector Machines (SVM) as a component for AdaBoost.","acronyms":[[118,121],[58,61]],"long-forms":[[93,116],[33,56]]},{"text":"lar expressions. So the detection of factoid expression  can be achieved by Finite State Automaton(FSA). ","acronyms":[[94,97]],"long-forms":[[71,92]]},{"text":"As an extension, Zhang et al (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).","acronyms":[[95,98],[132,135]],"long-forms":[[67,93],[104,130]]},{"text":" SYSTEM ARCHITECTURE The TIA utilizes for MUC-3 was developed from the AD-TIA (Alternate Domain TIA) . This system, shown","acronyms":[[67,73],[38,43],[25,28]],"long-forms":[[75,95]]},{"text":"give an indication as to what the vibhakti\/TAM are.  Words with PSP (postposition) and NST (noun with spatial and temporal properties) tags are generally","acronyms":[[64,67],[34,46],[87,90]],"long-forms":[[69,81]]},{"text":" Parsed* Recall t Prec\/Rec t MLP Prob t  Left Corner (LC) 21797 91.75 9000 .76399 .78156 .175928  LB o LC 53026 96.75 7865 .77815 .78056 .359828 ","acronyms":[[54,56],[29,32],[98,100],[103,105]],"long-forms":[[41,52]]},{"text":" 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random CoNLL-03","acronyms":[[60,62],[107,110],[118,126],[83,86]],"long-forms":[[39,58],[63,69]]},{"text":"We conducted experiments on a number of different datasets: (1) the English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al, 1993) with standard POS","acronyms":[[97,100],[163,166]],"long-forms":[[76,95]]},{"text":"CoTrain vs. BaseCN2 1.8E-07 0.00257 0.000182 CoTrain vs. BaseCN3 1.27E-06 0.00922 0.000765 CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329","acronyms":[[107,109],[103,106],[150,153],[154,156]],"long-forms":[[91,98]]},{"text":"We used five retrieval systems to generate  relevance scores for query-document pairs:  Fuzzy Boolean (FB). This system translates a query ","acronyms":[[103,105]],"long-forms":[[88,101]]},{"text":"clickthrough data. Among ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133?142.","acronyms":[[84,87],[22,25],[26,32]],"long-forms":[[47,75]]},{"text":"Hajdinjak and Mihelic? The PARADISE Evaluation Framework Number of help messages (NHM) and help-message ratio (HMR), i.e., the number and the ratio of system?s help messages;","acronyms":[[82,85],[111,114]],"long-forms":[[57,80],[91,109]]},{"text":" 4.3 The Limited-Memory BFGS Algorithm The limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimization algorithm (Nocedal and Wright 1999).","acronyms":[[64,70],[24,28]],"long-forms":[[43,62]]},{"text":"4 Sagan for MT appraisals Sagan for MT evaluation is based on a crux devel-opment to approaches the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the diploma of semantic equivalence between two text fragments. STS is associated to both Recognizing Textual En-tailment (RTE) and Paraphrase Acknowledging, but  54","acronyms":[[371,374],[12,14],[36,38],[132,135],[153,156],[315,318]],"long-forms":[[338,369],[98,126]]},{"text":"to prevent this class of mistakes. To put it another way, we hoped to exploit the correlation between named-entities and noun phrase (NP) boundaries. A","acronyms":[[134,136]],"long-forms":[[121,132]]},{"text":" In Trials ofthe 12th International Conference on  Computational Linguistics (COLING), Budapest. ","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"on the SMTnews dataset, with an increased in the Pearson correlate of over 0.10. MSRpar (MPar) is the only dataset in which TLsim (S?aric?","acronyms":[[90,94],[125,130]],"long-forms":[[82,88],[7,10]]},{"text":"wk} be its vocabulary. Among the monolingual setting, the Vector Space Model (VSM) is a k-dimensional space Rk, in which the text tj ?","acronyms":[[76,79],[106,108]],"long-forms":[[56,74]]},{"text":"words. In Proceedings of the International Conference on Computational Linguistics (COLING). ","acronyms":[[84,90]],"long-forms":[[57,82]]},{"text":" 1 Introduction Massive Open Online Courses (MOOCs), executing by organizations such as Coursera, have been among","acronyms":[[45,50]],"long-forms":[[16,43]]},{"text":"renowned that Figure 1(a) can be represented by an equivalent hierarchical Chino Restaurant Process (CRP) (Aldous, 1985) as in Figure 1(b). ","acronyms":[[100,103]],"long-forms":[[72,98]]},{"text":" 1 Introduction Recurrent Neural Network (RNN)-basis conditional language models (LM) have been shown to","acronyms":[[42,45],[82,84]],"long-forms":[[16,40],[65,80]]},{"text":"Figure 2: Heat map of the relevance scores w s, j between the target domain Usenet (UN) with the other domains on ACE 2004 data set.","acronyms":[[84,86],[114,117]],"long-forms":[[76,82]]},{"text":" This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past","acronyms":[[92,95],[127,130]],"long-forms":[[57,90],[101,125]]},{"text":"We forming a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel  corpus.","acronyms":[[83,89],[40,46],[27,31]],"long-forms":[[63,81],[32,38]]},{"text":"Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Aiding Vector Machine (SVM) utilise an RBF kernel with hyperparameters optimised via cross-","acronyms":[[118,121],[132,135]],"long-forms":[[94,116]]},{"text":"establishment (CR)  emotion (EM)  motion (MO)  perception (PC) ","acronyms":[[37,39],[10,12],[24,26],[54,56]],"long-forms":[[29,35],[0,8],[15,22],[42,52]]},{"text":"English utilize the frequency dictionary of Arabian (Buckwalter and Parkinson, 2011) and the Corpus of Contemporary American English (COCA) top 5,000 words (Davies, 2010).","acronyms":[[131,135]],"long-forms":[[100,121]]},{"text":"The columns in the first section of the table represent different settings of the p# parameter, with highest performance for each adjusted count paragon shown in bold. p# values were chosen to displaying a representative assortment of performance. P = phoneme model; NOR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM = local minimum strategy. The best perform local minimum model is shaded.","acronyms":[[300,302],[330,332],[256,258]],"long-forms":[[305,317],[335,348],[241,248],[261,272],[284,292]]},{"text":"Maximum Entropy Markov Model (MEMM)-based word segmenter with Conditional Indiscriminate Fields (CRF)based chunking; 3.","acronyms":[[89,92]],"long-forms":[[62,87]]},{"text":"  2. PrecisionCorrectTransliteration  (PTrans)  The precision is going to be computed used the ","acronyms":[[39,45]],"long-forms":[[5,36]]},{"text":"dominated by various Root Nodes. In this table, for each  possible Root Node category (RN), its corresponding Head  Node (HN), Dependent Nods\/8 (DN) and Control Peculiarities (CF), ","acronyms":[[89,91],[124,126],[147,149],[173,175]],"long-forms":[[69,78],[112,122],[129,145],[155,171]]},{"text":" NPHIL = Stray NP: Volume I: Syntax,  SUBJ = Subject: H__~e reads., ","acronyms":[[38,42]],"long-forms":[[45,52]]},{"text":"tion using a method identical to FOIL (Quin-  lan, 1990) and bottom-up generalization using  Least General Generalizations (LGG's). Ad- ","acronyms":[[122,127],[31,35]],"long-forms":[[91,120]]},{"text":"dimensional space, in which both texts and terms are represented by means of Domain Vectors (DVs), i.e. vectors representing the domain relevances among the linguistic object and","acronyms":[[93,96]],"long-forms":[[77,91]]},{"text":"3 CLaC Methodology Preprocessing consists of tokenizing, lemmatizing, sentence splitting, and part of speech (POS) tagged. ","acronyms":[[110,113],[2,6]],"long-forms":[[94,108]]},{"text":"Two types of initial parameter configurations were tried for BFGS; initial parameters have the same fixed values, or were chosen randomly. Steepest Descent (SD) was used as online training where some portion (i.e. chunk) of the training data were used during an iteration.","acronyms":[[157,159],[61,65]],"long-forms":[[139,155]]},{"text":"ate the surface shape in the opposite direction.  Amazon?s Mechanical Turk (MTurk) is becoming an essential tools for creating annotated resources","acronyms":[[75,80]],"long-forms":[[58,73]]},{"text":"twelve NE tags. The NE tagged corpus has been  used to develop Named Entity Recognition (NER)  system in Bengali using pattern directed shallow ","acronyms":[[89,92],[7,9],[20,22]],"long-forms":[[63,87]]},{"text":"It  is embedded to the C-value approach for  automatic term recognition (ATR), in the  form of weights constructed from statisti- ","acronyms":[[73,76],[23,30]],"long-forms":[[45,71]]},{"text":"plementation of SVR, with tuned parameters.  Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-","acronyms":[[80,87],[16,19]],"long-forms":[[57,78]]},{"text":"  Proceedings of the 2014 Conference on Empirical Means in Natural Vocabulary Processing (EMNLP), pages 1181?1191, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"that first contains TOOV and awarded by the scoured  engine. Average_Rank (A_Rank) is the average position of TOOV in the returned snippets.","acronyms":[[72,78],[20,24],[107,111]],"long-forms":[[58,70]]},{"text":"In Proceedings of the International Conference on World Large Web (WWW), pages 641?650. ","acronyms":[[66,69]],"long-forms":[[50,64]]},{"text":"puts to a system and the outputs it is intended to produce. During Machine Translation (MT), such resources taking the form of sentence-aligned parallel corpora of","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":" 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful ap-","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"coord dep coord (c) Previous conjunct headed (PH) Je vois Jean , Paul et Mary","acronyms":[[46,48]],"long-forms":[[20,44]]},{"text":"2008) since it also includes structural information of arguments. It is based on the Argumentation Markup Language (AML) that models argument components in a XML-based tree structure.","acronyms":[[116,119],[158,161]],"long-forms":[[85,114]]},{"text":"Since this is a binary classification task, we have 5 different tags: B-L (Started of a literal chunk), I-L (Interiors of a literal chunk), B-I (Startup an Idiomatic chunk), I-I (Inside an Idiomatic chunk),","acronyms":[[106,109],[139,142],[70,73],[175,178]],"long-forms":[[111,130],[144,166],[75,97],[180,199]]},{"text":"92  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 28?36, Montre?al, Canada, June 7?8, 2012.","acronyms":[[90,95]],"long-forms":[[31,88]]},{"text":"the favorite word attachments.  EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents","acronyms":[[49,52]],"long-forms":[[33,47]]},{"text":" Table 1: Classifier features in predicate disambiguation (PredDis), argument identification (ArgId), and argument labeling (ArgLab).","acronyms":[[94,99],[59,66],[125,131]],"long-forms":[[69,92],[33,57],[106,123]]},{"text":"Transactions of the Associations for Computational Linguistics (TACL), 1:25?36. ","acronyms":[[63,67]],"long-forms":[[0,61]]},{"text":"full PTB, using 1st sense information. All results  are shown as labelled attachment score (LAS). ","acronyms":[[92,95],[5,8]],"long-forms":[[65,90]]},{"text":" From the set of erroneous instances: True Positive (TP) ML class 6= student class False Negative (FN) ML class = student class","acronyms":[[53,55],[99,101],[57,59],[103,105]],"long-forms":[[38,51],[83,97]]},{"text":"Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven","acronyms":[[156,159],[92,96],[188,192]],"long-forms":[[127,154]]},{"text":" The pattern we utilize consists of a mix between the  part of speech (POS) tags and the mention tags for  the words in the training case.","acronyms":[[68,71]],"long-forms":[[52,66]]},{"text":"1 Introduction Kernel techniques are considered the most effective techniques for various relation extraction (RE) tasks on both general (f.g. newspaper text) and","acronyms":[[108,110]],"long-forms":[[87,106]]},{"text":"usually similar with that in word sense disambiguation (WSD), including bag of word lemmas  in the sentence, n-grams and parts of speech (POS)  in a window, etc.","acronyms":[[138,141],[56,59]],"long-forms":[[121,136],[29,54]]},{"text":"Concepts across categories Hilke Reckman and Crit Cremers Leiden University Centered for Linguistics (LUCL) Leiden, Netherlands","acronyms":[[100,104]],"long-forms":[[58,98]]},{"text":"4 Learns Algorithms We evaluation four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and de-","acronyms":[[97,100],[153,155],[134,137]],"long-forms":[[72,95],[140,150],[103,111]]},{"text":"sisting of analyses automatically created by schemes participating in the recent BioNLP Shared Task (ST) 2011. At providing for the","acronyms":[[101,103],[81,87]],"long-forms":[[88,99]]},{"text":"Huang and Jiang, 2005), which is usage to represent all derivations (i.e. parse trees) for a given  sentence under a context free grammar (CFG). A ","acronyms":[[139,142]],"long-forms":[[117,137]]},{"text":"1(c)) for each k. 3.3.2 PLEB PLEB (Point Location in Equal Balls) was first proposed by Indyk and Motwani (1998) and further","acronyms":[[29,33],[24,28]],"long-forms":[[35,64]]},{"text":" More recently, Galley and Quirk (2011) have made linear programs MERT (LP-MERT) as an exact search algorithm that reaches the global op-","acronyms":[[81,88]],"long-forms":[[56,79]]},{"text":"References TREC (Text REtrieval Conference) : http:\/\/trec.nist.gov\/ NTCIR (NII-NACSIS Test Collection for IR Systems) project: http:\/\/research.nii.ac.jp\/ntcir\/index-en.html","acronyms":[[75,85],[11,15],[68,73],[106,108]],"long-forms":[[17,42]]},{"text":"work for sentence level feature extraction. In the Window Processing component, each token is further represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the","acronyms":[[132,134],[159,161]],"long-forms":[[117,130],[140,157]]},{"text":"recursive noun phrases (NPs), main verb groups (MVs), and a common annotation for adjectival and adverbial phrases (APs). Example (3) be-","acronyms":[[116,119],[24,27],[48,51]],"long-forms":[[97,114],[10,22],[30,46]]},{"text":" The implemented system has three main modules: the Feature Extractor (FE), the Generalized Iterative Scaling (GIS), and the Classifica-","acronyms":[[71,73],[111,114]],"long-forms":[[52,69],[80,109]]},{"text":"ing (NLP) applications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query\/question expansion (Moldovan and","acronyms":[[78,80],[5,8],[50,52]],"long-forms":[[58,76],[27,48]]},{"text":"RERANKED 56.2 13.5 57.3 12.7 ORACLE 85.0 70.3 80.4 60.0 Table 2: Word accuracies and mistake rate shrinking (ERR) in percentages for English-to-Japanese MTL risen","acronyms":[[108,111],[152,155]],"long-forms":[[85,106]]},{"text":"Answer Pinpointing. In Proceedings of the DARPA Human Language Technology Conference (HLT). ","acronyms":[[86,89],[42,47]],"long-forms":[[48,73]]},{"text":"is that of the earliest centroid.  The Naive Bayes (NB) classifier is based on a probabilistic model which presumes conditional in-","acronyms":[[51,53]],"long-forms":[[38,49]]},{"text":" The system defines the position of the main part of  the situation relative to the point of discourse (PS). The ","acronyms":[[104,106]],"long-forms":[[87,102]]},{"text":"that these heuristics have much effect not only in  the inductive inference (regular SVM) but also in  transductive inference (TSVM), especially when  the untagged data is large.","acronyms":[[127,131],[85,88]],"long-forms":[[103,125]]},{"text":"4.1 Preprocessing  HTML Page Parsing  The Documenting Object Model (DOM) is an application programming interface used for parsing ","acronyms":[[65,68],[19,23]],"long-forms":[[42,63]]},{"text":"James Clark. 1999. XSL transformations (XSLT). W3C Recommendation, 16 November.","acronyms":[[40,44]],"long-forms":[[19,38]]},{"text":"therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). ","acronyms":[[129,132]],"long-forms":[[92,127]]},{"text":"triplet model since it is based on word triplets, is not trained discriminatively but employs the classical maximum likelihood approaching (MLE) instead. ","acronyms":[[134,137]],"long-forms":[[105,123]]},{"text":"connective label?. The results are reported for Same Sentence (SS) and Previous Sentence (PS) models, and the joined results for each of the argu-","acronyms":[[63,65],[90,92]],"long-forms":[[48,61],[71,88]]},{"text":"The Cell or Tissue Type classes was split into two fine grained classes, CELL and CLNE (cell line). ","acronyms":[[83,87],[74,78]],"long-forms":[[89,98]]},{"text":"Web Search 1 Figure 1: Architecture of the Multi-task Deep Neural Network (DNN) for Representation Learning: The lower layers are shared across all tasks, while top layers are task-specific.","acronyms":[[75,78]],"long-forms":[[54,73]]},{"text":"we have established direct contact with the south korean delegation through tribal elders .  Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted paraphrasing translation process (TP), and a human reference translation.","acronyms":[[182,184],[241,243]],"long-forms":[[162,180],[220,239]]},{"text":"2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conver-","acronyms":[[69,73]],"long-forms":[[47,67]]},{"text":"and a segment relation are identified.  Topic break index (TBI) takes the value of 1 or 2: the boundary with TBI=2 is less con-","acronyms":[[59,62],[109,112]],"long-forms":[[40,57]]},{"text":"                                                    2  In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone  number, and WWW.","acronyms":[[148,151],[114,118],[190,193]],"long-forms":[[140,146],[108,112]]},{"text":"edges. The NEs includes personal name(PRE), location name(LOC) and organization name(ORG). ","acronyms":[[85,88],[11,14],[38,41],[58,61]],"long-forms":[[67,79],[24,32],[44,52]]},{"text":"ing Language Understanding Engine), and afterwards analyze its performance on the task?s exchanging texts. BLUE encompasses of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules.","acronyms":[[160,162]],"long-forms":[[146,158]]},{"text":"EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Tableau 1: Aimed accuracy (DA) for WSJ10, section 23. ","acronyms":[[88,90],[0,3],[34,39]],"long-forms":[[69,86]]},{"text":"Chunking Trees (CARAT) 86.17 86.21 Linear Features (Kl) 90.79 90.46 Kl w\/o using LM characteristics (Kl-LM) 84.24 84.06 Composite Kernel (Kc: MST+Kl) 92.98 92.67","acronyms":[[89,94],[15,17],[48,50],[130,133],[134,136]],"long-forms":[[64,79],[0,13]]},{"text":"2 D IA        1 In order to test this hypothesis,            1   a double-stranded oligonucleotide probe that corresponds to bp +10 to +60 of the CCR3 gene was prepared, 3 0 NN            referred to as E1-FL (exon 1- full length, Figure 2A). 3 D A    1 1   This is the exact sequence 3 D N          that was deleted in the CCR3(-exon1).pGL3 plasmid 3 D N          that demonstrated decreased activity 3 D N      1   compared to the full length 1.6 kb construct [27].","acronyms":[[203,208],[146,150],[174,176],[324,328]],"long-forms":[[210,229]]},{"text":"190  troductory phase (GREET-INTRODUCE-TOPIC), the  negotiation phase (NEGOTIATE) and the shutdown  phase (FINISH).","acronyms":[[71,80],[106,112]],"long-forms":[[52,69],[90,104]]},{"text":"NEARING A PROJECT SUGGESTION TO OTTA BOARD  As the  Urns,,. C6ngress Office of Techdblogy Rating (OTA) programmed  sttidy on te lecomunica t ions ,  computers and information p o l i c i e s  approaches ","acronyms":[[100,103],[30,34]],"long-forms":[[67,98]]},{"text":"known as NP The corpus of English Wikipedia pages, known as EnWiki NP ( * NP) Hidden Markov Model (HMM) is used to solve ...","acronyms":[[99,102],[74,76],[67,69],[9,11]],"long-forms":[[78,97]]},{"text":" After detecting a new indefinite description (as ETA(x) : unlversity(x)) ReP  begets a newest \"referential object'\" (RefO). During the discours6 (after the ","acronyms":[[116,120],[50,56],[74,77]],"long-forms":[[94,112]]},{"text":"use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR)  These are mostly taken from the classifications ","acronyms":[[89,92],[4,7],[16,19],[26,29],[45,48],[66,69]],"long-forms":[[72,87],[0,3],[9,15],[21,25],[32,44],[50,65]]},{"text":"fn is tfi, and foov is fq in Feature (1).  v. Df_Rank (D-Rank): It is similar to SRank and computed based on Rank(i)= ","acronyms":[[55,61],[81,86]],"long-forms":[[46,53]]},{"text":"further republish the baseline results of Schnabel and Sch?utze (2014) employs the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger. ","acronyms":[[131,135],[87,90]],"long-forms":[[101,129]]},{"text":"in a Swedish Clinical Corpus Hercules Dalianis, Maria Skeppstedt Department of Computer and Regimes Sciences (DSV) Stockholm University","acronyms":[[110,113]],"long-forms":[[65,108]]},{"text":"In all other tests, including all closed  tests, City Academics of Hong Kong (CityU)  open test and Microsoft Research (MSR) open  test, we trained our system utilizes the relevant ","acronyms":[[121,124],[79,84]],"long-forms":[[101,119],[49,64]]},{"text":"It includes the four original partners  of the LATER project and the following new partners: University of Amsterdam (UvA) in the Netherlands, Free University of Bolzano-Bozen (FUB) ","acronyms":[[118,121],[47,52],[177,180]],"long-forms":[[93,116],[143,175]]},{"text":"there is a link to the next node).  Prompts (PT) occur when the tutor attempts to elicit a meaningful contribution from the student.","acronyms":[[45,47]],"long-forms":[[36,43]]},{"text":"In Proc. of the 4th Workshop  on Treebanks and Linguistic Theories (TLT), pages  149?160.","acronyms":[[68,71]],"long-forms":[[33,66]]},{"text":"Abstract We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which","acronyms":[[84,86]],"long-forms":[[63,82]]},{"text":"  Abstract  Over the last few years, machine translation (MT) has transformed from an academic research  platform into a productivity or gisting tool adopted by several end users.","acronyms":[[58,60]],"long-forms":[[37,56]]},{"text":"Multiword Expressions (MWEs) are one of the deadlocked blocks for more precise Natural Language Processing (NLP) systems.","acronyms":[[107,110],[23,27]],"long-forms":[[78,105],[0,21]]},{"text":"Figure 1 shows the example of the inputs format of ACABIT in XML makes use of which conforms to Document Kind Definition (DTD) in Figure 2.","acronyms":[[121,124],[50,56],[60,63]],"long-forms":[[95,119]]},{"text":"(A~.TG, ~ROC)I ^  (SUBS, SUSU, VT),  (ARTG, PR.OC)I ^  (SUBS, SUSU, VT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment ","acronyms":[[76,80],[99,103],[4,6],[9,12],[19,23],[25,29],[31,33],[38,42],[44,49],[56,60],[62,66],[68,70]],"long-forms":[[83,97],[106,116]]},{"text":"edge associated with it.  Definitions 3 (Informative Feature Extraction (IF)) We define the Informative-Features(IF ) feature","acronyms":[[72,74],[112,114]],"long-forms":[[40,59],[91,110]]},{"text":"  The analogy of two words is the least common  ancestor information content(IC), and therefore, the  higher the information content is, the more similar ","acronyms":[[80,82]],"long-forms":[[60,78]]},{"text":"Kanayama et al99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55 Haruno et al98 Probabilistic model (DT + Boosting) EDR (50,000) 85.03 Fujio et al98 Probabilistic model (ML) EDR (190,000) 86.67 Table 4: Comparison with the related work","acronyms":[[174,176],[38,40],[43,47],[49,52],[105,107],[120,123],[178,181]],"long-forms":[[167,172]]},{"text":"(I) it performs a translation between  intermediate languages constructed  over source language (SL) and target  language (TL) respectively (called ","acronyms":[[97,99],[123,125]],"long-forms":[[80,95],[105,121]]},{"text":"standing that shares tasks with OIE. AMR parsing (Banarescu et al, ), semantic role labeling (SRL) (Toutanova et al, 2008; Punyakanok et al, 2008)","acronyms":[[94,97],[37,40],[32,35]],"long-forms":[[70,92]]},{"text":"In that case partial semantic mapping will take place where no Logical Form is being built and only referring expressions are asserted in the Discourse Model ? but see below.  3.2 Lexical Information The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG (see [13]) terms using f-structures as domains and grammatical functions as entry points into the structure. We show here below the architecture of the system.","acronyms":[[270,272],[328,331]],"long-forms":[[255,268]]},{"text":"documents. Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA.","acronyms":[[78,81],[17,21],[102,105]],"long-forms":[[57,76]]},{"text":" The constructed ontology is assessment using  cluster purity (CP), instances knowledge (IK),  and relation concept (RC).","acronyms":[[62,64],[88,90],[116,118]],"long-forms":[[46,60],[67,86],[98,114]]},{"text":"nuqaqa llakiy qhachqa p?achakunata churakurqani.  Abbreviations: AMLQ = Academia Mayor de la Lengua Quechua en Cusco, norma = normalized, span = Spanish orthography, boliv = (longtime) Bolivian orthography Tables 1: Different Orthographies with Corresponding Standardized Version","acronyms":[[65,69]],"long-forms":[[72,107]]},{"text":"is shown in PLATE 1. The second part is the  Speedy Piano Server (PPS), which is an IVR  (interactive voice responded) server with a Dialogic ","acronyms":[[66,69],[84,87]],"long-forms":[[45,64]]},{"text":"dominates the other.  Marcu?s Nuclearity Principle (NP) Marcu 1996 provides an alternative to the immediate interpretation and","acronyms":[[52,54]],"long-forms":[[30,50]]},{"text":"than have them specified in a tag dictionaries.  The Lexicon HMM (Lex-HMM) extends the Pitman-Yor HMM (PYP-HMM) described by","acronyms":[[64,71],[101,108]],"long-forms":[[51,62],[85,99]]},{"text":" Finally there is a set of measures relating to the receiver operate characteristic (ROC) curves, which measures the discrimination of the scores for","acronyms":[[87,90]],"long-forms":[[52,85]]},{"text":"structure by outset computation the similarity of each proposition to the others using a Latent Dirichlet Allocation (LDA) model. LDA is a genera-","acronyms":[[115,118],[127,130]],"long-forms":[[86,113]]},{"text":" 2 Swedish FrameNet Swedish FrameNet (SweFN), which began in 2011, is part of Swedish FrameNet++ (Borin et al.,","acronyms":[[38,43]],"long-forms":[[20,36]]},{"text":"5 Learning Algorithms We used deux non-parametric learning approaches, Support Vector Machinery (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs)","acronyms":[[95,99],[162,165]],"long-forms":[[70,93],[142,160]]},{"text":"ture subset selection [2][4][8]. Yang and Pederson  found information gain (IG) and chi-square test (CHI)  most effective in aggressive term removal without los-","acronyms":[[76,78],[101,104]],"long-forms":[[58,74],[84,99]]},{"text":"sLDA is a model that is an extension of Latent Dirichlet Allocation (LDA) (Blei et al, 2003) that models each document as having an output variable in addition to","acronyms":[[69,72],[0,4]],"long-forms":[[40,67]]},{"text":"boundaries. Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is used to provide word boundaries as well as inaccu-","acronyms":[[89,96]],"long-forms":[[56,87]]},{"text":"Classification, Word Sense Disambiguation, etc. In  Natural Language Processing (NLP), one of the  most used resources for WSD and other tasks is ","acronyms":[[81,84],[123,126]],"long-forms":[[52,79]]},{"text":"knowledge. The use of Proposition Stores as  Background Knowledge Bases (BKB) have been  argued to be useful for improving parsing, co-","acronyms":[[73,76]],"long-forms":[[45,71]]},{"text":"MSEAS (MS,MSEA, VP,i, OUT)  (1) Start with VP = VAR ({X1, \" \" ,X.}), MSEA = f~,  i=1, and OUT = O. When the computation is completed, MS is bound to the set of active ","acronyms":[[90,93],[0,5],[7,9],[10,14],[16,18],[22,25],[43,45],[48,51],[69,73],[134,136]],"long-forms":[[96,119]]},{"text":"report false positives. This can be quantified by the positive predictive value (PPV), or probability that a research finding is true.","acronyms":[[81,84]],"long-forms":[[54,79]]},{"text":"Pour vs e r i f y  th'e r e l a t i o n s h i p s  between the s t a t i s t i c a l  models of word  importance and t h s  vector space modelled, dcsument c o l l e c t i o n s  are utilised i n  tres  d i f f e r f n t  subjec t  a reas ,  encompassing aerodynamics (cRAN),  meds (MED) and  worldwide a f f a i r s  (TIME).","acronyms":[[275,278],[257,261]],"long-forms":[[265,273]]},{"text":"As first step we try to map the linguistic triple  into an ontology triple, by utilised an adjustment of  Aqualog?s Relation Similarity Service (RSS).  ","acronyms":[[142,145]],"long-forms":[[113,140]]},{"text":" 3. Construct a character list (CH)3, in which the characters are top 20 frequency in training","acronyms":[[32,34]],"long-forms":[[16,25]]},{"text":"  MTI. The original Medical Text Indexer (MTI)  system, shown in Figure 1, consists of an infra-","acronyms":[[42,45],[2,5]],"long-forms":[[20,40]]},{"text":"by (Punyakanok et al, 2004). The process is devising as an integer linear programs (ILP) problem that takes as inputs the confidences over each","acronyms":[[89,92]],"long-forms":[[61,87]]},{"text":"~  = unrecognized input token.)  (ABC) = (A(BC)) = ((AB)C). Tim singletonbidi- ","acronyms":[[53,57]],"long-forms":[[42,46]]},{"text":"2 As a matter of fact, Figure 1 only shows 8 columns, although  the CoNLL-X format includes two additional columns for the  projective head (PHEAD) and projective dependency relation  (PDEPREL), which have not been used in our work.","acronyms":[[141,146],[68,75],[185,192]],"long-forms":[[124,139],[152,182]]},{"text":"Theorem LG is NP-complete.  Bin Packages (BP) which is NP-complete is  polynomial-t ime Karp reducible to LG.","acronyms":[[41,43],[8,10],[14,16],[105,107],[54,56]],"long-forms":[[28,39]]},{"text":" Experimenting are performed on two datasets, the English Penn Treebank (PTB) dataset using the standard train, dev and test partition, and the ARK","acronyms":[[71,74]],"long-forms":[[56,69]]},{"text":" Temporal Types Probable Values (tags) Timeline (TL) past, present, future Day of Week (DOW) Mon, Tue, . . . ,","acronyms":[[49,51],[88,91]],"long-forms":[[39,47],[75,86]]},{"text":"Not normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1) Table 2: Binary logistic regression 10-fold crossed validated with different feature normalization approaches: Scores inside brackets are when the female speaker data is removed; S = Insisted, US = Unstressed, MCB = Majority Class Baseline. ","acronyms":[[260,262],[277,280]],"long-forms":[[265,275],[283,306]]},{"text":"In the next section we will explain the concep-  tual model of ILMs by means of the KADS Domain  Description Language (DDL) proposed in Schreiber  (Schreiber et al, 1993).","acronyms":[[119,122],[63,67],[84,88]],"long-forms":[[97,117]]},{"text":"Accept? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the","acronyms":[[66,70],[45,48]],"long-forms":[[54,64],[24,32]]},{"text":" The CBDF similarity values between  100,000-word subsets of Initial French (OF),  French translated from English (EF), from ","acronyms":[[78,80],[5,9],[116,118]],"long-forms":[[61,76],[84,114]]},{"text":"English using the frequency dictionary of Arabic (Buckwalter and Parkinson, 2011) and the Corpus of Contemporary American English (COCA) top 5,000 words (Davies, 2010).","acronyms":[[131,135]],"long-forms":[[100,121]]},{"text":"ond accusative objects, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, ORC = clausal object, EP = expletive es 9AG = genitive adjunct","acronyms":[[122,124],[23,25],[36,38],[58,60],[85,87],[101,103],[140,143]],"long-forms":[[127,136],[28,34],[41,56],[63,83],[90,99],[106,120],[146,162]]},{"text":"sources Tony Mullen and Nigel Neck National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku","acronyms":[[73,76]],"long-forms":[[38,71]]},{"text":"sists of given entities and their relation expression.  Here, we use a Salient Referent List (SRL) to obtain contextual structure.","acronyms":[[94,97]],"long-forms":[[71,92]]},{"text":"show how to architectural MWE-aware training resource for them.  The corpora utilised in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?,","acronyms":[[128,131],[22,31],[179,182]],"long-forms":[[111,126],[162,177]]},{"text":"Translation. In Proceedings of the 13th International  Conference on Computational Linguistics (COLING-90),  Vol.","acronyms":[[96,105]],"long-forms":[[55,94]]},{"text":"4 GTS with the Idea of Coordination Problem Game GTS (Game Theoretic Semantics) has been developed as an alternative semantics where major se-","acronyms":[[49,52],[2,5]],"long-forms":[[54,78]]},{"text":"Table 5: Sample Hindi complex predicates   5 Corpus and pre-processing  Basic Travel Expressions Corpus (BTEC)  contain travels conversations is used for ","acronyms":[[105,109]],"long-forms":[[72,103]]},{"text":"FA8750-09-C-0181. The second author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.","acronyms":[[82,85]],"long-forms":[[52,80]]},{"text":" In Table 3 the number of questions that get a higher and lower reciprocal rank (RR) after applying the individual lexico-semantic resources are","acronyms":[[81,83]],"long-forms":[[64,79]]},{"text":"Answer Pinpointing. Among Proceedings of the DARPA Human Language Technology Conference (HLT). ","acronyms":[[86,89],[42,47]],"long-forms":[[48,73]]},{"text":"call this set of features Feat-IV.  Table 3 demonstrates the word error rate(WER) improvement enabled by our binary subsampling","acronyms":[[77,80]],"long-forms":[[61,75]]},{"text":"based method is slightly better. The two systems share the same topic relevance score (REL) and sentiment score, but the sentence-ranking method","acronyms":[[87,90]],"long-forms":[[70,79]]},{"text":"2.1 Download Initial Collection        The Yahoo Full Coverage Collection (YFCC) was  downloaded from http:\/\/fullcoverage.yahoo.com during ","acronyms":[[75,79]],"long-forms":[[43,73]]},{"text":"2.2 Singular Value Decomposition Given any matrix S, its singular value decomposition (SVD) is S = U?V T . The matrix Sk =","acronyms":[[87,90],[99,102]],"long-forms":[[57,85]]},{"text":"Branching quantification in DTS has partially been discussed in [7] and [8], in which we compared DTS with First Order Logic (FOL). However, FOL is limited in that it allows to","acronyms":[[126,129],[28,31],[98,101],[141,144]],"long-forms":[[107,124]]},{"text":"Similarly, to create a fully text-bound subset, family memberships relations (MEMBER) were resolved into single edges and adequate references to","acronyms":[[80,86]],"long-forms":[[57,78]]},{"text":"We apply the combination patterns to the training corpus, and count pairs of True Positives (TP) and False Positives (FP). The scores are calculated","acronyms":[[118,120],[93,95]],"long-forms":[[101,116],[77,91]]},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is considered one of the most momentous prob-","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"tistical machine translation. In Proceedings of the Machine Translation Summit (MT-Summit). ","acronyms":[[80,89]],"long-forms":[[52,78]]},{"text":"mechanism for accurate named entity  (NORTHEASTERN) translation in English?Chinese  question answering (QA). This mecha-","acronyms":[[94,96],[38,40]],"long-forms":[[74,92],[23,35]]},{"text":"The  suggestions made here for the organization of space are only a working  set for which l i t t le  justification can be offered: location (LOC)--a  neutral statement of position, contact--in physical contact, and -* near ","acronyms":[[143,146]],"long-forms":[[133,141]]},{"text":" In Proceedings of the International Conference on Language Resources and Evaluation (LREC). ","acronyms":[[86,90]],"long-forms":[[51,69]]},{"text":"quences here).  1PARTMOD=participial modifier, PERS=prepositional modifier, POBJ=object of preposition.","acronyms":[[47,51],[16,24],[76,80]],"long-forms":[[52,65],[25,45],[81,102]]},{"text":"thematic structure, and defined well formedness conditions on the thematic architecture and on the relation between thematic structure (TH) and syntactic dominance (IDS) structure.","acronyms":[[133,135],[162,164]],"long-forms":[[113,121],[0,8]]},{"text":"In this paper, we propose  methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs)  based WOEs detection models identify the sentence segments contained afflictions.","acronyms":[[111,115],[57,61],[124,128],[188,192]],"long-forms":[[84,109]]},{"text":"4.4 Word Sense Induction In this section, we present an evaluation of our model on the word sense launching (WSI) missions. The","acronyms":[[109,112]],"long-forms":[[87,107]]},{"text":"10?16). Results for per-predication (PR) and per-whole-graph (GRPH) tagging percentage accuracies are listed. (","acronyms":[[62,66],[37,39]],"long-forms":[[55,60],[24,35]]},{"text":"ptishes a rather inconsequential change with respect o a  previously non-existent link or with respect to a link  No impairment (NI) Q Confusion (C)  Q Mislearning (ML) Q Insufficient Learning (IL) ","acronyms":[[129,131],[165,167],[194,196]],"long-forms":[[114,127],[152,163],[171,192],[135,144]]},{"text":" 1 Introduction Verb Phrase Ellipsis (VPE) is an anaphoric constructing in which a verbal constituent has been omitted.","acronyms":[[38,41]],"long-forms":[[16,36]]},{"text":"an efficacious bottom-up algorithm. The asymptotic period complexity of this search is O(SR) where S is the number of source nodes andR is the number","acronyms":[[85,87]],"long-forms":[[73,79]]},{"text":"For phrase which failed to be guessed by  tile guessing regulation we applied the standard method  of classifying them as common nouns (NN) if they  are not capitalised inside a sentencing and proper ","acronyms":[[130,132]],"long-forms":[[123,128]]},{"text":"ing at combining the strengths of dissimilar grammars, we describes a synthetic synchronicity grammar (SSG), which tentatively in this paper, integrates a syn-","acronyms":[[100,103]],"long-forms":[[69,98]]},{"text":" Opennlp maxent1, an implementation of Maximum Entropy (ME) modeling, is used as the classification tool.","acronyms":[[56,58]],"long-forms":[[39,54]]},{"text":"a r e  the fo l lowing  -- Performer,  Object, Goal, Source,  Locat ion,   Means, Cause, and Enabler  -- and (2) s t r u c t u r a l  c a s e s ,  which a r e   R E E L  ( r e l a t i v e  c l a u s e )  and COMP (compound). I w i l l  not  e x p l a i n  ","acronyms":[[208,212]],"long-forms":[[214,222]]},{"text":" 2 Named Entity Extraction  Named Entity Recognition (NER) is useful in  NLP applications such as question responding, ","acronyms":[[54,57],[73,76]],"long-forms":[[28,52]]},{"text":"In  Proc. of Intelligent Tutoring Systems (ITS). ","acronyms":[[43,46]],"long-forms":[[13,41]]},{"text":"Figure 3: Dialogue system architecture    The Dialogue Manager (DM) utilizing sceneobject attributes, such as type, angle or interval ","acronyms":[[64,66]],"long-forms":[[46,62]]},{"text":"phrases (NPs) (representing 49.6% of the total number of phrases), verb phrases (VPs), prepositional phrases (PPs), adjectival phrases (ADJPs), and quantity phrases (QPs), representing 99.1% of","acronyms":[[136,141]],"long-forms":[[116,134]]},{"text":" 2Regularized parses (henceforth, \"parse trees\") are  like F-structures of Lexical Ftmction Grammar (LFG),  except, hat a dependency structure is used.\"","acronyms":[[101,104]],"long-forms":[[75,99]]},{"text":"Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA). ","acronyms":[[203,207],[127,134]],"long-forms":[[162,201],[92,125]]},{"text":"patterns indicative of SLI. In this work, we use Language Models (LMs) for this task since they are a powerful statistical measure of language usage","acronyms":[[66,69],[23,26]],"long-forms":[[49,64]]},{"text":"transcripts produced with Automatic Speech Acknowledgment  (ASR) systems tend to contain several recognition errors,  leading to low Information Retrieval (IR) performance  (Oard et al, 2007).","acronyms":[[150,152],[57,60]],"long-forms":[[127,148],[26,54]]},{"text":"20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), T = total WER (%).","acronyms":[[137,139],[99,104],[227,230]],"long-forms":[[142,153],[182,191],[201,211],[221,226]]},{"text":"Throughout this paper, we portray an initial  applying of a general spoken language interface, the  Carnegie Mellon Spoken Language Shell (CM-SLS) which  provides voice interface services to a variable number of applica- ","acronyms":[[138,144]],"long-forms":[[99,136]]},{"text":"PER (PN), only PER candidates beginning with  the family name is considered. For PER (FN), a  candidate is generated only if all its composing ","acronyms":[[86,88],[0,3],[5,7],[15,18]],"long-forms":[[77,84]]},{"text":"We utilised the following label set: S-O (not in maze); S-M (single word maze); B-M (beginning of multi-word 72","acronyms":[[75,78],[32,35],[51,54]],"long-forms":[[80,98],[56,72]]},{"text":"ifications) to be the first on the COMPS list, and further assigns a supportive value for an additional feature INV (reversing) on verbs. This feature may","acronyms":[[110,113],[35,40]],"long-forms":[[115,123]]},{"text":"that provides a good compression rate of the text.  3.2 Byte Pair Encoding (BPE) Byte Pair Encoding (BPE) (Gage, 1994) is a sim-","acronyms":[[76,79],[101,104]],"long-forms":[[56,74],[81,99]]},{"text":"initions of state cannot be sensitive to (sometimes critical) aspects of the dialogue context, such as the user?s last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of","acronyms":[[134,136]],"long-forms":[[119,132]]},{"text":" 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as templates of temporal, or","acronyms":[[64,68]],"long-forms":[[37,62]]},{"text":"4.4.2 Optimization Of maximize the intents in (6), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al, 2009).","acronyms":[[94,97]],"long-forms":[[65,92]]},{"text":"IGT-XML. At the heartland of the model is a representation of interlinearized glossed text (IGT). Building","acronyms":[[88,91],[0,7]],"long-forms":[[58,86]]},{"text":"Web Search 1 Silhouette 1: Architecture of the Multi-task Deep Neural Network (DNN) for Representation Learning: The lower layers are exchanging across all tasks, while top layers are task-specific.","acronyms":[[75,78]],"long-forms":[[54,73]]},{"text":" For this reason, NIST assessors not only marked  the segments sharing between system units (SU)  and model units (MU), they additionally indicated the ","acronyms":[[92,94],[18,22],[114,116]],"long-forms":[[78,90],[101,112]]},{"text":"Associated Press Worldstream English Service (APW) ? The New York Times Newswire Service (NYT) ? The Xinhua News Agency English Service (XIE) For each source, Gigaword articles are classified into several types, including newswire advisories, etc.  We restricted our investigations to actual news stories.","acronyms":[[137,140],[46,49],[90,93]],"long-forms":[[101,135],[0,28],[57,71]]},{"text":"Topic: Short, usually controversial statement that defines the subject of interest.   Context Dependent Claim (CDC): General, and concise statement, that directly supports or contests  the given Topic.","acronyms":[[111,114]],"long-forms":[[86,109]]},{"text":"Argument Filtering Argument  Boundary Detection (ABD) module ?)???????","acronyms":[[49,52]],"long-forms":[[29,47]]},{"text":"DSTG = Adverb s t r i r ig   mRTOVO = For + Subject -+ to -+ Object  NA = N t- Adjective  NASOBJBE - N + as - t- Object  of be -","acronyms":[[69,71],[0,4],[29,35],[90,98]],"long-forms":[[74,88]]},{"text":"However of utilized graph-based consensus  confidence as features in the log-linear model, we  perform structured label propagation (Struct-LP) to  re-rank the n-best list directly, and the similarity ","acronyms":[[130,139]],"long-forms":[[100,128]]},{"text":"more, 1976; Fillmore, 1982). The categories of events are the semantic frames of the lexical units (LUs) that evoke them, and the functions associated with the","acronyms":[[97,100]],"long-forms":[[82,95]]},{"text":"2.1 Description of the procedure Two specialized topics In this study MA student translator were invited to prepare for simultaneous interpreting tasks on two specialised item: speedily reactors (FR) and Seabed minerals (SM). They","acronyms":[[196,198],[221,223],[70,72]],"long-forms":[[181,194],[204,219]]},{"text":"in table 4. As we can see a small improvement is obtained for the interpretation error rate (IER) with the integrated strategy (strat2).","acronyms":[[93,96]],"long-forms":[[66,91]]},{"text":"They  ha 1 Personal Name (PN); Date or Time (DT); Location Name  (LN); Team Behalf (TN); Competition Title (CARAT); Personal ","acronyms":[[26,28],[45,47],[66,68],[82,84],[106,108]],"long-forms":[[11,24],[31,43],[50,63],[71,80],[87,104]]},{"text":"1 Introduction  Turkish Discourse Bank (TDB) is the first discourse-annotated corpus of Turkish, which follows the  principles of Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) and includes annotations for dis-","acronyms":[[156,160],[40,43]],"long-forms":[[130,154],[16,38]]},{"text":"and Causal Relationship. In proceedings of the Association for Computational Linguistics (ACL). ","acronyms":[[87,90]],"long-forms":[[50,85]]},{"text":"a different word, such as job, in a given context) by defining a one-class learning algorithm based on support vector machines (SVM). They train a one-","acronyms":[[128,131]],"long-forms":[[103,126]]},{"text":"In Proceedings of the 2005 International conference on Intelligent User Interfaces (IUI), pages 137?144. ACM Press.","acronyms":[[84,87],[105,108]],"long-forms":[[55,82]]},{"text":"For this task we train and test three different statistical models: an n-gram language model, a maximum entropy modeling (MaxEnt) and a (linear) help vector machine (SVM).","acronyms":[[119,125],[166,169],[71,77]],"long-forms":[[96,111],[142,164]]},{"text":" Succour Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text","acronyms":[[49,53]],"long-forms":[[24,47]]},{"text":"complementary features ( Section 3.3).  3.2 Mine Labeled Sequential Patterns ( LSP ) Labeled Sequential Patterns (LSP).","acronyms":[[81,84],[115,119]],"long-forms":[[51,78],[87,114]]},{"text":"the percentage of words that were placed in a segment perfectly identical to that in the reference. The dialogue act based metric (DER) was proposed in Zimmermann et al (2005). In this metric a word is","acronyms":[[131,134]],"long-forms":[[104,129]]},{"text":"given themes.  The Maximal Marginal Relevance (MMR) summarization method, which is based on a","acronyms":[[46,49]],"long-forms":[[18,44]]},{"text":"Training uses balanced data (50:50). Testing uses two class distributions (C.D.): 50:50 (balanced) and Natural Distribution (N.D.). Improvements of our method are statistically significant with p<0.005 based on paired t-test.","acronyms":[[125,129],[75,78]],"long-forms":[[103,123],[54,72]]},{"text":"Introduction  The pro jec t  note  presents  the  computer  programmed  GECO (GEometry COnsu l te r ) ,  wh ich  creates   exp lanat ions  (descr ip t ions )  o f  geometr i wo l  ","acronyms":[[69,73]],"long-forms":[[75,96]]},{"text":"Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) . ","acronyms":[[257,262],[67,70],[76,79]],"long-forms":[[214,255]]},{"text":"Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City Uni-","acronyms":[[131,133],[170,173]],"long-forms":[[107,129],[145,161]]},{"text":"Table 3: Results on DevTest and Test Sets compared with the Average Performance in CoNLL?07. LAS = Labelled Attachment Score, UAS = Unlabelled Attachment Score, LAcc = Label Accuracy, AV = Average score.","acronyms":[[126,129],[184,186],[93,96],[161,165],[83,88]],"long-forms":[[132,159],[189,196],[99,124],[168,182]]},{"text":"and why they should be adhered to? involving a coordinated phrase in the object position consisting of an NP (najprostsze zasady ? the most basic principles?)","acronyms":[[106,108]],"long-forms":[[110,121]]},{"text":"the reference than the rest. Considering this, we use a Longest Common Subsequence(LCS) groundwork criteria to calculate s(x, y).","acronyms":[[83,86]],"long-forms":[[56,81]]},{"text":"Nearly 2,500 sets of related words in  LLOCE are organized according to 14 subjects and 129 topics (TOP). Cross references (REF) between sets,  topics, and subjects are also given to show various inter-sense r lations not captured within the same topic.","acronyms":[[124,127]],"long-forms":[[112,122]]},{"text":"namely, the set {? ( D)d |ad = a}, where ad is the author of document d. An alternative approach is LDA-S (LDA with a single document per author), where each author?s documents are concatenated into a single document in a preprocessing step, LDA is run","acronyms":[[100,105],[242,245]],"long-forms":[[107,124]]},{"text":"from the remaining pool of data.  The intrinsic stopping criterion (ISC) we proposing here focussed on the latter aspect of the ideal stop-","acronyms":[[68,71]],"long-forms":[[38,66]]},{"text":"al., 2005; Vapnik, 1998) based on the Gaussian  Radial Basis kernel functions (RBF). We tuned ","acronyms":[[78,81]],"long-forms":[[48,76]]},{"text":" 1 Introduction Natural Language Inference (NLI), i.e. the task of determining whether an NL hypothesis can be in-","acronyms":[[44,47],[90,92]],"long-forms":[[16,42]]},{"text":" 1 Introduction The approach to factoid question answering (QA) that we adoptions was first described in (Whittaker et","acronyms":[[60,62]],"long-forms":[[40,58]]},{"text":"tags and word.  Rich Morphological Features (Rich-MF): In addition to the elementary features we use the am-","acronyms":[[45,52]],"long-forms":[[16,43]]},{"text":"SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78 ?","acronyms":[[56,62],[0,4],[27,32]],"long-forms":[[65,87],[7,26],[35,55]]},{"text":"Keyword 0.168 0.168 0.157 6.3 Chalkboard 3: Title quality as compared to the reference for the hierarchy discriminative (HD), flat discriminative (FD), hierarchy generative (HG), flat","acronyms":[[119,121],[145,147],[175,177]],"long-forms":[[90,117],[124,143],[150,173]]},{"text":"The alignments produced by MEBA were  compared to the ones produced by YAWA and  evaluated against the Gold Standard (GS)1 annotations used in the Word Alignment Shared ","acronyms":[[118,120],[27,31],[71,75]],"long-forms":[[103,116]]},{"text":"ratings of IP strategies. In the following we use a Reinforcement Learning (RL) as a statistics planning frames (Sutton and Barto,","acronyms":[[76,78],[11,13]],"long-forms":[[52,74]]},{"text":"negative has an incorrect, but plausible, stress pattern, u. We adopt a Support Vector Machine (SVM) solution to these ranking constraints as described by","acronyms":[[96,99]],"long-forms":[[72,94]]},{"text":"taking et al (2005)) for the representation of meaning.  LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copestake, 2002), a development surroundings for constraint-based grammars.","acronyms":[[112,115],[55,61]],"long-forms":[[82,110]]},{"text":"fn is tfi, and foov is fq in Featured (1).  v. Df_Rank (D-Rank): It is similar to SRank and computed based on Grades(i)= ","acronyms":[[55,61],[81,86]],"long-forms":[[46,53]]},{"text":"by using RBM to implement the middle layers,  since RBM can be learned very urgently by the  Contrastive Variance (CD) approach. ","acronyms":[[116,118],[9,12],[52,55]],"long-forms":[[92,114]]},{"text":"distinct verbs, not occurrences. The seventh column shows the number of verbs that  were misclassified (MC)--the sum of false positives and false negatives. The eighth ","acronyms":[[104,106]],"long-forms":[[89,102]]},{"text":"shown in (1).2  2~Vc use lhe fo l low ing  abbrev ia t ions :  NOM : nominat ive ;   ACC = accusat ive ;  AI)N = adnomina l ;  CI. = c lass i l ier ;  ARGSTR ","acronyms":[[85,88],[106,110]],"long-forms":[[91,98],[113,121]]},{"text":"923 DOs, Active: \"AGENT STRING AUX active-verb-element DETERMINER * POSTMOD\"DOs, Passive: \"DETERMINER * AUX active-verb-element element\"TVs, Active: \"AGENT STRING AUX * DETERMINER active-noun- element POSTMOD\"TVs, Passive:\"DET active-noun-element AUX * POSTMOD\" Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. ","acronyms":[[318,321],[345,348],[31,34],[104,107],[163,166],[4,7],[76,79],[247,250],[136,139],[209,212]],"long-forms":[[302,316],[327,343]]},{"text":"morpheme omitted.   We adopt Support Vector Machines(SVM) as  the device by which a given adnoun clause is ","acronyms":[[53,56]],"long-forms":[[29,51]]},{"text":"PP ??  ( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ? ","acronyms":[[9,11],[0,2],[31,33],[50,52]],"long-forms":[[14,17]]},{"text":"bath?). The builder can selects to represent binaries as either relational noun words (RELNP) or generalized transitive verbs (VP\/NP).","acronyms":[[88,93],[128,133]],"long-forms":[[63,86],[121,126]]},{"text":"lexical chaining.  4.1 LDA Manner Method (LDA-MM) The LDA-MM approach places all word tokens that","acronyms":[[40,46],[52,58]],"long-forms":[[23,38]]},{"text":"This makes it hard to unearth particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the conversations at a high-level. Ours goal in this work was to create a simple instrument which would permitting people to rapid ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comments cords. 2 How CoFi works For a given set of sightings, we create a distinct CoFi cases.","acronyms":[[453,457],[625,629],[564,568]],"long-forms":[[459,473]]},{"text":" At least for case schemata we have as first al-   ternative the choice between the realization types :CLAUSE  and :NG (noun group). For semantic structures from titles we ","acronyms":[[116,118]],"long-forms":[[120,130]]},{"text":"representatives popular heterogeneous corpora, i.e. 232 Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD).","acronyms":[[78,81],[109,112]],"long-forms":[[60,76],[87,107]]},{"text":" Ours focus on the following languages: German (DE), French (FR), Italian (IT), and Dutch (NL).","acronyms":[[59,61],[73,75],[46,48],[89,91]],"long-forms":[[51,57],[64,71],[38,44],[82,87]]},{"text":"Therefore, the standard metrics widely used in sequential subject segmentation for monologues and dialogs, such as Pk and WindowDiff(WD), are similarly not applicable.","acronyms":[[131,133],[113,115]],"long-forms":[[120,129]]},{"text":" A baseline system was also implemented utilizing  the principle of most frequent sense (MFS),  where each word sense distribution was recovered ","acronyms":[[85,88]],"long-forms":[[64,83]]},{"text":"Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores","acronyms":[[108,111]],"long-forms":[[80,106]]},{"text":"Hodellng Temporal Knowledge  Hodellng time, dasoite its olovlous importance, has  proved an elusive goals for artificial Intelligence (AI). ","acronyms":[[134,136]],"long-forms":[[109,132]]},{"text":"tence All the indexes dove ., in which All should be tagged as a predeterminer (PDT).10 Most occurrences of All, however, are as a determiner (DT, 106\/135 vs","acronyms":[[80,83],[143,145]],"long-forms":[[65,78],[131,141]]},{"text":"is Tws, for \"Translator's Workstation.\" We also used the  C-based X11 toolkit called MOTIF (Motif, 1991) and its Com-  monLisp interface called CLM (Babatz et.","acronyms":[[85,90]],"long-forms":[[92,97]]},{"text":"We focusing on syntax, esp. noun phrase (NP) syntax from the beginning.","acronyms":[[40,42]],"long-forms":[[27,38]]},{"text":"We appraisal the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000)","acronyms":[[74,76]],"long-forms":[[51,62]]},{"text":"  Abstract  Over the last few yr, machine translation (MT) has transformed from an academic research  platform into a productivity or gisting tool adopted by several terminating users.","acronyms":[[58,60]],"long-forms":[[37,56]]},{"text":"tried two types of expansion, one mainly using synonyms (SYN), and one mainly using hypernyms or related links (LNK). ","acronyms":[[112,115],[57,60]],"long-forms":[[105,110],[47,55]]},{"text":"October-2001 w\/out 2000 SPA 30.88 \u0000 95.68 \u0000 0.08 \u0000 Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim = Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl.","acronyms":[[203,206],[24,27],[95,99],[147,150],[226,228]],"long-forms":[[209,219],[153,162]]},{"text":"(equivalent to relation). Their relation instances are named entity(NE)-mention pairs conforming to a set of pre-specified rules.","acronyms":[[68,70]],"long-forms":[[55,67]]},{"text":"pound verb (CompV) is framed with these two  verbs. But, the second Light Verb (LV) maggio be  a part of another Sprawling Predicate (CP).","acronyms":[[80,82]],"long-forms":[[68,78]]},{"text":"spectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcasts news (BNews).","acronyms":[[80,85],[128,133]],"long-forms":[[70,78],[112,126]]},{"text":"Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping goals for tongue blade constriction degree (TBCD), lip aperture (LA), and glottis (GLO).","acronyms":[[134,138],[155,157],[173,176]],"long-forms":[[100,132],[141,153],[164,171]]},{"text":"5 5 of such properties is acyclicity, as in Hidden Markov Models (HMMs). For","acronyms":[[66,70]],"long-forms":[[44,64]]},{"text":"the bend (AUC) from the trained detector on the heldout test set. Outcomes are reported in terms of the mean and standard diversion (SD) of the AUC across all splits. ","acronyms":[[133,135],[11,14],[144,147]],"long-forms":[[113,131]]},{"text":"Expanding  on a proposal of Nfichieis (1982), we classify verbs  as subject equi (SEqui), object equi (OEqul), sub-  ject raising (SRals ing)  or objects raising (ORuls ing) ","acronyms":[[84,89],[105,110],[164,173],[133,142]],"long-forms":[[70,82],[92,103],[148,162],[113,131]]},{"text":"2 Dialogue data The dialogue corpus used to perform the experiments is the Switchboard database (SWBD). It","acronyms":[[97,101]],"long-forms":[[75,95]]},{"text":"For each sense s i of the targets words, we place a Hierarchical Dirichlet process (HDP) before on the mixture proportion to latent concepts shown as follows:","acronyms":[[83,86]],"long-forms":[[51,81]]},{"text":"on the SMTnews dataset, with an increase in the Pearson correlation of over 0.10. MSRpar (MPar) is the only dataset in which TLsim (S?aric?","acronyms":[[90,94],[125,130]],"long-forms":[[82,88],[7,10]]},{"text":"curacy of an automatic classifier means to compare its output with the correct semantic tags on a Gold Standard (GS) dataset. Within our formal","acronyms":[[113,115]],"long-forms":[[98,111]]},{"text":"gave as input. The crawler generates the  Universal Resource Locator (URL) address for the  index (first) page of any particular dating.","acronyms":[[74,77]],"long-forms":[[46,72]]},{"text":"II: irrelevant input due to ASR errors or noise.  We approve logistic regression (LR)-based conversations act tagging approach (Tur et al.,","acronyms":[[80,82],[28,31]],"long-forms":[[59,78]]},{"text":" As an example, consider a user who is looking for information on digital video recorders (DVR), in particular, on how she can use a DVR with a","acronyms":[[91,94],[133,136]],"long-forms":[[66,89]]},{"text":"Table 1 FactBank annotation scheme. CT = certain; PR = probable; PS = possible; U = underspecified; + = conducive; ?","acronyms":[[36,38],[50,52],[65,67]],"long-forms":[[41,48],[55,63],[70,78],[84,98],[104,112]]},{"text":"ing Language Understanding Engine), and subsequently analyze its performance on the task?s shared texts. BLUE consists of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules.","acronyms":[[160,162]],"long-forms":[[146,158]]},{"text":"using the DSO corpus, which involves sentences drawn from two multiple corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They","acronyms":[[131,134],[10,13],[102,104]],"long-forms":[[110,129],[88,100]]},{"text":"SemEval 2012 compete for evaluating Natural  Language Processing (NLP) systems introduces a  new task called Semantic Textual Similarity (STS)  (Agirre et al, 2012).","acronyms":[[140,143],[70,73]],"long-forms":[[111,138],[40,68]]},{"text":"2013. Overview of the trail curation (PC) task of bioNLP shared task 2013.","acronyms":[[40,42]],"long-forms":[[22,38]]},{"text":"139  Computational Linguistics Volume 18, Number 2  (18a) sense of JUMPS = jumping. ","acronyms":[[67,71]],"long-forms":[[74,81]]},{"text":"In especial, the work of (Pietra et al, 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter evaluation and the Gibbs sampler","acronyms":[[104,107]],"long-forms":[[76,102]]},{"text":"C H A N G E .  H A V E  CSEXCH FOB BEPGFF IN 1 4   ANTEST CALLEC FOR 23\"NASREL: \" (AACC) S D =  24, RES= 3. TOP= 1:s ","acronyms":[[65,68],[83,87],[89,92],[100,103],[108,111],[24,30],[31,34],[35,41]],"long-forms":[[51,64]]},{"text":"labeled as detrimental; otherwise, the inspecting is labeled as positive.  3.2 Lexicon-Based Modes in Chinese Language: LEX(CN) This method first translates English sentiment lexica into Chinese lexica, and then","acronyms":[[118,120],[114,117]],"long-forms":[[96,112]]},{"text":"DEPICTION (DPC) TOPIC (TPC) SYNONYMY-NAME (SYN) CAUSALITY (CSL) PART-WHOLE (PW) MANNER (MNR) ANTONYMY (ANT) JUSTIFICATION (JST) HYPERNYMY (ISA) MEANS (MNS) PROBABILITY OF EXISTENCE (PRB) GOAL (GOL) ENTAIL (ENT) ACCOMPANIMENT (ACC) POSSIBILITY (PSB) BELIEF (BLF)","acronyms":[[182,185],[43,46],[11,14],[23,26],[193,196],[206,209],[226,229],[244,247],[257,260],[59,62],[76,78],[88,91],[103,106],[123,126],[151,154],[139,142]],"long-forms":[[156,167],[28,36],[0,9],[16,21],[187,191],[198,204],[211,224],[231,242],[249,255],[48,57],[64,74],[80,86],[93,101],[108,121],[144,149]]},{"text":" 156 The Basque Dependency Treebank (BDT) is a dependency treebank in its original design, due to","acronyms":[[37,40]],"long-forms":[[9,35]]},{"text":"in the dictionary to the following categories: protesters : NP seized : (S\\NP )\/NP several : NP\/NP","acronyms":[[70,74],[57,59],[77,79],[90,95]],"long-forms":[[60,68]]},{"text":"TI = fTW; F; ADV; TO; V A; V C; V V; Pg Everyone genre of the indicators, e.g. TW , contains a set of words, such as TW = twlist = ftw","acronyms":[[113,115],[75,77],[18,21],[13,16],[0,2]],"long-forms":[[118,124]]},{"text":"verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP). This ","acronyms":[[84,86],[35,37]],"long-forms":[[65,82],[23,33]]},{"text":"egories (left side) and with respect to the assessor?s own expertise (right side). ( Keys: B=favourable, LB=potentially beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE) the assessor had no idea from which stipulation an","acronyms":[[150,152],[104,106]],"long-forms":[[153,172],[92,102],[107,124],[128,137],[141,148],[176,183],[187,190]]},{"text":" 5 Conclusions The spoken language understanding (SLU) system discussed in this paper is entirely statistical based.","acronyms":[[50,53]],"long-forms":[[19,48]]},{"text":"We also wanted to deciding if information about 6http:\/\/www.isi.edu\/?ravichan\/YASMET.html dialog acts (DA) helps the classify task. If we","acronyms":[[104,106]],"long-forms":[[91,102]]},{"text":"tf iD j  is the inverse document frequency(IDF)  of the j-th word calculated as below: ","acronyms":[[43,46]],"long-forms":[[16,41]]},{"text":"validation. This is carried out for the tweet text (TEXT), user-declared location (MB-LOC) and user-declared time zone (MB-TZ).","acronyms":[[52,56],[83,89],[120,125]],"long-forms":[[46,50],[59,81],[95,118]]},{"text":"4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments. STS is related to both Recognizing Textual En-tailment (RTE) and Paraphrase Recognition, but  54","acronyms":[[371,374],[12,14],[36,38],[132,135],[153,156],[315,318]],"long-forms":[[338,369],[98,126]]},{"text":"\u0011\u000f \u0019 \u0019 \u0019 \u0019 \u0019 \u0018 Figure 3: Scored dependency forest 2.5 Semantic Dependency Graph (SDG) The SDG is a semantic-label word DG designed","acronyms":[[81,84],[90,93],[119,121]],"long-forms":[[54,79]]},{"text":"called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S consists of two modules: (1) a language generation module (LGM) and (2) a speech generation module (SGM) which turns the generated text into a speech signal.","acronyms":[[227,230],[12,15],[7,10],[164,167],[268,271]],"long-forms":[[199,225],[242,266]]},{"text":"ajaynagesh@cse.iitb.ac.in Abstract Information Extraction (IE) has become an indispensable tool in our quest to handle the data","acronyms":[[59,61]],"long-forms":[[35,57]]},{"text":"follows. NLC(:A): the analysed of concepts that playing  a role in natural anguage; (NL)CA: the lattice the-  Karaphuis and Sarbo 205 Natural Language Concept Analysis ","acronyms":[[82,84],[9,12],[85,87]],"long-forms":[[64,79]]},{"text":"eal papers. Unwell PTvcccdings of thc Pac'lific Sym-  posium on Biocomp'uting'98 (PSB'98), .Jan-  1uAYy.","acronyms":[[79,85],[16,27]],"long-forms":[[51,77]]},{"text":"the previous section. We contrast this metric with Normalized Pointwise Mutual Information (NPMI) which uses only the events A = X+a and B = X","acronyms":[[92,96]],"long-forms":[[51,90]]},{"text":"validation. This is carried out for the tweet text (TEXT), user-declared location (MB-LOC) and user-declared moment zone (MB-TZ).","acronyms":[[52,56],[83,89],[120,125]],"long-forms":[[46,50],[59,81],[95,118]]},{"text":"maps: 2-d space-filling approach. ACM Transactions on Graphics (TOG), 11(1):92?99. ","acronyms":[[64,67],[34,37]],"long-forms":[[38,62]]},{"text":"all characters are included as features; full remove (P4), where all special Twitter features like user names, URLs, hashtags, retweet (RT ) tags, and emoticons are stripped; and replacing Twitter fea-","acronyms":[[136,138],[54,56],[111,115]],"long-forms":[[127,134]]},{"text":"uation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al 2001) distributed by the Linguistic Data Consortium (LDC)3. The RST-DTB","acronyms":[[124,127],[48,55],[135,142]],"long-forms":[[96,122],[24,46]]},{"text":"descriptions 3800 4247 Table 2: Properties of the annotated two subcorpora, genetics (GEN) and computational language (CL)","acronyms":[[86,89],[122,124]],"long-forms":[[76,84],[95,120]]},{"text":"sented by the S node) are extracted. ( 2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as hav-","acronyms":[[94,97],[42,45],[47,50],[89,92]],"long-forms":[]},{"text":"supervised labels to train our user model.  We use Amazon Mechanical Turk (MTurk) to collect data.","acronyms":[[76,81]],"long-forms":[[59,74]]},{"text":"paradigm (Berners-Lee, 2006), which requires the use of uniform recourse identifiers (URIs), the hypertext transfer protocol (HTTP), standard representation formats (such as RDF) and tying to","acronyms":[[126,130],[86,90],[174,177]],"long-forms":[[97,124],[56,84]]},{"text":"provided as input. The crawler generates the  Universal Resource Locator (URL) address for the  index (first) page of any particular date.","acronyms":[[74,77]],"long-forms":[[46,72]]},{"text":"SASKATCHEWAN(d1,d1)?SK(d2,d2) 4 Experiments The use of WordNet (WN) in the term similarity function introduces a before knowledge whose impact","acronyms":[[54,56],[0,2],[10,12]],"long-forms":[[45,52]]},{"text":"ferent corpora, Academia Sinica (AS), Town  University of Hong Kong (HK), Peking Campus (PK), and Microsoft Research Asia (MSR),  each of which has its own definition of a word.","acronyms":[[127,130],[33,35],[69,71],[93,95]],"long-forms":[[102,120],[16,31],[58,67],[74,91]]},{"text":"trieval process. ( Zhou and Wade, 2009b) proposed a Latent Dirichlet Allocation (LDA)based method to model the latent structure of ","acronyms":[[81,84]],"long-forms":[[52,79]]},{"text":"The results of dependency (ZPar-eager, Ours-standard, Ours-PS and Mate-tools) and constituent parsers (BerkeleyParser and ZPar-con) are measured by the unlabeled accuracy score (UAS), labeled accuracy score (LAS) and bracketing f-measure (BF), respectively. ","acronyms":[[208,211],[239,241],[54,61],[178,181],[122,130],[27,37]],"long-forms":[[184,206],[217,229],[152,176]]},{"text":"paradigm (Berners-Lee, 2006), which requires the use of uniform resource identifiers (URIs), the hypertext transfer protocol (HTTP), standard representation formats (such as RDF) and links to","acronyms":[[126,130],[86,90],[174,177]],"long-forms":[[97,124],[56,84]]},{"text":" 4.1 KNN classifications The basic thinking of the K nearest neighbor (KNN) classification algorithm is to use already categorized","acronyms":[[66,69],[5,8]],"long-forms":[[46,64]]},{"text":"2012) for our experiment. It consists of 12 common label: NOUN, VERB, ADJ (adjectives), ADV (adverb), PRON (pronoun), DET (deter-","acronyms":[[69,72],[86,89],[100,104],[116,119]],"long-forms":[[74,83],[91,97],[106,113],[121,126]]},{"text":"icantly better performance than GIZA++. We apart evaluated Support Vector Machines (SVM) classifiers on the same fiirst order feature space and","acronyms":[[83,86],[32,38]],"long-forms":[[58,81]]},{"text":"Semantic Computing Group Cognitive Interaction Technology ? Center of Excellence (CITEC), Bielefeld University, Germany","acronyms":[[82,87]],"long-forms":[[60,80]]},{"text":"The second and last step to generate qwn-ppv(s) consists of propagating over a WordNet graph to achieve a Custom PageRanking Vector (PPV), one for each polarity.","acronyms":[[138,141],[37,44]],"long-forms":[[105,136]]},{"text":"1979). Most tool and resources developed for natural language processing (NLP) of Arabic are designed for MSA.","acronyms":[[75,78],[107,110]],"long-forms":[[46,73]]},{"text":"Table 1: Summary of distance values between the 380 observed A-N pairs and the predictions from each model (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression).","acronyms":[[108,111],[122,125],[61,64],[142,146]],"long-forms":[[112,120],[126,140],[147,179]]},{"text":"that together with the BOW it yields higher accuracy. Their results exhibitions a significant 1 The bilateral rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant responding.","acronyms":[[110,112],[23,26]],"long-forms":[[93,108]]},{"text":"input format. For instance, if the input is annotated with word and PoS (WP), so must be the translation model.","acronyms":[[73,75]],"long-forms":[[59,71]]},{"text":"To estimate the weights ? i in formula (1), we  usage Less Error Rate Training (MERT) algorithm, which is widely used for phrase-based ","acronyms":[[81,85]],"long-forms":[[52,79]]},{"text":"Table 3: The effect of nouveau features on the development set for English. UAS = unlabeled attachment notation; UEM = unlabeled exact match.","acronyms":[[72,75],[106,109]],"long-forms":[[78,104],[112,133]]},{"text":"of not requiring so much copying. On the con-  trary, constraint unification (CU) (Hasida 1986,  Tuda et al 1989), a disjunctive unification ","acronyms":[[78,80]],"long-forms":[[54,76]]},{"text":"AB to letter l ji ? A where A is a regular letter alphabet and AB=A?{B} contains B as an abstract morpheme lancer symbol","acronyms":[[63,65]],"long-forms":[[66,70]]},{"text":"grading.  The Content Assessment Module (CAM) presented in Bailey (2008) and Bailey and Meurers","acronyms":[[41,44]],"long-forms":[[14,39]]},{"text":"2003) ? Minimum Error Rates Training (MERT) (Och, 2003) on a held-out development set, target","acronyms":[[37,41]],"long-forms":[[8,35]]},{"text":"i );3 MFS = Maximal Freq Sequences(d1 i","acronyms":[[6,9]],"long-forms":[[12,37]]},{"text":"navigation3  SCAN was developed initially for the TREC-6  Talked Document Retrieve (SDR) task, which em-  ploys the NIST\/DARPA HUB4 Broadcast News cor- ","acronyms":[[85,88],[117,127],[128,132]],"long-forms":[[58,83],[50,54]]},{"text":"puterization, Tech Report 6-CICC-MT55 (1995)  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182, Rock, Colorado, June 2009.","acronyms":[[136,141],[33,42]],"long-forms":[[95,134]]},{"text":" 2 (Durrett and Klein, 2013) call this error incorrect new (FN). ","acronyms":[[56,58]],"long-forms":[[45,54]]},{"text":" 246  AO = all objects  MO = coupled objects ","acronyms":[[6,8],[24,26]],"long-forms":[[11,22],[29,43]]},{"text":"4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset. ","acronyms":[[77,82],[38,41]],"long-forms":[[84,96]]},{"text":"mance.  We investigated chopping criteria based on a fixed number of mots (FIXES), at  speaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentence ","acronyms":[[76,81],[105,109],[123,128]],"long-forms":[[53,74],[115,121]]},{"text":"(FW), SeekBw (BW), ScrollFw (FS), ScrollBw (BS), Ratechange Widen (RCI), Ratechange Decrease (RCD). ","acronyms":[[97,100],[1,3],[14,16],[29,31],[44,46],[70,73]],"long-forms":[[76,95],[6,12],[19,27],[34,42],[49,68]]},{"text":"Clark, 2010; Sun, 2011b; Li, 2011) are generally trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al, 2005), and perform quite well on correspond essays sets.","acronyms":[[117,120]],"long-forms":[[99,115]]},{"text":" NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). ","acronyms":[[71,74]],"long-forms":[[49,69]]},{"text":"of-the-art dependency parser, the 2014 online version. 4) PPAD Naive Bayes(NB) is the same as PPAD but utilise a generative paragon, as opposed to","acronyms":[[75,77],[58,62],[94,98]],"long-forms":[[63,73]]},{"text":"tities e1, e2, ei,.., eE on translated English documents through aforementioned step, meanwhile, we consider all noun phrases(NP) in original Chinese documents and generate mention candidates","acronyms":[[126,128]],"long-forms":[[113,124]]},{"text":"1.10% 0.20% 18.86% 0    We determine Simple MNP (SMNP) whom  length is less than 5 words and Complete MNP ","acronyms":[[46,50],[100,103]],"long-forms":[[34,44]]},{"text":"Participants in this investigated included 39 kiddies with typical development (TD) and 21 children with autism spectrum disorder (ASD). ASD was di-","acronyms":[[125,128],[74,76],[131,134]],"long-forms":[[99,123],[53,72]]},{"text":"of the first studies to investigate such constancy is Genzel and Charniak (2002), in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per sig-","acronyms":[[139,142]],"long-forms":[[116,137]]},{"text":"Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distancing measures","acronyms":[[87,89],[51,53]],"long-forms":[[65,85],[33,49]]},{"text":"semantic links between NEs extracted from the replies sentence and the question focus word, which encodes the hoped lexical answer type (LAT). We","acronyms":[[139,142],[23,26]],"long-forms":[[118,137]]},{"text":"ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definitive articles), MARCOS (demonstratives), INF (infinitives), ITJ (interjections), NP (noun","acronyms":[[85,87],[109,112],[0,3],[18,21],[33,35],[52,54],[66,68],[131,134],[150,153],[171,173]],"long-forms":[[89,106],[114,128],[5,15],[23,30],[37,49],[56,63],[70,83],[136,147],[155,168],[175,179]]},{"text":" This  suspens ion takes place dur ing un i f icat ion  of  the Flat Concurrent  Pro log (FCP) pred icate   (see below), into which expert  rout ines are ","acronyms":[[90,93]],"long-forms":[[64,88]]},{"text":"ponent Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al.,","acronyms":[[63,65]],"long-forms":[[67,87]]},{"text":"Abstract   In recent years, statistical approaches on  ATR (Automatic Term Recognition) have  achieved well results.","acronyms":[[55,58]],"long-forms":[[60,86]]},{"text":"Adjust of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used feature learning tech-","acronyms":[[110,112]],"long-forms":[[89,108]]},{"text":"approaches. In Lawsuits of the National Conference on Artificial Intelligence (AAAI), pages 419?424. ","acronyms":[[82,86]],"long-forms":[[57,80]]},{"text":"Table 7 showings the effects of merging the sub-  ject accessibility bias with both recency biases and  the restricted souvenir bias (RM). The results in ","acronyms":[[129,131]],"long-forms":[[105,122]]},{"text":"2004). Accordingly, we define in-NE probability  to help delete and create named entities (NE). ","acronyms":[[91,93],[30,35]],"long-forms":[[75,89]]},{"text":"information nuggets?, called Summary Content Units (SCUs), which are short, atomic statements of facts con-","acronyms":[[52,56]],"long-forms":[[29,50]]},{"text":"interested in formalisms which are being  using or have applications in the domain  of machine translation (MT). These can ","acronyms":[[107,109]],"long-forms":[[86,105]]},{"text":"The modifier*\"its\" of TOK188 was converted to a modifier of the form  (POSSBY SCAFFOLD184), which was semantically processed to make TOK188 a  LOCPART (LOCationlPAIiT) SFRAME whwe SEMOBJ (SEMantic ODJect) is  SCAFFOLD1 84; idelltif'ication of the location referents of TOK 188 yieldad the two ","acronyms":[[143,150],[22,28],[71,77],[78,89],[133,139],[168,174],[180,186],[188,196],[197,203],[209,218],[269,272]],"long-forms":[[152,166]]},{"text":"A Named Entity Labeler for Deutsch: Exploiting Wikipedia and Distributional Clusters. In Proceedings of the Conference on Language Resources and Assessed (LREC), pages 552?556, La Valletta, Malta.","acronyms":[[156,160]],"long-forms":[[121,139]]},{"text":"The straight case is the one mentioned above, treating all ingredients on the PARTS listed equally (EQUAL). As a second op-","acronyms":[[95,100]],"long-forms":[[86,93]]},{"text":"61 Tables 2: Three sort of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP","acronyms":[[78,81],[98,101],[124,127],[141,144]],"long-forms":[[84,89],[72,76],[130,139],[104,113]]},{"text":"(? baseline?) and MT (Madnani et al, 2012). RAE","acronyms":[[18,20],[44,47]],"long-forms":[[22,32]]},{"text":"impulses are possibly found. It is realized with very  simple space management transition networks (SMTNs),  in which $EXP, a distinguished symbol on an arc, ","acronyms":[[100,105]],"long-forms":[[62,98]]},{"text":"Figure 3: System I performance for each relationship (CC=CAUSE-EFFECT, IA=INSTRUMENTAGENCY, PP=PRODUCT-PRODUCER, OE=ORIGIN-ENTITY, TT=THEME-TOOL,","acronyms":[[50,52],[67,69],[88,90],[109,111],[127,129]],"long-forms":[[53,65],[70,86],[91,107],[112,125],[130,140]]},{"text":"Lexical feature show a much more mixed result. Type?Token Ratio (TTR) is only important for documented classification, whereas most of the","acronyms":[[66,69]],"long-forms":[[48,64]]},{"text":"Table 5: Final test accuracies for Chinese. UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score.","acronyms":[[78,81]],"long-forms":[[84,105]]},{"text":"4.2 Cognate based features Dictionaries mostly fail to returns translation entries for named entities (NEs) or specialized terminology.","acronyms":[[102,105]],"long-forms":[[86,100]]},{"text":"are contained in a XML file and each query  consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Topic  question(DESC),Topic Narrative(NARR) and ","acronyms":[[90,93],[19,22],[107,112],[130,134],[152,156]],"long-forms":[[83,88],[101,106],[142,151]]},{"text":"We begin by describing how for our typical model, the Viterbi EM objective can be formulated as a mixed integer quadratic programming (MIQP) problem with nonlinear constraints (Figure 2).","acronyms":[[135,139]],"long-forms":[[98,133]]},{"text":"SEMANTICS. The main component of SeSyn is a rule system (the syntax) which transforms the Semantic Analysis  (SA) of any bestowed sentence into a Surface Structure (SS) of that sentences. The SAs constituted meanings ina higher order ","acronyms":[[162,164],[33,38],[110,113],[188,191]],"long-forms":[[143,160],[90,107]]},{"text":"341  Proceedings of the 2014 Conference on Empirical Means in Natural Linguistic Processing (EMNLP), pages 1070?1080, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"The paper first provides a brief overview of Lexical Functional Grammar, and the Penn Arabic Treebank (ATB). The next section presents","acronyms":[[103,106]],"long-forms":[[86,101]]},{"text":" 1 Introduction Natural Language Inference (NLI), i.e. the task of determining whether an NL hypotheses can be in-","acronyms":[[44,47],[90,92]],"long-forms":[[16,42]]},{"text":"sided brevity penalty (C = 0.01). Table 2 shows the average compression rates (CompR) for McDonald (2006) and our model (STSG) as well as their perfor-","acronyms":[[79,84],[121,125]],"long-forms":[[60,77]]},{"text":"76 4 Multi-media Information Networks A Multimedia Information Network (MINet) is a structured collection made up of a set of multimedia documents (e.g., texts and images) and links between these documents.","acronyms":[[72,77]],"long-forms":[[40,70]]},{"text":"segmentation While the model structure is reminiscent of a factorial hidden Markov modelling (HMM), there are notable differences that prevent the direct application of","acronyms":[[90,93]],"long-forms":[[69,88]]},{"text":" Figure 1: Graphical representation of the phrase pair topic (PPT) model. ","acronyms":[[62,65]],"long-forms":[[43,60]]},{"text":"Abstract We consider the trouble of correcting errors made by English as a Second Language (ESL) writers and addressing two issues that are essen-","acronyms":[[92,95]],"long-forms":[[62,90]]},{"text":" In order to rate models M1, M2, M3 in compares to the vector space model (VERSUS) using MSTs, STs and CTs as alternative hierarchi-","acronyms":[[77,79],[87,91],[93,96],[101,104]],"long-forms":[[57,69]]},{"text":"functions to SGML-mark the input.  Prompt Partial Parser (FPP) .  The ultimate ","acronyms":[[56,59],[13,17]],"long-forms":[[35,54]]},{"text":"BP = log (Prob(N2 | N1)) (2) ? Pointwise Mutual Info (PMI): Defined as a measure of how collocated deux words are,","acronyms":[[61,64],[0,2]],"long-forms":[[31,59],[5,28]]},{"text":"Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos","acronyms":[[128,130],[40,43],[167,170]],"long-forms":[[112,126],[0,38],[136,165]]},{"text":"These  probabilities could be estimated from training cases  with Maximum Likelihood Evaluations (MLE). Let l ","acronyms":[[97,100]],"long-forms":[[66,95]]},{"text":"corresponding to the point P.  Once a n  object has been added to the geometric model by specifying values  for its GSTART, GSIZE, and ROTN (rotation), the geometric coordinates for any  location on the object may be obtained by calling the funtion EXECLOCA with the ","acronyms":[[135,139],[249,257],[116,122],[124,129]],"long-forms":[[141,149]]},{"text":" We trained a dialogue independent (DI) class based LM and dialog dependent (DD) grammar basis LM. In all LMs n is set to 3.","acronyms":[[75,77],[34,36],[93,95],[104,107]],"long-forms":[[64,73],[14,32]]},{"text":"and EN-DE) with token frequency, sense distribution and most frequent translations ordered by the corresponding senses (T = temporal, CO = concession, CT = contrast). ","acronyms":[[134,136],[151,153],[4,9]],"long-forms":[[139,149],[156,164],[124,132]]},{"text":"tion string in discourse: PER, GPE, ORG, LOC,  FAC, NPER (NOMINAL PER), NGPE, NORG,  NLOC, NFAC, PPER (PROUNOUN PER),  PGPE, PORG, PLOC, and PFAC.","acronyms":[[97,101],[26,29],[31,34],[36,39],[41,44],[47,50],[52,56],[72,76],[78,82],[85,89],[91,95],[119,123],[125,129],[131,135],[141,145]],"long-forms":[[103,115],[58,69]]},{"text":"The seed set is compiled from popular mental and emotional state dictionaries, including the Profile of Mood States (POMS) (McNair et al.,","acronyms":[[117,121]],"long-forms":[[93,115]]},{"text":"segment of a conversation. We will therefore use the  terms initiating conversational participant (ICP) and other  conversational participant(s) (OCP) to distinguish the initi- ","acronyms":[[99,102],[146,149]],"long-forms":[[60,97],[108,141]]},{"text":"figure 1. To preventing confusion, we refer to this basic  unit throughout as a Temporal Unit (TU). ","acronyms":[[90,92]],"long-forms":[[75,88]]},{"text":" We have three versions of reference summaries based on summarization ratio(SR): 10%, 15% and 20% respectively.","acronyms":[[76,78]],"long-forms":[[56,75]]},{"text":"Tables 79 showings, for each emotion categorize, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets.","acronyms":[[98,100]],"long-forms":[[82,96]]},{"text":"lined previously, in that its intent is entity extract from raw news wires from the news entities Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input","acronyms":[[121,124],[164,167]],"long-forms":[[99,119]]},{"text":"hdaume,marcu \u0001 @isi.edu Abstract Entity detection and tracking (EDT) is the task of identifying textual cites","acronyms":[[64,67]],"long-forms":[[33,62]]},{"text":" 3 Dataset & Experimental Setup We use the First Certificate in English (FCE) ESOL examination scripts2 (upper-intermediate level as-","acronyms":[[73,76]],"long-forms":[[43,71]]},{"text":"methods to identify such targets. The first mode depends on identifying noun groups (NG). We con-","acronyms":[[87,89]],"long-forms":[[74,85]]},{"text":"rule in a CFG. It can  therefore fill the ACity (ArrivalCity) or the  DCity (DepartureCity) slot, and instantiate a ","acronyms":[[42,47],[10,13],[70,75]],"long-forms":[[49,60],[77,90]]},{"text":"distinctive, we regard the sentiment  classification as a sequence labeling problem and  use conditional random field (CRFs) model to  capture the relation between two neighbor ","acronyms":[[122,126]],"long-forms":[[96,120]]},{"text":"etiquette, lemmatization, etc.). For corpus query, we employ the Corpus Query Transformer (CQP) (CWB; Evert, 2004) which works on the basis of","acronyms":[[87,90]],"long-forms":[[63,85]]},{"text":"We also mark conjunct clauses with the feature nosubj if they are neither headed by an imperative nor contain a child node with the grammatical function SB (subject) or EP (expletive). This is useful in order to correctly parse","acronyms":[[153,155],[169,171]],"long-forms":[[157,164],[173,182]]},{"text":"3  Chinese NER Utilize CRFs Model Integrating Multiple Specifics  Besides the text feature(TXT), simplified part-ofspeech (POS) feature, and small-vocabulary-","acronyms":[[88,91],[11,14],[21,25],[120,123]],"long-forms":[[75,87],[105,118]]},{"text":" 1 Introduction Machine Translation (MT) can be addressed as a structured forecasting task (Brown et al.,","acronyms":[[37,39]],"long-forms":[[16,35]]},{"text":"CTexT. 2011. Afrikaans WordNet. Centro for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.","acronyms":[[60,65],[0,5]],"long-forms":[[32,58]]},{"text":"Features. Onto Proceedings of the 21st Conference on Computational Linguistics (COLING). ","acronyms":[[78,84]],"long-forms":[[51,76]]},{"text":"semantic F1 of 85.63 for English.  Time Expression Identification (TEI) and Normalization (TEN): We use the time module","acronyms":[[67,70]],"long-forms":[[35,65]]},{"text":"non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions.","acronyms":[[86,89]],"long-forms":[[64,84]]},{"text":"Chiang, Hua. Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt. Edu      Abstract This work outline the participation of the University of Texas Sante Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenges.","acronyms":[[190,198]],"long-forms":[[136,188]]},{"text":"We have adapted the listings from Rambow et al(2006) for Arabian, and call it here CORE12. It involves the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), collaboration (C), particle","acronyms":[[149,151],[162,164],[180,182],[194,197],[218,221],[78,84]],"long-forms":[[138,147],[118,122],[128,132],[154,160],[167,178],[185,192],[200,216],[224,235],[241,252]]},{"text":"Aux this end, we employ the  27 Adaptive Hierarchical Density Histograms (AHDH) as visual feature vectors, due to the doing that they  have illustrated discriminative power between binary complex drawings (Sidiropoulos et al.,","acronyms":[[73,77]],"long-forms":[[31,71]]},{"text":"Given an OOV word a and its IV version b we have extracted character transformations rules from a to b use the longest commons substring (LCS) algorithm (See Table 5).","acronyms":[[138,141],[9,12],[28,30]],"long-forms":[[112,136]]},{"text":"of units. '  i Note that unlike RST, Veins Theory (VT) is not  preoccupied with the genera of relations which hold ","acronyms":[[51,53],[32,35]],"long-forms":[[37,49]]},{"text":" English For Englishman dataset, we follow the standard hyphenate of Penn Treebank (PTB), using sections 2-21 for training, section 22 as de-","acronyms":[[78,81]],"long-forms":[[63,76]]},{"text":"bines the output of all the 13 base models introduced previously. We implemented the meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results","acronyms":[[132,136]],"long-forms":[[107,130]]},{"text":"Verb classification performance (exact, recall, and F for MRS are macro-averaged). Global accuracy supplemented by 95% binomial confidence intervals (CI). ","acronyms":[[153,155],[62,64]],"long-forms":[[131,151]]},{"text":"Multi-document individual name resolution, Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.","acronyms":[[124,127]],"long-forms":[[81,122]]},{"text":"the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.  European Language Resources Association (ELRA). ","acronyms":[[148,152],[71,75]],"long-forms":[[107,146],[36,53]]},{"text":"derivations obtained from all discontinue states in the chart.  3.6 Minimal Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a","acronyms":[[81,84]],"long-forms":[[61,79]]},{"text":"We conduct an extrinsic evaluation to compare  the different versions of ArSenL on the task of  subjectivity and sentiment analysis (SSA). We ","acronyms":[[133,136],[73,79]],"long-forms":[[96,131]]},{"text":"ious learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas.","acronyms":[[76,79],[117,120]],"long-forms":[[51,74]]},{"text":"representing natural language in a traversable graph, composed of propositions and their semantic interrelations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation","acronyms":[[148,151]],"long-forms":[[117,146]]},{"text":"various respects. While CKY requires a grammar in Chomsky Normal Form (CNF), LSCP takes an ordinary PG grammar, since no equivalent of the","acronyms":[[71,74],[24,27],[77,81],[100,102]],"long-forms":[[50,69]]},{"text":"859  Proceedings of the 13th Annually Meeting of the Especial Interest Group on Discourse and Dialogue (SIGDIAL), pages 217?226, Seoul, South Korea, 5-6 July 2012.","acronyms":[[101,108]],"long-forms":[[51,99]]},{"text":"SEPA parameters are S = 13, 000, N = 20. For both rows, SEPA results for the in-domain (gauche) and adaptation (middle) scenarios are compared to the trusting (CB) and minimum length (ML) baselines. The","acronyms":[[183,185],[0,4],[55,59],[159,161]],"long-forms":[[167,181]]},{"text":"score word pairs for relatedness (on a scale of 0 to 10), which is in contrast to the similarity judgments requested of the Miller and Charles (MC) and Rubenstein and Goodenough (RG) participants.","acronyms":[[144,146],[179,181]],"long-forms":[[124,142],[152,177]]},{"text":"of Machine Translation and present an implemetation of a morphological analyser for Amharic using Xerox Finite State Tools (XFST). The different","acronyms":[[124,128]],"long-forms":[[98,122]]},{"text":" Since we planned to eventually test our algorithms in  word recognition on the Resource Management (RM)  database, our phone classification experiments were also ","acronyms":[[101,103]],"long-forms":[[80,99]]},{"text":"Name Discrimination by Clustering Similar Contexts, Proceedings of the World Wide Web Conference (WWW). ","acronyms":[[98,101]],"long-forms":[[71,85]]},{"text":"He washed it. With Kamp's  discourse representation theory (DRT) (Kamp 1981; Kamp and Reyle 1993) a discourse  representation structure (DRS) in which the reference to the pronoun he is constrained ","acronyms":[[60,63],[137,140]],"long-forms":[[27,58],[100,135]]},{"text":"AJCL = AmericanJournal of Computational  Linguistics (1974-present)  SNLP = Researches in Natural Language Processin~  (Cambridge University Press Monograph ","acronyms":[[69,73],[0,4]],"long-forms":[[76,114],[7,52]]},{"text":"marn et al 2007). The training procedure usually employs an expectation maximization (EM) procedure and the resulting transducer can be used to","acronyms":[[86,88]],"long-forms":[[60,84]]},{"text":"fying the native language based on the manner of speaking and writing a second language is borrowed from Second Language Acquisition (SLA), where this is known as vocabulary conveyance.","acronyms":[[134,137]],"long-forms":[[105,132]]},{"text":" 2. Tezt routing, tezt fdter\/n9, and SDI (selective dissemination of information): These terms  refer to a loose collection of text classification tasks such as managing personal electronic mail, ","acronyms":[[37,40]],"long-forms":[[42,65]]},{"text":" 4 Algorithms 4.1 Topic?sentence graph matching (GM) We treat a sentence and a topic as erections.","acronyms":[[49,51]],"long-forms":[[33,47]]},{"text":"cation), TMP (time), DIS (discourse connectives), PRP (aim) or DIR (direction). Negations (NEG) and modals (MOD) are also marked.","acronyms":[[95,98],[9,12],[21,24],[50,53],[67,70],[112,115]],"long-forms":[[84,93],[26,35],[55,62],[72,81],[104,110]]},{"text":" The communal modeling is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.","acronyms":[[81,86]],"long-forms":[[58,79]]},{"text":" 3. Ephemeral Phrases (TRP) We hypothesize that a more cohesive essay, being easier for a","acronyms":[[26,29]],"long-forms":[[4,24]]},{"text":" 1 Introduction Language identification (LangID) is the problem of determining what natural language a document is written in.","acronyms":[[41,47]],"long-forms":[[16,39]]},{"text":"this results in minimum expected word error rate (WER) hypothesis (Mangu et al, 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim","acronyms":[[122,125]],"long-forms":[[102,120]]},{"text":"Note that the autoPS heuristic for ranks senses is a more exact estimator than the WordNet most?frequent?sense (MFS). ","acronyms":[[116,119],[14,20]],"long-forms":[[95,114]]},{"text":"adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). ","acronyms":[[119,122]],"long-forms":[[103,117]]},{"text":"Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification employs the Ontology Web Language (OWL) to list valid values for objects of the defined categories. ","acronyms":[[123,126],[7,10]],"long-forms":[[100,121]]},{"text":" What we describe is part of a theory of language (knowledge and processing)  called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge ","acronyms":[[99,101]],"long-forms":[[85,97]]},{"text":"- coordination (COORD) : traite les c,'~s imples de  coordination,  - statistique (STAT) : utilis6 sur des s6quences qu'il  est impossible de d6sambigui'ser A l'aide ","acronyms":[[83,87],[16,21]],"long-forms":[[70,81],[0,14]]},{"text":"produced by a transliteration system 1. Word Accuracy in Top-1 (ACC) Also known as Word Error Rate, it measures correct-","acronyms":[[64,67]],"long-forms":[[45,53]]},{"text":" (5) Rank Value:  i. Top Rank (T-Rank): The rank of snippet  that first contains the candidate.","acronyms":[[31,37]],"long-forms":[[21,29]]},{"text":"machine learning algorithm is used to discover the morph set of the parlance in question, using less description length (MDL) as an optimization criterion.","acronyms":[[124,127]],"long-forms":[[96,122]]},{"text":"belief updating model. In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approach","acronyms":[[81,85]],"long-forms":[[40,79]]},{"text":" We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al, 2005), for updating the weighted.","acronyms":[[43,47]],"long-forms":[[49,69]]},{"text":"637  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32, Sofia, Bulgaria, August 9, 2013.","acronyms":[[70,77]],"long-forms":[[36,68]]},{"text":"1 (TICCL) we increasingly developed in prior projects is now TICCLops (TICCL online processing system). TICCLops is a fully","acronyms":[[58,66],[3,8],[101,109]],"long-forms":[[68,98]]},{"text":"ging. We ran this set of experimenting on the parte of Chinese Treebank 4 (CTB-4)6. Ninety percent of the","acronyms":[[72,77]],"long-forms":[[52,70]]},{"text":" 1 Introduction Use natural language processing (NLP) techniques to mine software corpora such as code com-","acronyms":[[51,54]],"long-forms":[[22,49]]},{"text":"mantic content Ks, sends a message m to R and R  interprets m as meaning CR. CS = cn is a neces-  sary condition for this turn of communication to ","acronyms":[[77,79],[15,17],[73,75]],"long-forms":[[82,87]]},{"text":"proper textual evidences. We wording this task as an Integer Linear Programming (ILP). Instead of","acronyms":[[83,86]],"long-forms":[[55,81]]},{"text":"TV programs they watch.  Collaborative filtering (CF) (Resnick et al, 1994; Breese et al, 1998) and content-based (or","acronyms":[[50,52],[0,2]],"long-forms":[[25,48]]},{"text":" 5.2 Named Entities As standard named entity acknowledgment (NER) systems do not capture categories that are relevant to","acronyms":[[58,61]],"long-forms":[[32,56]]},{"text":" We first examined three randomly selected portions of the listing  in the American Heritage Word Frequency Book (AHWFB).side by side  with the corresponding entry lists of the American Heritage School Dic- ","acronyms":[[114,119]],"long-forms":[[75,112]]},{"text":"during the last two years. In this first of four installerments, the  Association of Data Processing Service Organizations, Inc. (ADAPSO) is  considered with respect to its membership, charter, organization and ","acronyms":[[130,136]],"long-forms":[[70,122]]},{"text":"Sparse Lexicalised features and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268?275.","acronyms":[[143,148],[53,56]],"long-forms":[[88,141]]},{"text":"Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexi-cons accessible to average users. 1 Introduction Speech recognition (SR) systems use pronuncia-tion lexicons to map words into the phoneme-like units used for acoustic modeling. Text-to-speech (TTS) systems also make use of pronunciation lexicons, both internally and as ?","acronyms":[[235,237],[360,363]],"long-forms":[[215,233],[344,358]]},{"text":"attributes of the observation vectors and a  specific label).we can represent the input-output  pairs via joint feature map (JFM)  1","acronyms":[[125,128]],"long-forms":[[106,123]]},{"text":"counted through specifier words. A Malay case, where biji is the count classifier (CL) for fruit, is given in (1).","acronyms":[[88,90]],"long-forms":[[76,86]]},{"text":"Sofa. The viewer makes use the open-source library Java Speeches Toolkit (JSTK)5. ","acronyms":[[72,76]],"long-forms":[[51,70]]},{"text":"Fazly and Stevenson, 2007), our approaching is to compare the context vector of a VNC with the composed vector of the sneezing and noun (V-N) component unit of the VNC when they occur in iso-","acronyms":[[130,133]],"long-forms":[[115,128]]},{"text":"The word lattices of the HUB-1 corpus are geared acyclic graphs in the HTK Standards Lattice Format (SLF), consisting of a set of vertices and a set of edges.","acronyms":[[102,105],[25,30],[73,76]],"long-forms":[[77,100]]},{"text":"CrossT values). The regression model predicted  Figure 4: ST alignment crossings (CrossS), as generated  when checking the ST against the TT ","acronyms":[[82,88],[0,6],[58,60],[123,125],[138,140]],"long-forms":[[71,80]]},{"text":"1 Introduction Newswire text has long been a primary target for natural language processing (NLP) techniques such as information extraction, summarization, and ques-","acronyms":[[93,96]],"long-forms":[[64,91]]},{"text":"string is a false supportive (FP). Each gold standard gene mentioning is counted as a false negative (FN) if it is not identified by the approach.","acronyms":[[97,99],[28,30]],"long-forms":[[81,95],[12,26]]},{"text":"Specifically, we achieve a roughly 10% improvement in precision on text from the information technology (IT) business presse via post hoc rule-based error reducing.","acronyms":[[105,107]],"long-forms":[[81,103]]},{"text":"on the labels from DSlabels+MinCut: (4) MaxEnt with named entities (NE); (5) MaxEnt with NE and semantic (SEM) features; (6) CRF with NE; (7) MaxEnt with NE and sequential (SQ) features;","acronyms":[[106,109],[68,70],[89,91],[125,128],[134,136],[142,148],[154,156],[173,175],[40,46]],"long-forms":[[96,104],[52,66]]},{"text":"grammatical features from a diachronic corpus of academic English, and visualise our extraction results with Structured Parallel Coordinates (SPC), a tool for the visualisation of structured multidi-","acronyms":[[142,145]],"long-forms":[[109,140]]},{"text":"objectives translation: the gunman was killing by police .  The Penn English Treebank (PTB) (Marcus et al, 1993) is our source of syntactic information, largely","acronyms":[[82,85]],"long-forms":[[59,80]]},{"text":"al., 2005) to promptly find possible answers, given the relational conjunction (RC) of the question. ","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":"The open schoolroom  word (host) tends to be uninflected, and only  the light verb (LV) carries tense, contract  and aspect markers.","acronyms":[[79,81]],"long-forms":[[67,77]]},{"text":"2008) and hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007) utilised the executing in (Turian et al, 2010), Hellinger PCA (H-PCA) (Lebret and Collobert, 2014) and our connective-based representative (Bllip).","acronyms":[[143,148]],"long-forms":[[128,141]]},{"text":"contrast, L\/RMC = left\/right-most char,  L\/RMW = left\/right-most word, VS = vowel  sequences, HYPH = hyphenation, CASE =  case, PM = parenthesized material.","acronyms":[[94,98],[10,15],[41,46],[71,73],[114,118],[128,130]],"long-forms":[[101,112],[18,38],[49,69],[76,92],[122,126],[133,155]]},{"text":"vv@iiit.ac.in Abstract Acknowledgment of Named Entities (NEs) is a difficult process in Indian languages like Hindi,","acronyms":[[54,57]],"long-forms":[[38,52]]},{"text":"Syntactic Tree  Ambiguous Semantic  Expression (EFL)  Unambiguous Semantic ","acronyms":[[48,51]],"long-forms":[[36,46]]},{"text":"2-21. Onto Chinese, we experienced on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conver-","acronyms":[[69,73]],"long-forms":[[47,67]]},{"text":"  Abstract  Named entity recognition (NER) is nowadays an important task, which is responsi-","acronyms":[[38,41]],"long-forms":[[12,36]]},{"text":" 3.3 Linear-chain CRF for Extraction The aligning CRF (AlignCRF) model describe in Section 3.1 is able to predict labels for a text","acronyms":[[56,64],[18,21]],"long-forms":[[41,54]]},{"text":" 1 Introduction  Lexical Acquisition (LA) processes strongly rely on  basic assumptions embodied by the source informa- ","acronyms":[[38,40]],"long-forms":[[17,36]]},{"text":"systematic way.  The MIME (Managing Information in Doctor Emergencies)1 project is developing technology to","acronyms":[[21,25]],"long-forms":[[27,70]]},{"text":"though domain sublanguages are characterized by specific vocabularies, a well-defined border between specific sublanguages (SLs) and general language (GL) vocabularies is difficult to establish","acronyms":[[124,127],[151,153]],"long-forms":[[110,122],[133,149]]},{"text":"The thesis will examine two main areas: modelling cohesive devices within sentences and modelling discourse relations (DRs) across sentences.","acronyms":[[119,122]],"long-forms":[[98,117]]},{"text":" ? WHNP_NN_IN  Syntactic Parse Trees (TP)  72","acronyms":[[38,40],[3,13]],"long-forms":[[25,36]]},{"text":" Throughout addition, the user can supply relevance judgements on  any document by clicking Rel (relevant), NRel (not rel-  evant), or PRel (admittedly relevant).","acronyms":[[100,104],[84,87],[127,131]],"long-forms":[[106,121],[89,97],[133,150]]},{"text":"known that Figure 1(a) can be represented by an equivalent hierarchical Chinese Restaurant Process (CRP) (Aldous, 1985) as in Figure 1(b). ","acronyms":[[100,103]],"long-forms":[[72,98]]},{"text":"cation and domain in question, can provide an efficient means to deal with a number of natural language processing (NLP) task.","acronyms":[[116,119]],"long-forms":[[87,114]]},{"text":"5.1 Data  We have two sorting of training data from general  realms: Labeled Data (LD) and Unlabeled Data  (UD).","acronyms":[[81,83],[106,108]],"long-forms":[[67,79],[89,103]]},{"text":"Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al, 2000; Moore and Paris,","acronyms":[[139,142]],"long-forms":[[110,137]]},{"text":" 1 Introduction Question Replying (QA) from structured data, such as DBPedia (Auer et al.,","acronyms":[[36,38],[70,77]],"long-forms":[[16,34]]},{"text":"X at Y (verb phr:~se, noun phrase)  X a.m. (corot?rand word)  After step (I) is finished, steps (II)-(IV) are repeated  recursively.","acronyms":[[97,99]],"long-forms":[[77,88]]},{"text":"Afterward, alternative estimation strategies for unsupervised learnt have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005).","acronyms":[[126,128]],"long-forms":[[102,124]]},{"text":"structural and behavioral parts have to be absolutely specified at this plano.  2.3 Overshadow Modeling Framework (EMF) We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-","acronyms":[[107,110],[130,133]],"long-forms":[[79,105]]},{"text":"67  Proceedings of the 2014 Conference on Experimental Modes in Natural Language Processing (EMNLP), pages 189?195, October 25-29, 2014, Doha, Qatar.","acronyms":[[92,97]],"long-forms":[[42,90]]},{"text":"Nostra work is most similar to the content selection method of the multimedia speaking system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004).","acronyms":[[95,98]],"long-forms":[[100,132]]},{"text":"1.10% 0.20% 18.86% 0    We define Simple MNP (SMNP) whose  length is less than 5 words and Complete MNP ","acronyms":[[46,50],[100,103]],"long-forms":[[34,44]]},{"text":"most blogged about articles? of the New York Moments (NYT)1. ","acronyms":[[52,55]],"long-forms":[[36,50]]},{"text":"Specifically, the groups include children with ASD with language impairment (ALI); ASD with no language impairment (ALN); SLI only; and typically formulated (TD), which is","acronyms":[[116,119],[47,50],[77,80],[122,125],[159,161]],"long-forms":[[83,114],[56,75],[137,157]]},{"text":"namely the overall accuracy (Total-A) and the recall with respect to in-vocabulary words (IV-R),  OOV words (OOV-R) or multi-POS words (MTR).","acronyms":[[109,114],[90,94],[125,128],[136,139]],"long-forms":[[98,107],[69,82]]},{"text":"ayman,R.Gaizauskas@dcs.shef.ac.uk Abstract Disambiguating named entities (NE) in running text to their correct interpretations in a specific knowledge base (KB) is an important problem in NLP.","acronyms":[[74,76],[157,159],[188,191]],"long-forms":[[58,72],[141,155]]},{"text":"based chunking; 3. MEMM-based word segmenter with Support Vector Equipments (SVM)-based chunking.","acronyms":[[75,78],[19,23]],"long-forms":[[50,73]]},{"text":"? Negat ive Precis ion ( I~P)  :  * F -measure  (FM) ? ( ~2+I)?PP?PR \/32 ?","acronyms":[[49,51],[25,28]],"long-forms":[[36,46]]},{"text":"  Abstract  In an attempt to extend Penn Discourse Tree Bank (PDTB) \/ Turkish Discourse Bank (TDB)  style annotations to spoken Turkish, this paper presents the first attempt at annotating the explicit ","acronyms":[[62,66],[94,97]],"long-forms":[[36,60],[70,92]]},{"text":" 2.6 Adverb  Group (AdvG)   Adverb group (AdvG) is used in the realization  of several circumstantial functions given in Sec- ","acronyms":[[42,46],[20,24]],"long-forms":[[28,40]]},{"text":"The resulting unit denominates a concept which belongs to the language for special purposes (LSP). ","acronyms":[[93,96]],"long-forms":[[62,91]]},{"text":"Knowledge-free initiation of morphology using latent semantic analysis. In Lawsuits of the Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67?72, Lisbon, Portugal.","acronyms":[[139,149]],"long-forms":[[93,137]]},{"text":"without any adaptation.  Laplacian SVM (L-SVM) This is a semisupervised learning method based on label","acronyms":[[40,45]],"long-forms":[[25,38]]},{"text":"Silhouette 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff system, REF = reference). OOVs and their trans-","acronyms":[[96,99],[32,35],[46,50],[70,77],[114,118]],"long-forms":[[102,111],[38,44],[53,61],[80,87]]},{"text":"the procedures that manipulate an  intermediate representat ion of the query,  cal led the Meta Query Language (MQL). ","acronyms":[[112,115]],"long-forms":[[91,110]]},{"text":"We suppose that the possessor of a noun phrase is  the subject or the noun phrase's nearest opic that  has a semantic mark,er HUM (human) or a seman-  tic marker AN I (animal).","acronyms":[[126,129],[162,164]],"long-forms":[[131,136]]},{"text":"initiates algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a little number of linguistic","acronyms":[[116,119]],"long-forms":[[84,114]]},{"text":"the feature structural which are associated with each node, which prohibiting certain compositions, are not shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intended, as","acronyms":[[158,162]],"long-forms":[[141,156]]},{"text":"As first step we try to map the linguistic triple  into an ontology triple, by using an adaptation of  Aqualog?s Relation Similarity Service (RSS).  ","acronyms":[[142,145]],"long-forms":[[113,140]]},{"text":"egorization. ACM Transactions on Information Plan (TOIS), 12(3):233?251. ","acronyms":[[54,58],[13,16]],"long-forms":[[17,52]]},{"text":"al., 2005; Joachims et al, 2009) formulation, as indicated in Optimization Problem 1 (OP1), to learnt a weight vector w.","acronyms":[[82,85]],"long-forms":[[58,80]]},{"text":"  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"several systems have been developed such as BITS (Bilingual Internet Proof Searched) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation","acronyms":[[107,114],[44,48],[163,169]],"long-forms":[[116,135],[50,80],[171,193]]},{"text":"274  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192?1202, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]]},{"text":"Table 1: Purity (Pu), collocation (Co), and F1 scores of our modelling and the syntactic baseline in percent. Performance on arguments (Argn), adjuncts (ArgM), and overall result (Arg*) are indicated separately. ","acronyms":[[149,153],[35,37],[177,180],[132,136],[17,19]],"long-forms":[[121,130],[22,33],[9,15]]},{"text":" 2 Swedish FrameNet Swedish FrameNet (SweFN), which launch in 2011, is part of Swedish FrameNet++ (Borin et al.,","acronyms":[[38,43]],"long-forms":[[20,36]]},{"text":"Abstract We contour a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual","acronyms":[[67,69]],"long-forms":[[52,65]]},{"text":"versational participants. This type of HMM is called a speaker HUM (SHMM) and has been successfully utilizing to model two-party conversa-","acronyms":[[68,72],[39,42]],"long-forms":[[55,66]]},{"text":"Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing scholar, SDE=Software Evolution Mechanics, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer, WN-lemma=WordNet lemmatization, McCCJ=McClosky-Charniak-Johnson","acronyms":[[138,141],[73,75],[94,97],[173,180],[199,205],[222,230],[249,257],[281,286]],"long-forms":[[142,171],[76,92],[98,125],[181,197],[206,220],[231,247],[258,279],[287,312]]},{"text":"boring dependency structures. CC = coordinate concatenate, LA = left adjoining, and RA = right adjoining.","acronyms":[[59,61]],"long-forms":[[64,78]]},{"text":"prim. = primary source; C06?C09 = CoNLL 2006?2009; I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip","acronyms":[[68,70],[90,92],[112,114],[128,131],[156,159],[189,191],[194,196],[197,200]],"long-forms":[[73,88],[95,103]]},{"text":"Chinese Semantic Dictionary (CSD) for  Chinese-English machine translation, the  Chinese Concept Dictionary (CCD) for  cross-language text processing, the multi-level ","acronyms":[[109,112],[29,32]],"long-forms":[[81,107],[0,27]]},{"text":"Translation retrieval (TR) is a description of this process of electing from the TM a set of translation records (TRecs) of maximum L1 similarity to a given entry.","acronyms":[[115,120],[23,25],[82,84]],"long-forms":[[94,113],[0,21]]},{"text":"Having presented the sequent parser, we now shows its embedding in the learning algorithm GraSp (Grammar of Speech). ","acronyms":[[89,94]],"long-forms":[[96,113]]},{"text":" 1 Introduction  Statistical Machine Translation(SMT) is presently the state of the art solution to the machine ","acronyms":[[49,52]],"long-forms":[[17,47]]},{"text":"has been discussed extensively in various formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.","acronyms":[[128,131],[62,65],[89,91]],"long-forms":[[104,126]]},{"text":"(Bikel et al, 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised","acronyms":[[132,135]],"long-forms":[[107,130]]},{"text":"state%1:03:00; ? AB = abstraction%1:03:00 \\ (event%1:03:00 ? sate%1:03:00).","acronyms":[[17,19]],"long-forms":[[22,41]]},{"text":"Entity lier (EL) recognizes mentions in a text and associates them to their corresponding entries in a knowledge base (KB), for examples, Wikipedia","acronyms":[[122,124],[16,18]],"long-forms":[[106,120],[0,14]]},{"text":"PCFG score (SP ) 49.5 50.0 TSG score (ST ) 49.5 49.7 Charniak score (SC) 50.0 50.0 l + S3 61.0 64.3","acronyms":[[69,71],[0,4],[12,14],[27,30],[38,40]],"long-forms":[[62,67]]},{"text":"Since by default we returns up to RT=100  search engine results to user, we will extracts the  top RQ=RT\/(#newQuery+1) entrance from results of  each new query and original query.","acronyms":[[97,99],[33,35]],"long-forms":[[100,115]]},{"text":"known formalisms uch as Head Driven Expressions  Structure Grammar (HPSG), Lexical Functional  Grammar (LFG) or Slot Grammars (SG), because  SUG allowed a modular and computational ","acronyms":[[122,124],[63,67],[99,102],[136,139]],"long-forms":[[107,120],[24,61],[70,97]]},{"text":"We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by utilise a Helped Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).","acronyms":[[132,135]],"long-forms":[[108,130]]},{"text":"Pr(f |e) Pr(e) (2) where Pr(f |e) is the translation model and Pr(e) is the goals vocabulary model (LM). This ap-","acronyms":[[99,101]],"long-forms":[[83,97]]},{"text":"Revue des Sciences de l?Education (RSE) ? Traduction, Terminologie et R?daction (TTR) ?","acronyms":[[81,84],[35,38]],"long-forms":[[42,79],[0,33]]},{"text":"Table 6 shows the number of true positive (TP), truthful negative (TN), false positive (FP) and false bad (FN) of models for the stocks. ","acronyms":[[108,110],[43,45],[63,65],[84,86]],"long-forms":[[92,106],[33,41],[48,61],[68,82]]},{"text":"simardm@iro.umontreal.ca Abstract The term translation spotting (TS) refers to the task of identifying the target-language (TL)","acronyms":[[65,67]],"long-forms":[[43,63],[107,122]]},{"text":"ENT E1 E2 (b) Feature Paired Tree(FPT) ENT","acronyms":[[34,37]],"long-forms":[[14,32]]},{"text":"functions(CPU: Celeron TM 366, RAM: 64M).  2) Prediction precision(PP) =  number of words with correct BPs(CortBP) ","acronyms":[[67,69],[10,13],[23,25],[103,106],[107,113],[31,34]],"long-forms":[[46,66]]},{"text":"Background (BKG) the background of the study Problem (PROB) the research problem Method (METH) the methods used Result (RES) the results achieved","acronyms":[[89,93],[12,15],[54,58],[120,123]],"long-forms":[[81,87],[0,10],[45,52],[112,118]]},{"text":"utilizing a set of data-driven terms.  We investigated how likely term frequency (TF) based RF is to discover HEWs.","acronyms":[[78,80],[88,90],[106,110]],"long-forms":[[62,76]]},{"text":"prim. = primary backgrounds; C06?C09 = CoNLL 2006?2009; I10 = ICONS 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the internal and outside CS); RT UAS = unlabeled attachment score of the roundtrip","acronyms":[[68,70],[90,92],[112,114],[128,131],[156,159],[189,191],[194,196],[197,200]],"long-forms":[[73,88],[95,103]]},{"text":"The semantic role labeling (SRL) can be divided into two separate phase: semantic role classifications (SRC) and post inference (PI). ","acronyms":[[129,131],[28,31],[104,107]],"long-forms":[[113,127],[4,26],[74,102]]},{"text":"Therefore, the standard metrics widely used in sequential topic segmentation for monologues and dialogs, such as Pk and WindowDiff(WD), are also not applicable.","acronyms":[[131,133],[113,115]],"long-forms":[[120,129]]},{"text":"ISREM 1 iff the candidate occurs 2 or more sentences before the anaphor POSITION 1 iff the antecedent occurs before anaphor SEMANTIC ROLE LABELLING (SR) IVERB 1 iff the governing verb of the given candidate is an issue verb","acronyms":[[149,151],[0,5],[72,80],[153,158]],"long-forms":[[124,137]]},{"text":" 1 Introduction Approaches to Machine Translation (MT) using Data-Oriented Parsing (DOP: (Bod, 1998; Bod et","acronyms":[[51,53],[84,87]],"long-forms":[[30,49],[61,82]]},{"text":"pora. In Proceedings of the 22nd Globally Conference on Computational Linguistics (COLING), pages 993?1000.","acronyms":[[88,94]],"long-forms":[[61,86]]},{"text":" On the other hand, the lowered in performance for the composite hallmarks vector baseline (CFV) may be attributed to the data sparseness phenomenon","acronyms":[[90,93]],"long-forms":[[55,79]]},{"text":"comparability between documents that are  choose for exploitation in the currents research,  are Mutual Infromation (MI) and Normalized  Mutual Infroamtion (NMI).","acronyms":[[116,118],[156,159]],"long-forms":[[96,114],[124,154]]},{"text":"\u0011\u000f \u0019 \u0019 \u0019 \u0019 \u0019 \u0018 Figure 3: Scored dependency forest 2.5 Semantic Dependence Graph (SDG) The SDG is a semantic-label word DG designed","acronyms":[[81,84],[90,93],[119,121]],"long-forms":[[54,79]]},{"text":"NIST-06. The bilingual training corpus comes from Linguistic Data Consortium (LDC)6, which consists of 3.4M sentence pairs with 64M\/70M Chi-","acronyms":[[78,81],[0,7]],"long-forms":[[50,76]]},{"text":"In  Proceedings of the 16th Globally Conference on  Computational Linguistics (COLING-96), Copenhagen,  Denmark, pp 459-465.","acronyms":[[84,93]],"long-forms":[[57,82]]},{"text":"tion). All markables have named entity types such as FACILITY, GPE (geopolitical entity), PERSON, LOCATION, ORGANIZATION, PERSON, VEHI-","acronyms":[[63,66]],"long-forms":[[68,87]]},{"text":" A conversation legislation is defined as a pair consisting of a communicative function (CF) and a semantic content (SC): a =< CF,SC >.","acronyms":[[77,79],[105,107],[115,117],[118,120]],"long-forms":[[53,75],[87,103]]},{"text":"tics.  Language Data Consortium (LDC). 2013.","acronyms":[[35,38]],"long-forms":[[7,33]]},{"text":"vincent.claveau@irisa.fr christian.raymond@irisa.fr rs?SUMA?Dans cet article, nous d?crivons notre involvement au D?fi Fouille de Texte (DeFT) 2012. Ced?fi consistait en l?attribution automatique de mots-cl?s ?","acronyms":[[137,141],[52,57]],"long-forms":[[81,135]]},{"text":"ond accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9AG = genitive adjunct","acronyms":[[122,124],[23,25],[36,38],[58,60],[85,87],[101,103],[140,143]],"long-forms":[[127,136],[28,34],[41,56],[63,83],[90,99],[106,120],[146,162]]},{"text":"Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) designate the ratio of three  sorts of bracketing numbers in all bracketing ","acronyms":[[72,75],[18,21],[43,46]],"long-forms":[[54,70],[0,17],[24,42]]},{"text":"term t appears in position i nearly the entity.  Bigram Context (BCON): The bigram-based  context model was built in a equivalent way to UCON, ","acronyms":[[65,69],[134,138]],"long-forms":[[49,63]]},{"text":"volution model (DTCNN). Figure 1 illustrates an example from the Flick Reviews (MR) dataset (Pang and Lee, 2005).","acronyms":[[80,82],[16,21]],"long-forms":[[65,78]]},{"text":"patterns allows a 4-way classification of slopes of lines: fast raising, rising, level, falling.  These are the 4 Fundamental Pattern Featured (FPF). A conjunction of 2 or  3 (of the 4) ","acronyms":[[143,146]],"long-forms":[[113,141]]},{"text":"MTCL = Machine Translations and Computational  Linguistics (1965-1968)  AJCL = AmericanJournal of Computational  Linguistics (1974-present) ","acronyms":[[71,75],[0,4]],"long-forms":[[78,110],[7,57]]},{"text":"and in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by","acronyms":[[135,139],[83,90]],"long-forms":[[104,133]]},{"text":" ? Self-training Segmenters (STS): two variant models were defined by the approaching re-","acronyms":[[29,32]],"long-forms":[[17,27]]},{"text":" 2 Tutorial Dialogue Setting and Data My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve science learning and understanding for students in grades 3-5.","acronyms":[[56,60]],"long-forms":[[38,54]]},{"text":" In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems.","acronyms":[[82,85]],"long-forms":[[34,57]]},{"text":"we maximize the log likelihood J(?) using a simple optimization technique called stochastic gradient descent (SGD). N,W","acronyms":[[110,113]],"long-forms":[[81,108]]},{"text":"Onto each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Penalties to Phrase (S-Ph), Expression to Word (Ph-W) and Word to Sense (W-Se), participants are required to re-","acronyms":[[119,123]],"long-forms":[[103,117]]},{"text":"The third is end poste (EP), after a predi-  cate. Pre poste (PreP) and post position (PostP)  are provided for adverbs as modifiers.","acronyms":[[68,72],[93,98],[27,29]],"long-forms":[[54,66],[78,91],[13,25]]},{"text":"Computing Center ,  Academy of Sc iences ,  Hosoow, USSR  1.  Personal  Computer Systems (POS) represent  nowadays a  s ign i f teaut  t rend  in  the professiona~, and amateur use of  ","acronyms":[[90,93],[52,56]],"long-forms":[]},{"text":" 5.2 Named Entities As standard named entity recognition (NER) systems do not capture categories that are relevant to","acronyms":[[58,61]],"long-forms":[[32,56]]},{"text":"ror rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.","acronyms":[[102,105],[10,14],[70,72]],"long-forms":[[85,100]]},{"text":"search tool, but it is insufficient for domain-  specific text, such as that encountered in  the MUCs (Message Understanding Confer-  ences).","acronyms":[[97,101]],"long-forms":[[103,132]]},{"text":"the classification of various verb groups (generic verbs versus specific verbs) based on the semantic distance with Latent Semantic Analysis (LSA) and Cluster Analysis.","acronyms":[[142,145]],"long-forms":[[116,140]]},{"text":"many competing approaches to tagging problems including Hid Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Sphere (CRFs).","acronyms":[[116,121],[78,82],[154,158]],"long-forms":[[89,114],[56,76],[127,152]]},{"text":"3 Model We introduce a topic-model based approach to declarative knowledge (DK) acquisition and describe how this knowledge can be applied to two unsuper-","acronyms":[[76,78]],"long-forms":[[53,74]]},{"text":"tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000","acronyms":[[142,146],[77,82]],"long-forms":[[116,140]]},{"text":" 1 Introduction Word Sense Disambiguation(WSD) is the processed of assigning a meaning to a word founded on the context","acronyms":[[42,45]],"long-forms":[[16,41]]},{"text":"SN = Sa-inflection oun (nominal verb)  CM = case marker (-nom\/-acc argument)  PT = particle (other arguments)  VB = verb ","acronyms":[[78,80],[0,2],[39,41],[111,113]],"long-forms":[[83,91],[5,22],[44,55],[116,120]]},{"text":"Rule 3: Issue word followed swiftly by a verb (Example (3)).   Qp = matter word + headword in the following Verb Phrase(VP) or NP chunk  Rule 4: Question word followed by a passive VP (Example (4)).","acronyms":[[129,131],[136,138],[190,192]],"long-forms":[[117,127]]},{"text":"gramming for probabilistic program. In International Workshop on Statistical Relational AI (StarAI). ","acronyms":[[96,102]],"long-forms":[[69,94]]},{"text":"and stochastic optimization. In Proceedings of the Conference on Learning Theory (COLT), pages 257?269.","acronyms":[[82,86]],"long-forms":[[51,80]]},{"text":"Note that the proponents of the BootCaT method seem to acknowledge this evolution, see for example Marco Baroni?s talk at this year?s BootCaTters of the world unite (BOTWU) workshop: ?","acronyms":[[166,171],[32,39]],"long-forms":[[134,164]]},{"text":"  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 67?68, Reykjavik, Sweden, Avril 27, 2014.","acronyms":[[71,76]],"long-forms":[[37,69]]},{"text":"Text5 vacations(0.432) humor(0.23) 0.099 blues(0.15) Table 2: Percent Agreement(PA) to handbook extracted index terms","acronyms":[[78,80]],"long-forms":[[60,76]]},{"text":"performance\" (MOPs) presented here with  results we still need from system-external  \"measures of effectiveness\" (MOEs)25 MOE-  based methods evaluate (i) baseline unaided ","acronyms":[[114,118],[14,18],[122,125]],"long-forms":[[86,111]]},{"text":" 4.1 BRAT The brat rapid annotation tool (BRAT) is an opensource web-based annotation tool that supports a","acronyms":[[42,46]],"long-forms":[[14,40]]},{"text":"After grammatical ambiguities are removed  by the stochastic parser, the phrase is divided  into noun phrases(NP) and verb phrases(VP),  giving, ","acronyms":[[110,112],[131,133]],"long-forms":[[97,108],[118,129]]},{"text":"Revue des Sciences of l?Education (RSE) ? Traduction, Terminologie et R?daction (TTR) ?","acronyms":[[81,84],[35,38]],"long-forms":[[42,79],[0,33]]},{"text":"appear although ModP and FocP are optional.  projections such as NegP (negation phrase) will  not be discussed although we assume there must ","acronyms":[[65,69],[16,20],[25,29]],"long-forms":[[71,86]]},{"text":"five COMPARE3, and five RECOMMEND CPs.  Everyone of the 112 textplans (TPs) that produced 4","acronyms":[[67,70],[34,37]],"long-forms":[[56,65]]},{"text":"In this shared task we test the feasibility of eliciting parallel data for Machine Translation (MT) using Mechanical Turk (MTurk). MT poses an interesting","acronyms":[[123,128],[96,98],[131,133]],"long-forms":[[106,121],[75,94]]},{"text":"Syntactic features from SLA research (SLASYN) ? Mean length of clause (MLC) ?","acronyms":[[71,74],[24,27],[38,44]],"long-forms":[[48,69]]},{"text":"? Negat ive Precis ion ( I~P)  :  * F -measures  (FM) ? ( ~2+I)?PP?PR \/32 ?","acronyms":[[49,51],[25,28]],"long-forms":[[36,46]]},{"text":" 1 The following abbreviations are utilizes POSS = possessive prefix\/suffix; LOC = locative suffix; OBV = obviative suffix;","acronyms":[[40,44],[73,76],[96,99]],"long-forms":[[47,57],[79,87],[102,111]]},{"text":"We use two different windows to define a triggering environment: one for morpheme and another for its part of speech (POS) tagged. Figure 2 exposition ","acronyms":[[118,121]],"long-forms":[[102,116]]},{"text":"That is, it learn some new knowledge.  Static Interactive Learning (SIL): Whenever the  system encounters a condemnation out o f  i t s  processing ","acronyms":[[69,72]],"long-forms":[[40,67]]},{"text":"occurs from multiple production sources, we  proposal an extend to this genre of technique  in the form of a Group Sparse Model (GSM)  which enforces sparsity with a L2,1 norm instead ","acronyms":[[132,135]],"long-forms":[[112,130]]},{"text":" TCGs, such as Quadrant the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever broadens","acronyms":[[58,60],[36,39],[1,5]],"long-forms":[[45,56],[15,34]]},{"text":"chanical Turk service. Two classifiers, Na??ve Bayes (NB) and a aid vector machine (SVM), were implemented on the tokenized and stemmed state-","acronyms":[[88,91],[54,56]],"long-forms":[[64,86],[40,52]]},{"text":"did. We utilizing files 1-270, 400-554, and 600-931 as source domain training data (STrain), docket 271300 as source domain testing data (STest) and files","acronyms":[[79,85],[132,137]],"long-forms":[[50,72],[104,130]]},{"text":"In Procedural of the Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 376?387.","acronyms":[[95,102]],"long-forms":[[22,93]]},{"text":" 3.3 Prosodic Model Training We choose to use a support vector machine (SVM) classifier1 for the prosodic model based on previous","acronyms":[[72,75]],"long-forms":[[48,70]]},{"text":"put language context to bias translation choices is in some sense a neurological network analogy to the PSD (phrase sensing disambiguation) approach for context-dependent translation probabilities of","acronyms":[[98,101]],"long-forms":[[103,130]]},{"text":"AO = all objects  MO = matched objects  TF = text filtering  FM = F-measures ","acronyms":[[40,42],[0,2],[18,20],[61,63]],"long-forms":[[45,59],[5,16],[23,38],[66,76]]},{"text":"ZZ incipient optional verb complements  statement(Q) -...  verb_complementsO(VC). ","acronyms":[[75,77],[0,2]],"long-forms":[[57,73]]},{"text":"nextpos part of speech of next word in the sentence determiner if the word has a determiner prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition insidequotes if the word is inside quotes","acronyms":[[157,159]],"long-forms":[[135,155]]},{"text":"In Proc. of the Lectures on Computational Natural Language Learning (CoNLL), 7.","acronyms":[[71,76]],"long-forms":[[30,69]]},{"text":"validating the performance of our method:  1. Precision@N (P@N). P@N measures how ","acronyms":[[57,60],[63,66]],"long-forms":[[44,55]]},{"text":"many competing approaches to tagging problems including Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs).","acronyms":[[116,121],[78,82],[154,158]],"long-forms":[[89,114],[56,76],[127,152]]},{"text":"(Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels.","acronyms":[[78,80]],"long-forms":[[52,76]]},{"text":"other models, such as vector space model (VSM), Okapi model (Robertson et al, 1994) or language model (LM). The pipeline meth-","acronyms":[[103,105],[42,45]],"long-forms":[[87,101],[22,40]]},{"text":"bath?). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP\/NP).","acronyms":[[88,93],[128,133]],"long-forms":[[63,86],[121,126]]},{"text":"All words are labeled as basic or not basic according to Ogden?s Basic Frenchman 850 (BE850) list (Ogden, 1930).3 In edict to measure the lexical complexity","acronyms":[[84,89]],"long-forms":[[65,82]]},{"text":"TGTM PR=pr ,  pkr ,  b rs   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw  Silhouette 26 ","acronyms":[[62,64],[0,4],[5,7],[27,31],[32,34],[57,61]],"long-forms":[[65,67],[8,10],[36,39]]},{"text":"Model tiers, there is an intermediate stages of analysis , characterized by a  formal language , resp r  - The Engliaboriented Formal Language ( EFL) , which containa  constant^ that  correspond to the terms of Frenchman, This language is wed to represent the ","acronyms":[[144,147]],"long-forms":[[110,141]]},{"text":"operating system.  For production deployment we  used Message Driven Beans (MDBs)using IBM  Websphere Application Server? (","acronyms":[[76,80],[87,90]],"long-forms":[[54,74]]},{"text":"XC (compound),   NN (noun),   NNP (proper ","acronyms":[[17,19],[0,2],[30,33]],"long-forms":[[21,25],[4,12]]},{"text":"tice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM.","acronyms":[[91,94]],"long-forms":[[70,89]]},{"text":"Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,  Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on  Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  ","acronyms":[[210,214],[147,151]],"long-forms":[[166,208]]},{"text":"uation test sets. Equal Error Rates (EER), where FA = FR, are afforded in Table 5. Results on EVAL","acronyms":[[54,56],[37,40],[49,51],[91,95]],"long-forms":[[18,35]]},{"text":"tends the comparison set to players of AS Roma.  Prepositional phrases (PPs), gerunds, and relative clauses introduce additional complexity.","acronyms":[[72,75],[39,41]],"long-forms":[[49,70]]},{"text":"One is manually typing the text transcriptions which we regarded as the Correcting Recognition Result (CRR) transcription, and another is the ASR outcomes which","acronyms":[[100,103],[139,142]],"long-forms":[[72,98]]},{"text":"                                                           8  Feature type: CB=Clause boundary based feature type,  PT=Parse tree based feature type  9A ?","acronyms":[[116,118],[76,78]],"long-forms":[[119,129],[79,94]]},{"text":" In integrating this approach into a dialog system, we see that the dialog manager (DM) no longer determines surface strings to send to the TTS system, as is often the case in current dialog systems.","acronyms":[[84,86],[140,143]],"long-forms":[[68,82]]},{"text":"http:\/\/www.cs.ualberta.ca\/?yx2\/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003).","acronyms":[[81,84]],"long-forms":[[60,79]]},{"text":"Domains  In this section we compare some characteristics of the  Francais Trips Domain (ETD) and the English Spon-  taneous Scheduling Task (ESST).","acronyms":[[88,91],[141,145]],"long-forms":[[65,86],[101,139]]},{"text":"5.1 Candidate Generation We generate a list of candidate antecedents by first extracting all VPs and ADJPs (and all contiguous combinations of their components) from the underway","acronyms":[[101,106],[93,96]],"long-forms":[[108,126]]},{"text":"carded in LSI-based approaches. We dub our paragon ONETA (OrthoNormal Explicit Topic Analysis) and empirically show that on a cross-lingual recover","acronyms":[[49,54],[10,13]],"long-forms":[[56,91]]},{"text":"not unequivocal about this. ? P5E2N4S3, F W A Computer Processable Linguistics (CPL) (Clark et al. 2005) is a controlled variant of","acronyms":[[74,77],[27,35]],"long-forms":[[43,72]]},{"text":"Translation  The first method compared is a transitive  translation using MT (machine translation). The ","acronyms":[[74,76]],"long-forms":[[78,97]]},{"text":"Table 4: Evaluate results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order Language Model); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector). ","acronyms":[[113,115],[127,129],[44,47],[78,82],[149,151]],"long-forms":[[117,124],[131,146],[84,110],[153,178]]},{"text":"words, from this the subscript b (bag-of-words).  Subtree Kernel (SbtK) is one of the simplest tree kernels as it only creates finishes subtrees, i.e.,","acronyms":[[66,70]],"long-forms":[[50,64]]},{"text":"Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Naturel Language Treatment searcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.","acronyms":[[94,97],[73,75],[138,143],[178,186],[204,206],[239,241]],"long-forms":[[98,125],[76,92],[144,169],[187,195],[242,244],[207,226]]},{"text":" IHMM-based: He et al (2008) propose an  indirect hidden Markov model (IHMM) for hypothesis alignment.","acronyms":[[71,75],[1,5]],"long-forms":[[41,69]]},{"text":"terial with relevant events will be effected along the 1MUMIS is an on-going EU-funded project within the Information Society Programmed (IST) of the European Union, section Human Language Technician (HLT).","acronyms":[[131,134]],"long-forms":[[102,121]]},{"text":"BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs. ","acronyms":[[105,107],[133,135],[0,2],[30,32],[51,54],[90,92]],"long-forms":[[110,127],[138,145],[5,28],[35,49],[57,88],[95,103]]},{"text":"2.1 Collection Tasks To collect our data, we used two different types of human intelligence tasks (HITs). Across types 1, the","acronyms":[[99,103]],"long-forms":[[73,97]]},{"text":" 2.5 Noise Contrastive Appraisal Noise contrastive estimation (NCE) is another sampling-based technique (Hyv?arinen, 2010;","acronyms":[[64,67]],"long-forms":[[34,62]]},{"text":"of the reparandum coincides with the termination of  the fluent portion of the utterance, which we term the  INTERRUPTION SITE (IS). The DISFLUENCY INTERVAL ","acronyms":[[128,130]],"long-forms":[[109,126]]},{"text":"CIMA is an online information center maintained by the Spanish Agency for Medicines and Health Products (AEMPS). CIMA provides","acronyms":[[105,110],[0,4],[113,117]],"long-forms":[[84,103]]},{"text":" Intro  Categorial Grammars (CGs) consist of two compo-  nents: (i) a lexicon, which assigns syntactic types ","acronyms":[[36,39]],"long-forms":[[15,34]]},{"text":"tion of the reference xamples takes place.  Translation Memories (TMs) are such purely  memory based MT-systems.","acronyms":[[66,69],[101,103]],"long-forms":[[44,64]]},{"text":"Instead of using graph-based consensus  confidence as features in the log-linear model, we  perform structured label propagation (Struct-LP) to  re-rank the n-best list directly, and the similarity ","acronyms":[[130,139]],"long-forms":[[100,128]]},{"text":"telling. In Computers in Entertainment (CIE), Association for Computing Machinery (ACM), volume 5.","acronyms":[[83,86],[40,43]],"long-forms":[[46,81],[12,38]]},{"text":"Several costed algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improve Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno ways (L-BFGS) (Nocedal and Wright","acronyms":[[131,134]],"long-forms":[[112,129]]},{"text":"sions to identify stylistic shifts in paraphrasing, allowing us to discriminate stylistic properties in the Paraphrase Database (PPDB) with high accuracy. Sec-","acronyms":[[128,132]],"long-forms":[[89,126]]},{"text":"Publishes results for two unattended algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al (2010), on this widely-used benchmark corpus are","acronyms":[[108,112],[55,58]],"long-forms":[[96,106]]},{"text":"4 Hierarchic Autoepistemic  Logic  Autoepistemic (AE) logic was devised by Moore  \\[I0\\] as a reconstruction of McDermott's nonmono- ","acronyms":[[50,52]],"long-forms":[[35,48]]},{"text":"tions.  3.1 Simple Classification (SC) Given an expression x consisting of n words x1,","acronyms":[[35,37]],"long-forms":[[12,33]]},{"text":"lines are publishing findings by Salle and Nivre 2008 (HN08), Maier et al 2012 (M12), van Cranenburgh 2012 (C12), Kallmeyer and Maier 2013 (KM13), van Cranenburgh and Bod 2013 (CB13), and Versley 2014a, 2014b (V14a, V14b).","acronyms":[[137,141],[52,56],[77,80],[105,108],[174,178],[207,217]],"long-forms":[[111,135],[31,50],[60,75],[87,103],[148,172],[185,205]]},{"text":"1 In t roduct ion   The main goal of the proposed project is to develop  a language model(LM) that uses syntactic structure. ","acronyms":[[90,92]],"long-forms":[[75,88]]},{"text":"We utilize four groups of datasets. The first grouped comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":"qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, np), qdet?1.0 DET(the),","acronyms":[[52,54],[8,10],[30,32],[76,79]],"long-forms":[[56,60]]},{"text":"J~:u phom:,l:ical (graphemic) empressio~ in to  5 l \\ [ ,+vei~  tecLogt+ah~illa~ieg (levm+l o+ US+st abbrev,  TR) + ,+:mr+ar:e  synkam (SR) + (neP'pfie\/r+ic5 (HR) '+ phnnemics ,~nd phnnetit:ts  (g raphmaics} , ,  Eact+ ef the  ieve l~ i s  in terpreted  a~ a set, ","acronyms":[[136,138],[110,112],[159,161]],"long-forms":[[128,134]]},{"text":"categories, as shown in Figure 1. Messages may  involve a request (REQ), provide information  (INF), or fall into the category of interpersonal ","acronyms":[[67,70],[95,98]],"long-forms":[[58,65],[81,92]]},{"text":"DUC 2007. In Proceedings of the Seventh Document Understanding Conference (DUC), Rochester, NY.","acronyms":[[75,78],[0,3],[92,94]],"long-forms":[[40,73]]},{"text":"ZZ initial optional verb complements  statement(Q) -...  verb_complementsO(VC). ","acronyms":[[75,77],[0,2]],"long-forms":[[57,73]]},{"text":" Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, utilize Medical Subject Headings (MeSH) and Uniting Physicians Language System (UMLS) as lexical resources for representing each","acronyms":[[155,159],[198,202]],"long-forms":[[129,153],[165,196]]},{"text":" Conventional machine learning techniques, such as Stealth Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful","acronyms":[[72,75],[104,106]],"long-forms":[[51,70],[81,96]]},{"text":" ? Because Dependency Grammar (DG) directly outline the functional relations between  words, and s dependency trees has not any non-terminal nodes, DG is suitable for our ","acronyms":[[31,33],[149,151]],"long-forms":[[11,29]]},{"text":"tical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discriminative models.","acronyms":[[125,128],[31,35],[89,92]],"long-forms":[[98,123],[0,29],[65,87]]},{"text":"(ST), i.e. any node of a tree along with all its descendants. A subset tree (SST) exploited by the SubSetTreeKernel is a more","acronyms":[[77,80],[1,3]],"long-forms":[[64,75],[99,109]]},{"text":"feeling (FE)  food (FO)  group (GR)  location (LO) ","acronyms":[[32,34],[9,11],[20,22],[47,49]],"long-forms":[[25,30],[0,7],[14,18],[37,45]]},{"text":"User-generated content (UGC), and especial the microblog genre, has become an interesting resource for Natural Language Treatment (NLP) tools and applications.","acronyms":[[133,136],[24,27]],"long-forms":[[104,131],[0,22]]},{"text":"A workaround is to curb the possible tag hopefuls per position by using either morphological analyzers (MAs), dictionaries or heuristics (Hajic?,","acronyms":[[110,113]],"long-forms":[[85,108]]},{"text":" 1 Introduction Currently the Machine Translation (MT) research community attempts to seamlessly integrate both","acronyms":[[51,53]],"long-forms":[[30,49]]},{"text":" 3.1.1 Pre-Processing For Feature Extraction Phrase Analysis(PA): Using basic syntactic analysis (shallow parsing), the PA module re-builds","acronyms":[[61,63],[120,122]],"long-forms":[[45,59]]},{"text":"(Koehn, 2004a). Also, they extendedWSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a).","acronyms":[[78,81]],"long-forms":[[49,76]]},{"text":" The sentence-level extraction is done with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was shown to give good re-","acronyms":[[68,71]],"long-forms":[[51,66]]},{"text":"2.4.1 English Gigaword We created large-scale n-gram language models using English Gigaword Second Edition6 (EGW). ","acronyms":[[109,112]],"long-forms":[[75,91]]},{"text":"five folds and evaluate them on the fifth 29 Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows (SW). ??","acronyms":[[120,122],[88,95],[145,147]],"long-forms":[[102,118]]},{"text":"(? basic?) and MT (Madnani et al, 2012). RAE","acronyms":[[18,20],[44,47]],"long-forms":[[22,32]]},{"text":"These two representations are associated in an abstract type map, called AAM (Abstract Associative Cartography). This","acronyms":[[73,76]],"long-forms":[[78,102]]},{"text":"document is zero.  Agglomerative Hierarchical Clustering (AHC)  AHC is a bottom-up hierarchical clustering ","acronyms":[[58,61],[64,67]],"long-forms":[[19,56]]},{"text":"perceptual space between each pair. We can do this with a multidimensional scaling (MDS) algorithm. Let us call","acronyms":[[84,87]],"long-forms":[[58,82]]},{"text":"BP = log (Prob(N2 | N1)) (2) ? Pointwise Mutual Information (PMI): Defined as a measure of how collocated two words are,","acronyms":[[61,64],[0,2]],"long-forms":[[31,59],[5,28]]},{"text":"otherwise as uniform as possible (Berger et al, 1996). maximum entropy model (MaxEnt) is known to easily combine diverse features and","acronyms":[[78,84]],"long-forms":[[55,70]]},{"text":"tence pairs were selected from the WMT Giga corpus if the perplexity of their French part with respect to a language model (LM) trained on French news data was below a given threshold.","acronyms":[[124,126],[35,38]],"long-forms":[[108,122]]},{"text":"  1. RecallCorrectTransliteration  (RTrans)  The recall is going to be calculating using the ","acronyms":[[36,42]],"long-forms":[[5,33]]},{"text":"{yvchen, yww, anatoleg, air}@cs.cmu.edu Abstract Spoken dialogue systems (SDS) typically require a predefined semantic ontology","acronyms":[[74,77]],"long-forms":[[49,72]]},{"text":"ence.  Semantic Textual Analogy (STS) is the task of judging the similarity of two sentences on a scale","acronyms":[[36,39]],"long-forms":[[7,34]]},{"text":"our modes in this domain. The analysis of disparity (ANOVA) and Tukey?s honestly significant differences (HSD) tests on the classification accuracies","acronyms":[[53,58],[106,109]],"long-forms":[[31,51],[72,104]]},{"text":"In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003).","acronyms":[[148,151]],"long-forms":[[123,146]]},{"text":"DEP = dependency type ? MOR = morphological characters (set) ?","acronyms":[[24,27],[0,3]],"long-forms":[[30,43],[6,16]]},{"text":"(NL) into formal meaning representations (MR), and are applied for both parsing of textual input and in Spoken Language Understanding (SLU). In data-","acronyms":[[135,138],[1,3],[42,44]],"long-forms":[[104,133],[17,40]]},{"text":"Frame representat ion ( for  mapping 4):  \\[agent\/an\\] (i\/PRP)  5PO$ abbreviations: PRP=personal pro-  noun, AUX=auxiliary verb, VB=main verb (non-inflected), ","acronyms":[[84,87],[109,112],[129,131]],"long-forms":[[88,101],[113,122],[137,141]]},{"text":"One part of the work is directed towards developing computational methods to facilitate the manual construction of SweFN. We have so far focused on three tasks: (1) semantic role labeling (SRL) (Johansson et al.,","acronyms":[[189,192],[115,120]],"long-forms":[[165,187]]},{"text":"The first step is the identification of the presence of a graphical image in the web page by a  Browser Helper Object (BHO) (Elzer et al., 2007).","acronyms":[[119,122]],"long-forms":[[96,117]]},{"text":"Query key words and concepts Token and concept Indexing Knowledge Fount Adapters (KSAs)   integrate and deliver content from ","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":"the parameters. We trained our networking with stochastic gradient descent (SGD), mini-batches and adagrad updating (Duchi et al, 2011), using","acronyms":[[73,76]],"long-forms":[[44,71]]},{"text":"The Meter Corpus selecting as the test data  is a collection of court reports from the British Press Association (PA) and some leading  British newspaper (Gaizauskas 2001; Clough ","acronyms":[[111,113]],"long-forms":[[92,109]]},{"text":"qn  f.g., ? MP (Member of Parliament)? ","acronyms":[[12,14]],"long-forms":[[16,36]]},{"text":"functions. The latter three correspond to the three Gene Ontology (GO) (Ashburner et al, 2000) toplevel sub-ontologies, and terms of these types were","acronyms":[[67,69]],"long-forms":[[52,65]]},{"text":"The last column lists the Spearman rank order correlation (?) of the rankings with the Berlin and Kay (B&K) ranks. ","acronyms":[[103,106]],"long-forms":[[87,101]]},{"text":"2003). Recently, Huang et al (2015) showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs com-","acronyms":[[85,88],[120,129]],"long-forms":[[59,83]]},{"text":" The non-standard mots in text are referred to as Out of Vocabulary (OOV) words. The nor-","acronyms":[[70,73]],"long-forms":[[51,68]]},{"text":" PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0 ZC05 (Zettlemoyer and Collins 2005) 79.3 ? ","acronyms":[[54,58]],"long-forms":[[60,88]]},{"text":"2004. A maxi-mum-entropy Chinese parser amplify by transformation-based learning. ACM Transactions on Asiatic Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu.","acronyms":[[143,148]],"long-forms":[[88,141]]},{"text":"corpus (DUC2002) and the second is automatically extracted from related web news stories (WNS) automatically extracted.","acronyms":[[90,93],[8,15]],"long-forms":[[72,88]]},{"text":"cal work is extensive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking.","acronyms":[[90,93]],"long-forms":[[57,88]]},{"text":"nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w\/o B.O.) srl 0 487 437 63 13 0.9740 0.1260 55.0% Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives, TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no","acronyms":[[165,167],[186,188],[208,210],[229,231],[255,257],[260,262],[268,270],[273,275],[277,281]],"long-forms":[[170,184],[191,206],[213,227],[234,249],[284,293]]},{"text":"actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations eas-","acronyms":[[75,78]],"long-forms":[[50,73]]},{"text":"tor. Together, these modifications reduce the BLEU score by 1.49 BLEU points (BP)9 at the largest training size.","acronyms":[[78,80],[46,50]],"long-forms":[[65,76]]},{"text":"category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and ","acronyms":[[75,77],[27,29],[52,56],[103,105],[136,139]],"long-forms":[[80,100],[32,49],[59,73],[118,134],[142,151]]},{"text":"admit cue phrase. Five plan followed a pure token classification approach (TC) for cue detection while others used sequential labeling tech-","acronyms":[[83,85]],"long-forms":[[52,81]]},{"text":" 310 X. Fan et al  Causality Na?ve Bayesian Classifier (CNB): For a document represented by a binary-valued vector d=(X1 ,X2 , ?,","acronyms":[[56,59]],"long-forms":[[19,43]]},{"text":"We further validate our approach on a large publicly available manipulation action dataset (MANIAC) from (Aksoy et al, 2014), achieving promising ex-","acronyms":[[92,98]],"long-forms":[[63,82]]},{"text":"168 Figure 1: Plots of concreteness versus. imageability scores for literal vs. nonliteral words in the VUAMC (Conc=concreteness, Imag=imageability, NL=nonliteral, L=verbatim) concrete than the reliant\/s; H","acronyms":[[145,147],[100,105],[107,111],[126,130]],"long-forms":[[148,158],[112,124],[131,143],[162,169]]},{"text":" The growing need to  manage and search large video collections presents  a challenge to traditional information retrieval (IR)  technologies.","acronyms":[[124,126]],"long-forms":[[101,122]]},{"text":"tion). All markables have named entity types such as FACILITY, GPE (geopolitical entity), PERSON, LOCATION, ORGANIZATIONS, PERSON, VEHI-","acronyms":[[63,66]],"long-forms":[[68,87]]},{"text":"Ph i lade lph ia ,  PA  191C4  .ABSTRACT  Tree Adjoining Grammar (TAG) is a formalism for natural  language grammars.","acronyms":[[66,69],[20,22]],"long-forms":[[42,64],[0,16]]},{"text":"(pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (level uni), independently encoding the parts of speech (POS) of the words preceding and following the adjective, respec-","acronyms":[[142,145]],"long-forms":[[125,140]]},{"text":" Because of data sparseness, we cannot reliably use a  maximum chance estimator (MLE) for bigram prob-  abilities.","acronyms":[[85,88]],"long-forms":[[55,83]]},{"text":"knowledge that we can get from these examples the required information to parse a new input sentence .  In our  approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema  where each SSTC describes a sentence, a representation tree as well as the correspondence b tween substrhzgs in ","acronyms":[[193,197],[229,233]],"long-forms":[[154,191]]},{"text":"entire search process.  (b) EPN for the first optimum solution (EPN-F): The number of the expanded problems when","acronyms":[[64,69]],"long-forms":[[28,45]]},{"text":"September\/NNP \\] \\[O .\/. \\]  we can extracted following chunk habits:  NP=NULL 90 PRP 99 VBZ  VP=PRP 99 VBZ 99 DT ","acronyms":[[74,78],[10,13],[71,73],[82,85],[89,92],[94,96],[97,100],[104,107],[111,113]],"long-forms":[]},{"text":"NG = ngrams, E = emoticon replacement, IR = informal register replacement, TL = tweet length, RT = retweet count, SVO = subject-verbobject structures. %","acronyms":[[94,96],[0,2],[39,41],[75,77],[114,117]],"long-forms":[[99,106],[5,11],[17,25],[44,61],[80,92],[120,138]]},{"text":"This article describes the collaborative work on applying  the newly proposed ISO standard for conversations act  annotation to the Switchboard Dialogue Act (SWBD-DA)  Corpus, as portions of our on-going efforts to promote ","acronyms":[[153,160],[78,81]],"long-forms":[[127,151]]},{"text":" 3.3.1 Determinantal Point Processes Determinantal point processes (DPPs) are distributions over subsets that jointly prefer quality of","acronyms":[[68,72]],"long-forms":[[37,66]]},{"text":"which allows POS tagged and chunked data to be represented (including recursion), and Shakti Standard Layout (SSF)2. The editor lets","acronyms":[[110,113],[13,16]],"long-forms":[[86,108]]},{"text":"= argmaxjP (zi = j|sk).  4.2 Lexical Chain Segmenter (LCSeg) Our second model is the lexical chain based seg-","acronyms":[[54,59]],"long-forms":[[29,52]]},{"text":"ENTITY_A appos Figure 2: Dependency tree (DT) for the entity blinded sentences ?","acronyms":[[42,44]],"long-forms":[[25,40]]},{"text":"stable functional definition across languages. These categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition,","acronyms":[[84,87],[101,104],[115,118],[129,132]],"long-forms":[[90,99],[107,113],[121,127],[135,145]]},{"text":"features to argument classification models, but also  represent full parsing information as constraints in  integer linear programs (ILP) to resolve label inconsistencies.","acronyms":[[133,136]],"long-forms":[[108,131]]},{"text":"3 Results  The experiments were completed using the revised  RTE3 development set (RTE3Devmt) before the  RTE3Test results were released.","acronyms":[[83,92],[106,114]],"long-forms":[[61,81]]},{"text":"specifier phrase with one of a set of (usually numeric) specifiers; the specifier phrase typically occurs in apposition or as a genitive modifier (GEN) to the head noun.","acronyms":[[147,150]],"long-forms":[[128,136]]},{"text":"2. We propose a new topic modeling, Topic Sentiment Latent Dirichlet Allocation (TSLDA), which can capture the matter and sentiment si-","acronyms":[[78,83]],"long-forms":[[33,76]]},{"text":"web. We require parallel data to build a statistical machine translation (SMT) system that translates from German into Sim-","acronyms":[[74,77]],"long-forms":[[41,72]]},{"text":" 1 Introduction Multiword Expressions (MWEs) are commonly defined as ?","acronyms":[[39,43]],"long-forms":[[16,37]]},{"text":"Model of Argumentation. Procedural of American Association .for  Artificial Intelligence (AAAI) Conference: 313-315. ","acronyms":[[91,95]],"long-forms":[[39,89]]},{"text":"to any text, so we shall comment on them.  Frequent Candidates (FC) ? this is a boosting score","acronyms":[[64,66]],"long-forms":[[43,62]]},{"text":"special right category before COO > (5) the common left coordination category > (6) the other special right category > (7) the free cross-clause clausal category (IC) > (8) the common left cross-clause category > (9) the free cross-clause punctuations (PUS). ","acronyms":[[253,256],[30,33],[163,165]],"long-forms":[[239,251],[56,68]]},{"text":"The SCBD structure.  prepositional phrases (PPs), verb phrases (VPs), and adverbial expression  (APs).","acronyms":[[44,47],[64,67],[4,8],[94,97]],"long-forms":[[21,42],[50,62],[74,91]]},{"text":"to any text, so we shall comment on them.  Frequent Contestants (FC) ? this is a boosting score","acronyms":[[64,66]],"long-forms":[[43,62]]},{"text":"reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).  Entity coherence, which is based on the routing the referents of noun phrases (NPs) relate resultant clauses in the text, is an notable aspect of textual organization.","acronyms":[[134,137]],"long-forms":[[120,132]]},{"text":"ghar.  Parse: The modern town of [NP (np Hyderabad)  (punct ,) (advP about 50 km south of Navi ","acronyms":[[34,36],[61,65]],"long-forms":[[38,40]]},{"text":"? A chunking rule:  PP = prep, NP#1, if (pythontest(#1)). ","acronyms":[[20,22]],"long-forms":[[25,29]]},{"text":"vides brief details of each annotation dimension.   2.1 Knowledge Type (KT)  This dimension is responsible for capturing the ","acronyms":[[72,74]],"long-forms":[[56,70]]},{"text":"fifth folds and evaluate them on the fifth 29 Table 1: Results for passage retrieval for TREC-QA use disjoint windows (DW) and sliding windows (SW). ??","acronyms":[[120,122],[88,95],[145,147]],"long-forms":[[102,118]]},{"text":"Abstract In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues","acronyms":[[51,56]],"long-forms":[[58,78]]},{"text":"To enabled for portability, the SABA parser translates its natural  vocabulary inputs into an ~mtermediate\" s mantic network formalism  called SF (for \"Sentence Formalism'), presented in details in (Binot,  1984, 1985).","acronyms":[[138,140],[30,34]],"long-forms":[[147,165]]},{"text":" 2.2 W3C Semantic Web The World Wide Web (WWW) was once designed to be as simple, as decentralized and as interop-","acronyms":[[42,45],[5,8]],"long-forms":[[26,40]]},{"text":"2013).  Currently the most active framenet research teams are working on Swedish FrameNet (SweFN) (Borin et al.,","acronyms":[[91,96]],"long-forms":[[73,89]]},{"text":"vector built from training set. However, this  wo lead to many out of vocabulary (OOV)  cases, in addition to protracted vector.","acronyms":[[85,88]],"long-forms":[[66,83]]},{"text":"  1 Introduction  Sign Language (SL) is a visual-gestural language, using the whole upper body articulators ","acronyms":[[33,35]],"long-forms":[[18,31]]},{"text":"? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples  The difficulty of identifying anonymous words in ","acronyms":[[43,47],[11,15]],"long-forms":[[34,42],[2,10]]},{"text":"Collins et al. ( 2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF).","acronyms":[[63,65],[107,110]],"long-forms":[[39,61],[81,105]]},{"text":"training data (Surdeanu et al, 2008). We report chant (PU), collocation (CO), and their harmonic mean (F1) assessed on gold arguments in two set-","acronyms":[[56,58],[74,76]],"long-forms":[[48,54],[61,72]]},{"text":"ticPhrase, Predicate, Argument, ? the MILE Data Categories (MDC) which constitute the attributes and values to adorn","acronyms":[[60,63]],"long-forms":[[38,58]]},{"text":"For the training of the SMT engines, we usage two parallel Spanish-English corpora provided by the organiser: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M paral-","acronyms":[[124,126],[24,27]],"long-forms":[[114,122]]},{"text":"mannequins use a word embedding size of 200, while the hidden layer(s) size is stationary at 400, with all hidden units using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x) as activation function.","acronyms":[[146,150],[165,168]],"long-forms":[[123,144]]},{"text":"disallowed by the space-saving algorithm.  Least Freshly Employs (LRU) When the volume of flow in a text stream rapidly increases, it is likely to relate to a burst of a certain topic.","acronyms":[[62,65]],"long-forms":[[41,60]]},{"text":"form (FFC); from this decision he\/she formulates a natural language utterance with certain peculiarities including the sentence type (SeTp) the subject type (SuTp) and rating (Punct).","acronyms":[[129,133],[6,9],[153,157],[176,181]],"long-forms":[[114,127],[139,151],[163,174]]},{"text":" The two-level analysis of the mentioned forms ap-  pears below ST = sm'face tape, PT -- pattern  tape, 115.\\[' -- root tape, VT : loud{sin tat)e , and ","acronyms":[[60,62]],"long-forms":[[65,77]]},{"text":"Abstract  This paper presents a new bootstrapping  approach to named entity (NE)  classify.","acronyms":[[77,79]],"long-forms":[[63,75]]},{"text":"augmented by a set of PRIDES-specific Common  Gateway Interfaces (CGIs), communicates with the  client via Hypertext Transport Protocol (HTTP). A ","acronyms":[[137,141],[22,37],[66,70]],"long-forms":[[107,135],[38,64]]},{"text":"Greater  technically, for each syntactic feature {sf1, sf2, ...,  sfn} of the set SF (Syntactic Features) represented  in the lexical typology, we define the objective of our ","acronyms":[[79,81]],"long-forms":[[83,101]]},{"text":"There are three options: French (FR), Spanish (SP), or, Merged languages (ML), where the outcomes are obtaining by merging the English output of FR","acronyms":[[74,76],[33,35],[47,49],[143,145]],"long-forms":[[56,72],[25,31],[38,45]]},{"text":"Interpreting news requires identifying its constituent events. Information extraction (IE) makes this feasible by considering only events of a specified type,","acronyms":[[87,89]],"long-forms":[[63,85]]},{"text":"crimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). ","acronyms":[[143,146],[25,29]],"long-forms":[[126,141]]},{"text":"number, fifteen predefined verbs (Paul, 2010)  are chosen as Light Verbs (LVs) for framing  the compound verbs (CompVs).  A manually ","acronyms":[[112,118],[74,77]],"long-forms":[[96,110],[61,72]]},{"text":"formation from non-expert bilingual speakers. The  Translation Correction Tool (TCTool) is a userfriendly online tool that allows users to add, delete ","acronyms":[[80,86]],"long-forms":[[51,78]]},{"text":"Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a problematic themes in Natural Language Processing","acronyms":[[110,112],[77,79]],"long-forms":[[85,108],[56,75]]},{"text":"! JIM: { Person67 \/ Person83 \/ Name18 \/ (TYPE=&Person, SEX=Male, NAME=NamelS) }  Ultimately, the association between grammatical functions and ","acronyms":[[65,69]],"long-forms":[[70,76]]},{"text":"2.2 Conditional Random Fields Conditional random field (CRF) was an enlarging of both Maximum Entropy Model (MEMs) and Hidden Markov Model (HMMs) that was firstly","acronyms":[[109,113],[56,59],[141,145]],"long-forms":[[86,107],[30,54],[119,139]]},{"text":"(Bjo?rne et al 2011).  The Turku Event Extraction System (TEES)1 is an open source program for extracting events and re-","acronyms":[[58,62]],"long-forms":[[27,56]]},{"text":"In Proc. of the Eighth  Text Retrieval Conference (TREC-8), pages 151162.","acronyms":[[51,57]],"long-forms":[[16,49]]},{"text":"2.4 Longest Common Substring  Given two fetters, T of length n and H of lengths m,  the Longest Common Sub-string (LCS) problem  (Dana, 1999) will find the longest string that is a ","acronyms":[[114,117]],"long-forms":[[87,105]]},{"text":"In Proceedings of the Workshop on Incremental Parsing: Bringing Engineers and Cognition Together (ACL), pages 50?57, Stroudsburg, PA.","acronyms":[[100,103],[132,134]],"long-forms":[]},{"text":"ley (2004). This framework for linguistic semantics is called Uni\u0002ed Eventity Representation (UER), because it is a true extension of the UML and not","acronyms":[[94,97],[138,141]],"long-forms":[[62,92]]},{"text":"  1 Introduction  INTERA (Integrated European language data  Repository Area, Contract 22076Y2C2DMAL2) is ","acronyms":[[18,24]],"long-forms":[[26,76]]},{"text":"would have higher perplexity.  Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally cor-","acronyms":[[48,50]],"long-forms":[[41,46]]},{"text":"Extract Abbreviations NE = Named Entity CE = Correlated Entity","acronyms":[[25,27],[43,45]],"long-forms":[[30,42],[48,65]]},{"text":"Because LSA is closely related to principle component analysing (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analyze (OPCA) can leverage","acronyms":[[128,131],[8,11],[64,67],[84,87],[176,180]],"long-forms":[[96,126],[34,62],[137,174]]},{"text":"{EVENT 2 {AND {SUBTYPE DIE} {PERSONA  $foo}}}  2.4 Graphical User Interface (GUI)  For some applications such as database ","acronyms":[[76,79]],"long-forms":[[50,74]]},{"text":"NEDcost = EDcost\/length (4) ? The Match Number(MN): The match number is the number of words","acronyms":[[47,49],[0,7],[10,16]],"long-forms":[[34,45]]},{"text":"(reduced dimensions). The general idea behind the  Pseudo Relevance Feedback (PRF) (Croft &  Harper, 1979) or its more recent variation called ","acronyms":[[78,81]],"long-forms":[[51,76]]},{"text":"Frustratingly easy domain adaptation.  Throughout Association for Computational Linguistics (ACL). ","acronyms":[[85,88]],"long-forms":[[42,83]]},{"text":"In addition, we also  experimented with different combinations of  translation models (TM), phrase-based and  factor-based, trained on various datasets to ","acronyms":[[87,89]],"long-forms":[[67,85]]},{"text":"tion, Generation, Question Answering (QA), etc.  STS is related to both Textual Entailment (TE) and Paraphrasing, but diversified in a number of shapes","acronyms":[[92,94],[38,40],[49,52]],"long-forms":[[72,90],[18,36]]},{"text":"vp?np?pp .83 .80 .83 .80 .83 .80 vp?pp?pp .75 .74 .75 .74 .75 .74 Lexical associations (LAsim) Sequencing PrEC PrPGR RecEC RecPGR F-SRV F-SPGR","acronyms":[[87,92]],"long-forms":[[66,85]]},{"text":"and in part by the TerraSwarm Research Center, one of six centres supported by the STARnet phase of the Centered Center Research Program (FCRP) a Semiconductor Research Corporate program sponsored by","acronyms":[[135,139],[83,90]],"long-forms":[[104,133]]},{"text":"and EN-DE) with token frequency, sense distribution and most frequent translations ordered by the corresponding senses (T = lobe, CO = concession, CARAT = contrast). ","acronyms":[[134,136],[151,153],[4,9]],"long-forms":[[139,149],[156,164],[124,132]]},{"text":"Setting P0.1 P0.25 P0.33 P0.5 Best-F1 ContextSim (CS) 42.9 69.6 60.7 58.7 49.6 SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9 (a) from baseline models","acronyms":[[92,94],[50,52]],"long-forms":[[79,90],[38,48]]},{"text":"Section 7 concludes this article.  2 Automatic Speech Recognition (ASR)  Thai ASR research focused on two major topics.","acronyms":[[67,70],[78,81]],"long-forms":[[37,65]]},{"text":"4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): y?","acronyms":[[125,128],[18,26],[52,55]],"long-forms":[[105,123]]},{"text":"  Secondly, as for the word order of prepositional  phrases (PP), Arabic and English are similar in  that PPs generally appear at the end of the sen-","acronyms":[[61,63],[106,109]],"long-forms":[[52,59]]},{"text":"the divergence of their distributions in the targets and backgrounds. A support vector machine (SVM) was used to learnt to classifying between the targets and","acronyms":[[96,99]],"long-forms":[[72,94]]},{"text":" 207  Informational Contribution (IC) function for each element  in a pair.","acronyms":[[34,36]],"long-forms":[[6,32]]},{"text":"2008; Metzler and Cai, 2011).  Work in Content Founded Image Retrieval (CBIR) (Datta et al., 2008) has advances from systems that","acronyms":[[70,74]],"long-forms":[[39,68]]},{"text":"3.3 Sentiment Modelled The design of the sentiment model used in our system was basis on the assumption that the vistas expressed should be exceedingly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. Ours designed an interface that allow annotators to fulfill the annotations outdoor of AMT so that they could participate anonymously.","acronyms":[[400,403]],"long-forms":[[376,398]]},{"text":"Univers i ty  of Vienna  The first part of this paper is dedicated to an overv iew  of the parser of the system VIE-LANG (Viennese Language  Understanding System).","acronyms":[[112,120]],"long-forms":[[122,139]]},{"text":"{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com Abstract Onto natural language issue answering (QA) systems, questions often contain terms and","acronyms":[[101,103]],"long-forms":[[81,99]]},{"text":"= Task Defines RAW SCORI~  ((! OMM(~OST x number of messages)  - (INFCOST x nnmber  o f  in fe rences)  ","acronyms":[[31,34]],"long-forms":[[36,60]]},{"text":" 2. Tezt routing, tezt fdter\/n9, and SDI (selective dissemination of information): These terms  refer to a loose collection of text classifying mission such as managing personal electronic mail, ","acronyms":[[37,40]],"long-forms":[[42,65]]},{"text":" 6 Finished Segmentation  For wrongly spelled or OOV (out of vocabulary)  Urdu words, the system may forcibly break the ","acronyms":[[45,48]],"long-forms":[[50,67]]},{"text":" 1 Introductions Electroencephalography (EEG) and magnetoencephalography (MEG) are similar methods for","acronyms":[[40,43],[73,76]],"long-forms":[[16,38],[49,71]]},{"text":"In order to take the transitivity of outscoping relations into account, we use the transitive closure (TC) of DAGs. Let G+ =","acronyms":[[103,105],[110,114]],"long-forms":[[83,101]]},{"text":"are routinely followed by a number n ? 0. DU = discourse unit, ce = conversational event, K = DRS, u = utterance, sem","acronyms":[[40,42],[92,95]],"long-forms":[[45,54],[101,110]]},{"text":"3.4 Algorithm The algorithm first splits the data into appropriate units (SL=source language, TL=target language): 1.","acronyms":[[74,76],[94,96]],"long-forms":[[77,92],[97,112]]},{"text":"son, 2012; Vlas and Robinson, 2011). Due to its expressiveness, natural language (NL) became a popular medium of communication between users and","acronyms":[[82,84]],"long-forms":[[64,80]]},{"text":"et al, 2000b). A more complex asks targets the Aircraft Preserving Manual (AMM) of the Airbus A320 (Rinaldi et al, 2002b).","acronyms":[[83,86]],"long-forms":[[54,81]]},{"text":"ticular, this includes a model of the grounding process (Clark, 1996) that involves recognition and build of common overland units (CGUs) (see (Traum, 2003)). ","acronyms":[[137,141]],"long-forms":[[116,135]]},{"text":"sity is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).","acronyms":[[123,128],[52,56]],"long-forms":[[95,121]]},{"text":"subject and object with the ground truth. ETS\/ETO = Emotions towards subject\/object, MAS=mean absolute error, and RMSE= root mean square error","acronyms":[[85,88],[42,49],[114,118]],"long-forms":[[89,102],[52,83],[120,142]]},{"text":"e i Algorithm 1 Sparse projection (SP) Require: v \/\/ Vocabulary: vector of n words","acronyms":[[35,37]],"long-forms":[[16,33]]},{"text":"Since all covariance matrices are auspicious semi-definite, the quadratic program (QP) remains convex in w?, ","acronyms":[[81,83]],"long-forms":[[62,79]]},{"text":"We used five retrieval schemes to generate  relevance scores for query-document peers:  Fuzzy Boolean (FB). This system translates a query ","acronyms":[[103,105]],"long-forms":[[88,101]]},{"text":"Extended Markup Vocabulary (XML) is a pro-  posed standard (XML, 1997) specified by the World  Wide Web Consortium (W3C). Among XML, tags and ","acronyms":[[114,117]],"long-forms":[[93,112]]},{"text":"participate in the interpretation f the CLS (e.g., elements bearing the thematic roles assigned by  the predicate, etc.). DPSs (DP structures) semantically characterize noun phrases. They consist ","acronyms":[[122,126],[40,43]],"long-forms":[[128,141]]},{"text":"directly from the speech signal. In newer years, a variant of dynamic times warping (DTW) has been proposed to find reoccurring patterns in the speech","acronyms":[[85,88]],"long-forms":[[63,83]]},{"text":" ) are based on the pairwise mutual information (PMI) between two phrases.","acronyms":[[49,52]],"long-forms":[[20,47]]},{"text":"guages: German, French and Italian, with German  usually serving as the source language (SL),  French and Italian as the target language (TL). ","acronyms":[[138,140],[89,91]],"long-forms":[[121,136],[72,87]]},{"text":"A similar embedding method, called ? Global Vector (GloVe)?, was","acronyms":[[52,57]],"long-forms":[[37,50]]},{"text":"quency weighted rappel appraise. We used a  Japanese frequency dictionary (FD) generated  from the Japanese EDR corpus (Isahara, 2007) to ","acronyms":[[77,79]],"long-forms":[[55,75]]},{"text":"future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) plan learn an extractor for each aiming relation from la-","acronyms":[[79,81]],"long-forms":[[55,77]]},{"text":"tions. This model has three main steps including Locale Term Weighting (LTW), World Term Weighting (GTM), and Fuzzy Clustering (Algo-","acronyms":[[71,74]],"long-forms":[[49,69]]},{"text":"Holes, 2004). Plus tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.","acronyms":[[82,85],[114,117]],"long-forms":[[53,80]]},{"text":"smoothing as the evaluation metric.  Best v.s. Stays (BR) To score the best hypothesis in the n-best set","acronyms":[[53,55]],"long-forms":[[37,51]]},{"text":"It  is at this critical point, when care is being trans-  ferred from the Operating Salle (OR) to the ICU  and monitoring is at a minimal, that the pa- ","acronyms":[[90,92],[101,104]],"long-forms":[[74,88]]},{"text":"chanical Turk service. Two classifiers, Na??ve Bayes (NB) and a support vector machine (SVM), were applied on the tokenized and stemmed state-","acronyms":[[88,91],[54,56]],"long-forms":[[64,86],[40,52]]},{"text":"tion access mission. Current approaches to AZ rely on supervised machine learning (ML). ","acronyms":[[81,83],[41,43]],"long-forms":[[63,79]]},{"text":"particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence  restrictions (FCRs), and feature specification defaults (FSDs)  - -  and four universal components - - a theory of syntactic fea- ","acronyms":[[188,192],[48,50],[92,94],[145,149]],"long-forms":[[156,186],[27,46],[73,90],[108,143]]},{"text":"POS tag distribution. We also use features based on part of speech (POS). We tag use","acronyms":[[68,71],[0,3]],"long-forms":[[52,66]]},{"text":"actual object and \\[AI is the word that represents A. CS : = speaking A;yes refers A \\[A\\] ; yes  (111) Resource Situation(RS)  A resource situation is defined for each individual in a discourse; it ","acronyms":[[123,125],[54,56]],"long-forms":[[104,122]]},{"text":"transcription is carried out by using dynamic  programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).","acronyms":[[102,105],[169,172]],"long-forms":[[90,100],[158,167]]},{"text":"Interpreting news requires detect its constituent events. Information extraction (CI) makes this feasible by considering only events of a specified type,","acronyms":[[87,89]],"long-forms":[[63,85]]},{"text":"single weight matrixW . In contrast, the CVG uses a syntactically untied RNN (SU-RNN) which has a set of such weights.","acronyms":[[78,84]],"long-forms":[[52,76]]},{"text":"Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH). ","acronyms":[[131,136],[68,70],[74,76]],"long-forms":[[90,112]]},{"text":"ordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB =","acronyms":[[116,119],[24,26],[40,43],[69,71],[97,100],[141,143],[154,157],[180,182],[195,197],[214,216]],"long-forms":[[122,132],[29,38],[46,67],[74,95],[103,114],[146,152],[160,178],[185,193],[200,212]]},{"text":"curacy of an automatic classifier means to compare its output with the correcting semantic tags on a Gold Standard (GS) dataset. Indoors our formal","acronyms":[[113,115]],"long-forms":[[98,111]]},{"text":"words words  Fig. 1 Structure of Bunruigoihy6 (BGH)  This paper focuses on classifications only nouns in terms of ","acronyms":[[47,50]],"long-forms":[[33,45]]},{"text":"larger segments.  Speech Systems Incorporated (SSI) has been using a  version of this two-stage cascade of the MMI encoders in the ","acronyms":[[47,50],[111,114]],"long-forms":[[18,45]]},{"text":"2.1 Collection Tasks To collect our data, we used two different types of human intelligence tasks (HITs). In type 1, the","acronyms":[[99,103]],"long-forms":[[73,97]]},{"text":"Researchers have ex-plored the topic of CLI in the areas of lexical style (Jarvis et al 2012a), lexical n-grams (Jarvis & Paquot, 2012), character n-grams (Tsur & Rappo-prot, 2007), using variables related to cohesion, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), error patterns (Bestgen, et al 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al 2012b; Koppel et al 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).  Such studies have demonstrated relatively strong success rates for classifying an L2 writing sample based on the L1 of the writer. For instance, Jarvis and Paquot (2012), using 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al 2009) achieved a 53.6% classification accuracy for 12 groups of L1s. Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge taken from the computational tool Coh-","acronyms":[[813,817],[40,43],[590,592],[621,623]],"long-forms":[[772,811]]},{"text":"al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instruc-","acronyms":[[91,94]],"long-forms":[[57,89]]},{"text":"to ? constraints? in interactive topic modelling (ITM) (Hu et al, 2014).","acronyms":[[47,50]],"long-forms":[[21,45]]},{"text":"as the World Wide Web. Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech- ","acronyms":[[66,68],[124,127]],"long-forms":[[44,64],[96,123]]},{"text":"the method described in Section 3.2). We also present the number of linear equations (L.Eq.) employs","acronyms":[[86,91]],"long-forms":[[68,84]]},{"text":"somewhat streamlining version of Webber's original rule  schema, says that for any formulas that meets the  structural description (SD), a discourse ntity identified  by the ID formula is to be constructed.","acronyms":[[129,131],[171,173]],"long-forms":[[105,127]]},{"text":"learning community: ? Rectified Linear Unit (ReLU) (Nair and Hinton, 2010): max(0, x);","acronyms":[[45,49]],"long-forms":[[22,43]]},{"text":"The current state of the art within the comment summarisation field is to cluster comments using Latent Dirichlet Allocation (LDA) topic modelling (Khabiri et al, 2011; Ma et al, 2012;","acronyms":[[126,129]],"long-forms":[[97,124]]},{"text":"The idea is simply to count the mmlber of new  words introduced ow'x a moving interval and 1)roduce  what he calls a vocabulary managemenl profile (VMI'),  or lneasurements at intervals.","acronyms":[[148,152]],"long-forms":[[117,146]]},{"text":"The\/AT table\\]NN is\/BEZ ready\/J\/.\/.  (PPS = subject pronoun; MD = modal; V'B =  verb (no inflection); AT = article; NN = noun;  BEZ ffi present 3rd sg form of \"to be\"; Jl = ","acronyms":[[102,104],[116,118],[38,41],[61,63],[73,76]],"long-forms":[[107,114],[121,125],[66,71],[80,84],[44,59]]},{"text":"Initiative (ECI) plans to distribute akin material in a multiplicity of languages. Even  greater esources are expected from the Linguistic Data Consortium (LDC). And the ","acronyms":[[154,157],[12,15]],"long-forms":[[126,152]]},{"text":"  The project used the Linguamatics Interactions  Information Extraction (I2E) platform. This ","acronyms":[[73,76]],"long-forms":[[49,71]]},{"text":"words, respectively. We include two versions of this general model; Continuous Bag of Words (CBOW) that predicts a word based on the context, and Skip-","acronyms":[[93,97]],"long-forms":[[68,91]]},{"text":" We examine deux resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al, 2013) and WordNet (Fell-","acronyms":[[79,83]],"long-forms":[[58,77]]},{"text":"or database .  Superficially, DEFT resembles a Natural language Understanding (NLUI) system ; instead, there are key differences .","acronyms":[[79,83]],"long-forms":[[47,77]]},{"text":"mark concept are sent to the kernel-based location belief tracker, while all other concepts are sent to a Dynamic Probabilistic Ontology Trees (DPOT) semantic belief tracker, whose structure is shown in","acronyms":[[144,148]],"long-forms":[[106,142]]},{"text":" 1 Introduction Natural Language Processing (NLP) and Machine Learning (ML) are making a considerable impact in","acronyms":[[45,48],[72,74]],"long-forms":[[16,43],[54,70]]},{"text":" Introduction  Categorial Grammars (CGs) consist of two compo-  nents: (i) a lexicon, which assigns syntactic types ","acronyms":[[36,39]],"long-forms":[[15,34]]},{"text":"SK(d1,d1)?SK(d2,d2) 4 Experiments The use of WordNet (WN) in the term similarity function introduces a prior knowledge whose impact","acronyms":[[54,56],[0,2],[10,12]],"long-forms":[[45,52]]},{"text":" Before we discuss the significant sentence in answer mails, we classified answer mails into three types: (1) direct answer (DA) mail, (2) questioner?s reply (QR) mail, and (3) the others.","acronyms":[[125,127],[159,161]],"long-forms":[[110,123],[139,157]]},{"text":" 1 Introduction Ever since Question Answering (QA) emerged as an active investigative field, the community has slowly","acronyms":[[47,49]],"long-forms":[[27,45]]},{"text":"(equivalent to relation). Their relation instances are named entity(NE)-mention pairs conforming to a set of pre-specified regulatory.","acronyms":[[68,70]],"long-forms":[[55,67]]},{"text":" Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0","acronyms":[[87,91],[109,114],[66,72]],"long-forms":[[75,85],[98,107],[43,64]]},{"text":"linguistic resources  ? Semantic Analysis Module (SAM) interpreting  LPM output using application knowledge ","acronyms":[[50,53],[69,72]],"long-forms":[[24,48]]},{"text":" On the other hand, the decline in performance for the composite feature vector baseline (CFV) may be attributed to the data sparseness phenomenon","acronyms":[[90,93]],"long-forms":[[55,79]]},{"text":" 3 Conditional Random Fields  Conditional Random Fields (CRFs) are a type of  discriminative probabilistic model proposed for ","acronyms":[[57,61]],"long-forms":[[30,55]]},{"text":"\\[Harman, 1996\\] D. Harman. Overview of the Forth  Text RetrievalConference (TREC-4). Onto Proceedings ","acronyms":[[77,83]],"long-forms":[[44,75]]},{"text":" ? Backward Looking (BL)\/Forward Looking (FL) featured (14 to 22) are mostly extracted from ut-","acronyms":[[42,44],[21,23]],"long-forms":[[25,40],[3,19]]},{"text":"exist solely to guide processing. These words, known  as function words (FW's), are quite common, and  include articles, prepositions, and auxiliary verbs.","acronyms":[[73,77]],"long-forms":[[57,71]]},{"text":"come. 2008. Decided strictly local (SL) languages.","acronyms":[[37,39]],"long-forms":[[21,35]]},{"text":"This gave soar to a relatively new research zona within the emerging field of Textto-Text Generation (TTG) called Multiple-Choice Test Item Generation (MCTIG).1","acronyms":[[102,105],[152,157]],"long-forms":[[78,100],[114,150]]},{"text":" 4 Algorithms 4.1 Topic?sentence graph matching (GM) We treat a sentence and a topic as graphs.","acronyms":[[49,51]],"long-forms":[[33,47]]},{"text":"ceedings of the 10th International Conference on Text, Speech, and Dialogue (TSD-2007), Lecture Notes in Computer Science (LNCS), Springer-Verlag.","acronyms":[[123,127],[77,80]],"long-forms":[[88,121]]},{"text":"2014).  The RST Rhetoric Treebank (RST-DT) (Carlson et al.,","acronyms":[[36,42]],"long-forms":[[12,34]]},{"text":"92\\]. This selective approach led to significant  results in some restricted applications (ATIS...). ","acronyms":[[91,98]],"long-forms":[[77,89]]},{"text":"These interfaces stand to play a critical role in the  ongoing migration of interaction fi'oln the desktop  to wireless portable computing devices (PI)As, next-  generation phones) that offer limited screen real es- ","acronyms":[[148,150]],"long-forms":[[120,146]]},{"text":"To estimate the weights ? i in formula (1), we  use Minimum Error Rate Training (MERT) algorithm, which is widely used for phrase-based ","acronyms":[[81,85]],"long-forms":[[52,79]]},{"text":"of money to fulfil tasks that are simple for humans but difficult for computers. Examples of these Human Intelligence Tasks (HITs) range from labeling images to moderating blog observations to providing commentaries on the relevance of consequence for a search query.","acronyms":[[126,130]],"long-forms":[[100,124]]},{"text":"End point SIP user agents: Ces are the SIP terminates points that exchange SIP signaling messages with the SIP Application server (AS) for call control.","acronyms":[[126,128],[41,44],[70,73],[102,105]],"long-forms":[[106,124],[10,13]]},{"text":"Next subsections exhibition the results achieved by TIPSem system in each one of the TempEval-2 tasks for English (EN) and Spanish (ES). More-","acronyms":[[109,111],[46,52],[79,89],[126,128]],"long-forms":[[100,107],[117,124]]},{"text":"Throughout frame semantics, the meaning of words or word expressions, besides called target words (TW), comprises aspects of conceptual structures, or frames, that de-","acronyms":[[88,90]],"long-forms":[[74,86]]},{"text":"factoid ones - as well as new elements ? such as  expected polarity type (EPT). However, opi-","acronyms":[[74,77]],"long-forms":[[50,72]]},{"text":"la AR  TD  par PREP (prEposition)  main SUBS (substamif)  REC8 (rEcursif simple) ","acronyms":[[40,44],[3,5],[7,9],[15,18],[58,62]],"long-forms":[[46,55],[21,32],[64,72]]},{"text":"Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks (HEUR-B), and minimal phrases (HEUR-P) as de-","acronyms":[[125,131],[26,30],[84,87],[142,148],[172,178]],"long-forms":[[94,123]]},{"text":"In the next experiments, we run experiments across three different locales in Places domain: United Kingdom (GB), Australia (AU), and India (IN).","acronyms":[[125,127],[109,111],[141,143]],"long-forms":[[114,123],[93,107],[134,139]]},{"text":"Utilized the observation that LL is correlated with MRR on the same data set, we expect that optimizing LL on a development set (LLdev) will also improvement MRR on an assess set (MRReval).","acronyms":[[126,131],[27,29],[49,52],[151,154],[177,184]],"long-forms":[[101,120]]},{"text":"1982, 1984; Clark 1992; Cremers 1996; Arts 2004). The present article will examine its consequences for the generation of referring expressions (GRE). In doing this, we dis-","acronyms":[[145,148]],"long-forms":[[108,143]]},{"text":"(Eisner, 1996). We defining role value labeled precision (RLP) and role value labeled remind (RLR) on dependency links as follows:","acronyms":[[92,95],[56,59]],"long-forms":[[65,90],[26,54]]},{"text":"rithms for learning neuropsychological and demographic data which are then used for the forecasting of Clinical Dementia Rating (CDR) scores for diversified sub-types of Dementia and other cog-","acronyms":[[128,131]],"long-forms":[[102,126]]},{"text":"1998) as the reference. In Chinese FrameNet, the predicates, called lexical units (LU), evoke frames which circa correspond to several","acronyms":[[83,85]],"long-forms":[[68,81]]},{"text":"As observed by Melamed (2000), the problem of conclusions the best set of links is the maximum-weighted bipartite matching (MWBM) problem: Given a weights bipartite graph G = (V1 ? V2, E) with","acronyms":[[117,121]],"long-forms":[[107,115],[159,164]]},{"text":"2214  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 1?9, Gothenburg, Sweden, April 26-30 2014.","acronyms":[[74,76],[32,36]],"long-forms":[[54,72]]},{"text":"nese personal naming system. Therefore, we hold  Chinese personal name disambiguation (CPND) to  explore those problems.","acronyms":[[87,91]],"long-forms":[[49,85]]},{"text":"  Figure 2.  Words Correct (WC) scores from Teachers  and Machine scoring at the reader level (n = 87).","acronyms":[[28,30]],"long-forms":[[13,26]]},{"text":" In Proceedings of the 17th International Conference on Computational Linguistics (COLING), Montreal, Canada.","acronyms":[[83,89]],"long-forms":[[56,81]]},{"text":"non relevant texts has the lowered expectation. Figure 1 describes the probability density function (PDF ) for domain frequency scores of the ATHLETE domain","acronyms":[[99,102],[140,145]],"long-forms":[[69,97]]},{"text":"1 Introduction  In this paper we address the event extraction task  defined in Automatic Content Extraction (ACE)1  program.","acronyms":[[109,112]],"long-forms":[[79,107]]},{"text":"BG 80,757 1.34 EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian","acronyms":[[58,60],[70,72],[15,17],[0,2],[82,84],[94,96]],"long-forms":[[61,68],[73,80],[85,92],[97,106]]},{"text":"and tfidf of unigrams, bigrams, and trigrams.  DAL (Dictionary of Affect in Language) is a psycholinguistic resource to measure the emo-","acronyms":[[47,50]],"long-forms":[[52,84]]},{"text":"demanding.  Latent Semantic Analytic (LSA) (Deerwester et al.,","acronyms":[[38,41]],"long-forms":[[12,36]]},{"text":"sic and Young, 2011; Williams, 2010; Young et al., 2010) and Bayesian network (BN)-based methods (Raux and Ma, 2011; Thomson and Young,","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and purposes sentences are using for training statistical","acronyms":[[81,83],[51,55]],"long-forms":[[60,79],[8,49]]},{"text":"A Robust Algorithm for the Tree Edit Distancing.  Proceedings of the VLDB Endowment (PVLDB), 5(4):334?345.","acronyms":[[83,88]],"long-forms":[[48,71]]},{"text":"types through various features based on e.g., partof-speech tagging (POS) and named entity recognition (NER). ","acronyms":[[104,107],[69,72]],"long-forms":[[78,102],[46,59]]},{"text":"pus (Mitchell et al, 2003)1. Both the Newswire (NWIRE) and Spreading News (BNEWS) chapters where split into 60-20-20% document-based par-","acronyms":[[75,80],[48,53]],"long-forms":[[59,73],[38,46]]},{"text":"of not requiring so much photocopied. On the con-  trary, constraint unification (CU) (Hasida 1986,  Tuda et al 1989), a disjunctive monotheism ","acronyms":[[78,80]],"long-forms":[[54,76]]},{"text":"evaluating the attribute subsets. Hun evaluation is based on consistency (CBF) and correlation (CFS). ","acronyms":[[76,79],[98,101]],"long-forms":[[63,74],[85,96]]},{"text":" In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 1?9.","acronyms":[[63,69],[71,76]],"long-forms":[[50,61]]},{"text":"Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and","acronyms":[[81,84]],"long-forms":[[53,79]]},{"text":"This paper describes a set of experiments on two sub-tasks of Quality Assessment of Machine Translation (MT) output.","acronyms":[[105,107]],"long-forms":[[84,103]]},{"text":"maries C are overall better for answering questions than summaries B. Comparison between B and C (B-C) precision recall","acronyms":[[98,101]],"long-forms":[[89,96]]},{"text":" We incorporated two sets of linguistic features into a maximum entropy (MaxEnt) model and devise aMaxEnt-based binary classifier to predict the cat-","acronyms":[[70,76],[97,103]],"long-forms":[[53,68]]},{"text":"proaches, namely Bisecting K-Means (Steinbach et al.,  2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA).","acronyms":[[92,95],[135,138]],"long-forms":[[66,90]]},{"text":"Sentence) and mcdonalds (2006) on a sentence-by-sentence basis. Table 1 denotes the compression rates (CompR) for the three systems and evaluates the quality of their output use grammatical relations F1.","acronyms":[[100,105]],"long-forms":[[81,98]]},{"text":"NE = Named Entity CE = Correlated Entity EP = Entity Profiles SVO = Subject-Verb-Object","acronyms":[[41,43],[0,2],[18,20],[61,64]],"long-forms":[[46,60],[5,17],[23,40],[67,86]]},{"text":"city. City and state can then be used to select a local area language model (LM) for recognizing listing names.","acronyms":[[77,79]],"long-forms":[[61,75]]},{"text":"Normalization (WCCN) (Dehak et al., 2011) and Eigen Factor Radial (EFR) (Bousquet et al., 2011).","acronyms":[[67,70],[15,19]],"long-forms":[[46,65]]},{"text":"with common sense knowledge.       Natural language processing (NLP) techniques  such as part of speech tagging and parse tree gen-","acronyms":[[64,67]],"long-forms":[[35,62]]},{"text":"Ph i lade lph ia ,  PA  191C4  .ABSTRACT  Trees Adjoining Grammar (TAG) is a formalism for natural  language grammars.","acronyms":[[66,69],[20,22]],"long-forms":[[42,64],[0,16]]},{"text":"whole sentence length.  SyntaxBased (SyntB): contextual features have been computed according to the ?","acronyms":[[38,43]],"long-forms":[[25,36]]},{"text":"Visweswariah et al (2011) regarded the preordering problem as a Voyage Salesman Problem (TSP) and applied TSP solvers for gain reordered words.","acronyms":[[92,95],[109,112]],"long-forms":[[64,90]]},{"text":"come. 2008. Deciding strictly local (SL) languages.","acronyms":[[37,39]],"long-forms":[[21,35]]},{"text":"qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, qnp), qdet?1.0 DET(the),","acronyms":[[52,54],[77,80],[30,32],[8,10]],"long-forms":[[56,60]]},{"text":"it as a model for compiling their own corpora.  The Russian National Corpus (RNC) has been released by the group of specialists from different organi-","acronyms":[[77,80]],"long-forms":[[52,75]]},{"text":"rules, along with a few lexical ordinances involving a list of stop phrases, rhetoric cue phrases and wordlevel parts of speech (POS) tags. First, paragraph","acronyms":[[125,128]],"long-forms":[[108,123]]},{"text":" 2. Computer  Aided Wri t ing (CAW)  A computer system for a writer is basically a ","acronyms":[[31,34]],"long-forms":[[4,29]]},{"text":"In addition, results from the machine learning based model are refined by a rule-based postprocessing, which is implemented using a finite state transducer (FST). The","acronyms":[[157,160]],"long-forms":[[132,155]]},{"text":"Text REtrieval Conference (TREC)1. The TREC 1The Text REtrieval Conference (TREC) is a series of evaluations of fully automatic Q\/A systems","acronyms":[[76,80],[27,31],[39,43],[128,131]],"long-forms":[[49,74]]},{"text":"Inference rules for predicates have been identified as an sizable element in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information","acronyms":[[132,134]],"long-forms":[[112,130]]},{"text":"? similar count? score (SC) is calculated as the number of personage that match between two","acronyms":[[24,26]],"long-forms":[[17,22]]},{"text":"In this paper we present our entry to the WMT?13 shared tasks: Quality Assessment (QE) for machine translation (MT). ","acronyms":[[111,113],[42,48],[82,84]],"long-forms":[[90,109],[62,80]]},{"text":"4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning","acronyms":[[85,90],[41,46]],"long-forms":[[48,83],[4,39]]},{"text":"posed web-based semantic similarity measurement: Jaccard, Dice, Overlap, PMI (Bollegala et al, 2007), Normalized Google Distancing (NGD) (Cilibrasi and Vitanyi, 2007), Sahami and Heil-","acronyms":[[127,130],[70,73]],"long-forms":[[99,125]]},{"text":"sists of given entities and their relation expression.  Here, we use a Salient Referent Listings (SRL) to obtain contextual structural.","acronyms":[[94,97]],"long-forms":[[71,92]]},{"text":" Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). ","acronyms":[[83,87]],"long-forms":[[52,81]]},{"text":"3.1 Data and preprocessing All embeddings are trained on 22 million tokens from the the North American News Text (NANT) corpus (Graff, 1995).","acronyms":[[114,118]],"long-forms":[[88,112]]},{"text":"not explicit about this. ? P5E2N4S3, F W A Computer Processable Language (CPL) (Clark et al. 2005) is a controlled variant of","acronyms":[[74,77],[27,35]],"long-forms":[[43,72]]},{"text":"Amongst the divergent learning algorithms available in QUEST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been display to performs very well","acronyms":[[135,138],[53,58],[103,106]],"long-forms":[[112,133]]},{"text":"P2 = < ash, c >}.  3-3 Mixed Fetters Adjunct Vocabulary (MAL)  We now have two different styles of rules in G, namely, the ","acronyms":[[54,57]],"long-forms":[[23,52]]},{"text":"be expressed via correspondences. We will defines a  variant of SSTC called synchronous SSTC (S-SSTC).  ","acronyms":[[93,99],[63,67]],"long-forms":[[75,91]]},{"text":" 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of discernment the similarity of a pair of punishments on","acronyms":[[62,65]],"long-forms":[[33,60]]},{"text":"inference of lexical semantic roles Subsequently the training phase, a testing procedure uses the Markov Chain Monte Carlo (MCMC) inference engine can be used to infer role labels.","acronyms":[[118,122]],"long-forms":[[92,116]]},{"text":" 66 approve a minimum Bayes risk (MBR) approach, with the goal of finding the graph with the lowest","acronyms":[[32,35]],"long-forms":[[12,30]]},{"text":"+ ??????????)  random Markov Clustering Algorithm (MCL)  (Dongen, 2000) ","acronyms":[[51,54]],"long-forms":[[22,49]]},{"text":"machine learning algorithm is used to discover the morph set of the language in question, using minimum description length (MDL) as an optimization criterion.","acronyms":[[124,127]],"long-forms":[[96,122]]},{"text":"1 Introduction ? Language Model (LM) Growing? refers to adding","acronyms":[[33,35]],"long-forms":[[17,31]]},{"text":"cation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).","acronyms":[[92,94],[124,128]],"long-forms":[[80,90],[100,123]]},{"text":"paper is the following:  Def. A generative system (GS) is a sequence TI,... ,Tn of TS,  whe~'~-TR(Tl,... ,Tn) is a relation between strings and D-trees and ","acronyms":[[51,53],[83,85],[95,97]],"long-forms":[[32,49]]},{"text":"\\[ Classroom (Tag) Kernel Nouns  act (AC)  an~ (AN)  art~fact (AR) ","acronyms":[[44,46]],"long-forms":[[39,42]]},{"text":"124   Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 202?203, Vancouver, October 2005.","acronyms":[[79,83]],"long-forms":[[31,77]]},{"text":"This paper discusses the automatic labeling of semantic relations in nominalized noun phrases (NPs) using a support vector machines learning algorithm.","acronyms":[[95,98]],"long-forms":[[81,93]]},{"text":" [and, therefore, so]  Contrastive Connectives (CC)  men den ?","acronyms":[[48,50]],"long-forms":[[23,46]]},{"text":"Schauder 91). The grammar is divided into an LD  (linear dominance) and an LP (linear precedence) part  so that the piecewise construction of syntactic ","acronyms":[[75,77],[45,47]],"long-forms":[[79,96],[50,67]]},{"text":"appear in the tagged training data. In this paper, we call their method the latent variable method (LVM). ","acronyms":[[101,104]],"long-forms":[[77,99]]},{"text":" The improvement of PAS analysis would benefit many natural language processing (NLP) applications, such as information extraction, summariza-","acronyms":[[81,84],[20,23]],"long-forms":[[52,79]]},{"text":"2.1 Purver, Ginzburg and Healey (PGH) Purver, Ginzburg and Healey (2003) investigated CRs in the British National Corpus (BNC) (Burnard, 2000).","acronyms":[[122,125],[33,36],[86,89]],"long-forms":[[97,120],[4,31]]},{"text":"However, a study regarding maximal recall shows that we do not remove too many true positives (TPs) (more details in Section 4.1).","acronyms":[[95,98]],"long-forms":[[79,93]]},{"text":"tract syntactic features for FB-LTAG.  We use with Sejong Treebank (SJTree) which  contains 32 054 eojeols (the unity of segmenta-","acronyms":[[68,74]],"long-forms":[[51,66]]},{"text":"A Machine Learning based Approach to Evaluating Retrieval Systems Huyen-Trang Vu and Patrick Gallinari Laboratory of Computer Science (LIP6) University of Pierre and Marie Curie","acronyms":[[135,139]],"long-forms":[[103,133]]},{"text":"Re-moving PHI from these free text portions requires application of techniques from natural language processing that are capable of detect phrases of specific types base on the lexical content (the words that make up the phrases) and the surround-ing words.  3 Current Methods and Metrics Fortunately, the problem of determine sorts of information in free text is a well-studied problem in the natural language processing community. We can leverage several decades of investigative on infor-mation extraction and the named entity identifica-tion problems in particular, comprising assorted community assessed such as the Message Un-derstanding Conferences (MUC) (Grishman & Sundheim, 1996) and the subsequent Automation Content Extract (ACE) evaluations1 ? both fo-cused on extraction from newswire -- as well as assessments of biomedical entity extraction from the published documentaries e.g., in the BioCreative evaluations (Krallinger, et al, 2008).","acronyms":[[743,746]],"long-forms":[[713,741]]},{"text":"CTexT. 2011. Afrikaans WordNet. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.","acronyms":[[60,65],[0,5]],"long-forms":[[32,58]]},{"text":"to say\" and the \"what to say\" is still an open  question. TOWELS (rags, 1999) proposes astandard  edifice for the data, but leaves the ","acronyms":[[58,62]],"long-forms":[[64,68]]},{"text":"lingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents","acronyms":[[100,103],[50,53],[68,71],[73,77]],"long-forms":[[90,98]]},{"text":"n?5\u0001WZWZ7V?Zo+Y#?ZWA<\u0007E  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224, October 25-29, 2014, Doha, Qatar.","acronyms":[[113,118]],"long-forms":[[63,111]]},{"text":"reporting deceptive positives. This can be quantified by the positive predictive value (PPV), or probability that a research finding is true.","acronyms":[[81,84]],"long-forms":[[54,79]]},{"text":"chore. As reported in (Liu et al 2013a), we utilize a genetic algorithm (GA) (Cormen et al 2001) to au80","acronyms":[[69,71]],"long-forms":[[50,67]]},{"text":"have to be induced from parallel corpora.  An inversion transduction grammar (ITG) strikes a good balance between STGs and SDTGs,","acronyms":[[78,81],[114,118],[123,128]],"long-forms":[[46,76]]},{"text":"for re-ranking in the context of name tagging.  Maximum Entropy models (MaxEnt) has  been insanely successful for many NLP classifi-","acronyms":[[74,80],[122,125]],"long-forms":[[48,63]]},{"text":"Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer. ","acronyms":[[125,127],[35,38],[57,60],[86,90]],"long-forms":[[128,143],[61,84],[91,123]]},{"text":"the percentage of tokens that are assigned the rightness head and dependency tagging, as well as the unlabeled attachment score (UAS), that is, the percentage of tokens with the correct head, and the label accuracy (LA), that is, the ratio of tokens with the correcting dependency label.","acronyms":[[212,214],[125,128]],"long-forms":[[196,210],[97,123]]},{"text":"scikit-learn python library 3 : Naive Bayes (NB), Nearest Neighbor (NN), Decision Tree (DT), Ran-","acronyms":[[45,47],[68,70],[88,90]],"long-forms":[[32,43],[50,66],[73,86]]},{"text":"guage varieties: the language of native speakers (N), the language of advanced, highly fluent nonnative speakers (NN), and translationese (T). We","acronyms":[[114,116],[50,52],[139,141]],"long-forms":[[97,103],[33,39],[123,137]]},{"text":"{wangruibo,gaoyahui}@sxu.edu.cn Succinct Across this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the","acronyms":[[79,82]],"long-forms":[[56,78]]},{"text":" 1 I n t roduct ion   The development of Natural Linguistics (NL) systems  for data recovery has been a central issue in NL Pro- ","acronyms":[[59,61],[119,121]],"long-forms":[[41,57]]},{"text":" As an example consider the many alignments in Figure 4, with the gold standard alignment (GRAM) on the left and the generated alignment (GA) on","acronyms":[[95,97]],"long-forms":[[70,83]]},{"text":"NLP track report. In Proceedings of the 5th  Text REtrieval Conference (TREC-5). ","acronyms":[[72,78],[0,3]],"long-forms":[[40,70]]},{"text":"  1 Introduction  Innumerable Natural Language Processing (NLP)  applications need to recognize when the meaning ","acronyms":[[52,55]],"long-forms":[[23,50]]},{"text":"ranking modeling on this data set, including Support Vector Appliance (SVM) with a linear kernel, SVM with a radial basis function (RBF) kernel and Logistic Regression (LR).","acronyms":[[128,131],[67,70],[94,97],[165,167]],"long-forms":[[105,126],[43,65],[144,163]]},{"text":"We use two pathways to measurement contribution in terms of graphemes: contseq(w, b) is the length of the longest prefix\/suffix of word w which blend b initiation\/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and b. This yields four featured:","acronyms":[[215,218]],"long-forms":[[187,213]]},{"text":" 1 Introduction Natural Language Processing (NLP) systems often consist of a series of NLP components, each formed","acronyms":[[45,48],[87,90]],"long-forms":[[16,43]]},{"text":"noisy and potentially unreliable observations.  While scenario template creation (STC) is a difficult problem, its evaluation is arguably more dif-","acronyms":[[82,85]],"long-forms":[[54,80]]},{"text":"feeling (FE)  feeding (FO)  group (GR)  location (LO) ","acronyms":[[32,34],[9,11],[20,22],[47,49]],"long-forms":[[25,30],[0,7],[14,18],[37,45]]},{"text":"Campus of Massachusetts-Boston  Abstract  Word sense disambiguation (WSD) is one of  the main challenges in Computational ","acronyms":[[73,76]],"long-forms":[[46,71]]},{"text":"parsing. At Tenth International Conference on Parsing Technologies (IWPT), pages 121?132, Prague, Czech Republic.","acronyms":[[68,72]],"long-forms":[[18,66]]},{"text":"columbia, edu  Abstract  Concept To Speech (CTS) scheme are  closely related to two other types of ","acronyms":[[44,47]],"long-forms":[[25,42]]},{"text":"treebank. In: Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24? ","acronyms":[[86,89]],"long-forms":[[51,84]]},{"text":"Table 1). Firstly, each term candidate is mapped to  a specific canonical representative (CR) by  semantically isomorphic transforms.","acronyms":[[90,92]],"long-forms":[[64,88]]},{"text":"robe such conflicting constraints. In this methodology, the owner of the TM generates a Phrase Table (PT) from it, and makes it accessible to the user following","acronyms":[[100,102],[71,73]],"long-forms":[[86,98]]},{"text":"(DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) ?","acronyms":[[123,125],[1,3],[13,15],[27,29],[41,43],[54,56],[68,70],[81,83],[93,95],[109,111],[137,139],[154,156]],"long-forms":[[114,121],[18,25],[32,39],[46,52],[59,66],[73,79],[86,91],[98,107],[128,135],[145,152]]},{"text":"\u0000 -movement (mostly wh-movement: WH), hollow complementizers (COMP), empty units (UNIT), and traces representing pseudo-attachments","acronyms":[[61,65],[33,35],[81,85]],"long-forms":[[44,59],[74,78]]},{"text":"is showed by the dotted black line.  The receiver operating traits (ROC) curves in Figures 2 and 3 demonstrate perfor-","acronyms":[[79,82]],"long-forms":[[44,77]]},{"text":"numeroinen se on (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).","acronyms":[[63,65],[18,20],[107,109]],"long-forms":[[68,77],[23,32],[112,128]]},{"text":"explanations of these metrics are described below.  Symptoms name recognition rate (RRdet),  recognition error rate (RERdet) and recognition ","acronyms":[[83,88],[116,122]],"long-forms":[[65,81],[92,114]]},{"text":"2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).  Most semantic role labeling (SRL) systems to date conceptualize the tasks as a supervises learned problem and rely on role-annotated data for model training.","acronyms":[[117,120]],"long-forms":[[93,115]]},{"text":"all level of syntactic represent-  ation, name\\].y, D-structure,  S-structure, and LF (logical form). ","acronyms":[[84,86]],"long-forms":[[88,100],[55,64],[69,78]]},{"text":"removed for expository reasons.  rewrites into an (optional) sentence adjunct (SA), a  theme, a verbphrase and subject's right adjunct ","acronyms":[[79,81]],"long-forms":[[61,77]]},{"text":"city. Ville and state can then be used to select a local area vocabulary model (LM) for recognizing listing names.","acronyms":[[77,79]],"long-forms":[[61,75]]},{"text":"Center for Language Technology. After accomplishing the task concerning named entity (NE)  identification, we go on studying identification ","acronyms":[[86,88]],"long-forms":[[72,84]]},{"text":"Figure 1: Overall architecture of Sentiment Classifier when a word is used with highly positive (HP), positive (P), highly negative (HN), negative (N) or objective (O) meaning based on a sentiment sense inven-","acronyms":[[133,135],[97,99]],"long-forms":[[116,131],[80,95],[102,110],[138,146],[154,163]]},{"text":"boring dependency structures. CC = coordinate concatenate, LA = left adjoining, and RA = right neighboring.","acronyms":[[59,61]],"long-forms":[[64,78]]},{"text":"Features. In Proceedings of the 21st Conference on Computational Linguistics (COLING). ","acronyms":[[78,84]],"long-forms":[[51,76]]},{"text":"grammatical individual and number (1PS, 1PP, 2P, 3PS, 3PP), the quantified pronouns (QUANT), and a grouping including all other expressions (OTHER). ","acronyms":[[134,139],[81,86]],"long-forms":[[115,132],[60,79]]},{"text":" Ours ran three parsing experiments: (i) lieu the value of the surface form (FORM) of pronominal prepositions with their lemma form (LEMMA), for","acronyms":[[80,84],[136,141]],"long-forms":[[74,78],[124,129]]},{"text":"the MACH-III expert system, we should begin with  a brief description. Functional hierarchy (FH) is a  new paradigm for organizing expert system ","acronyms":[[93,95],[4,12]],"long-forms":[[71,91]]},{"text":"tence pairs were selected from the WMT Giga corpus if the perplexity of their Anglais portion with respect to a language paragon (LM) trained on French news data was below a given threshold.","acronyms":[[124,126],[35,38]],"long-forms":[[108,122]]},{"text":"As noted by Melamed (2000), the problem of finding the best set of links is the maximum-weighted bipartite matching (MWBM) problem: Given a weighted bipartite graph G = (V1 ? V2, E) with","acronyms":[[117,121]],"long-forms":[[107,115],[159,164]]},{"text":"ellieioncy has been drastically improved for n)orpho-  logical analysis by representing large dictionaries with  Finite State Automata (FSA) and by representhig two-  level rnles and le?ical hlforination with finite-state ","acronyms":[[136,139]],"long-forms":[[113,134]]},{"text":"Table 1. Hedging and accuracy of each derived feature for RTE3 revised development gathering  (RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All). ","acronyms":[[135,143],[59,63],[97,106],[161,165],[178,185]],"long-forms":[[113,133]]},{"text":"by adapting the baseline model to four adaptation domains. In particular, we attempt to interpret the results given in terms of the character error rate (CER) by  correlating them with the characteristics of the adaptation domain measured us-","acronyms":[[154,157]],"long-forms":[[132,152]]},{"text":"1 Scope and Prior Work We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.","acronyms":[[120,123]],"long-forms":[[88,118]]},{"text":"EOPAS (PARADISEC tool) for text interlinear text and media analysis  2. NLTK (Natural Linguistics Toolkit) for texts analytics with linguistic data (Bird, Klein, & Loper, 2009)  3.","acronyms":[[72,76],[0,5],[7,16]],"long-forms":[[78,102]]},{"text":"Run 3 100% 18 (9.0%) 38 (19.0%)  Table 7.  Effect of Translations (E-C)   ","acronyms":[[66,69]],"long-forms":[[43,49]]},{"text":"Scores for PK    Because Academia Sinica corpora (AS) are  provided by us, we are not allowed to participate ","acronyms":[[50,52],[11,13]],"long-forms":[[25,40]]},{"text":"identified linguistic context, the task is to predicted the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model.","acronyms":[[98,102]],"long-forms":[[73,96]]},{"text":" 1 In t roduct ion   Language Understanding (LU) has been the focus of much research work in the last twenty ears. ","acronyms":[[45,47]],"long-forms":[[21,43]]},{"text":" The output is a prediction of whether the tweet is inside region (IR) or outside region (OR). We","acronyms":[[67,69],[90,92]],"long-forms":[[52,65],[74,88]]},{"text":"for a exhaustive comparison: ? Mean absolute error (MAE) measures how closely predictions resemble their observed","acronyms":[[55,58]],"long-forms":[[34,53]]},{"text":"Taking into account the above strategies, we suggested three concrete DS to CS conversions: Flat transformation with H auxiliary symbol (FlatH). ","acronyms":[[131,136],[68,70],[74,76]],"long-forms":[[90,112]]},{"text":"108   Proceedings of the 3rd Seminars on Hybrid Approaching to Translation (HyTra) @ EACL 2014, page 1, Gothenburg, Sweden, April 27, 2014.","acronyms":[[75,80],[84,88]],"long-forms":[[41,73]]},{"text":" Type Short time members Long time members Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon) Social networks Facebook, fb","acronyms":[[68,70],[93,95]],"long-forms":[[72,85],[97,112]]},{"text":" Examples of lexical and contextual rules learned by  the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb, ","acronyms":[[92,94],[73,76],[115,117],[146,148],[162,165]],"long-forms":[[97,105],[79,90],[120,144],[151,160],[168,172]]},{"text":"If we put these two constraints together we achieve the restrictions MINS = MAXS, which means that the area where quantifiers take scope (the MAXS-","acronyms":[[73,77],[66,70],[139,143]],"long-forms":[]},{"text":"of a multi-class document categorization. We introduce PRBEP (precision recall rupture yet point) as a measure which is popular in the area of infor-","acronyms":[[55,60]],"long-forms":[[62,95]]},{"text":"Subsequently we name the TD consisting of expression from gold training set and tagged test set and as Na??ve TD (NTD) for its unbalanced coverage in training and test set.","acronyms":[[104,107],[22,24]],"long-forms":[[93,102]]},{"text":"tions for this sentence.  Figure 5: The LF (left) and MRS (right) representations for the condemnation ?","acronyms":[[40,42],[54,57]],"long-forms":[[44,48]]},{"text":"583  Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7, New York City, New York, June 2006.","acronyms":[[68,72],[86,95]],"long-forms":[[24,66]]},{"text":"inforced by the proposing method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used.","acronyms":[[68,70]],"long-forms":[[53,66]]},{"text":" (UT = Utterance as actually made by the user, UR = Utterance as concedes by the system, SU = System utterance).","acronyms":[[47,49],[2,4],[91,93]],"long-forms":[[52,61],[7,16],[96,112]]},{"text":"Silhouette 2: Heat map of the relevance scores w s, j between the targeting domain Usenet (UN) with the other domains on ACE 2004 data set.","acronyms":[[84,86],[114,117]],"long-forms":[[76,82]]},{"text":"Other formats have been suggested for dictionary shares,  notably those developed under the Text Encoding Initiatives  using SGML (Standard Generalized Markup Language). We ","acronyms":[[125,129]],"long-forms":[[131,167]]},{"text":"In the next experiments, we run experiments across three different locales in Places domain: Unidos Kingdom (GB), Australia (AU), and Indies (IN).","acronyms":[[125,127],[109,111],[141,143]],"long-forms":[[114,123],[93,107],[134,139]]},{"text":"Three transliteration models have been used that  can generate the Hindi transliteration from an  Englishman named entity (NE). An Brits NE is ","acronyms":[[120,122],[136,138]],"long-forms":[[106,118]]},{"text":"Onto example, PropBank annotates 8,037 ARGM-MNR relationship (10.7%) out of 74,980 adjunct-like arguments (ARGMs). There are verbs","acronyms":[[103,108],[38,46]],"long-forms":[[92,101]]},{"text":"@\"' itself is counted. Another way is to counting how  many parts of words (PWs) are eontMnmd in the  SA.","acronyms":[[73,76],[99,101]],"long-forms":[[57,71]]},{"text":"\u0003\u0003\t\f\u0006\u0011\u000b\u0006\b strategies(Lewis, 1992).  We uses likelihood threshold(PT) strategy where each document is assigned to the categories above a thresh-","acronyms":[[65,67]],"long-forms":[[43,64]]},{"text":"? iyi = 0 This is a quadratic programming (QP) problem and we can always found the global maximum of","acronyms":[[43,45]],"long-forms":[[20,41]]},{"text":"in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is saying the other a storytelling.","acronyms":[[81,84]],"long-forms":[[68,79]]},{"text":"_ _eI$X=A\u0007I&HLH7K5HOG\u0007X5HOGPMLHLK ^ CWX=A$X=APH U\u0003I&K5X\u0010K5HOG\u0007X5H&GflMLHJa\u0007HLK5MOI5CEc\u0007CEG! ","acronyms":[[44,47]],"long-forms":[[48,90]]},{"text":" 1 Introduction  Spoken language translation (SLT) has become  more essential due to globalization.","acronyms":[[46,49]],"long-forms":[[17,44]]},{"text":"While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values.","acronyms":[[485,487],[576,578],[20,23]],"long-forms":[[467,483]]},{"text":"It is well known that for English, the automatic transforms of a constituency parser?s output to dependency format can accomplish competitive unlabeled attachment scores (ULA) to a dependence parser?s output trained on automatically converted trees","acronyms":[[168,171]],"long-forms":[[139,159]]},{"text":"  SVM Classification  SVM (Support Vector Machines) has attracted  much attention since it was introduced in (Boser et ","acronyms":[[22,25],[2,5]],"long-forms":[[27,50]]},{"text":"Implicit Incongruity (IMP) Boolean Incongruity of extracted implicit phrases (Rilof et.al, 2013) Explicit Incongruity (EXP) Integer Number of times a word follows a word of opposite polarity","acronyms":[[119,122],[22,25]],"long-forms":[[97,105]]},{"text":"cause de la limite des outils informatiques li?s ? son traitement  automatique,  ce  qui  rend  difcile  son  adh?sion  ?  ses  cons?urs  dans  le domaine des nouvelles technologies de l'information et de la communication (NTIC). Par cons?quent, un ensemble de recherches scientifques et linguistiques sont lanc?es pour rem?dier ?","acronyms":[[223,227]],"long-forms":[[159,221]]},{"text":"ate the surface form in the opposite direction.  Amazon?s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources","acronyms":[[75,80]],"long-forms":[[58,73]]},{"text":"Each extractor receives a sentence as input and determines which noun phrases (NPs) in the sentence are fillers for the event role.","acronyms":[[79,82]],"long-forms":[[65,77]]},{"text":"n?5\u0001WZWZ7V?Zo+Y#?ZWA<\u0007f  Proceedings of the 2014 Conference on Empirical Mode in Natural Language Processing (EMNLP), pages 1216?1224, October 25-29, 2014, Doha, Qatar.","acronyms":[[113,118]],"long-forms":[[63,111]]},{"text":" 3.4 MAP Inference Maximum a posteriori (MAP) inference seeks the solution to","acronyms":[[41,44],[5,8]],"long-forms":[[19,39]]},{"text":" 6. Rulings Tree (DT) - with 12,782 MWEs of D5.","acronyms":[[19,21]],"long-forms":[[4,17]]},{"text":" 2 (Durrett and Klein, 2013) call this error false new (FN). ","acronyms":[[56,58]],"long-forms":[[45,54]]},{"text":"Expanding  on a suggestion of Nfichieis (1982), we classify verbs  as subject equi (SEqui), object equi (OEqul), sub-  ject raising (SRals ing)  or object raising (ORuls ing) ","acronyms":[[84,89],[105,110],[164,173],[133,142]],"long-forms":[[70,82],[92,103],[148,162],[113,131]]},{"text":"Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To","acronyms":[[132,135]],"long-forms":[[104,130]]},{"text":"Each word is considered as an  instance. Maximum Entropy (MaxEnt) is employs in  this paper.","acronyms":[[58,64]],"long-forms":[[41,56]]},{"text":"given threshold, this binary feature fires.  4.3 Variant Feature (VAR) In the variant cipher, the plaintext is written into","acronyms":[[66,69]],"long-forms":[[49,64]]},{"text":"To v e r i f y  th'e r e l a t i o n s h i p s  between the s t a t i s t i c a l  models of word  importance and t h s  vector space model, dcsument c o l l e c t i o n s  are used i n  three  d i f f e r e n t  subjec t  a reas ,  including aerodynamics (cRAN),  medicine (MED) and  world a f f a i r s  (TIME).","acronyms":[[275,278],[257,261]],"long-forms":[[265,273]]},{"text":" A project that is based on a roughly similar notion of text meaning representation (TMR) concepts is the ?","acronyms":[[85,88]],"long-forms":[[56,83]]},{"text":"gros and is often simplified.  Because we use belief propagation (BP) as basic to compare to, and as a subroutine in our pro-","acronyms":[[67,69]],"long-forms":[[47,65]]},{"text":"predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy  score, H_AVE = human average scoring.9   ","acronyms":[[109,114],[36,38],[51,55],[79,83]],"long-forms":[[117,130],[28,34],[41,48],[58,71],[86,100]]},{"text":" - 19  -  Both L I and L 2 are CSL's (Context receptive languages). They ","acronyms":[[31,36]],"long-forms":[[38,65]]},{"text":" 1 Introduction Named entity acknowledgment (NER) is the most studied information extraction (IE) task.","acronyms":[[42,45],[91,93]],"long-forms":[[16,40],[67,89]]},{"text":"152 Kuhn A Survey and Classification of Controlled Natural Languages Controlled Language for Crisis Management (CLCM) (Temnikova 2010, 2011, 2012) is a language for writing instructions about how to deal with crisis situations.","acronyms":[[112,116]],"long-forms":[[69,110]]},{"text":"e-mail: lynet te@goldilocks.lcs.mit.edu  SUCCINCT  The Air Travels Information System (ATIS) domain serves as  the common task for DARPA spoken language system re- ","acronyms":[[86,90],[130,135]],"long-forms":[[55,84]]},{"text":"2.2 Hidden Markov Models One simple family of models for part-of-speech induction are the Hidden Markov Models (HMMs), in which there is a sequence of hidden state vari-","acronyms":[[112,116]],"long-forms":[[90,110]]},{"text":"(BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models qualified on the English Circulated News (BN) corpus (430 hours of audio) gave to us by IBM (Chen et al, 2009).","acronyms":[[113,115],[1,3],[163,166]],"long-forms":[[97,111]]},{"text":"1 Introduction We broadened a popular model, latent Dirichlet al location (LDA), to unbounded streams of documents.","acronyms":[[72,75]],"long-forms":[[42,70]]},{"text":"Langman dictionary.  Maximum Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Rat-","acronyms":[[38,44]],"long-forms":[[21,36]]},{"text":"The lacks of a  fully comprehensive bilingual dictionary including  the entries for all named agency (NEs) renders the  task of transliteration necessary for certain natural ","acronyms":[[103,106]],"long-forms":[[87,101]]},{"text":"relative-resource?, i.e.  EuroWordNet (EWN).1   In this paper we start by briefly recalling the ","acronyms":[[39,42]],"long-forms":[[26,37]]},{"text":" ? The Match Rate(MR): The match rate is the match number normalized","acronyms":[[18,20]],"long-forms":[[7,16]]},{"text":"For example, NNS  (noun ? plural) became NN (noun). ","acronyms":[[41,43],[13,16]],"long-forms":[[45,49],[19,25]]},{"text":"tice to both characteristics mentioned above. The central construct in this framework is  that of context factor (CF). A CF is defined by a scope, which is a collection of individ- ","acronyms":[[114,116],[121,123]],"long-forms":[[98,112]]},{"text":"1 Introduction Linguistics researches have shown that action verbs often denote some change of state (CoS) as the results of an action, where the change of state of-","acronyms":[[99,102]],"long-forms":[[82,97]]},{"text":"The difference of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-) disparity (Cover and Thomas 1991): Fixednesssyn (v, n)","acronyms":[[136,139]],"long-forms":[[118,134]]},{"text":"1 25   2. Loca l  Word  Grouper  (LWG)  The funct ion  of th i s  b lock  is to fo rm ","acronyms":[[34,37]],"long-forms":[[10,31]]},{"text":"lexicon tool, with a classification phase based on  Featured-Based kernel such as SL kernel and TreeBased kernel such as Dependency tree (DT) kernel  (Culotta and Sorensen, 2004) and Phrase Structure ","acronyms":[[138,140],[82,84]],"long-forms":[[121,136]]},{"text":"The GALE manual WA corpus and the Chinese to English corpus from the shared task of the NIST open machine translation (OpenMT) 2006 evaluation 6 were employed as the experimental corpus","acronyms":[[119,125],[4,8],[16,18],[88,92]],"long-forms":[[93,117]]},{"text":"gorithm used belongs to the family of algorithms described by Covington (2001), and the classifiers are formed use support vector machines (SVM) (Vapnik, 1995).","acronyms":[[143,146]],"long-forms":[[118,141]]},{"text":"In Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING), pages 120?128. ","acronyms":[[91,98]],"long-forms":[[40,89]]},{"text":"email: allan.ramsay@manchester.ac.uk Debora Field University of Sheffield (UK) email: D.Field@sheffield.ac.britons","acronyms":[[75,77]],"long-forms":[[50,73]]},{"text":"Part of FrameNet is also a corpus of 135,000 annotated example sentences from the British National Corpus (BNC). ","acronyms":[[107,110]],"long-forms":[[82,105]]},{"text":"al., 2005; Vapnik, 1998) based on the Gaussian  Radial Basis kernel function (RBF). We tuned ","acronyms":[[78,81]],"long-forms":[[48,76]]},{"text":"The representative vectors for each phoneme category consist of the mean vectors of the Gaussian Mixtures Model (GMM). ","acronyms":[[112,115]],"long-forms":[[88,110]]},{"text":" In Proc. Rich Text 2004 Fall Workshop (RT-04F). ","acronyms":[[40,46]],"long-forms":[[10,29]]},{"text":" To solve this problem, we introduce  Document oriented Preference Sets(DoPS). The ","acronyms":[[72,76]],"long-forms":[[38,70]]},{"text":"standing that exchanging tasks with OIE. AMR parsing (Banarescu et al, ), semantic role label (SRL) (Toutanova et al, 2008; Punyakanok et al, 2008)","acronyms":[[94,97],[37,40],[32,35]],"long-forms":[[70,92]]},{"text":"ditto Shi-fen three-hours ten-minute, i.e. 'three-  ten'), post-position phrases (GPs), preposition  words (PPs), br adverbs (ADVs). They all shares ","acronyms":[[128,132],[82,85],[110,113]],"long-forms":[[119,126],[88,108],[59,80]]},{"text":"stable functional definition during languages. These categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition,","acronyms":[[84,87],[101,104],[115,118],[129,132]],"long-forms":[[90,99],[107,113],[121,127],[135,145]]},{"text":"knowledge in building the target domain classifier, we propose a novel optimization method based on the Na??ve Bayesian (NB) framework and stochastic gradient descent.","acronyms":[[121,123]],"long-forms":[[104,119]]},{"text":"transitions, depending on whether the backward-looking center of Ui?1 is maintained or not in Ui and on whether CB(Ui) is also the most highly ranked entity (CP) of Ui: Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most highly ranked CF (CP) of Ui (i.e., CP(Ui) = CB(Ui))","acronyms":[[190,193],[112,114],[94,96],[65,67],[158,160],[196,198],[252,254],[256,258],[282,284],[273,275]],"long-forms":[[176,188],[199,201]]},{"text":"during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased as independent bloggers in August 2008.6 Because","acronyms":[[119,122],[34,36],[51,53],[75,77],[92,94],[133,135],[126,128]],"long-forms":[[102,117],[20,32],[40,49],[57,73],[81,90]]},{"text":"the best among all other symmetrization heuristics. The other was a Tree Edit Distance (TED) model, popularly used in a series of NLP appli-","acronyms":[[88,91],[130,133]],"long-forms":[[68,86]]},{"text":" We used Moses (Koehn et al 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al 2011) to trans-","acronyms":[[91,93],[109,114]],"long-forms":[[75,89]]},{"text":"Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein,  1998), latent semantic analysis (LSA) (Gong and  Liu, 2001).","acronyms":[[115,118],[50,53]],"long-forms":[[89,113],[22,48]]},{"text":"generative models which are respectively estimated  on their corresponding named entity lists employs  maximum likelihood estimate (MLE), together  with smoothing methods4.","acronyms":[[132,135]],"long-forms":[[101,130]]},{"text":"The results can be saw at:  http:\/ \/c lwww.essex.ac.uk\/w3c\/.  The project was  funded by JISC (the Joint Information Plans Com-  mittee of the UK Upper Education Funding Coun- ","acronyms":[[90,94],[146,148]],"long-forms":[[100,138]]},{"text":" 1 Introduction In this paper, we propose TroFi (Trope Finder), a about unsupervised clustering method for sep-","acronyms":[[42,47]],"long-forms":[[49,61]]},{"text":"ISREM 1 iff the hopefuls occurs 2 or more sentences before the anaphor POSITION 1 iff the antecedent occurs before anaphor SEMANTIC ROLE LABELLING (SR) IVERB 1 iff the govern verb of the given hopefuls is an issue verb","acronyms":[[149,151],[0,5],[72,80],[153,158]],"long-forms":[[124,137]]},{"text":"ferent setups with this parameter. We compare the following setups: (1) The majority baseline (BL) i.e., opting the most frequent labels (SR). (","acronyms":[[95,97],[139,141]],"long-forms":[[85,93]]},{"text":"these 153,014 verb-noun collocations.  We used 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)  as the Japanese thesaurus.","acronyms":[[65,68],[71,75]],"long-forms":[[48,63]]},{"text":"tation for the joint learning process. Specifically, we make use of the latent structural SVM (LS-SVM) (Yu and Joachims, 2009) formulation.","acronyms":[[95,101]],"long-forms":[[72,93]]},{"text":"BG 80,757 1.34 EN 94,725 2.58 Table 2: Corpus statistical: SR=Serbians, SL=Slovene, EN=English, BG=Bulgarian","acronyms":[[58,60],[70,72],[15,17],[0,2],[82,84],[94,96]],"long-forms":[[61,68],[73,80],[85,92],[97,106]]},{"text":"de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment under an attitude predicate (say), the events in Examples (6a) and (6b) are assessed as certain (CT+), whereas the words highly confident in Example (6c) trigger PR+, and may in Example (6d) leads to PS+.","acronyms":[[181,184],[246,249],[284,287]],"long-forms":[[172,179]]},{"text":"non-interactions   Y They are widely distributed and mediate all of the known biologic effects of  angiotensin II (AngII) through a variety of signal transduction systems, including activation of phospholipases C and A2, inhibition of adenylate cyc-","acronyms":[[115,120]],"long-forms":[[99,113]]},{"text":"Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via cross-","acronyms":[[118,121],[132,135]],"long-forms":[[94,116]]},{"text":"2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features.","acronyms":[[82,84],[90,93]],"long-forms":[[61,80]]},{"text":" At fewer for case schemata we have as first al-   ternative the choice between the realization types :CLAUSE  and :NG (noun groupings). For semantic structures from titles we ","acronyms":[[116,118]],"long-forms":[[120,130]]},{"text":"   Additionally, Mean Reciprocity Rank (MRR) is  also reported.","acronyms":[[39,42]],"long-forms":[[17,37]]},{"text":"Tags Base NP modifier NN (common noun), M (measurement word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR (proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (rating) Base NP head NN (common noun), M (measures word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR","acronyms":[[184,186]],"long-forms":[[188,199]]},{"text":"belief from the faith tracker in 6.3% of the dialogs.  The mean Word Mistake Rate (WER) per worker on the test set is 27.5%.","acronyms":[[82,85]],"long-forms":[[65,80]]},{"text":" The practical application of flame-based knowledge-based systems, such as in expert systems, requires the maintenance of  potentially very prodigious amounts of declarative knowledge stored in their acquaintance bases (KBs). As a KB raising in size and ","acronyms":[[212,215],[223,225]],"long-forms":[[195,210]]},{"text":"to cover terms frequently used by students, such as acronyms: E.L.C. (the English Language Center), R.O.C. (Republic of China), and so on. Other","acronyms":[[100,106],[62,67]],"long-forms":[[108,125],[74,97]]},{"text":"CHBWGE, H l V E  CSERCH FOR HERGEF IN 10  merge features  i n t o  node 10 (making it a schwa)  ANTEST CALLED FOB 211SCFIHDA (AACC) , SD= 3. RES= 7.","acronyms":[[126,130],[134,136],[0,6],[8,15],[17,23],[28,34],[24,27],[141,144]],"long-forms":[[96,124]]},{"text":"TIGER treebank. In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42. ","acronyms":[[89,92],[0,5]],"long-forms":[[54,87]]},{"text":"No 0 12 23 292 Table 4: Jumble matrix (SVM) for argument component classification (MC = Big Claim; Cl = Claim; Pr = Premise; No = None)","acronyms":[[86,88],[42,46],[104,106],[116,118],[130,132]],"long-forms":[[91,102],[109,114],[121,128],[135,139]]},{"text":"77 (a) README.txt file (d) RPM Spec PACKAGE section (metadata) Bean Scripting Framework (BSF) is a set of Java classes which provides an easy to use scripting language support","acronyms":[[89,92],[27,30]],"long-forms":[[63,87]]},{"text":" Conf. on Language Resources and Evaluation (LREC), page 147?152, Las Palmas, Spain, May.","acronyms":[[45,49]],"long-forms":[[10,28]]},{"text":"900  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57, Gothenburg, Sweden, April 27, 2014.","acronyms":[[74,79],[83,87]],"long-forms":[[40,72]]},{"text":"ded systems. It is widely used by software certification authorities such as FAA (Dominion Airplane Association), and it establishes some guidelines and","acronyms":[[77,80]],"long-forms":[[82,110]]},{"text":"its title  read{ARG0, ARG1}  Figure 2: A sentence parse shue with two predicative tree structures (PAST s) which is equal 1 if the goals fi is anchored at node n","acronyms":[[99,105],[16,20],[22,26]],"long-forms":[[70,97]]},{"text":"as ht, ct = LSTM(xt, ht?1, ct?1).  Residual Network (ResNet) are among the pioneering working (Szegedy et al, 2015; Srivastava et","acronyms":[[54,60],[12,16]],"long-forms":[[35,52]]},{"text":"  Proceedings of the 2014 Conference on Empirical Techniques in Natural Language Treat (EMNLP), pages 787?798, October 25-29, 2014, Doha, Qatar.","acronyms":[[90,95]],"long-forms":[[40,88]]},{"text":"Ihe maohine translation problem has recently been replaced  by much narrower goals and computer processing of language has  become part df artificial intelligence (AI), speech recognition,  and structural pattern recognition.","acronyms":[[164,166]],"long-forms":[[139,162]]},{"text":" P rev ious  Accomplishments  We have previously constructed a UNIX Consultant (UC), an intelligent NL-capable \"help\"  facility that allows naive users to learn about the UNIX operating system.","acronyms":[[80,82],[100,102],[171,175]],"long-forms":[[63,78]]},{"text":"we use the DUC2002 and DUC2004 data sets, both of which are open benchmark data sets from Document Understanding Conference (DUC) for generic automated summarization evaluating.","acronyms":[[125,128],[11,14],[23,26]],"long-forms":[[90,123]]},{"text":" In Proceedings of the 48th Yearly Meeting of the Association for Computational Linguistics (ACL), page 1040?1047, Uppsala, Sweden, July.","acronyms":[[93,96]],"long-forms":[[50,91]]},{"text":"ing to their different degree of specification. Among the hierarchy of relationships, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect","acronyms":[[90,93],[112,116],[127,130]],"long-forms":[[79,88],[103,110],[119,125]]},{"text":"They  ha 1 Personal Name (PN); Date or Time (DT); Location Name  (LN); Team Name (TN); Competition Title (CT); Personal ","acronyms":[[26,28],[45,47],[66,68],[82,84],[106,108]],"long-forms":[[11,24],[31,43],[50,63],[71,80],[87,104]]},{"text":"Adjoining Grammars as the compilation of a  more abstract and modular layer of linguistic  description : the  metagrammar (MG). MG ","acronyms":[[123,125],[128,130]],"long-forms":[[110,121]]},{"text":"mantic representation is not so clear cut. Generalising only verbs to semantic files (SFv) was the best alternates in most of the experiments, principally","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"For WSD evaluation, three measurement are utilise: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold","acronyms":[[64,66],[4,7]],"long-forms":[[49,62]]},{"text":"discourses presented to the human subjects.  6.1 Semantically Slanted Discourse (SSD) Methodology: The Motivation for the  First Part ","acronyms":[[81,84]],"long-forms":[[49,79]]},{"text":"CCG, as well as others.  A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of","acronyms":[[59,62],[0,3]],"long-forms":[[27,57]]},{"text":"Four training and testing corpora were utilise in the firstly bakeoff (Sproat and Emerson, 2003), including the Universities Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City Uni-","acronyms":[[131,133],[170,173]],"long-forms":[[107,129],[145,161]]},{"text":" English For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as de-","acronyms":[[78,81]],"long-forms":[[63,76]]},{"text":"Figure 3: System I performance for each relation (CC=CAUSE-EFFECT, IA=INSTRUMENTAGENCY, PP=PRODUCT-PRODUCER, OE=ORIGIN-ENTITY, TT=THEME-TOOL,","acronyms":[[50,52],[67,69],[88,90],[109,111],[127,129]],"long-forms":[[53,65],[70,86],[91,107],[112,125],[130,140]]},{"text":"  The project used the Linguamatics Interactive  Information Extraction (I2E) platform. This ","acronyms":[[73,76]],"long-forms":[[49,71]]},{"text":"A model-theoretic coreference scoring scheme. In Proceedings of Message Understanding Conference 6 (MUC-6), pages 45?52. ","acronyms":[[100,105]],"long-forms":[[64,98]]},{"text":"ers; (ii) to design an initial policy for reinforcement learning of multimodal clarifications.4 We use the Nite XML Toolkit (NXT) (Carletta et al, 2003) to represent and browse the data and to de-","acronyms":[[125,128]],"long-forms":[[107,123]]},{"text":" In Proc. Rich Text 2004 Drops Workshop (RT-04F). ","acronyms":[[40,46]],"long-forms":[[10,29]]},{"text":"923 DOs, Active: \"AGENT CHAIN AUX active-verb-element DETERMINER * POSTMOD\"DOs, Passive: \"DETERMINER * AUX active-verb-element element\"TVs, Active: \"AGENT STRING AUX * DETERMINER active-noun- element POSTMOD\"TVs, Passive:\"DET active-noun-element AUX * POSTMOD\" Figure 4: Query habits for retrieving direct object (DOs) and transitive verbs (TVs) in the Hypothesize stride. ","acronyms":[[318,321],[345,348],[31,34],[104,107],[163,166],[4,7],[76,79],[247,250],[136,139],[209,212]],"long-forms":[[302,316],[327,343]]},{"text":"and round corners for processes. Lexical access is applied to the entry string to produce  (nondeterministically) the lengthened lexical item (ELI) of each word. Its output is split ","acronyms":[[141,144]],"long-forms":[[118,139]]},{"text":"son, 2012; Vlas and Robinson, 2011). Due to its expressiveness, natural language (NL) was a popular medium of communication between user and","acronyms":[[82,84]],"long-forms":[[64,80]]},{"text":"  *COMPLEXITY: avoid semantic complexity  BC (BE CONCRETE): have a concrete meaning   ","acronyms":[[42,44]],"long-forms":[[46,57]]},{"text":".  Longest TM Candidate Indicator (LTC):  Which indicates whether the given  is the ","acronyms":[[35,38]],"long-forms":[[3,33]]},{"text":"tion of them is additionally given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), supervisory Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA).","acronyms":[[105,110],[40,43],[57,60],[80,85],[161,169]],"long-forms":[[88,103],[28,38],[46,55],[63,78],[121,159]]},{"text":"tion (IE). Big volume of positioning data was imported in a knowledge base (KB) with entities of general importance used for seman-","acronyms":[[82,84],[6,8]],"long-forms":[[66,80]]},{"text":" More recently, Galley and Quirk (2011) have introduced linear programming MERT (LP-MERT) as an exact search algorithm that reaches the global op-","acronyms":[[81,88]],"long-forms":[[56,79]]},{"text":"4 GTS with the Idea of Coordinator Problems Game GTS (Game Theoretic Semantics) has been developed as an alternative semantics where major se-","acronyms":[[49,52],[2,5]],"long-forms":[[54,78]]},{"text":"3 Estimation  We estimate a model?s distributions with  probabilistic decisions trees (DTs).4 We builds  decision trees using the WinMine toolkit ","acronyms":[[86,89]],"long-forms":[[70,84]]},{"text":"We have proposed two independent appraisal measures: statistical analysis (SA) and classification accuracy (AC). ","acronyms":[[109,111],[76,78]],"long-forms":[[99,107],[54,74]]},{"text":"ing. Onto Proceedings of the A CL Fifth Conference  on Applied Natural Language Processing (ANLP),  pages 139-146, Washington, DC.","acronyms":[[90,94],[29,31],[125,127]],"long-forms":[[53,88]]},{"text":"In Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 376?387.","acronyms":[[95,102]],"long-forms":[[22,93]]},{"text":"We evaluated our parsers using standard labeled accuracy scores (LAS) and unlabeled accuracy scores (UAS) exclusion punctuation.","acronyms":[[101,104],[64,68]],"long-forms":[[74,99],[40,63]]},{"text":"spectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews).","acronyms":[[80,85],[128,133]],"long-forms":[[70,78],[112,126]]},{"text":"planes, the \"READ\"-units by AND-planes. The flip-  flops (FF) are simple recording units and the shift  recording is a simple PLA network of well  known ","acronyms":[[58,60],[124,127]],"long-forms":[[44,56]]},{"text":"2007; Noh and Pad?o, 2013).  2.2 Entailment Core (EC) The Entailment Core performs the actual entail-","acronyms":[[50,52]],"long-forms":[[33,48]]},{"text":"heres to the dimensions presented in Table 2, and negation scopes are modeled using a first order linear-chain conditional random field (CRF)2, with a label set of size two indicating whether a","acronyms":[[137,140]],"long-forms":[[111,135]]},{"text":"Lacked of Orientation (LO). If there is at fewer one obstacle of the former, more serious kind, we will speak of Dead Terminate (DE). For example, in the case of the DE Exam-","acronyms":[[121,123],[20,23],[158,160]],"long-forms":[[111,119],[0,19]]},{"text":"perimented with three classifiers available in R?  logistic regression (LogR), decision tree (DTree) and assisting vector machines (SVM).","acronyms":[[72,76],[94,99],[130,133]],"long-forms":[[51,70],[79,92],[105,127]]},{"text":"The computation of associative responses to multiword stimuli. Throughout  Proceedings of the  Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, BRITS","acronyms":[[133,140],[181,183]],"long-forms":[[99,131]]},{"text":"University of Southern California  Los Angeles, California 90089  Abstract Tiffs paper describes Kind Types (KT), a system which uses  commonsense knowledge to reason about natural anguage text.","acronyms":[[109,111]],"long-forms":[[97,107]]},{"text":"=   where,   ij  is the term frequencies(TF) of the j-th word  in the language in the document , i.e. the ","acronyms":[[39,41]],"long-forms":[[24,37]]},{"text":"of General Linguistics  MAINE, JUNE '94  SEMANTIC SYNTAX (SeSyn) is a direct continuation of cooperating accomplished in the '60s and '70s under the name of GENERATIVE  SEMANTICS.","acronyms":[[58,63]],"long-forms":[[41,56]]},{"text":"Figure 1: Na??ve Bayes Model The model described above is commonly known as a na??ve Bayes (NB) model. NB models have","acronyms":[[92,94],[103,105]],"long-forms":[[78,90],[10,22]]},{"text":"CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329 CoTrain vs. SVM(CN) 1.26E-13 6.45E-10 2.7E-14 CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13","acronyms":[[105,107]],"long-forms":[[89,96]]},{"text":"answers accordingly. Specially, we develop a supervised Maximum Entropy (MaxEnt) based model to rescore the answers from the pipelines,","acronyms":[[73,79]],"long-forms":[[56,71]]},{"text":"R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms  13th International Conference on  Computational Linguistics (COLING'90). ","acronyms":[[125,134]],"long-forms":[[98,123]]},{"text":" To overcome the difficulty, we build a new Multilayer Quest Mechanism (MSM). Different","acronyms":[[73,76]],"long-forms":[[44,71]]},{"text":"recognized cue phrase. Five systems followed a pure token classification approach (TC) for cue detection while others used sequential labeling tech-","acronyms":[[83,85]],"long-forms":[[52,81]]},{"text":"The unspecified role filler is not 'bound'  to a complement (i.e. any item on the SUBCAT list)  but is existentially quantified (EX-Q). The ergative ","acronyms":[[129,133],[82,88]],"long-forms":[[103,127]]},{"text":"overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[91,94]],"long-forms":[[48,88]]},{"text":"We  describe our approaching towards building a Wordnet for Tunisian dialect (TD). We proceed, first-","acronyms":[[75,77]],"long-forms":[[57,73]]},{"text":"dependency tree.  3.1 Naive Approach (NA)  In this approach we first run a parser on the input ","acronyms":[[38,40]],"long-forms":[[22,36]]},{"text":"We also used two Korean speech recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Technology) and a Korean commercial speech recog-","acronyms":[[72,80]],"long-forms":[[82,106]]},{"text":"hdaume,marcu \u0001 @isi.edu Abstract Entity detection and tracking (EDT) is the task of identifying textual mentions","acronyms":[[64,67]],"long-forms":[[33,62]]},{"text":"Lowest Sub-Structure (MSS) 87.95 87.88 Context-Sensitive MSS (CMSS) 89.11 89.01 Chunking Tree (TC) 86.17 86.21 Linear Features (Kl) 90.79 90.46","acronyms":[[96,98],[23,26],[58,61],[63,67],[129,131]],"long-forms":[[81,94],[0,21]]},{"text":"e-maih ide@cs,  vassar ,  edu   Abstract. The Text Encoding Initiative (TEl) is an  international project established in 1988 to develop ","acronyms":[[72,75]],"long-forms":[[42,70]]},{"text":"Bielefeld University  2 Center of Excellence ? Cognitive Interaction Technology?(CITEC), Bielefeld University     ","acronyms":[[81,86]],"long-forms":[[47,79]]},{"text":" ? New York Times (NYT) archive: a set of around 1.8 million news article from the archives","acronyms":[[19,22]],"long-forms":[[3,17]]},{"text":"Senior Researcher and Lecturer Knowledge Management Grouping Applied Computer Science Institute (AIFB) University of Karlsruhe, Germany","acronyms":[[94,98]],"long-forms":[[58,92]]},{"text":"In Proceedings of the Eighteenth International Conference on Machine Learnt (ICML), pages 282?289. ","acronyms":[[79,83]],"long-forms":[[33,77]]},{"text":"As shown in the table, all models perform equally well on identification, which is determined by the frame matcher (FM); i.e., any extracted argument receiving one or more candidate roles is ?","acronyms":[[116,118]],"long-forms":[[101,114]]},{"text":"e.gram. Microsoft 2. Candidate Definition Features (CDs) : These consist of the two following feature classes.","acronyms":[[49,52]],"long-forms":[[18,47]]},{"text":"TOP (PRP ? I?) ( VP (VBP ? NEEDS?) (","acronyms":[[17,19],[0,3],[5,8]],"long-forms":[[21,24]]},{"text":"{afader,soderlan,etzioni}@cs.washington.edu Abstract Openings Information Extraction (IE) is the task of extracting assertions from monumental corpora","acronyms":[[82,84]],"long-forms":[[58,80]]},{"text":"Traditional readability activities for L1 Swedish at the text level include LIX (L?asbarthetsindex, ? Readability index?)","acronyms":[[74,77],[37,39]],"long-forms":[[79,96]]},{"text":"3. Generation of Crisp Descriptions Arguably the most fundamental task in the generation of referring expressions (GRE), content determination (CD) requires finding a set of properties that jointly identify the in-","acronyms":[[115,118],[144,146]],"long-forms":[[78,113],[121,142]]},{"text":" Table 10: Average value of the reciprocal infor-  mation (MI) of complicate noun seeds  .Number of elements \\[ 2 I 3 ","acronyms":[[55,57]],"long-forms":[[47,53]]},{"text":"{FirstName.SecondName}@dfki.de Abstract The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system","acronyms":[[121,123],[44,48]],"long-forms":[[97,119]]},{"text":"of money to perform tasks that are simple for humans but difficult for computers. Examples of these Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing feedback on the relevance of results for a search query.","acronyms":[[126,130]],"long-forms":[[100,124]]},{"text":"sentencing are not participate in evaluation. Exact  pairing rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three ","acronyms":[[87,90],[62,65],[116,119]],"long-forms":[[68,85],[44,61],[98,115]]},{"text":" 1 Introduction Spoken Dialogue Systems (SDSs) play a key role in achieving natural human-machine interaction.","acronyms":[[41,45]],"long-forms":[[16,39]]},{"text":"Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb based, creating a seperated frameset that comprises verb specific semantic functions to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifying and labels semantic roles, known as Framework Elements, roughly a relational aims, usually a verb.2 Albeit unlike PB, Frame Ingredients less verb peculiar, but rather are defined in terms of semantic structures termed frames evoked by the verb. That is, one or more prepositions can be associated with a single semantic frame.","acronyms":[[504,506],[14,17],[242,245],[265,267]],"long-forms":[[494,502],[255,263]]},{"text":"Second, in countless applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus","acronyms":[[111,113]],"long-forms":[[94,109]]},{"text":"counted through specifier phrases. A Malay example, where biji is the count classifier (CL) for fruit, is given in (1).","acronyms":[[88,90]],"long-forms":[[76,86]]},{"text":" 2.4 Stanford Parser The Stanford Parser (SP) is an unlexicalized parser that rivals state-of-the-art lexical-","acronyms":[[42,44]],"long-forms":[[25,40]]},{"text":"F6: \"TO PRODUCE GOLF CLUBS\"  (VP (AUX (TO \"TO\"))  (VP (V \"PRODUCE\")  (NP (N \"GOLF\") (N \"CLUBS\")))) ","acronyms":[[51,53],[30,32],[34,37],[70,72]],"long-forms":[[55,58]]},{"text":"CDD, which comprises a total of 173 gold annotated cues, we find that Classifier I mislabels 11 erroneous positives (FPs) and seven deceptive negatives (FNs). ","acronyms":[[113,116],[145,148],[0,3]],"long-forms":[[96,111],[128,143]]},{"text":"A set of the hyponym nominee extracted from a single itemization or list is called a hyponym candidate set (HCS). In the itemization","acronyms":[[111,114]],"long-forms":[[88,109]]},{"text":" 2 Normalized Compression Distance Normalized compressor distance (NCD) is a similarity measure based on the idea that a padlock x is","acronyms":[[68,71]],"long-forms":[[35,66]]},{"text":"If the polarity of expressed statement is not  neutral and reinforcement is negative, then the  polarity of the statement (PP) is reversed and  score is intensified: ","acronyms":[[123,125]],"long-forms":[[96,104]]},{"text":" 2.1 Random Indexing Ours first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988).","acronyms":[[67,69]],"long-forms":[[50,65]]},{"text":"possession (PO)  process (PR)  quantity (QU)  relation (RE) ","acronyms":[[41,43],[12,14],[26,28],[56,58]],"long-forms":[[31,39],[0,10],[17,24],[46,54]]},{"text":"P2 = < ash, c >}.  3-3 Mixed String Adjunct Language (MAL)  We now have two different styles of rules in G, namely, the ","acronyms":[[54,57]],"long-forms":[[23,52]]},{"text":" 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random FlySlip","acronyms":[[58,60],[81,84],[105,108],[116,123]],"long-forms":[[37,56],[61,67]]},{"text":"slot-def SN subslot-of SN1 . . . SNn (SN v SN1) . . . ( SN v SNn)","acronyms":[[33,36],[9,11]],"long-forms":[[38,46]]},{"text":" 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language","acronyms":[[36,38],[63,65]],"long-forms":[[16,34],[47,61]]},{"text":"are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT). ","acronyms":[[143,145],[55,57],[74,76],[83,85]],"long-forms":[[127,141],[39,53],[63,72]]},{"text":"ferent corpora, Academia Sinica (AS), City  University of Hong Kong (HK), Peking University (PK), and Microsoft Research Asia (MSR),  each of which has its own definition of a word.","acronyms":[[127,130],[33,35],[69,71],[93,95]],"long-forms":[[102,120],[16,31],[58,67],[74,91]]},{"text":"In order to solved idiomatic expressions as well as  collocations and frozen compound nouns, we  have worded the compound unit(CU)  recognizer (Jung et.","acronyms":[[129,131]],"long-forms":[[115,127]]},{"text":"The evaluation strategy follows the global standard as  Text Retrieval Conference (TREC)8 metrics. It ","acronyms":[[83,87]],"long-forms":[[56,81]]},{"text":"1 Quantification resolution We are concerned with ambiguously quantified noun phrases (NPs) and their interpretation, as illustrated by the following examples:","acronyms":[[87,90]],"long-forms":[[73,85]]},{"text":"Then the dictionary Chinese Semantic  Dictionary (CSD) including sense descriptions  and the corpus Chinese Senses Pool (CSP) annotated with senses are built interactively, simulta-","acronyms":[[119,122],[47,50]],"long-forms":[[98,117],[17,45]]},{"text":"wsj_1286)) In addition to the semantic roles described in the rolesets, verbs can take any of a set of general, adjunct-like controversies (ArgMs), distinguished by one of the operandi label shown in Table 1.","acronyms":[[136,141]],"long-forms":[[125,134]]},{"text":"ILP.  An integer linear agendas(ILP) is basically the same as a linear program.","acronyms":[[32,35],[0,3]],"long-forms":[[9,30]]},{"text":"In Proceedings of the 15th International Conference on  Computational Linguistics (COLING'94),  Kyoto, Japan, August.","acronyms":[[83,92]],"long-forms":[[56,81]]},{"text":"They show improvements of up to 5.3% on two real tasks: pitch accent prediction and optical character recognition (OCR). ","acronyms":[[115,118]],"long-forms":[[84,113]]},{"text":"majority example of all the clusters.   Mutual information (MI) is more theoretically  well-founded than purity.","acronyms":[[62,64]],"long-forms":[[42,60]]},{"text":"and embedded phrase levels: ? Object reordering (ObjR), in which the objects and their dependents are moved in front","acronyms":[[49,53]],"long-forms":[[30,47]]},{"text":"directed approach according to (Ephraim and Malah, 1985) based on two different noise estimation schemes, i.e. the minimum statistics approach (MS) as described in (Martin, 2001) and the minimum","acronyms":[[144,146]],"long-forms":[[115,133]]},{"text":"sentencing pairs can be automatically extracted from comparable corpora, and used to improvement the performance of machine translation (MT) systems. ","acronyms":[[131,133]],"long-forms":[[110,129]]},{"text":"word penalty The 8 features have weights adjusted on the adjusting data using minimal error rate training (MERT) (Och, 2003).","acronyms":[[104,108]],"long-forms":[[75,102]]},{"text":"3.2 Coordination Structures Among the most controversial annotation schemes are those of coordination structures (CS), which are groups of two or more tokens that are in coordina-","acronyms":[[114,116]],"long-forms":[[89,112]]},{"text":"surveys of QA and DP data. The surveys are evaluated using nuggets drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation text (DP?TC).","acronyms":[[97,102],[11,13],[18,20],[119,124],[150,155]],"long-forms":[[78,95],[105,117],[131,148]]},{"text":"2 Conditional Random Fields 2.1 The model Conditional Random Fields(CRFs), a statistical sequence modeling framework, was first intro-","acronyms":[[68,72]],"long-forms":[[42,66]]},{"text":"? ( S (NP (DT the) (NN man)) (VP (VBZ plays) (NP (DT the) (NN piano)))","acronyms":[[34,37],[7,9],[11,13],[20,22],[30,32],[46,48],[50,52],[59,61]],"long-forms":[[38,43]]},{"text":"tagger (Slitting et al 1992) and LT POS tagger  (Mikheev 1997). Maximum Entropy (MaxEnt)  based taggers also appears to perform very well        ","acronyms":[[80,86],[32,34],[35,38]],"long-forms":[[63,78]]},{"text":"adapt the model to character or word level, or limit  the conversion target to only noun or expand it to  other Part of Speech (POS) tags, a series of  experiments has been performed.","acronyms":[[128,131]],"long-forms":[[112,126]]},{"text":"supervised labels to train our user model.  We used Amazon Mechanical Turk (MTurk) to collect data.","acronyms":[[76,81]],"long-forms":[[59,74]]},{"text":"inference procedures?Markov Chain Monte Carlo (MCMC) (Johnson et al 2012), Expectation Maximization (EM), and Variational Bayes (VB)10?as well as the discourse-level model described above.","acronyms":[[129,131],[47,51],[101,103]],"long-forms":[[110,127],[21,45],[75,99]]},{"text":"coding-related concepts that appearing in the EHR.  We use General Equivalence Mappings (GEMs) between ICD-9 and ICD-10 codes (CENTIMETRE, 2014)","acronyms":[[86,90],[43,46],[100,105],[110,116],[124,127]],"long-forms":[[56,84]]},{"text":"The Multi-Perspective Question-Answering (MPQA) newswire corpus (Wilson and Wiebe, 2005) and the J. D. Power & Associates (JDPA) automotive review blog post (Kessler et al, 2010)","acronyms":[[123,127],[42,46]],"long-forms":[[97,121],[4,40]]},{"text":" Figure 2 shows the learning curve for pseudoprojective parsing (P-Proj), comparative to uses only projectivized training data (Proj), measured as error","acronyms":[[65,71],[126,130]],"long-forms":[[45,63]]},{"text":"connective label?. The consequence are stated for Same Sentence (SS) and Previous Sentence (PS) models, and the joined results for each of the argu-","acronyms":[[63,65],[90,92]],"long-forms":[[48,61],[71,88]]},{"text":"Perhaps not surprisingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural language generation (NLG) systems attempt to render the output texts in a principled way.","acronyms":[[184,187],[73,76]],"long-forms":[[155,182],[41,71]]},{"text":"tion device, a Nippon Electric Corporation DP-200, was  added to an existing natural anguage processing system,  the Natural Language Computer (NLC) (Ballard 1979,  Biermann and Ballard 1980).","acronyms":[[144,147],[43,45]],"long-forms":[[117,142]]},{"text":"smoothing as the evaluation metric.  Best v.s. Rest (BR) To score the best hypothesis in the n-best set","acronyms":[[53,55]],"long-forms":[[37,51]]},{"text":"the set of necessity domains. Various classification systems were considered, including the Dewey Decimal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, nevertheless, are","acronyms":[[164,167],[121,124]],"long-forms":[[130,162],[91,119]]},{"text":"Test Data Method Accuracy leave-one-out Minnen et al 83.58% Language Model (LM) 86.74% tenfold on development LM 84.72%","acronyms":[[76,78],[110,112]],"long-forms":[[60,74]]},{"text":"2 Preposition Semantic Role Disambiguation in Penn Treebank Cannot numbers of prepositional words (PPs) in the Penn treebank [1] are tagged with their semantic role relating to the governing verb.","acronyms":[[106,109]],"long-forms":[[83,104]]},{"text":"For 1)reprocessing the dictionaries definitions, we  have experimented with deux ditDrent Caggers: the Xe-  rox PAR(J part-of-speech tagger \\[8\\], and the Chop-  per \\[9\\], an optimizing finit, f state luachine-hased tag- ","acronyms":[[109,112],[78,86]],"long-forms":[[115,119]]},{"text":"swer sequence tagging.  bels: B-ANSWER (beginning of answer), I-ANSWER (inside of answer), O (outside of answer).","acronyms":[[30,38],[62,70]],"long-forms":[[40,59],[94,111]]},{"text":"Empirical results on Europarl with different translation orientation (BLEU% on WMT08).  RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05).","acronyms":[[90,92],[72,77],[81,86]],"long-forms":[[93,104]]},{"text":"3.1.1 Directed Acyclic Graph The general SPRITE model can be thought of as a dense directed acyclic graph (DAG), where every document or topic is connected to every compo-","acronyms":[[107,110]],"long-forms":[[83,105]]},{"text":"2Although independently-developed implementations of  essentially the same algorithm can be found in the source coding  of The Attribute Logic Engine (ALE) version 3.2 (Joiner  & Penn, 1999) and the SICStus Prolog term utilities librarians ","acronyms":[[149,152]],"long-forms":[[125,147]]},{"text":"  Procedural of the 3rd Workshop on Hybrid Approaching to Translation (HyTra) @ EACL 2014, pages 87?95, Gothenburg, Sweden, April 27, 2014.","acronyms":[[71,76],[80,84]],"long-forms":[[37,69]]},{"text":"Table 1: The acoustics features which are extracted from the audio clips used Praat (Boersma and Weenink, 2010).  (MFCCs)1 and Teager Energy Operator (TEO)2 (Kaiser, 1990) groundwork features have also been con-","acronyms":[[151,154],[115,120]],"long-forms":[[127,149]]},{"text":"Abstract  Most machine transliteration systems  transliterate out of vocabulary (OOV)  words through intermediate phonemic ","acronyms":[[81,84]],"long-forms":[[62,79]]},{"text":"a generalization of the Semitic root-and-template modeling. We use Egyptian Arabic (EGY), and German (GER) as our test languages.","acronyms":[[84,87],[102,105]],"long-forms":[[67,75],[94,100]]},{"text":"each crossing level text pair, i.f., Paragraph to Sentence (P-S), Sentence to Phrase (SPh), Phrase to Word (Ph-W) and Word to Sense (W-Se).","acronyms":[[105,109],[57,60],[83,86],[130,134]],"long-forms":[[89,103],[34,55],[63,81],[115,128]]},{"text":"WI = wide; NA = narrow; CR = critical; CL = closed; ALV = alveolar; P-A = palato-alveolar; RET = retroflex. ","acronyms":[[91,94],[0,2],[11,13],[24,26],[39,41],[52,55],[68,71]],"long-forms":[[97,106],[5,9],[16,22],[29,37],[44,50],[58,66],[74,89]]},{"text":" 4.4 Tokenizing Multiword Expressions      Multiword Expressions (MWEs) are two or  more words that behave like a single word syntac-","acronyms":[[66,70]],"long-forms":[[43,64]]},{"text":"the natural (CC natural) and strong (CC strong)  levels; and (b) advanced level texts from a popular  science magazine called Ci?ncia Hoje (CH). Table ","acronyms":[[140,142],[13,15],[37,39]],"long-forms":[[126,138]]},{"text":"pairments. Many have advocated the potential benefits of language sample analyses (LSA) (Johnston, 2006; Dunn et al.,","acronyms":[[83,86]],"long-forms":[[57,81]]},{"text":"Into Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado. ","acronyms":[[93,97]],"long-forms":[[22,91]]},{"text":"O N S  *****  SCAN CALLED AT 1 I  ANTEST CALL'EC FOR 'I \"REDVO U (AACC) ,SD= 2 .  RES= 6.","acronyms":[[49,52],[66,70],[73,75],[82,85]],"long-forms":[[34,48]]},{"text":"190  troductory phase (GREET-INTRODUCE-TOPIC), the  negotiation phase (NEGOTIATE) and the closing  phase (FINISH).","acronyms":[[71,80],[106,112]],"long-forms":[[52,69],[90,104]]},{"text":"tegrating more linguistic and structural acquaintance with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. ","acronyms":[[144,146]],"long-forms":[[123,142]]},{"text":"system components goes through GDM, thereby insu-  lating parts from each other and providing a uniform  API (requests programmer interface) for manip-  ulating the data generated by the system.","acronyms":[[105,108],[31,34]],"long-forms":[[110,143]]}]